diff --git a/build_ffmpeg.sh b/build_ffmpeg.sh
new file mode 100755
index 0000000000..d90cb75799
--- /dev/null
+++ b/build_ffmpeg.sh
@@ -0,0 +1,431 @@
+#!/usr/bin/env bash
+
+target_windows=false;
+target_android=false;
+debug=false;
+enable_ffplay=false;
+enable_ffprobe=false;
+enable_x264=false;
+enable_x265=false;
+enable_libaom=false;
+enable_libvpx=false;
+enable_libxml=false;
+enable_ffnvcodec=false;
+enable_vmaf=false;
+enable_shared=false;
+enable_gstreamer_support=false;
+dry_run_mode=false;
+android_arch=x86_64;
+custom_flags="";
+extra_config_flags=""
+product_line_flags=""
+enable_logan=false;
+enable_quadra=false;
+
+# on a windows msys2 environment, detect linux symlinks for netint files and replace them with the target of symlink
+# this function is primarily used for Netint internal git project where there are symlinks. Netint external releases will not have symlinks.
+# this function may only be called when working_directory is top level of FFmpeg folder
+# this function only scans for symlinks on this folder level and one level down
+# this function only scans for symlinks that begin with '../' or '../../'
+replace_symlinks_with_files_on_windows() {
+    file_list=( $(find . -type f \( -iname \*ni\*.c -o -iname \*ni\*.h -o -iname \*ni\*.cpp -o -iname \*ni\*.hpp -o -iname \*.sh \)) )
+    for file in ${file_list[@]}; do
+        if [[ $(head -q -c 6 $file) == "../../" ]]; then
+            if stat $(tail -q -c +4 $file) &> /dev/null; then
+                # echo "Replaced working symlink: ${file}"
+                cp $file $file.symlink
+                cp $(tail -q -c +4 $file) $file
+            fi
+        elif [[ $(head -q -c 3 $file) == "../" ]]; then
+            if stat $(cat $file) &> /dev/null; then
+                # echo "Replaced working symlink: ${file}"
+                cp $file $file.symlink
+                cp $(cat $file) $file
+            fi
+        fi
+    done
+}
+
+# undo the actions of replace_symlinks_with_files_on_windows() function
+# removes ".symlink" suffix from files and replaces pre-existing files with them
+restore_symlinks_on_windows() {
+    file_list=( $(find . -type f -name \*.symlink ) )
+    for file in ${file_list[@]}; do
+        mv $file ${file%.*}
+    done
+}
+
+# parse a flag with an arg in or after it
+# $1 flag pattern, $2 entire flag arg, $3 arg after flag arg
+# return 1 if path is in second arg (separated by space), else return 0. Store path in $extract_arg_ret
+extract_arg () {
+    unset extract_arg_ret
+    # check valid arg flag
+    if [ -n "$(printf "%s" ${2} | grep -Eoh "${1}")" ]; then
+        # check if path string is connected by '=' or is in following arg
+        if [ -n "$(echo "${2}" | grep -Eoh "${1}=")" ]; then
+            arg_str=`printf "%s" "${2}" | grep -Poh "${1}=\K.+"`;
+            # trim out leading and trailing quotation marks
+            extract_arg_ret=`echo "${arg_str}" | sed -e 's/^\(["'\'']\)//' -e 's/\(["'\'']\)$//'`;
+            return 0;
+        elif [ -n "$(printf "%s" ${2} | grep -Eoh "^${1}$")" ]; then
+            arg_str="${3}";
+            # trim out leading and trailing quotation marks
+            extract_arg_ret=`printf "%s" "${arg_str}" | sed -e 's/^\(["'\'']\)//' -e 's/\(["'\'']\)$//'`;
+            return 1;
+        else
+            echo "Unknown option '$2', exiting";
+            exit 1;
+        fi
+    else
+        echo "Target flag '$1' not found in '$2', exiting"; exit 1;
+    fi
+}
+
+if [ `whoami` = root ]; then
+    read -p "Do you wish to execute with sudo [Y/N]? " -n 1 -r
+    echo   
+    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
+        exit
+    fi
+fi
+
+while [ "$1" != "" ]; do
+    case $1 in
+        -h | --help) echo "Usage: ./build_ffmpeg.sh --quadra | --logan [OPTION]";
+                     echo "Compile FFmpeg for Logan/Quadra.";
+                     echo "Example: ./build_ffmpeg.sh --quadra";
+                     echo;
+                     echo "Options:";
+                     echo "-h, --help                    display this help and exit.";
+                     echo "-w, windows                   compile for windows.";
+                     echo "-a, --android                 compile for Android NDK.";
+                     echo "-g, --gdb                     compile for GDB.";
+                     echo "--quadra                      build for Quadra (enabled if neither --quadra or --logan selected).";
+                     echo "--logan                       build for Logan.";
+                     echo "--ffplay                      compile with ffplay (requires libsdl2 package).";
+                     echo "--ffprobe                     compile with ffprobe.";
+                     echo "--libx264                     compile with libx264 (requires libx264 package).";
+                     echo "--libx265                     compile with libx265 (requires libx265 package).";
+                     echo "--libaom                      compile with libaom (requires libaom package).";
+                     echo "--libvpx                      compile with libvpx-vp9 (requires libvpx-dev package).";
+                     echo "--libxml                      compile with libxml2 (requires libxml2-dev, liblzma-dev package).";
+                     echo "--ffnvcodec                   compile with ffnvcodec (requires ffnvcodec package).";
+                     echo "--vmaf                        compile with vmaf (requires libvmaf).";
+                     echo "--shared                      compile with shared libFF components.";
+                     echo "--gstreamer                   compile with Netint GStreamer support (FFmpeg 4.3.1 only).";
+                     echo "--dry                         dry run printing configs without building";
+                     echo "--android_arch \"<arch>\"       cross compile CPU arch when compiling for --android. [arm,arm64,x86,x86_64(default)]";
+                     echo "--custom_flags \"<flags>\"      compile with custom configuration flags";
+                     echo "";
+                     echo "Quadra required configuration flags:";
+                     echo "--enable-libxcoder --enable-ni_quadra --enable-gpl";
+                     echo "--extra-ldflags='-lm -ldl'";
+                     echo "--enable-pthreads --extra-libs='-lpthread'";
+                     echo "--enable-x86asm";
+                     echo "";
+                     echo "Logan required configuration flags:";
+                     echo "--enable-libxcoder_logan --enable-ni_logan --enable-gpl";
+                     echo "--extra-ldflags='-lm -ldl'";
+                     echo "--enable-pthreads --extra-libs='-lpthread'";
+                     echo "--enable-x86asm";
+                     exit 0
+        ;;
+        -w | windows)    target_windows=true
+        ;;
+        -a | --android)  target_android=true
+        ;;
+        -g | --gdb)      debug=true
+        ;;
+        --quadra)        enable_quadra=true
+        ;;
+        --logan)         enable_logan=true
+        ;;
+        --ffplay)        enable_ffplay=true
+        ;;
+        --ffprobe)       enable_ffprobe=true
+        ;;
+        --libx264)       enable_x264=true
+        ;;
+        --libx265)       enable_x265=true
+        ;;
+        --libaom)        enable_libaom=true
+        ;;
+        --libvpx)        enable_libvpx=true
+        ;;
+        --libxml)        enable_libxml=true
+        ;;
+        --ffnvcodec)     enable_ffnvcodec=true
+        ;;
+        --vmaf)          enable_vmaf=true
+        ;;
+        --shared)        enable_shared=true
+        ;;
+        --gstreamer)     enable_gstreamer_support=true
+        ;;
+        --dry)           dry_run_mode=true
+        ;;
+        --android_arch | --android_arch=*)  extract_arg "\-\-android_arch" "$1" "$2"; prev_rc=$?;
+                                            if [ "$prev_rc" -eq 1 ]; then shift; fi
+                                            android_arch=$extract_arg_ret
+        ;;
+        --custom_flags | --custom_flags=*)  extract_arg "\-\-custom_flags" "$1" "$2"; prev_rc=$?;
+                                            if [ "$prev_rc" -eq 1 ]; then shift; fi
+                                            custom_flags=$extract_arg_ret
+        ;;
+        *)               echo "Usage: ./build_ffmpeg.sh [OPTION]...";
+                         echo "Try './build_ffmpeg.sh --help' for more information"; exit 1
+        ;;
+    esac
+    shift
+done
+
+if [[ $enable_logan == "true" && $enable_quadra == "true" ]]; then
+    product_line_flags="--enable-libxcoder --enable-ni_quadra --enable-libxcoder_logan --enable-ni_logan"
+elif $enable_logan; then
+    product_line_flags="--enable-libxcoder_logan --enable-ni_logan --disable-filter=rotate_ni_quadra --disable-filter=bg_ni_quadra --disable-filter=crop_ni_quadra --disable-filter=yuv420to444_ni_quadra --disable-filter=yuv444to420_ni_quadra --disable-filter=hwupload_ni_quadra --disable-filter=overlay_ni_quadra --disable-filter=pad_ni_quadra --disable-filter=roi_ni_quadra --disable-filter=scale_ni_quadra --disable-filter=split_ni_quadra --disable-filter=xstack_ni_quadra"
+elif $enable_quadra; then
+    product_line_flags="--enable-libxcoder --enable-ni_quadra --disable-filter=hwupload_ni_logan"
+elif [[ $enable_logan == "false" && $enable_quadra == "false" ]]; then
+    product_line_flags="--enable-libxcoder --enable-ni_quadra --disable-filter=hwupload_ni_logan"
+    enable_quadra=true
+    enable_logan=false
+fi
+
+if $debug; then
+    extra_config_flags="${extra_config_flags} --disable-optimizations --disable-asm --disable-stripping --enable-debug=3"
+else
+    extra_config_flags="${extra_config_flags} --enable-x86asm --disable-debug"
+fi
+
+if $enable_ffplay; then
+    extra_config_flags="${extra_config_flags} --enable-ffplay"
+else
+    extra_config_flags="${extra_config_flags} --disable-ffplay"
+fi
+
+if $enable_ffprobe; then
+    extra_config_flags="${extra_config_flags} --enable-ffprobe"
+else
+    extra_config_flags="${extra_config_flags} --disable-ffprobe"
+fi
+
+if $enable_x264; then
+    extra_config_flags="${extra_config_flags} --enable-libx264"
+else
+    extra_config_flags="${extra_config_flags} --disable-libx264"
+fi
+
+if $enable_x265; then
+    extra_config_flags="${extra_config_flags} --enable-libx265"
+else
+    extra_config_flags="${extra_config_flags} --disable-libx265"
+fi
+
+if $enable_libaom; then
+    extra_config_flags="${extra_config_flags} --enable-libaom"
+else
+    extra_config_flags="${extra_config_flags} --disable-libaom"
+fi
+
+if $enable_libvpx; then
+    extra_config_flags="${extra_config_flags} --enable-libvpx"
+else
+    extra_config_flags="${extra_config_flags} --disable-libvpx"
+fi
+
+if $enable_libxml; then
+    extra_config_flags="${extra_config_flags} --enable-libxml2"
+else
+    extra_config_flags="${extra_config_flags} --disable-libxml2"
+fi
+
+if $enable_ffnvcodec; then
+    extra_config_flags="${extra_config_flags} --extra-cflags=-I/usr/local/cuda/targets/x86_64-linux/include --extra-ldflags=-L/usr/local/cuda/targets/x86_64-linux/lib --enable-cuda-nvcc --enable-cuda --enable-cuvid --enable-nvdec --enable-nvenc"
+else
+    extra_config_flags="${extra_config_flags} --disable-cuda-nvcc --disable-cuda --disable-cuvid --disable-nvdec --disable-nvenc"
+fi
+
+if $enable_vmaf; then
+    extra_config_flags="${extra_config_flags} --enable-libvmaf --enable-version3"
+else
+    extra_config_flags="${extra_config_flags} --disable-libvmaf"
+fi
+
+if $enable_shared; then
+    extra_config_flags="${extra_config_flags} --disable-static --enable-shared"
+else
+    extra_config_flags="${extra_config_flags} --enable-static --disable-shared"
+fi
+
+if $enable_gstreamer_support; then
+    extra_config_flags="${extra_config_flags} --extra-cflags=-DNI_DEC_GSTREAMER_SUPPORT"
+else
+    extra_config_flags="${extra_config_flags} --extra-cflags=-UNI_DEC_GSTREAMER_SUPPORT"
+fi
+
+extra_config_flags="${extra_config_flags} ${custom_flags}"
+
+if $target_windows; then
+    if $dry_run_mode; then # Dry-run mode args is a separate duplicate of wet-run mode args due to bash quotation passing limitations
+        echo ./configure \
+        --enable-cross-compile --arch='x86_64' \
+        --target-os='mingw32' --cross-prefix='x86_64-w64-mingw32-' \
+        --nm='x86_64-w64-mingw32-gcc-nm' \
+        --ar='x86_64-w64-mingw32-gcc-ar' \
+        --strip='strip' \
+        --pkg-config-flags='--static' \
+        --enable-gpl --enable-nonfree \
+        --extra-ldflags=\'-lm -Wl,--stack 4194304\' \
+        ${product_line_flags} \
+        --enable-w32threads --extra-libs=\'-lwinpthread -lws2_32\' \
+        --enable-encoders --enable-decoders --enable-avfilter --enable-muxers --enable-demuxers --enable-parsers \
+        ${extra_config_flags}
+    else # Dry-run mode args is a separate duplicate of wet-run mode args due to bash quotation passing limitations
+        replace_symlinks_with_files_on_windows
+        ./configure \
+        --enable-cross-compile --arch='x86_64' \
+        --target-os='mingw32' --cross-prefix='x86_64-w64-mingw32-' \
+        --nm='x86_64-w64-mingw32-gcc-nm' \
+        --ar='x86_64-w64-mingw32-gcc-ar' \
+        --strip='strip' \
+        --pkg-config-flags='--static' \
+        --enable-gpl --enable-nonfree \
+        --extra-ldflags='-lm -Wl,--stack 4194304' \
+        ${product_line_flags} \
+        --enable-w32threads --extra-libs='-lwinpthread -lws2_32' \
+        --enable-encoders --enable-decoders --enable-avfilter --enable-muxers --enable-demuxers --enable-parsers \
+        ${extra_config_flags}
+        if [ $? != 0 ]; then
+            echo -e "\e[31mConfiguration failed. Exiting...\e[0m"
+            restore_symlinks_on_windows
+            exit 1
+        else
+            make -j $(nproc)
+            restore_symlinks_on_windows
+            RC=$?
+        fi
+    fi
+elif $target_android; then
+    echo "android_arch: ${android_arch}"
+
+    if [ -z ${ANDROID_NDK_ROOT} ]; then
+        echo "You must set ANDROID_NDK_ROOT environment variable"
+        echo "Please download NDK r20b from https://developer.android.com/ndk/downloads/older_releases"
+        exit -1
+    fi
+
+    if [ "${android_arch}" = "arm" ]; then
+        ARCH=arm
+        ARCH2=armv7-a
+        CPU=armv7-a
+    elif [ "${android_arch}" = "arm64" ]; then
+        ARCH=arm64
+        ARCH2=aarch64
+        CPU=armv8-a
+    elif [ "${android_arch}" = "x86" ]; then
+        ARCH=x86
+        ARCH2=i686
+        CPU=i686
+    elif [ "${android_arch}" = "x86_64" ]; then
+        ARCH=x86_64
+        ARCH2=x86_64
+        CPU=x86_64
+    elif [ "${android_arch}" = "" ]; then
+        ARCH=x86_64
+        ARCH2=x86_64
+        CPU=x86_64
+    else
+        echo "Error - unknown option for --android_arch. Select from: [arm,arm64,x86,x86_64]"
+        exit -1
+    fi
+
+    echo "Building android ARCH=${ARCH}"
+
+    API=28
+    TOOLCHAIN=${ANDROID_NDK_ROOT}/toolchains/llvm/prebuilt/linux-x86_64
+    PREFIX=android/$ARCH
+
+    if $dry_run_mode; then # Dry-run mode args is a separate duplicate of wet-run mode args due to bash quotation passing limitations
+        echo ./configure \
+        --prefix=$PREFIX \
+        --enable-cross-compile \
+        --sysroot=$TOOLCHAIN/sysroot \
+        --arch=$ARCH2 \
+        --cpu=$CPU \
+        --target-os='android' \
+        --cc=$TOOLCHAIN/bin/$ARCH2-linux-android$API-clang \
+        --cross-prefix=$TOOLCHAIN/bin/$ARCH2-linux-android- \
+        --pkg-config=$(which pkg-config) \
+        --pkg-config-flags='--static' \
+        --enable-gpl --enable-nonfree \
+        --extra-ldflags=\'-fuse-ld=gold -lm\' \
+        ${product_line_flags} \
+        --extra-libs=-lgcc \
+        --enable-pic \
+        --extra-cflags=\'-DANDROID -D_ANDROID -D__ANDROID__\' \
+        --enable-pthreads \
+        --enable-encoders --enable-decoders --enable-avfilter --enable-muxers --enable-demuxers --enable-parsers \
+        ${extra_config_flags}
+    else # Dry-run mode args is a separate duplicate of wet-run mode args due to bash quotation passing limitations
+        ./configure \
+        --prefix=$PREFIX \
+        --enable-cross-compile \
+        --sysroot=$TOOLCHAIN/sysroot \
+        --arch=$ARCH2 \
+        --cpu=$CPU \
+        --target-os='android' \
+        --cc=$TOOLCHAIN/bin/$ARCH2-linux-android$API-clang \
+        --cross-prefix=$TOOLCHAIN/bin/$ARCH2-linux-android- \
+        --pkg-config=$(which pkg-config) \
+        --pkg-config-flags='--static' \
+        --enable-gpl --enable-nonfree \
+        --extra-ldflags='-fuse-ld=gold -lm' \
+        ${product_line_flags} \
+        --extra-libs=-lgcc \
+        --enable-pic \
+        --extra-cflags='-DANDROID -D_ANDROID -D__ANDROID__' \
+        --enable-pthreads \
+        --enable-encoders --enable-decoders --enable-avfilter --enable-muxers --enable-demuxers --enable-parsers \
+        ${extra_config_flags}
+        if [ $? != 0 ]; then
+            echo -e "\e[31mConfiguration failed. Exiting...\e[0m"
+            exit 1
+        else
+            make -j $(nproc)
+            RC=$?
+        fi
+    fi
+else
+    if $dry_run_mode; then # Dry-run mode args is a separate duplicate of wet-run mode args due to bash quotation passing limitations
+        echo ./configure \
+        --pkg-config-flags='--static' \
+        --enable-gpl --enable-nonfree \
+        --extra-ldflags='-lm' --extra-ldflags='-ldl' \
+        ${product_line_flags} \
+        --enable-pthreads --extra-libs='-lpthread' \
+        --enable-encoders --enable-decoders --enable-avfilter --enable-muxers --enable-demuxers --enable-parsers \
+        ${extra_config_flags}
+    else # Dry-run mode args is a separate duplicate of wet-run mode args due to bash quotation passing limitations
+        ./configure \
+        --pkg-config-flags='--static' \
+        --enable-gpl --enable-nonfree \
+        --extra-ldflags='-lm' --extra-ldflags='-ldl' \
+        ${product_line_flags}  \
+        --enable-pthreads --extra-libs='-lpthread' \
+        --enable-encoders --enable-decoders --enable-avfilter --enable-muxers --enable-demuxers --enable-parsers \
+        ${extra_config_flags}
+        if [ $? != 0 ]; then
+            echo -e "\e[31mConfiguration failed. Exiting...\e[0m"
+            exit 1
+        else
+            make -j $(nproc)
+            RC=$?
+        fi
+    fi
+fi
+
+if $enable_shared && [ ! -z $RC ] && [ $RC -eq 0 ]; then
+    echo "Reminder: after installing FFmpeg, run 'sudo ldconfig' to cache the shared libraries"
+fi
+exit $RC
diff --git a/configure b/configure
index ba5793b2ff..62c78daef9 100755
--- a/configure
+++ b/configure
@@ -291,6 +291,10 @@ External library support:
   --enable-libwebp         enable WebP encoding via libwebp [no]
   --enable-libx264         enable H.264 encoding via x264 [no]
   --enable-libx265         enable HEVC encoding via x265 [no]
+  --enable-libxcoder       enable NetInt Quadra Xcoder [no]
+  --enable-ni_quadra       enable NetInt Quadra HWaccel [no]
+  --enable-libxcoder_logan enable NetInt Quadra Xcoder [no]
+  --enable-ni_logan        enable NetInt Quadra HWaccel [no]
   --enable-libxavs         enable AVS encoding via xavs [no]
   --enable-libxavs2        enable AVS2 encoding via xavs2 [no]
   --enable-libxcb          enable X11 grabbing using XCB [autodetect]
@@ -1874,6 +1878,8 @@ EXTERNAL_LIBRARY_LIST="
     libvorbis
     libvpx
     libwebp
+    libxcoder
+    libxcoder_logan
     libxml2
     libzimg
     libzmq
@@ -1922,6 +1928,8 @@ HWACCEL_LIBRARY_LIST="
     $HWACCEL_LIBRARY_NONFREE_LIST
     libmfx
     mmal
+    ni_quadra
+    ni_logan
     omx
     opencl
 "
@@ -3387,6 +3395,18 @@ libx264_encoder_select="atsc_a53"
 libx264rgb_encoder_deps="libx264"
 libx264rgb_encoder_select="libx264_encoder"
 libx265_encoder_deps="libx265"
+h264_ni_quadra_decoder_deps="libxcoder"
+h265_ni_quadra_decoder_deps="libxcoder"
+jpeg_ni_quadra_decoder_deps="libxcoder"
+vp9_ni_quadra_decoder_deps="libxcoder"
+av1_ni_quadra_encoder_deps="libxcoder"
+h264_ni_quadra_encoder_deps="libxcoder"
+h265_ni_quadra_encoder_deps="libxcoder"
+jpeg_ni_quadra_encoder_deps="libxcoder"
+h264_ni_logan_decoder_deps="libxcoder_logan"
+h265_ni_logan_decoder_deps="libxcoder_logan"
+h264_ni_logan_encoder_deps="libxcoder_logan"
+h265_ni_logan_encoder_deps="libxcoder_logan"
 libxavs_encoder_deps="libxavs"
 libxavs2_encoder_deps="libxavs2"
 libxvid_encoder_deps="libxvid"
@@ -3718,6 +3738,7 @@ scale2ref_filter_deps="swscale"
 scale_filter_deps="swscale"
 scale_qsv_filter_deps="libmfx"
 scdet_filter_select="scene_sad"
+sdl_ni_quadra_filter_deps="swscale sdl2"
 select_filter_select="scene_sad"
 sharpness_vaapi_filter_deps="vaapi"
 showcqt_filter_deps="avformat swscale"
@@ -3812,7 +3833,7 @@ swscale_suggest="libm stdatomic"
 
 avcodec_extralibs="pthreads_extralibs iconv_extralibs dxva2_extralibs"
 avfilter_extralibs="pthreads_extralibs"
-avutil_extralibs="d3d11va_extralibs nanosleep_extralibs pthreads_extralibs vaapi_drm_extralibs vaapi_x11_extralibs vdpau_x11_extralibs"
+avutil_extralibs="d3d11va_extralibs nanosleep_extralibs pthreads_extralibs vaapi_drm_extralibs vaapi_x11_extralibs vdpau_x11_extralibs libxcoder_extralibs libxcoder_logan_extralibs"
 
 # programs
 ffmpeg_deps="avcodec avfilter avformat"
@@ -6673,6 +6694,8 @@ enabled libx264           && require_pkg_config libx264 x264 "stdint.h x264.h" x
                              check_cpp_condition libx262 x264.h "X264_MPEG2"
 enabled libx265           && require_pkg_config libx265 x265 x265.h x265_api_get &&
                              require_cpp_condition libx265 x265.h "X265_BUILD >= 70"
+enabled libxcoder         && require_pkg_config libxcoder xcoder ni_device_api.h ni_device_open
+enabled libxcoder_logan   && require_pkg_config libxcoder_logan xcoder_logan ni_device_api_logan.h ni_logan_device_open
 enabled libxavs           && require libxavs "stdint.h xavs.h" xavs_encoder_encode "-lxavs $pthreads_extralibs $libm_extralibs"
 enabled libxavs2          && require_pkg_config libxavs2 "xavs2 >= 1.3.0" "stdint.h xavs2.h" xavs2_api_get
 enabled libxvid           && require libxvid xvid.h xvid_global -lxvidcore
diff --git a/fftools/ffmpeg.c b/fftools/ffmpeg.c
index e7384f052a..b05e3aa0a2 100644
--- a/fftools/ffmpeg.c
+++ b/fftools/ffmpeg.c
@@ -1564,9 +1564,26 @@ static void print_report(int is_last_report, int64_t timer_start, int64_t cur_ti
             float fps;
             int64_t frame_number = ost->frame_number;
 
-            fps = t > 1 ? frame_number / t : 0;
+            // NETINT: add option to display windowed average FPS
+            if (ni_interval_fps > 0) { // if ni_interval_fps arg selected, calculate windowed average FPS
+                float interval = (cur_time - ost->ni_prev_fps_measurement_time)/ 1000000.0 ;
+                if (interval >= (float)ni_interval_fps) { // update fps at every ni_interval_fps
+                    fps = (frame_number - ost->ni_prev_frame_count) / interval;
+                    // store variables for tracking windowed average FPS in OutputStream object
+                    ost->ni_prev_fps = fps;
+                    ost->ni_prev_fps_measurement_time = cur_time;
+                    ost->ni_prev_frame_count = frame_number;
+                } else { // display FPS from previous interval if windowing interval not yet reached
+                    fps = ost->ni_prev_fps;
+                }
+            } else { // else, use default FFmpeg default fps calculation
+                fps = t > 1 ? frame_number / t : 0;
+            }
+
+            // NETINT: add option to display windowed average FPS
+            // in addition to default behavior, display 1 decimal place in FPS if ni_interval_fps selected
             av_bprintf(&buf, "frame=%5"PRId64" fps=%3.*f q=%3.1f ",
-                     frame_number, fps < 9.95, fps, q);
+                       frame_number, (fps < 9.95) || ni_interval_fps, fps, q);
             av_bprintf(&buf_script, "frame=%"PRId64"\n", frame_number);
             av_bprintf(&buf_script, "fps=%.2f\n", fps);
             av_bprintf(&buf_script, "stream_%d_%d_q=%.1f\n",
@@ -2419,6 +2436,9 @@ static int process_input_packet(InputStream *ist, const AVPacket *pkt, int no_eo
             ret = decode_video    (ist, repeating ? NULL : avpkt, &got_output, &duration_pts, !pkt,
                                    &decode_failed);
             if (!repeating || !pkt || got_output) {
+                if (got_output && repeating) { // NETINT: if a frame is already decoded no need to update DTS or PTS
+                    break;
+                }
                 if (pkt && pkt->duration) {
                     duration_dts = av_rescale_q(pkt->duration, ist->st->time_base, AV_TIME_BASE_Q);
                 } else if(ist->dec_ctx->framerate.num != 0 && ist->dec_ctx->framerate.den != 0) {
@@ -3128,6 +3148,41 @@ static int init_output_stream(OutputStream *ost, AVFrame *frame,
         if (ret < 0)
             return ret;
 
+        // NETINT: automatically enabling GenHdrs for MKV and HLS container format support
+        if (!strcmp(output_files[ost->file_index]->ctx->oformat->name, "matroska") ||
+            !strcmp(output_files[ost->file_index]->ctx->oformat->name, "hls") ||
+            !strcmp(output_files[ost->file_index]->ctx->oformat->name, "asf")) {
+            AVDictionaryEntry *t;
+            if ((t = av_dict_get(ost->encoder_opts, "xcoder-params", NULL, 0))) {
+                int i;
+                size_t len = strlen(t->value);
+                // Remove all colons if exist at the end of option values before appending GenHdrs
+                for(i=len-1; i>0; i--)
+                {
+                    if(t->value[i] == ':')
+                    {
+                        t->value[i] = '\0';
+                    }
+                    else
+                    {
+                        break;
+                    }
+                }
+                av_dict_set(&ost->encoder_opts, "xcoder-params", ":GenHdrs=1", AV_DICT_APPEND);
+            }else{
+                av_opt_set(ost->enc_ctx->priv_data, "xcoder-params", "GenHdrs=1", 0);
+            }
+
+            if (dec && ost->enc_ctx &&
+                (ost->enc_ctx->colorspace != dec->colorspace ||
+                 ost->enc_ctx->color_trc  != dec->color_trc  ||
+                 ost->enc_ctx->color_primaries != dec->color_primaries)) {
+                    ost->enc_ctx->colorspace = dec->colorspace;
+                    ost->enc_ctx->color_trc  = dec->color_trc;
+                    ost->enc_ctx->color_primaries = dec->color_primaries;
+            }
+        }
+
         if ((ist = get_input_stream(ost)))
             dec = ist->dec_ctx;
         if (dec && dec->subtitle_header) {
diff --git a/fftools/ffmpeg.h b/fftools/ffmpeg.h
index 391a35cf50..e8e9fb737d 100644
--- a/fftools/ffmpeg.h
+++ b/fftools/ffmpeg.h
@@ -62,6 +62,8 @@ enum HWAccelID {
     HWACCEL_NONE = 0,
     HWACCEL_AUTO,
     HWACCEL_GENERIC,
+    HWACCEL_NI_LOGAN,
+    HWACCEL_NI_QUADRA
 };
 
 typedef struct HWDevice {
@@ -474,6 +476,11 @@ typedef struct OutputStream {
     AVRational mux_timebase;
     AVRational enc_timebase;
 
+    // NETINT: add option to display windowed average FPS
+    int64_t ni_prev_fps_measurement_time;
+    int ni_prev_frame_count;
+    float ni_prev_fps;
+
     AVBSFContext            *bsf_ctx;
 
     AVCodecContext *enc_ctx;
@@ -620,6 +627,8 @@ extern float audio_drift_threshold;
 extern float dts_delta_threshold;
 extern float dts_error_threshold;
 
+extern float ni_interval_fps; // NETINT: windowed average FPS display window size
+
 extern int audio_volume;
 extern int audio_sync_method;
 extern enum VideoSyncMethod video_sync_method;
@@ -686,6 +695,14 @@ int ffmpeg_parse_options(int argc, char **argv);
 int videotoolbox_init(AVCodecContext *s);
 int qsv_init(AVCodecContext *s);
 
+#if CONFIG_NI_QUADRA
+int ni_quad_init(AVCodecContext *s);
+#endif
+
+#if CONFIG_NI_LOGAN
+int ni_logan_init(AVCodecContext *s);
+#endif
+
 HWDevice *hw_device_get_by_name(const char *name);
 int hw_device_init_from_string(const char *arg, HWDevice **dev);
 void hw_device_free_all(void);
diff --git a/fftools/ffmpeg_opt.c b/fftools/ffmpeg_opt.c
index 6e18a4a23e..c4d005c243 100644
--- a/fftools/ffmpeg_opt.c
+++ b/fftools/ffmpeg_opt.c
@@ -156,6 +156,8 @@ float audio_drift_threshold = 0.1;
 float dts_delta_threshold   = 10;
 float dts_error_threshold   = 3600*30;
 
+float ni_interval_fps = 0; // NETINT: add option to display windowed average FPS
+
 int audio_volume      = 256;
 int audio_sync_method = 0;
 enum VideoSyncMethod video_sync_method = VSYNC_AUTO;
@@ -956,7 +958,17 @@ static void add_input_streams(OptionsContext *o, AVFormatContext *ic)
                 ist->dec = avcodec_find_decoder(par->codec_id);
 
             // avformat_find_stream_info() doesn't set this for us anymore.
-            ist->dec_ctx->framerate = st->avg_frame_rate;
+
+            // NETINT: use bitstream framerate info when find_stream_info cannot estimate it
+            if ((st->avg_frame_rate.num != 0) && (st->avg_frame_rate.den != 0)){
+                ist->dec_ctx->framerate = st->avg_frame_rate;
+            }else if ((st->r_frame_rate.num != 0) && (st->r_frame_rate.den != 0)){
+                ist->dec_ctx->framerate = st->r_frame_rate;
+            }else{
+                av_log(NULL, AV_LOG_WARNING, "No framerate info found or probed -> using a default 30 fps\n");
+                ist->dec_ctx->framerate.num = 30;
+                ist->dec_ctx->framerate.den = 1;
+            }
 
             MATCH_PER_STREAM_OPT(frame_rates, str, framerate, ic, st);
             if (framerate && av_parse_video_rate(&ist->framerate,
@@ -1538,6 +1550,11 @@ static OutputStream *new_output_stream(OptionsContext *o, AVFormatContext *oc, e
 
     ost = ALLOC_ARRAY_ELEM(output_streams, nb_output_streams);
 
+    // NETINT: add option to display windowed average FPS
+    ost->ni_prev_fps_measurement_time = 0;
+    ost->ni_prev_frame_count = 0;
+    ost->ni_prev_fps = 0;
+
     ost->file_index = nb_output_files - 1;
     ost->index      = idx;
     ost->st         = st;
@@ -1675,7 +1692,8 @@ static OutputStream *new_output_stream(OptionsContext *o, AVFormatContext *oc, e
     MATCH_PER_STREAM_OPT(disposition, str, ost->disposition, oc, st);
     ost->disposition = av_strdup(ost->disposition);
 
-    ost->max_muxing_queue_size = 128;
+    // NETINT: change max_muxing_queue_size from 128 to 512 to alleviate muxing issues due to encoding latency
+    ost->max_muxing_queue_size = 512;
     MATCH_PER_STREAM_OPT(max_muxing_queue_size, i, ost->max_muxing_queue_size, oc, st);
 
     ost->muxing_queue_data_size = 0;
@@ -3764,6 +3782,8 @@ const OptionDef options[] = {
         "timestamp discontinuity delta threshold", "threshold" },
     { "dts_error_threshold", HAS_ARG | OPT_FLOAT | OPT_EXPERT,       { &dts_error_threshold },
         "timestamp error delta threshold", "threshold" },
+    { "ni_interval_fps", HAS_ARG | OPT_FLOAT | OPT_EXPERT,           { &ni_interval_fps }, // NETINT: add option to display windowed average FPS
+        "window size and reporting interval for moving average processing FPS calculation", "number" },
     { "xerror",         OPT_BOOL | OPT_EXPERT,                       { &exit_on_error },
         "exit on error", "error" },
     { "abort_on",       HAS_ARG | OPT_EXPERT,                        { .func_arg = opt_abort_on },
diff --git a/libavcodec/Makefile b/libavcodec/Makefile
index 457ec58377..01dc1abd89 100644
--- a/libavcodec/Makefile
+++ b/libavcodec/Makefile
@@ -18,6 +18,7 @@ HEADERS = ac3_parser.h                                                  \
           dxva2.h                                                       \
           jni.h                                                         \
           mediacodec.h                                                  \
+          ni_hevc_extradata.h                                           \
           packet.h                                                      \
           qsv.h                                                         \
           vdpau.h                                                       \
@@ -49,6 +50,7 @@ OBJS = ac3_parser.o                                                     \
        mathtables.o                                                     \
        mediacodec.o                                                     \
        mpeg12framerate.o                                                \
+       ni_hevc_extradata.o                                              \
        options.o                                                        \
        parser.o                                                         \
        parsers.o                                                        \
@@ -401,6 +403,18 @@ OBJS-$(CONFIG_H264_V4L2M2M_DECODER)    += v4l2_m2m_dec.o
 OBJS-$(CONFIG_H264_V4L2M2M_ENCODER)    += v4l2_m2m_enc.o
 OBJS-$(CONFIG_HAP_DECODER)             += hapdec.o hap.o
 OBJS-$(CONFIG_HAP_ENCODER)             += hapenc.o hap.o
+OBJS-$(CONFIG_H264_NI_LOGAN_DECODER)   += nidec_h264_logan.o nicodec_logan.o nidec_logan.o
+OBJS-$(CONFIG_H265_NI_LOGAN_DECODER)   += nidec_hevc_logan.o nicodec_logan.o nidec_logan.o
+OBJS-$(CONFIG_H264_NI_LOGAN_ENCODER)   += nienc_h264_logan.o nicodec_logan.o nienc_logan.o
+OBJS-$(CONFIG_H265_NI_LOGAN_ENCODER)   += nienc_hevc_logan.o nicodec_logan.o nienc_logan.o
+OBJS-$(CONFIG_H264_NI_QUADRA_DECODER)  += nidec_h264.o nicodec.o nidec.o
+OBJS-$(CONFIG_H265_NI_QUADRA_DECODER)  += nidec_hevc.o nicodec.o nidec.o
+OBJS-$(CONFIG_JPEG_NI_QUADRA_DECODER)  += nidec_jpeg.o nicodec.o nidec.o
+OBJS-$(CONFIG_VP9_NI_QUADRA_DECODER)   += nidec_vp9.o nicodec.o nidec.o
+OBJS-$(CONFIG_AV1_NI_QUADRA_ENCODER)   += nienc_av1.o nicodec.o nienc.o
+OBJS-$(CONFIG_H264_NI_QUADRA_ENCODER)  += nienc_h264.o nicodec.o nienc.o
+OBJS-$(CONFIG_H265_NI_QUADRA_ENCODER)  += nienc_hevc.o nicodec.o nienc.o
+OBJS-$(CONFIG_JPEG_NI_QUADRA_ENCODER)  += nienc_jpeg.o nicodec.o nidec.o
 OBJS-$(CONFIG_HCA_DECODER)             += hcadec.o
 OBJS-$(CONFIG_HCOM_DECODER)            += hcom.o
 OBJS-$(CONFIG_HEVC_DECODER)            += hevcdec.o hevc_mvs.o \
@@ -1188,8 +1202,11 @@ OBJS-$(CONFIG_H264_METADATA_BSF)          += h264_metadata_bsf.o h264_levels.o
 OBJS-$(CONFIG_H264_MP4TOANNEXB_BSF)       += h264_mp4toannexb_bsf.o
 OBJS-$(CONFIG_H264_REDUNDANT_PPS_BSF)     += h264_redundant_pps_bsf.o
 OBJS-$(CONFIG_HAPQA_EXTRACT_BSF)          += hapqa_extract_bsf.o hap.o
+OBJS-$(CONFIG_HEVC_FRAME_SPLIT_BSF)       += ni_hevc_frame_split_bsf.o ni_hevc_rbsp.o
 OBJS-$(CONFIG_HEVC_METADATA_BSF)          += h265_metadata_bsf.o h265_profile_level.o
 OBJS-$(CONFIG_HEVC_MP4TOANNEXB_BSF)       += hevc_mp4toannexb_bsf.o
+OBJS-$(CONFIG_HEVC_RAWTOTILE_BSF)         += ni_hevc_rawtotile_bsf.o ni_hevc_rbsp.o
+OBJS-$(CONFIG_HEVC_TILE_REPACK_BSF)       += ni_hevc_tile_repack_bsf.o
 OBJS-$(CONFIG_IMX_DUMP_HEADER_BSF)        += imx_dump_header_bsf.o
 OBJS-$(CONFIG_MJPEG2JPEG_BSF)             += mjpeg2jpeg_bsf.o
 OBJS-$(CONFIG_MJPEGA_DUMP_HEADER_BSF)     += mjpega_dump_header_bsf.o
diff --git a/libavcodec/allcodecs.c b/libavcodec/allcodecs.c
index bdfc2f6f45..19706a3571 100644
--- a/libavcodec/allcodecs.c
+++ b/libavcodec/allcodecs.c
@@ -813,6 +813,18 @@ extern const FFCodec ff_ac3_mf_encoder;
 extern const FFCodec ff_h263_v4l2m2m_encoder;
 extern const FFCodec ff_libaom_av1_decoder;
 /* hwaccel hooks only, so prefer external decoders */
+extern const FFCodec ff_h264_ni_quadra_decoder;
+extern const FFCodec ff_h265_ni_quadra_decoder;
+extern const FFCodec ff_vp9_ni_quadra_decoder;
+extern const FFCodec ff_jpeg_ni_quadra_decoder;
+extern const FFCodec ff_h264_ni_quadra_encoder;
+extern const FFCodec ff_h265_ni_quadra_encoder;
+extern const FFCodec ff_av1_ni_quadra_encoder;
+extern const FFCodec ff_jpeg_ni_quadra_encoder;
+extern const FFCodec ff_h264_ni_logan_decoder;
+extern const FFCodec ff_h265_ni_logan_decoder;
+extern const FFCodec ff_h264_ni_logan_encoder;
+extern const FFCodec ff_h265_ni_logan_encoder;
 extern const FFCodec ff_av1_decoder;
 extern const FFCodec ff_av1_cuvid_decoder;
 extern const FFCodec ff_av1_qsv_decoder;
diff --git a/libavcodec/bitstream_filters.c b/libavcodec/bitstream_filters.c
index 444423ae93..00eff138aa 100644
--- a/libavcodec/bitstream_filters.c
+++ b/libavcodec/bitstream_filters.c
@@ -39,8 +39,11 @@ extern const FFBitStreamFilter ff_h264_metadata_bsf;
 extern const FFBitStreamFilter ff_h264_mp4toannexb_bsf;
 extern const FFBitStreamFilter ff_h264_redundant_pps_bsf;
 extern const FFBitStreamFilter ff_hapqa_extract_bsf;
+extern const AVBitStreamFilter ff_hevc_frame_split_bsf;
 extern const FFBitStreamFilter ff_hevc_metadata_bsf;
 extern const FFBitStreamFilter ff_hevc_mp4toannexb_bsf;
+extern const AVBitStreamFilter ff_hevc_rawtotile_bsf;
+extern const AVBitStreamFilter ff_hevc_tile_repack_bsf;
 extern const FFBitStreamFilter ff_imx_dump_header_bsf;
 extern const FFBitStreamFilter ff_mjpeg2jpeg_bsf;
 extern const FFBitStreamFilter ff_mjpega_dump_header_bsf;
diff --git a/libavcodec/h264_mp4toannexb_bsf.c b/libavcodec/h264_mp4toannexb_bsf.c
index d11be455c2..248607656c 100644
--- a/libavcodec/h264_mp4toannexb_bsf.c
+++ b/libavcodec/h264_mp4toannexb_bsf.c
@@ -62,7 +62,8 @@ static void count_or_copy(uint8_t **out, uint64_t *out_size,
     *out_size += start_code_size + in_size;
 }
 
-static int h264_extradata_to_annexb(AVBSFContext *ctx, const int padding)
+// NETINT: add argument side_data and side_size for sequence changing
+static int h264_extradata_to_annexb(AVBSFContext *ctx, const int padding, uint8_t *side_data, size_t side_size)
 {
     H264BSFContext *s = ctx->priv_data;
     GetByteContext ogb, *gb = &ogb;
@@ -72,7 +73,12 @@ static int h264_extradata_to_annexb(AVBSFContext *ctx, const int padding)
     static const uint8_t nalu_header[4] = { 0, 0, 0, 1 };
     int length_size, pps_offset = 0;
 
-    bytestream2_init(gb, ctx->par_in->extradata, ctx->par_in->extradata_size);
+    // NETINT: add processing when sidedata is available
+    if (side_data == NULL) {
+        bytestream2_init(gb, ctx->par_in->extradata, ctx->par_in->extradata_size);
+    } else {
+        bytestream2_init(gb, side_data, side_size);
+    }
 
     bytestream2_skipu(gb, 4);
 
@@ -149,7 +155,7 @@ static int h264_mp4toannexb_init(AVBSFContext *ctx)
         av_log(ctx, AV_LOG_VERBOSE,
                "The input looks like it is Annex B already\n");
     } else if (extra_size >= 7) {
-        ret = h264_extradata_to_annexb(ctx, AV_INPUT_BUFFER_PADDING_SIZE);
+        ret = h264_extradata_to_annexb(ctx, AV_INPUT_BUFFER_PADDING_SIZE, NULL, 0);
         if (ret < 0)
             return ret;
 
@@ -176,6 +182,8 @@ static int h264_mp4toannexb_filter(AVBSFContext *ctx, AVPacket *opkt)
     uint8_t *out;
     uint64_t out_size;
     int ret;
+    size_t side_size = 0; // NETINT: for sequence changing
+    uint8_t *side = NULL;
 
     ret = ff_bsf_get_packet(ctx, &in);
     if (ret < 0)
@@ -190,6 +198,16 @@ static int h264_mp4toannexb_filter(AVBSFContext *ctx, AVPacket *opkt)
 
     buf_end  = in->data + in->size;
 
+    // NETINT: check new extra data which maybe contains new header
+    side = av_packet_get_side_data(in, AV_PKT_DATA_NEW_EXTRADATA, &side_size);
+    if (side) {
+        av_log(ctx, AV_LOG_TRACE, "h264_mp4toannexb_bsf found %d bytes of side data in pkt\n", side_size);
+        ret = h264_extradata_to_annexb(ctx, AV_INPUT_BUFFER_PADDING_SIZE, side, side_size);
+        if (ret < 0) {
+            av_log(ctx, AV_LOG_WARNING, "extra data parsing failed\n");
+        }
+    }
+
 #define LOG_ONCE(...) \
     if (j) \
         av_log(__VA_ARGS__)
diff --git a/libavcodec/h264_sei.c b/libavcodec/h264_sei.c
index 034ddb8f1c..8cf1574081 100644
--- a/libavcodec/h264_sei.c
+++ b/libavcodec/h264_sei.c
@@ -39,6 +39,10 @@
 #include "h264_sei.h"
 #include "sei.h"
 
+#if CONFIG_NI_LOGAN
+#include <ni_device_api_logan.h>
+#endif
+
 #define AVERROR_PS_NOT_FOUND      FFERRTAG(0xF8,'?','P','S')
 
 static const uint8_t sei_num_clock_ts_table[9] = {
@@ -60,6 +64,9 @@ void ff_h264_sei_uninit(H264SEIContext *h)
     h->afd.present                 =  0;
 
     av_buffer_unref(&h->a53_caption.buf_ref);
+#if CONFIG_NI_LOGAN
+    av_buffer_unref(&h->ni_custom.buf_ref);
+#endif
     for (int i = 0; i < h->unregistered.nb_buf_ref; i++)
         av_buffer_unref(&h->unregistered.buf_ref[i]);
     h->unregistered.nb_buf_ref = 0;
@@ -460,6 +467,33 @@ static int decode_film_grain_characteristics(H264SEIFilmGrainCharacteristics *h,
     return 0;
 }
 
+#if CONFIG_NI_LOGAN
+static int decode_ni_custom(H264SEINICustom *h, GetBitContext *gb, int size)
+{
+    int i, index;
+    ni_logan_all_custom_sei_t *p_all_custom_sei;
+    ni_logan_custom_sei_t *p_custom_sei;
+
+    h->buf_ref = av_buffer_allocz(sizeof(ni_logan_all_custom_sei_t));
+    if (!h->buf_ref)
+        return AVERROR(ENOMEM);
+
+    p_all_custom_sei = (ni_logan_all_custom_sei_t *) h->buf_ref->data;
+    index = p_all_custom_sei->custom_sei_cnt;
+    p_custom_sei = &p_all_custom_sei->ni_custom_sei[index];
+
+    for (i = 0; i < size; i++)
+        p_custom_sei->custom_sei_data[i] = get_bits(gb, 8);
+
+    /* default for AVC */
+    p_custom_sei->custom_sei_loc = NI_LOGAN_CUSTOM_SEI_LOC_BEFORE_VCL;
+    p_custom_sei->custom_sei_size = size;
+    p_custom_sei->custom_sei_type = h->type;
+    p_all_custom_sei->custom_sei_cnt++;
+    return 0;
+}
+#endif
+
 int ff_h264_sei_decode(H264SEIContext *h, GetBitContext *gb,
                        const H264ParamSets *ps, void *logctx)
 {
@@ -527,6 +561,13 @@ int ff_h264_sei_decode(H264SEIContext *h, GetBitContext *gb,
         default:
             av_log(logctx, AV_LOG_DEBUG, "unknown SEI type %d\n", type);
         }
+
+#if CONFIG_NI_LOGAN
+        if (type == h->ni_custom.type) {
+            ret = decode_ni_custom(&h->ni_custom, gb, size);
+        }
+#endif
+
         if (ret < 0 && ret != AVERROR_PS_NOT_FOUND)
             return ret;
         if (ret < 0)
diff --git a/libavcodec/h264_sei.h b/libavcodec/h264_sei.h
index f9166b45df..aa5234e64a 100644
--- a/libavcodec/h264_sei.h
+++ b/libavcodec/h264_sei.h
@@ -186,6 +186,13 @@ typedef struct H264SEIFilmGrainCharacteristics {
     int repetition_period;
 } H264SEIFilmGrainCharacteristics;
 
+#if CONFIG_NI_LOGAN
+typedef struct H264SEINICustom {
+    int type;
+    AVBufferRef *buf_ref;
+} H264SEINICustom;
+#endif
+
 typedef struct H264SEIContext {
     H264SEIPictureTiming picture_timing;
     H264SEIAFD afd;
@@ -198,6 +205,9 @@ typedef struct H264SEIContext {
     H264SEIGreenMetaData green_metadata;
     H264SEIAlternativeTransfer alternative_transfer;
     H264SEIFilmGrainCharacteristics film_grain_characteristics;
+#if CONFIG_NI_LOGAN
+    H264SEINICustom ni_custom; // NETINT: NI AVC custom SEI
+#endif
 } H264SEIContext;
 
 struct H264ParamSets;
diff --git a/libavcodec/h264_slice.c b/libavcodec/h264_slice.c
index d56722a5c2..b0c982cc47 100644
--- a/libavcodec/h264_slice.c
+++ b/libavcodec/h264_slice.c
@@ -457,6 +457,18 @@ int ff_h264_update_thread_context(AVCodecContext *dst,
     }
     h->sei.unregistered.x264_build = h1->sei.unregistered.x264_build;
 
+#if CONFIG_NI_LOGAN
+    // NETINT: custom AVC SEI
+    h->custom_sei_type       = h1->custom_sei_type;
+
+    av_buffer_unref(&h->sei.ni_custom.buf_ref);
+    if (h1->sei.ni_custom.buf_ref) {
+        h->sei.ni_custom.buf_ref = av_buffer_ref(h1->sei.ni_custom.buf_ref);
+        if (!h->sei.ni_custom.buf_ref)
+            return AVERROR(ENOMEM);
+    }
+#endif
+
     if (!h->cur_pic_ptr)
         return 0;
 
@@ -1431,6 +1443,18 @@ static int h264_export_frame_props(H264Context *h)
         h->sei.picture_timing.timecode_cnt = 0;
     }
 
+#if CONFIG_NI_LOGAN
+    // NETINT: custom AVC SEI
+    if (h->sei.ni_custom.buf_ref) {
+        H264SEINICustom *ni_custom = &h->sei.ni_custom;
+
+        AVFrameSideData *sd = av_frame_new_side_data_from_buf(cur->f, AV_FRAME_DATA_NETINT_CUSTOM_SEI, ni_custom->buf_ref);
+        if (!sd)
+            av_buffer_unref(&ni_custom->buf_ref);
+        ni_custom->buf_ref = NULL;
+    }
+#endif
+
     return 0;
 }
 
diff --git a/libavcodec/h264dec.c b/libavcodec/h264dec.c
index 2a5b53ea56..570a3b3a53 100644
--- a/libavcodec/h264dec.c
+++ b/libavcodec/h264dec.c
@@ -676,6 +676,9 @@ static int decode_nal_units(H264Context *h, const uint8_t *buf, int buf_size)
                 avpriv_request_sample(avctx, "Late SEI");
                 break;
             }
+#if CONFIG_NI_LOGAN
+            h->sei.ni_custom.type = h->custom_sei_type;
+#endif
             ret = ff_h264_sei_decode(&h->sei, &nal->gb, &h->ps, avctx);
             h->has_recovery_point = h->has_recovery_point || h->sei.recovery_point.recovery_frame_cnt != -1;
             if (avctx->debug & FF_DEBUG_GREEN_MD)
@@ -1056,6 +1059,10 @@ static const AVOption h264_options[] = {
     { "nal_length_size", "nal_length_size", OFFSET(nal_length_size), AV_OPT_TYPE_INT, {.i64 = 0}, 0, 4, VDX },
     { "enable_er", "Enable error resilience on damaged frames (unsafe)", OFFSET(enable_er), AV_OPT_TYPE_BOOL, { .i64 = -1 }, -1, 1, VD },
     { "x264_build", "Assume this x264 version if no x264 version found in any SEI", OFFSET(x264_build), AV_OPT_TYPE_INT, {.i64 = -1}, -1, INT_MAX, VD },
+#if CONFIG_NI_LOGAN
+    // NETINT: Extra h264 decoding option
+    { "custom_sei_passthru", "Specify a custom SEI type to passthrough", OFFSET(custom_sei_type), AV_OPT_TYPE_INT, {.i64 = -1}, -1, 254, VD },
+#endif
     { NULL },
 };
 
diff --git a/libavcodec/h264dec.h b/libavcodec/h264dec.h
index 9a1ec1bace..efe6b3c95a 100644
--- a/libavcodec/h264dec.h
+++ b/libavcodec/h264dec.h
@@ -545,6 +545,11 @@ typedef struct H264Context {
     ERContext er;
     int16_t *dc_val_base;
 
+#if CONFIG_NI_LOGAN
+    // NETINT: option as custom SEI type setting
+    int custom_sei_type;  ///< NI custom SEI type
+#endif
+
     H264SEIContext sei;
 
     AVBufferPool *qscale_table_pool;
diff --git a/libavcodec/hevc_mp4toannexb_bsf.c b/libavcodec/hevc_mp4toannexb_bsf.c
index f9a025a36b..ea181146fe 100644
--- a/libavcodec/hevc_mp4toannexb_bsf.c
+++ b/libavcodec/hevc_mp4toannexb_bsf.c
@@ -37,7 +37,8 @@ typedef struct HEVCBSFContext {
     int      extradata_parsed;
 } HEVCBSFContext;
 
-static int hevc_extradata_to_annexb(AVBSFContext *ctx)
+// NETINT: add argument side and side_size for sequence changing
+static int hevc_extradata_to_annexb(AVBSFContext *ctx, uint8_t *side, size_t side_size)
 {
     GetByteContext gb;
     int length_size, num_arrays, i, j;
@@ -46,7 +47,12 @@ static int hevc_extradata_to_annexb(AVBSFContext *ctx)
     uint8_t *new_extradata = NULL;
     size_t   new_extradata_size = 0;
 
-    bytestream2_init(&gb, ctx->par_in->extradata, ctx->par_in->extradata_size);
+    // NETINT: add processing when sidedata is available
+    if (side == NULL) {
+        bytestream2_init(&gb, ctx->par_in->extradata, ctx->par_in->extradata_size);
+    } else {
+        bytestream2_init(&gb, side, side_size);
+    }
 
     bytestream2_skip(&gb, 21);
     length_size = (bytestream2_get_byte(&gb) & 3) + 1;
@@ -106,7 +112,7 @@ static int hevc_mp4toannexb_init(AVBSFContext *ctx)
         av_log(ctx, AV_LOG_VERBOSE,
                "The input looks like it is Annex B already\n");
     } else {
-        ret = hevc_extradata_to_annexb(ctx);
+        ret = hevc_extradata_to_annexb(ctx, NULL, 0);
         if (ret < 0)
             return ret;
         s->length_size      = ret;
@@ -124,6 +130,11 @@ static int hevc_mp4toannexb_filter(AVBSFContext *ctx, AVPacket *out)
 
     int got_irap = 0;
     int i, ret = 0;
+    // NETINT: don't prepend extradata to IRAP frames if VPS/SPS/PPS already
+    // in the packet
+    int has_header = 0, has_vps = 0, has_sps = 0, has_pps = 0;
+    size_t side_size = 0; // NETINT: for sequence changing
+    uint8_t *side = NULL;
 
     ret = ff_bsf_get_packet(ctx, &in);
     if (ret < 0)
@@ -135,6 +146,15 @@ static int hevc_mp4toannexb_filter(AVBSFContext *ctx, AVPacket *out)
         return 0;
     }
 
+    // NETINT: check new extra data which maybe contains new header
+    side = av_packet_get_side_data(in, AV_PKT_DATA_NEW_EXTRADATA, &side_size);
+    if (side) {
+        ret = hevc_extradata_to_annexb(ctx, side, side_size);
+        if (ret < 0) {
+            av_log(ctx, AV_LOG_WARNING, "extra data parsing failed\n");
+        }
+    }
+
     bytestream2_init(&gb, in->data, in->size);
 
     while (bytestream2_get_bytes_left(&gb)) {
@@ -155,10 +175,14 @@ static int hevc_mp4toannexb_filter(AVBSFContext *ctx, AVPacket *out)
         }
 
         nalu_type = (bytestream2_peek_byte(&gb) >> 1) & 0x3f;
+        has_vps |= (HEVC_NAL_VPS == nalu_type);
+        has_sps |= (HEVC_NAL_SPS == nalu_type);
+        has_pps |= (HEVC_NAL_PPS == nalu_type);
+        has_header = (has_vps && has_sps && has_pps);
 
         /* prepend extradata to IRAP frames */
         is_irap       = nalu_type >= 16 && nalu_type <= 23;
-        add_extradata = is_irap && !got_irap;
+        add_extradata = is_irap && !has_header && !got_irap;
         extra_size    = add_extradata * ctx->par_out->extradata_size;
         got_irap     |= is_irap;
 
diff --git a/libavcodec/hevc_parser.c b/libavcodec/hevc_parser.c
index 59f9a0ff3e..c4b4509397 100644
--- a/libavcodec/hevc_parser.c
+++ b/libavcodec/hevc_parser.c
@@ -31,6 +31,8 @@
 #include "parser.h"
 
 #define START_CODE 0x000001 ///< start_code_prefix_one_3bytes
+// NETINT: start_code_4bytes handling
+#define START_CODE_4 0x00000001 ///< start_code_4bytes
 
 #define IS_IRAP_NAL(nal) (nal->type >= 16 && nal->type <= 23)
 #define IS_IDR_NAL(nal) (nal->type == HEVC_NAL_IDR_W_RADL || nal->type == HEVC_NAL_IDR_N_LP)
@@ -277,8 +279,7 @@ static int hevc_find_frame_end(AVCodecParserContext *s, const uint8_t *buf,
         if ((nut >= HEVC_NAL_VPS && nut <= HEVC_NAL_EOB_NUT) || nut == HEVC_NAL_SEI_PREFIX ||
             (nut >= 41 && nut <= 44) || (nut >= 48 && nut <= 55)) {
             if (pc->frame_start_found) {
-                pc->frame_start_found = 0;
-                return i - 5;
+                goto found;
             }
         } else if (nut <= HEVC_NAL_RASL_R ||
                    (nut >= HEVC_NAL_BLA_W_LP && nut <= HEVC_NAL_CRA_NUT)) {
@@ -287,14 +288,19 @@ static int hevc_find_frame_end(AVCodecParserContext *s, const uint8_t *buf,
                 if (!pc->frame_start_found) {
                     pc->frame_start_found = 1;
                 } else { // First slice of next frame found
-                    pc->frame_start_found = 0;
-                    return i - 5;
+                    goto found;
                 }
             }
         }
     }
 
     return END_NOT_FOUND;
+
+found:
+    pc->frame_start_found = 0;
+    if (((pc->state64 >> 3 * 8) & 0xFFFFFFFF) == START_CODE_4)
+        return i - 6;
+    return i - 5;
 }
 
 static int hevc_parse(AVCodecParserContext *s, AVCodecContext *avctx,
diff --git a/libavcodec/hevc_sei.c b/libavcodec/hevc_sei.c
index 631373e06f..315af07253 100644
--- a/libavcodec/hevc_sei.c
+++ b/libavcodec/hevc_sei.c
@@ -30,6 +30,10 @@
 #include "hevc_ps.h"
 #include "hevc_sei.h"
 
+#if CONFIG_NI_LOGAN
+#include <ni_device_api_logan.h>
+#endif
+
 static int decode_nal_sei_decoded_picture_hash(HEVCSEIPictureHash *s,
                                                GetByteContext *gb)
 {
@@ -374,6 +378,32 @@ static int decode_nal_sei_alternative_transfer(HEVCSEIAlternativeTransfer *s,
     return 0;
 }
 
+#if CONFIG_NI_LOGAN
+static int decode_nal_sei_ni_custom(HEVCSEINICustom *s, GetBitContext *gb, int size, int location)
+{
+    int i, index;
+    ni_logan_all_custom_sei_t *p_all_custom_sei;
+    ni_logan_custom_sei_t *p_custom_sei;
+
+    s->buf_ref = av_buffer_allocz(sizeof(ni_logan_all_custom_sei_t));
+    if (!s->buf_ref)
+        return AVERROR(ENOMEM);
+
+    p_all_custom_sei = (ni_logan_all_custom_sei_t *) s->buf_ref->data;
+    index = p_all_custom_sei->custom_sei_cnt;
+    p_custom_sei = &p_all_custom_sei->ni_custom_sei[index];
+
+    for (i = 0; i < size; i++)
+        p_custom_sei->custom_sei_data[i] = get_bits(gb, 8);
+
+    p_custom_sei->custom_sei_loc = location;
+    p_custom_sei->custom_sei_size = size;
+    p_custom_sei->custom_sei_type = s->type;
+    p_all_custom_sei->custom_sei_cnt++;
+    return 0;
+}
+#endif
+
 static int decode_nal_sei_timecode(HEVCSEITimeCode *s, GetBitContext *gb)
 {
     s->num_clock_ts = get_bits(gb, 2);
@@ -540,6 +570,13 @@ static int decode_nal_sei_message(GetByteContext *gb, void *logctx, HEVCSEI *s,
     ret = init_get_bits8(&message_gb, gb->buffer, payload_size);
     av_assert1(ret >= 0);
     bytestream2_skipu(gb, payload_size);
+
+#if CONFIG_NI_LOGAN
+    if (payload_type == s->ni_custom.type) {
+        return decode_nal_sei_ni_custom(&s->ni_custom, gb, payload_size, s->ni_custom.location);
+    }
+#endif
+
     if (nal_unit_type == HEVC_NAL_SEI_PREFIX) {
         return decode_nal_sei_prefix(&message_gb, &message_gbyte,
                                      logctx, s, ps, payload_type);
@@ -570,6 +607,9 @@ int ff_hevc_decode_nal_sei(GetBitContext *gb, void *logctx, HEVCSEI *s,
 void ff_hevc_reset_sei(HEVCSEI *s)
 {
     av_buffer_unref(&s->a53_caption.buf_ref);
+#if CONFIG_NI_LOGAN
+    av_buffer_unref(&s->ni_custom.buf_ref);
+#endif
 
     for (int i = 0; i < s->unregistered.nb_buf_ref; i++)
         av_buffer_unref(&s->unregistered.buf_ref[i]);
diff --git a/libavcodec/hevc_sei.h b/libavcodec/hevc_sei.h
index ef987f6781..d4b537ebbe 100644
--- a/libavcodec/hevc_sei.h
+++ b/libavcodec/hevc_sei.h
@@ -94,6 +94,14 @@ typedef struct HEVCSEIAlternativeTransfer {
     int preferred_transfer_characteristics;
 } HEVCSEIAlternativeTransfer;
 
+#if CONFIG_NI_LOGAN
+typedef struct HEVCSEINICustom {
+    AVBufferRef *buf_ref;
+    int location;
+    int type;
+} HEVCSEINICustom;
+#endif
+
 typedef struct HEVCSEITimeCode {
     int      present;
     uint8_t  num_clock_ts;
@@ -150,6 +158,10 @@ typedef struct HEVCSEI {
     HEVCSEIAlternativeTransfer alternative_transfer;
     HEVCSEITimeCode timecode;
     HEVCSEIFilmGrainCharacteristics film_grain_characteristics;
+#if CONFIG_NI_LOGAN
+    // NETINT: NI HEVC custom SEI
+    HEVCSEINICustom ni_custom;
+#endif
 } HEVCSEI;
 
 struct HEVCParamSets;
diff --git a/libavcodec/hevcdec.c b/libavcodec/hevcdec.c
index f8f981e838..30653259b7 100644
--- a/libavcodec/hevcdec.c
+++ b/libavcodec/hevcdec.c
@@ -50,7 +50,11 @@
 #include "internal.h"
 #include "profiles.h"
 #include "thread.h"
-#include "threadframe.h"
+#include "profiles.h"
+
+#if CONFIG_NI_LOGAN
+#include <ni_device_api_logan.h>
+#endif
 
 static const uint8_t hevc_pel_weight[65] = { [2] = 0, [4] = 1, [6] = 2, [8] = 3, [12] = 4, [16] = 5, [24] = 6, [32] = 7, [48] = 8, [64] = 9 };
 
@@ -2976,6 +2980,19 @@ static int set_side_data(HEVCContext *s)
         }
     }
 
+#if CONFIG_NI_LOGAN
+    // NETINT: NI HEVC custom SEI
+    if (s->sei.ni_custom.buf_ref) {
+        HEVCSEINICustom *ni_custom = &s->sei.ni_custom;
+        AVFrameSideData* sd = av_frame_new_side_data_from_buf(out,
+                                                              AV_FRAME_DATA_NETINT_CUSTOM_SEI,
+                                                              ni_custom->buf_ref);
+        if (!sd)
+            av_buffer_unref(&ni_custom->buf_ref);
+        ni_custom->buf_ref = NULL;
+    }
+#endif
+
     if (s->rpu_buf) {
         AVFrameSideData *rpu = av_frame_new_side_data_from_buf(out, AV_FRAME_DATA_DOVI_RPU_BUFFER, s->rpu_buf);
         if (!rpu)
@@ -3155,6 +3172,9 @@ static int decode_nal_unit(HEVCContext *s, const H2645NAL *nal)
             if (ret < 0)
                 goto fail;
         }
+#if CONFIG_NI_LOGAN
+        s->sei.ni_custom.type = s->custom_sei_type;
+#endif
         ret = ff_hevc_decode_nal_sei(gb, s->avctx, &s->sei, &s->ps, s->nal_unit_type);
         if (ret < 0)
             goto fail;
@@ -3288,6 +3308,9 @@ fail:
 static int decode_nal_units(HEVCContext *s, const uint8_t *buf, int length)
 {
     int i, ret = 0;
+#if CONFIG_NI_LOGAN
+    int got_slice = -1;
+#endif
     int eos_at_start = 1;
 
     s->ref = NULL;
@@ -3357,6 +3380,52 @@ static int decode_nal_units(HEVCContext *s, const uint8_t *buf, int length)
             && ff_hevc_nal_is_nonref(nal->type)) || nal->nuh_layer_id > 0)
             continue;
 
+#if CONFIG_NI_LOGAN
+        switch (nal->type) {
+            case HEVC_NAL_SEI_PREFIX:
+            case HEVC_NAL_SEI_SUFFIX:
+                if (got_slice >= 0) {
+                    /* We do NOT decide SEI location according to NALU type, we
+                     * detect the actual order of NALU in AVPacket instead.
+                     */
+                    s->sei.ni_custom.location = NI_LOGAN_CUSTOM_SEI_LOC_AFTER_VCL;
+                } else {
+                    s->sei.ni_custom.location = NI_LOGAN_CUSTOM_SEI_LOC_BEFORE_VCL;
+                }
+                break;
+            case HEVC_NAL_TRAIL_R:
+            case HEVC_NAL_TRAIL_N:
+            case HEVC_NAL_TSA_N:
+            case HEVC_NAL_TSA_R:
+            case HEVC_NAL_STSA_N:
+            case HEVC_NAL_STSA_R:
+            case HEVC_NAL_BLA_W_LP:
+            case HEVC_NAL_BLA_W_RADL:
+            case HEVC_NAL_BLA_N_LP:
+            case HEVC_NAL_IDR_W_RADL:
+            case HEVC_NAL_IDR_N_LP:
+            case HEVC_NAL_CRA_NUT:
+            case HEVC_NAL_RADL_N:
+            case HEVC_NAL_RADL_R:
+            case HEVC_NAL_RASL_N:
+            case HEVC_NAL_RASL_R:
+                // For soft decoding in FFmpeg we have to assure that the non
+                // VCL slice be decoded before any VCL slice to make the custom
+                // SEI pass through. So if any VCL is found before custom SEI in
+                // the packet we need to parse the suffix SEI (even it is
+                // labeled as prefix) ahead.
+                if (i != s->pkt.nb_nals - 1) {
+                    H2645NAL *last_nal = &s->pkt.nals[s->pkt.nb_nals - 1];
+                    if (last_nal->type != nal->type) {
+                        got_slice = i;
+                        continue;
+                    }
+                }
+            default:
+                break;
+        }
+#endif
+
         ret = decode_nal_unit(s, nal);
         if (ret >= 0 && s->overlap > 2)
             ret = AVERROR_INVALIDDATA;
@@ -3367,6 +3436,20 @@ static int decode_nal_units(HEVCContext *s, const uint8_t *buf, int length)
         }
     }
 
+#if CONFIG_NI_LOGAN
+    /* We need to decode slice as the last NALU for custom SEI decoding ahead */
+    if (got_slice >= 0) {
+        ret = decode_nal_unit(s, &s->pkt.nals[got_slice]);
+        if (ret >= 0 && s->overlap > 2)
+            ret = AVERROR_INVALIDDATA;
+        if (ret < 0) {
+            av_log(s->avctx, AV_LOG_WARNING,
+                   "Error parsing NAL unit #%d.\n", i);
+            goto fail;
+        }
+    }
+#endif
+
 fail:
     if (s->ref && s->threads_type == FF_THREAD_FRAME)
         ff_thread_report_progress(&s->ref->tf, INT_MAX, 0);
@@ -3740,6 +3823,10 @@ static int hevc_update_thread_context(AVCodecContext *dst,
     s->threads_number      = s0->threads_number;
     s->threads_type        = s0->threads_type;
 
+#if CONFIG_NI_LOGAN
+    s->custom_sei_type     = s0->custom_sei_type;
+#endif
+
     if (s0->eos) {
         s->seq_decode = (s->seq_decode + 1) & 0xff;
         s->max_ra = INT_MAX;
@@ -3789,6 +3876,9 @@ static int hevc_update_thread_context(AVCodecContext *dst,
     s->sei.mastering_display    = s0->sei.mastering_display;
     s->sei.content_light        = s0->sei.content_light;
     s->sei.alternative_transfer = s0->sei.alternative_transfer;
+#if CONFIG_NI_LOGAN
+    s->sei.ni_custom            = s0->sei.ni_custom;
+#endif
 
     ret = export_stream_params_from_sei(s);
     if (ret < 0)
@@ -3857,6 +3947,11 @@ static const AVOption options[] = {
         AV_OPT_TYPE_BOOL, {.i64 = 0}, 0, 1, PAR },
     { "strict-displaywin", "stricly apply default display window size", OFFSET(apply_defdispwin),
         AV_OPT_TYPE_BOOL, {.i64 = 0}, 0, 1, PAR },
+#if CONFIG_NI_LOGAN
+    // NETINT: Extra HEVC decoding option
+    { "custom_sei_passthru", "Specify the custom SEI type to passthrough", OFFSET(custom_sei_type),
+        AV_OPT_TYPE_INT, {.i64 = -1}, -1, 254, PAR },
+#endif
     { NULL },
 };
 
diff --git a/libavcodec/hevcdec.h b/libavcodec/hevcdec.h
index f6acf845ae..4f75747cf1 100644
--- a/libavcodec/hevcdec.h
+++ b/libavcodec/hevcdec.h
@@ -566,6 +566,11 @@ typedef struct HEVCContext {
     int is_nalff;           ///< this flag is != 0 if bitstream is encapsulated
                             ///< as a format defined in 14496-15
     int apply_defdispwin;
+#if CONFIG_NI_LOGAN
+    // NETINT: option as custom HEVC SEI type setting
+    int custom_sei_type;    ///< NI custom HEVC SEI type
+    int custom_sei_location;  ///< NI custom HEVC SEI location
+#endif
 
     int nal_length_size;    ///< Number of bytes used for nal length (1, 2 or 4)
     int nuh_layer_id;
diff --git a/libavcodec/ni_hevc_extradata.c b/libavcodec/ni_hevc_extradata.c
new file mode 100644
index 0000000000..7ca86b4b05
--- /dev/null
+++ b/libavcodec/ni_hevc_extradata.c
@@ -0,0 +1,100 @@
+/*
+ * NetInt HEVC tile bitstream filter common source code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ *
+ * Extract tile rows and columns number from HEVC AVPacket extra-data using
+ * FFmpeg coded bitstream APIs.
+ */
+
+#include "config.h"
+
+#include "libavutil/avassert.h"
+
+#include "avcodec.h"
+#include "bsf.h"
+#include "cbs.h"
+#include "cbs_h265.h"
+
+#include "ni_hevc_extradata.h"
+
+int av_hevc_extract_tiles_from_extradata(uint8_t *extradata,
+                                         size_t extradata_size, int *tile_row,
+                                         int *tile_col) {
+#if CONFIG_HEVC_FRAME_SPLIT_BSF
+    int i, ret;
+    AVCodecParameters par_in;
+    CodedBitstreamContext *cbc;
+    CodedBitstreamFragment td;
+    CodedBitstreamUnit *unit;
+    H265RawPPS *pps;
+
+    if (!extradata || !tile_row || !tile_col) {
+        ret = AVERROR(EINVAL);
+        av_log(NULL, AV_LOG_ERROR, "invalid arguments\n");
+        return ret;
+    }
+
+    ret = ff_cbs_init(&cbc, AV_CODEC_ID_HEVC, NULL);
+    if (ret < 0) {
+        av_log(NULL, AV_LOG_ERROR, "failed to initialize cbc\n");
+        return ret;
+    }
+
+    memset(&par_in, 0, sizeof(AVCodecParameters));
+    par_in.extradata      = extradata;
+    par_in.extradata_size = extradata_size;
+
+    memset(&td, 0, sizeof(CodedBitstreamFragment));
+
+    ret = ff_cbs_read_extradata(cbc, &td, &par_in);
+    if (ret < 0) {
+        av_log(NULL, AV_LOG_ERROR, "failed to read extradata\n");
+        goto out;
+    }
+
+    for (i = 0; i < td.nb_units; i++) {
+        unit = &td.units[i];
+        if (unit->type == HEVC_NAL_PPS) {
+            pps = (H265RawPPS *)unit->content;
+            *tile_row =
+                pps->tiles_enabled_flag ? pps->num_tile_rows_minus1 + 1 : 1;
+            *tile_col =
+                pps->tiles_enabled_flag ? pps->num_tile_columns_minus1 + 1 : 1;
+            break;
+        }
+    }
+
+out:
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_free(&td);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58) && (LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_free(cbc, &td);
+#else
+    ff_cbs_fragment_uninit(cbc, &td);
+#endif
+    ff_cbs_close(&cbc);
+    return ret;
+#else
+    return AVERROR(ENOSYS);
+#endif
+}
diff --git a/libavcodec/ni_hevc_extradata.h b/libavcodec/ni_hevc_extradata.h
new file mode 100644
index 0000000000..d0b5ff171c
--- /dev/null
+++ b/libavcodec/ni_hevc_extradata.h
@@ -0,0 +1,32 @@
+/*
+ * NetInt HEVC tile bitstream filter common code header
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_HEVC_EXTRADATA_H
+#define AVCODEC_HEVC_EXTRADATA_H
+
+#include <stddef.h>
+#include <stdint.h>
+
+int av_hevc_extract_tiles_from_extradata(uint8_t *extradata,
+                                         size_t extradata_size, int *tile_row,
+                                         int *tile_col);
+
+#endif
diff --git a/libavcodec/ni_hevc_frame_split_bsf.c b/libavcodec/ni_hevc_frame_split_bsf.c
new file mode 100644
index 0000000000..4368717830
--- /dev/null
+++ b/libavcodec/ni_hevc_frame_split_bsf.c
@@ -0,0 +1,755 @@
+/*
+ * NetInt HEVC frame split BSF common source code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ *
+ * This bitstream filter splits HEVC stream into packets containing just one
+ * frame and re-encoding them with tile flags so that the splited packets can
+ * be decoded independently.
+ */
+
+#include "libavutil/avassert.h"
+
+#include "avcodec.h"
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 91)
+#include "bsf_internal.h"
+#else
+#include "bsf.h"
+#endif
+#include "cbs.h"
+#include "cbs_h265.h"
+#include "ni_hevc_extradata.h"
+#include "ni_hevc_rbsp.h"
+
+struct tile_format {
+    int log2_ctb_size;
+    int num_tile_columns;
+    int num_tile_rows;
+    int ctb_width;
+    int ctb_height;
+    int width;
+    int height;
+
+    int *column_width;
+    int *row_height;
+    int *col_idx;
+    int *row_idx;
+};
+
+typedef struct HEVCFSplitContext {
+    AVPacket *buffer_pkt;
+    CodedBitstreamContext *cbc;
+    CodedBitstreamFragment temporal_unit;
+
+    struct tile_format *tiles[HEVC_MAX_PPS_COUNT];
+    struct tile_format **this_tile;
+    ni_bitstream_t *streams;
+
+    int tile_enabled;
+    int num_tiles;
+    int unit_offset;
+    int nb_slices;
+} HEVCFSplitContext;
+
+static int slice_addr_to_idx(HEVCFSplitContext *s, int slice_addr, int hid) {
+    int x, y;
+    struct tile_format *format = s->tiles[hid];
+
+    av_assert0(format);
+
+    x = slice_addr % format->ctb_width;
+    y = slice_addr / format->ctb_width;
+
+    return format->col_idx[x] + format->num_tile_columns * format->row_idx[y];
+}
+
+static void slice_geo(HEVCFSplitContext *s, int slice_idx, int *width,
+                      int *height, int hid) {
+    int x, y;
+    struct tile_format *format = s->tiles[hid];
+
+    av_assert0(format);
+
+    x = slice_idx % format->num_tile_columns;
+    y = slice_idx / format->num_tile_columns;
+
+    *width  = format->column_width[x];
+    *height = format->row_height[y];
+}
+
+static int hevc_frame_resolve_tiles(HEVCFSplitContext *s, AVBSFContext *ctx,
+                                    const H265RawSPS *sps,
+                                    const H265RawPPS *pps) {
+    int i, j;
+    int log2_ctb_size;
+    int limit;
+    struct tile_format *format;
+    int *index_buffer;
+    uint32_t index_len;
+
+    if (pps->pps_pic_parameter_set_id >= HEVC_MAX_PPS_COUNT) {
+        av_log(ctx, AV_LOG_ERROR, "pps id %d exceeds maximus\n",
+               pps->pps_pic_parameter_set_id);
+        return AVERROR(EINVAL);
+    }
+
+    if (!s->tiles[pps->pps_pic_parameter_set_id]) {
+        format = av_mallocz(sizeof(struct tile_format));
+        if (!format) {
+            av_log(ctx, AV_LOG_ERROR, "failed to allocate tile format\n");
+            return AVERROR(ENOMEM);
+        }
+        s->tiles[pps->pps_pic_parameter_set_id] = format;
+    } else {
+        format = s->tiles[pps->pps_pic_parameter_set_id];
+    }
+
+    if (!pps->tiles_enabled_flag) {
+        av_log(ctx, AV_LOG_ERROR, "tile enabled flags invalid\n");
+        return AVERROR(ENODEV);
+    }
+
+    log2_ctb_size = (sps->log2_min_luma_coding_block_size_minus3 + 3) +
+                    sps->log2_diff_max_min_luma_coding_block_size;
+    format->ctb_width =
+        (sps->pic_width_in_luma_samples + (1 << log2_ctb_size) - 1) >>
+        log2_ctb_size;
+    format->ctb_height =
+        (sps->pic_height_in_luma_samples + (1 << log2_ctb_size) - 1) >>
+        log2_ctb_size;
+    format->num_tile_columns = pps->num_tile_columns_minus1 + 1;
+    format->num_tile_rows    = pps->num_tile_rows_minus1 + 1;
+    format->log2_ctb_size    = log2_ctb_size;
+    format->width            = sps->pic_width_in_luma_samples;
+    format->height           = sps->pic_height_in_luma_samples;
+
+    av_log(
+        ctx, AV_LOG_DEBUG,
+        "ctb_size %d, ctb_width %d, ctb_height %d, uniform_spacing_flag %d\n",
+        log2_ctb_size, format->ctb_width, format->ctb_height,
+        pps->uniform_spacing_flag);
+
+    index_len = sizeof(int) * ((pps->num_tile_columns_minus1 + 1) +
+                               (pps->num_tile_rows_minus1 + 1) +
+                               format->ctb_width + format->ctb_height);
+
+    index_buffer = av_mallocz(index_len);
+    if (!index_buffer) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate index buffer\n");
+        return AVERROR(ENOMEM);
+    }
+
+    format->column_width = index_buffer;
+    format->row_height =
+        format->column_width + (pps->num_tile_columns_minus1 + 1);
+    format->col_idx = format->row_height + (pps->num_tile_rows_minus1 + 1);
+    format->row_idx = format->col_idx + format->ctb_width;
+
+    if (pps->uniform_spacing_flag) {
+        for (i = 0, limit = 0; i < pps->num_tile_columns_minus1; i++) {
+            format->column_width[i] =
+                (((i + 1) * format->ctb_width) /
+                     (pps->num_tile_columns_minus1 + 1) -
+                 (i * format->ctb_width) / (pps->num_tile_columns_minus1 + 1))
+                << log2_ctb_size;
+            limit += format->column_width[i];
+        }
+        format->column_width[i] = sps->pic_width_in_luma_samples - limit;
+
+        for (i = 0, limit = 0; i < pps->num_tile_rows_minus1; i++) {
+            format->row_height[i] =
+                (((i + 1) * format->ctb_height) /
+                     (pps->num_tile_rows_minus1 + 1) -
+                 (i * format->ctb_height) / (pps->num_tile_rows_minus1 + 1))
+                << log2_ctb_size;
+            limit += format->row_height[i];
+        }
+        format->row_height[i] = sps->pic_height_in_luma_samples - limit;
+    } else {
+        for (i = 0, limit = 0; i < pps->num_tile_columns_minus1; i++) {
+            format->column_width[i] = (pps->column_width_minus1[i] + 1)
+                                      << log2_ctb_size;
+            limit += format->column_width[i];
+        }
+        format->column_width[i] = sps->pic_width_in_luma_samples - limit;
+
+        for (i = 0, limit = 0; i < pps->num_tile_rows_minus1; i++) {
+            format->row_height[i] = (pps->row_height_minus1[i] + 1)
+                                    << log2_ctb_size;
+            limit += format->row_height[i];
+        }
+        format->row_height[i] = sps->pic_height_in_luma_samples - limit;
+    }
+
+    limit =
+        (format->column_width[0] + (1 << log2_ctb_size) - 1) >> log2_ctb_size;
+    for (i = j = 0; i < format->ctb_width; i++) {
+        if (i >= limit) {
+            j++;
+            limit += ((format->column_width[j] + (1 << log2_ctb_size) - 1) >>
+                      log2_ctb_size);
+        }
+        format->col_idx[i] = j;
+    }
+
+    limit = (format->row_height[0] + (1 << log2_ctb_size) - 1) >> log2_ctb_size;
+    for (i = j = 0; i < format->ctb_height; i++) {
+        if (i >= limit) {
+            j++;
+            limit += ((format->row_height[j] + (1 << log2_ctb_size) - 1) >>
+                      log2_ctb_size);
+        }
+        format->row_idx[i] = j;
+    }
+
+    /* dump index buffer */
+    for (i = 0; i < pps->num_tile_columns_minus1 + 1; i++) {
+        av_log(ctx, AV_LOG_DEBUG, "column_width: %d %d\n", i,
+               format->column_width[i]);
+    }
+
+    for (i = 0; i < pps->num_tile_rows_minus1 + 1; i++) {
+        av_log(ctx, AV_LOG_DEBUG, "row_width: %d %d\n", i,
+               format->row_height[i]);
+    }
+
+    for (i = 0; i < format->ctb_width; i++) {
+        av_log(ctx, AV_LOG_DEBUG, "column_idx: %d %d\n", i, format->col_idx[i]);
+    }
+
+    for (i = 0; i < format->ctb_height; i++) {
+        av_log(ctx, AV_LOG_DEBUG, "row_idx: %d %d\n", i, format->row_idx[i]);
+    }
+
+    return 0;
+}
+
+static int hevc_frame_init_tiles(HEVCFSplitContext *s, AVBSFContext *ctx) {
+    CodedBitstreamH265Context *priv = s->cbc->priv_data;
+    const H265RawSPS *sps;
+    const H265RawPPS *pps;
+    int i, ret;
+
+    for (i = 0; i < s->temporal_unit.nb_units; i++) {
+        CodedBitstreamUnit *unit = &s->temporal_unit.units[i];
+        if (unit->type == HEVC_NAL_PPS) {
+            pps = (H265RawPPS *)unit->content;
+            break;
+        }
+    }
+
+    if (i >= s->temporal_unit.nb_units) {
+        av_log(ctx, AV_LOG_ERROR, "cannot find valid header\n");
+        return AVERROR(ENODEV);
+    }
+
+    sps = priv->sps[pps->pps_seq_parameter_set_id];
+    if (!sps) {
+        av_log(ctx, AV_LOG_ERROR, "invalid pps data\n");
+        return AVERROR(EINVAL);
+    }
+
+    if (!pps->tiles_enabled_flag) {
+        av_log(ctx, AV_LOG_ERROR, "tile is disabled\n");
+        return AVERROR(ENODEV);
+    }
+
+    s->tile_enabled = !!pps->tiles_enabled_flag;
+    s->num_tiles =
+        (pps->num_tile_columns_minus1 + 1) * (pps->num_tile_rows_minus1 + 1);
+
+    ret = hevc_frame_resolve_tiles(s, ctx, sps, pps);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to resolve tiles\n");
+        return ret;
+    }
+
+    s->this_tile = &s->tiles[pps->pps_pic_parameter_set_id];
+    if (!s->this_tile) {
+        av_log(ctx, AV_LOG_ERROR, "invalid tile format\n");
+        return AVERROR(EINVAL);
+    }
+
+    return 0;
+}
+
+static int hevc_frame_encode_vps(HEVCFSplitContext *s, AVBSFContext *ctx,
+                                 CodedBitstreamUnit *unit, int tile_idx) {
+    ni_bitstream_t *stream = &s->streams[tile_idx];
+    H265RawVPS *vps        = unit->content;
+    int ret;
+
+    ni_write_nal_header(stream, HEVC_NAL_VPS, 0, 1);
+    ret = ni_hevc_encode_nal_vps(stream, vps);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to encode vps for tile %d\n",
+               tile_idx);
+        return ret;
+    }
+
+    return 0;
+}
+
+static int hevc_frame_tiles_encode_vps(HEVCFSplitContext *s, AVBSFContext *ctx,
+                                       CodedBitstreamUnit *unit) {
+    int i, ret = 0;
+
+    for (i = 0; i < s->num_tiles; i++) {
+        ret = hevc_frame_encode_vps(s, ctx, unit, i);
+        if (ret)
+            break;
+    }
+
+    return ret;
+}
+
+static int hevc_frame_encode_sps(HEVCFSplitContext *s, AVBSFContext *ctx,
+                                 CodedBitstreamUnit *unit, int tile_idx,
+                                 int hid) {
+    int ret, width, height;
+    ni_bitstream_t *stream = &s->streams[tile_idx];
+    H265RawSPS *sps        = unit->content;
+
+    slice_geo(s, tile_idx, &width, &height, hid);
+
+    av_log(ctx, AV_LOG_DEBUG, "tile_idx %d, pixel %dx%d\n", tile_idx, width,
+           height);
+
+    ni_write_nal_header(stream, HEVC_NAL_SPS, 0, 1);
+    ret = ni_hevc_encode_nal_sps(stream, sps, width, height);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to encode sps for tile %d\n",
+               tile_idx);
+        return ret;
+    }
+
+    return 0;
+}
+
+static int hevc_frame_tiles_encode_sps(HEVCFSplitContext *s, AVBSFContext *ctx,
+                                       CodedBitstreamUnit *unit, int hid) {
+    int i, ret = 0;
+
+    for (i = 0; i < s->num_tiles; i++) {
+        ret = hevc_frame_encode_sps(s, ctx, unit, i, hid);
+        if (ret)
+            break;
+    }
+
+    return ret;
+}
+
+static int hevc_frame_encode_pps(HEVCFSplitContext *s, AVBSFContext *ctx,
+                                 CodedBitstreamUnit *unit, int tile_idx) {
+    ni_bitstream_t *stream = &s->streams[tile_idx];
+    H265RawPPS *pps        = unit->content;
+    int ret                = 0;
+
+    ni_write_nal_header(stream, HEVC_NAL_PPS, 0, 1);
+    ret = ni_hevc_encode_nal_pps(stream, pps, 1, 1, 1);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to encode pps for tile %d\n",
+               tile_idx);
+    }
+
+    return ret;
+}
+
+static int hevc_frame_tiles_encode_pps(HEVCFSplitContext *s, AVBSFContext *ctx,
+                                       CodedBitstreamUnit *unit) {
+    int i, ret = 0;
+
+    for (i = 0; i < s->num_tiles; i++) {
+        ret = hevc_frame_encode_pps(s, ctx, unit, i);
+        if (ret)
+            break;
+    }
+
+    return ret;
+}
+
+static int hevc_frame_encode_slice_header(HEVCFSplitContext *s,
+                                          AVBSFContext *ctx,
+                                          CodedBitstreamUnit *unit,
+                                          int tile_idx, int hid) {
+    H265RawSlice *slice             = unit->content;
+    ni_bitstream_t *stream          = &s->streams[tile_idx];
+    CodedBitstreamH265Context *priv = s->cbc->priv_data;
+    H265RawPPS *pps                 = priv->pps[hid];
+    H265RawSPS *sps                 = priv->sps[pps->pps_seq_parameter_set_id];
+    int i, ret;
+
+    ni_write_nal_header(stream, slice->header.nal_unit_header.nal_unit_type, 0,
+                        1);
+
+    ret = ni_hevc_encode_nal_slice_header(stream, &slice->header, sps, pps, -1,
+                                          -1, 1 /* disable tile */, 0, 0,
+                                          1 /* independent */);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to encode slice header for tile %d\n",
+               tile_idx);
+        return ret;
+    }
+
+    av_assert0((slice->data_bit_start % 8) == 0);
+
+    for (i = 0; i < slice->data_size; i++) {
+        ni_put_bits(stream, 8, slice->data[i]);
+    }
+
+    return 0;
+}
+
+static int hevc_frame_init_bitstream(HEVCFSplitContext *s) {
+    int i;
+
+    s->streams = av_mallocz(s->num_tiles * sizeof(ni_bitstream_t));
+    if (!s->streams) {
+        return AVERROR(ENOMEM);
+    }
+
+    for (i = 0; i < s->num_tiles; i++) {
+        ni_bitstream_init(&s->streams[i]);
+    }
+
+    return 0;
+}
+
+static int hevc_frame_parse_tiles(HEVCFSplitContext *s, AVBSFContext *ctx,
+                                  H265RawSPS *sps, H265RawPPS *pps) {
+    struct tile_format *format = *s->this_tile;
+    int log2_ctb_size;
+
+    if (!pps->tiles_enabled_flag) {
+        av_log(ctx, AV_LOG_ERROR, "tile_enabled_flag unset\n");
+        return AVERROR(ENODEV);
+    }
+
+    if (sps->pic_width_in_luma_samples != format->width ||
+        sps->pic_height_in_luma_samples != format->height) {
+        av_log(ctx, AV_LOG_ERROR, "pixel size not match\n");
+        return AVERROR(ENODEV);
+    }
+
+    if (pps->num_tile_columns_minus1 + 1 != format->num_tile_columns ||
+        pps->num_tile_rows_minus1 + 1 != format->num_tile_rows) {
+        av_log(ctx, AV_LOG_ERROR, "tiles partition not match\n");
+        return AVERROR(ENODEV);
+    }
+
+    log2_ctb_size = (sps->log2_min_luma_coding_block_size_minus3 + 3) +
+                    sps->log2_diff_max_min_luma_coding_block_size;
+    if (log2_ctb_size != format->log2_ctb_size) {
+        av_log(ctx, AV_LOG_ERROR, "ctb size not match\n");
+        return AVERROR(ENODEV);
+    }
+
+    return 0;
+}
+
+static int hevc_frame_split_filter(AVBSFContext *ctx, AVPacket *out) {
+    HEVCFSplitContext *s       = ctx->priv_data;
+    CodedBitstreamFragment *td = &s->temporal_unit;
+    int i, ret, tile_idx = 0, new_size, *slice_addr;
+    H265RawSlice *slice;
+    ni_bitstream_t *stream = &s->streams[0];
+
+    if (!s->tile_enabled) {
+        av_assert0(s->tile_enabled);
+        goto passthrough;
+    }
+
+    if (s->buffer_pkt->data) {
+        av_assert0(s->nb_slices > 0);
+        goto slice_split;
+    }
+
+    ret = ff_bsf_get_packet_ref(ctx, s->buffer_pkt);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_DEBUG, "failed to get packet ref: 0x%x\n", ret);
+        return ret;
+    }
+
+    ret = ff_cbs_read_packet(s->cbc, td, s->buffer_pkt);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_WARNING, "Failed to parse temporal unit.\n");
+        goto passthrough;
+    }
+
+    s->nb_slices   = 0;
+    s->unit_offset = 0;
+    for (i = 0; i < td->nb_units; i++) {
+        CodedBitstreamUnit *unit = &td->units[i];
+        av_log(ctx, AV_LOG_DEBUG, "query index %d, unit type %d\n", i,
+               unit->type);
+
+        if (unit->type == HEVC_NAL_VPS) {
+            ret = hevc_frame_tiles_encode_vps(s, ctx, unit);
+            if (ret < 0) {
+                av_log(ctx, AV_LOG_ERROR, "failed to re-encode vps\n");
+                return ret;
+            }
+        } else if (unit->type == HEVC_NAL_SPS) {
+            if (i < td->nb_units - 1 && td->units[i + 1].type == HEVC_NAL_PPS) {
+                H265RawPPS *pps = td->units[i + 1].content;
+                H265RawSPS *sps = unit->content;
+                if (pps->pps_seq_parameter_set_id ==
+                    sps->sps_seq_parameter_set_id) {
+                    ret = hevc_frame_parse_tiles(s, ctx, sps, pps);
+                    if (ret < 0) {
+                        av_log(ctx, AV_LOG_ERROR, "failed to parse tiles\n");
+                        return ret;
+                    }
+
+                    ret = hevc_frame_tiles_encode_sps(
+                        s, ctx, unit, pps->pps_pic_parameter_set_id);
+                    if (ret < 0) {
+                        av_log(ctx, AV_LOG_ERROR, "failed to re-encode sps\n");
+                        return ret;
+                    }
+                } else {
+                    av_log(ctx, AV_LOG_ERROR,
+                           "seq_parameter_set_id mismatch: %d, %d\n",
+                           pps->pps_seq_parameter_set_id,
+                           sps->sps_seq_parameter_set_id);
+                    return AVERROR(EINVAL);
+                }
+            } else {
+                av_log(ctx, AV_LOG_ERROR, "failed to find PPS after SPS\n");
+                return AVERROR(EINVAL);
+            }
+        } else if (unit->type == HEVC_NAL_PPS) {
+            ret = hevc_frame_tiles_encode_pps(s, ctx, unit);
+            if (ret < 0) {
+                av_log(ctx, AV_LOG_ERROR, "failed to re-encode pps\n");
+                return ret;
+            }
+        } else if (unit->type <= HEVC_NAL_RSV_VCL31) {
+            if (s->nb_slices == 0) {
+                s->unit_offset = i;
+            }
+            s->nb_slices++;
+        }
+    }
+
+    if (s->nb_slices == 0) {
+        ret = AVERROR(EAGAIN);
+        goto end;
+    }
+
+slice_split:
+    for (i = s->unit_offset; i < td->nb_units; i++) {
+        CodedBitstreamUnit *unit = &td->units[i];
+        if (unit->type <= HEVC_NAL_RSV_VCL31) {
+            slice = unit->content;
+            tile_idx =
+                slice_addr_to_idx(s, slice->header.slice_segment_address,
+                                  slice->header.slice_pic_parameter_set_id);
+            av_assert0(tile_idx >= 0 && tile_idx < s->num_tiles);
+            av_log(ctx, AV_LOG_DEBUG, "slice_seg_addr %d, tile_idx %d\n",
+                   slice->header.slice_segment_address, tile_idx);
+
+            ret = hevc_frame_encode_slice_header(
+                s, ctx, unit, tile_idx,
+                slice->header.slice_pic_parameter_set_id);
+            if (ret) {
+                av_log(ctx, AV_LOG_ERROR, "failed to re-encode slice header\n");
+                av_assert0(0);
+            }
+
+            stream = &s->streams[tile_idx];
+            break;
+        }
+    }
+
+    new_size = (int)(ni_bitstream_count(stream) / 8);
+    ret      = av_new_packet(out, new_size);
+    if (ret < 0) {
+        return ret;
+    }
+
+    av_packet_copy_props(out, s->buffer_pkt);
+
+    slice_addr = (int *)av_packet_new_side_data(out, AV_PKT_DATA_SLICE_ADDR,
+                                                sizeof(*slice_addr));
+    av_assert0(slice_addr);
+    *slice_addr = tile_idx;
+
+    ni_bitstream_fetch(stream, out->data, new_size);
+    ni_bitstream_reset(stream);
+
+    s->unit_offset++;
+    if (s->unit_offset < td->nb_units) {
+        // To be continued...
+        return ret;
+    }
+
+end:
+    av_packet_unref(s->buffer_pkt);
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_reset(td);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_reset(s->cbc, td);
+#else
+    ff_cbs_fragment_uninit(s->cbc, td);
+#endif
+    return ret;
+
+passthrough:
+    av_packet_move_ref(out, s->buffer_pkt);
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_reset(td);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_reset(s->cbc, td);
+#else
+    ff_cbs_fragment_uninit(s->cbc, td);
+#endif
+
+    return ret;
+}
+
+static int hevc_frame_split_init(AVBSFContext *ctx) {
+    HEVCFSplitContext *s       = ctx->priv_data;
+    CodedBitstreamFragment *td = &s->temporal_unit;
+    int ret;
+
+    s->buffer_pkt = av_packet_alloc();
+    if (!s->buffer_pkt)
+        return AVERROR(ENOMEM);
+
+    ret = ff_cbs_init(&s->cbc, AV_CODEC_ID_HEVC, ctx);
+    if (ret < 0)
+        return ret;
+
+    s->cbc->decompose_unit_types    = NULL;
+    s->cbc->nb_decompose_unit_types = 0;
+    //    s->cbc->decompose_unit_types    =
+    //    (CodedBitstreamUnitType*)decompose_unit_types;
+    //    s->cbc->nb_decompose_unit_types =
+    //    FF_ARRAY_ELEMS(decompose_unit_types);
+
+    /* extradata is a must */
+    //    if (!ctx->par_in->extradata_size)
+    //        return AVERROR(ENODEV);
+    if (!ctx->par_in->extradata_size)
+        return 0;
+
+    ret = ff_cbs_read_extradata(s->cbc, td, ctx->par_in);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "Failed to parse extradata.\n");
+        goto fail_out;
+    }
+
+    ret = hevc_frame_init_tiles(s, ctx);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "Failed to initialize tiles\n");
+        goto fail_out;
+    }
+
+    ret = hevc_frame_init_bitstream(s);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to initialize bitstream\n");
+        goto fail_out;
+    }
+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_reset(td);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_reset(s->cbc, td);
+#else
+    ff_cbs_fragment_uninit(s->cbc, td);
+#endif
+    return 0;
+
+fail_out:
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_free(&s->temporal_unit);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_free(s->cbc, &s->temporal_unit);
+#else
+    ff_cbs_fragment_uninit(s->cbc, &s->temporal_unit);
+#endif
+    ff_cbs_close(&s->cbc);
+    return ret;
+}
+
+static void hevc_frame_split_flush(AVBSFContext *ctx) {
+    int i;
+    HEVCFSplitContext *s = ctx->priv_data;
+
+    for (i = 0; i < s->num_tiles; i++) {
+        ni_bitstream_reset(&s->streams[i]);
+    }
+
+    av_packet_unref(s->buffer_pkt);
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_reset(&s->temporal_unit);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_reset(s->cbc, &s->temporal_unit);
+#else
+    ff_cbs_fragment_uninit(s->cbc, &s->temporal_unit);
+#endif
+}
+
+static void hevc_frame_split_close(AVBSFContext *ctx) {
+    HEVCFSplitContext *s = ctx->priv_data;
+    int i;
+
+    for (i = 0; i < s->num_tiles; i++) {
+        ni_bitstream_deinit(&s->streams[i]);
+    }
+
+    for (i = 0; i < HEVC_MAX_PPS_COUNT; i++) {
+        if (s->tiles[i]) {
+            if (s->tiles[i]->column_width) {
+                av_freep(&s->tiles[i]->column_width);
+            }
+            av_freep(&s->tiles[i]);
+        }
+    }
+
+    av_freep(&s->streams);
+    av_packet_free(&s->buffer_pkt);
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_free(&s->temporal_unit);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_free(s->cbc, &s->temporal_unit);
+#else
+    ff_cbs_fragment_uninit(s->cbc, &s->temporal_unit);
+#endif
+    ff_cbs_close(&s->cbc);
+}
+
+static const enum AVCodecID hevc_frame_split_codec_ids[] = {
+    AV_CODEC_ID_HEVC,
+    AV_CODEC_ID_NONE,
+};
+
+const AVBitStreamFilter ff_hevc_frame_split_bsf = {
+    .name           = "hevc_frame_split",
+    .priv_data_size = sizeof(HEVCFSplitContext),
+    .init           = hevc_frame_split_init,
+    .flush          = hevc_frame_split_flush,
+    .close          = hevc_frame_split_close,
+    .filter         = hevc_frame_split_filter,
+    .codec_ids      = hevc_frame_split_codec_ids,
+};
diff --git a/libavcodec/ni_hevc_rawtotile_bsf.c b/libavcodec/ni_hevc_rawtotile_bsf.c
new file mode 100644
index 0000000000..da31c33a6f
--- /dev/null
+++ b/libavcodec/ni_hevc_rawtotile_bsf.c
@@ -0,0 +1,309 @@
+/*
+ * NetInt HEVC raw-to-tile BSF common source code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ *
+ * This bitstream filter re-encode HEVC NALUs (i.e. VPS, SPS, PPS and slice
+ * header) with tile flags. The slice segment address will be assigned in the
+ * slices or tiles respectively so as to repack them later.
+ */
+
+#include <libavutil/opt.h>
+#include <libavutil/internal.h>
+#include <math.h>
+
+#include "avcodec.h"
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 91)
+#include "bsf_internal.h"
+#else
+#include "bsf.h"
+#endif
+#include "hevc.h"
+#include "cbs.h"
+#include "cbs_h265.h"
+#include "ni_hevc_rbsp.h"
+
+
+typedef struct HEVCFtoTileContext {
+    AVPacket *buffer_pkt;
+    CodedBitstreamContext *cbc;
+    CodedBitstreamFragment temporal_unit;
+
+    int width;
+    int height;
+    int column; //total tile number in column
+    int row;    //total tile number in row
+    int x;
+    int y;
+
+    ni_bitstream_t stream;
+} HEVCFtoTileContext;
+
+
+static int hevc_rawtotile_encode_vps(HEVCFtoTileContext *s, AVBSFContext *ctx)
+{
+    CodedBitstreamH265Context *priv = s->cbc->priv_data;
+    ni_write_nal_header(&s->stream, HEVC_NAL_VPS, 0, 1);
+    return ni_hevc_encode_nal_vps(&s->stream, priv->active_vps);
+}
+
+static int hevc_rawtotile_encode_sps(HEVCFtoTileContext *s, AVBSFContext *ctx)
+{
+    CodedBitstreamH265Context *priv = s->cbc->priv_data;
+    ni_write_nal_header(&s->stream, HEVC_NAL_SPS, 0, 1);
+    return ni_hevc_encode_nal_sps(&s->stream, priv->active_sps, s->width, s->height);
+}
+
+static int hevc_rawtotile_encode_pps(HEVCFtoTileContext *s, AVBSFContext *ctx)
+{
+    CodedBitstreamH265Context *priv = s->cbc->priv_data;
+    ni_write_nal_header(&s->stream, HEVC_NAL_PPS, 0, 1);
+    return ni_hevc_encode_nal_pps(&s->stream, priv->active_pps, 2, s->column, s->row);
+}
+
+//TODO(tyroun): one extra memcpy for slice data, try to eliminate it
+static int hevc_rawtotile_slice(HEVCFtoTileContext *s, AVBSFContext *ctx, CodedBitstreamUnit *unit)
+{
+    int i;
+    H265RawSlice *slice = unit->content;
+    CodedBitstreamH265Context *priv = s->cbc->priv_data;
+
+    ni_write_nal_header(&s->stream, slice->header.nal_unit_header.nal_unit_type, 0, 1);
+
+    ni_hevc_encode_nal_slice_header(&s->stream, &slice->header, priv->active_sps, priv->active_pps,
+                                       s->width /* width */, s->height /* height  */, 2 /* enable tile */,
+                                       s->x, s->y, 1 /* independent */);
+
+    for (i = 0; i < slice->data_size; i++) {
+        ni_put_bits(&s->stream, 8, slice->data[i]);
+    }
+
+    return 0;
+}
+
+static int hevc_rawtotile_filter(AVBSFContext *ctx, AVPacket *out)
+{
+    HEVCFtoTileContext *s = ctx->priv_data;
+    CodedBitstreamFragment *td = &s->temporal_unit;
+    int i, ret, new_size, nb_slices, unit_offset;
+
+    ret = ff_bsf_get_packet_ref(ctx, s->buffer_pkt);
+    if (ret < 0) {
+        // EOF
+        return ret;
+    }
+
+    ret = ff_cbs_read_packet(s->cbc, td, s->buffer_pkt);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_WARNING, "Failed to parse temporal unit.\n");
+        goto passthrough;
+    }
+
+    nb_slices = 0;
+    unit_offset = 0;
+    for (i = 0; i < td->nb_units; i++) {
+        CodedBitstreamUnit *unit = &td->units[i];
+
+        if (unit->type == HEVC_NAL_VPS) {
+            ret = hevc_rawtotile_encode_vps(s, ctx);
+            if (ret < 0) {
+                av_log(ctx, AV_LOG_ERROR, "failed to re-encode vps\n");
+                av_assert0(0);
+            }
+        } else if (unit->type == HEVC_NAL_SPS) {
+            ret = hevc_rawtotile_encode_sps(s, ctx);
+            if (ret < 0) {
+                av_log(ctx, AV_LOG_ERROR, "failed to re-encode sps\n");
+                av_assert0(0);
+            }
+        } else if (unit->type == HEVC_NAL_PPS) {
+            ret = hevc_rawtotile_encode_pps(s, ctx);
+            if (ret < 0) {
+                av_log(ctx, AV_LOG_ERROR, "failed to re-encode pps\n");
+                av_assert0(0);
+            }
+        } else if (unit->type <= HEVC_NAL_RSV_VCL31) {
+            if (nb_slices == 0) {
+                unit_offset = i;
+            }
+            nb_slices++;
+        }
+    }
+
+    if (nb_slices == 0) {
+        ret = AVERROR(EAGAIN);
+        goto end;
+    }
+
+    for (i = 0; i < nb_slices; i++) {
+        CodedBitstreamUnit *unit = &td->units[i + unit_offset];
+        if (unit->type <= HEVC_NAL_RSV_VCL31) {
+            ret = hevc_rawtotile_slice(s, ctx, unit);
+            if (ret < 0) {
+                av_log(ctx, AV_LOG_ERROR, "failed to re-encode slice header\n");
+                av_assert0(0);
+            }
+            break;
+        }
+    }
+
+    new_size = ni_bitstream_count(&s->stream) / 8;
+    ret = av_new_packet(out, new_size);
+    if (ret < 0) {
+        return ret;
+    }
+
+    av_packet_copy_props(out, s->buffer_pkt);
+
+    ni_bitstream_fetch(&s->stream, out->data, new_size);
+    ni_bitstream_reset(&s->stream);
+
+end:
+    av_packet_unref(s->buffer_pkt);
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_reset(td);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58) && (LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_reset(s->cbc, td);
+#else
+    ff_cbs_fragment_uninit(s->cbc, td);
+#endif
+    return ret;
+
+passthrough:
+    av_packet_move_ref(out, s->buffer_pkt);
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_reset(td);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_reset(s->cbc, td);
+#else
+    ff_cbs_fragment_uninit(s->cbc, td);
+#endif
+    return 0;
+}
+
+static int hevc_rawtotile_init(AVBSFContext *ctx)
+{
+    HEVCFtoTileContext *s = ctx->priv_data;
+    CodedBitstreamFragment *td = &s->temporal_unit;
+    int ret;
+
+    s->buffer_pkt = av_packet_alloc();
+    if (!s->buffer_pkt)
+        return AVERROR(ENOMEM);
+
+    ret = ff_cbs_init(&s->cbc, AV_CODEC_ID_HEVC, ctx);
+    if (ret < 0)
+        return ret;
+
+    s->cbc->decompose_unit_types    = NULL;
+    s->cbc->nb_decompose_unit_types = 0;
+
+//    s->cbc->decompose_unit_types    = (CodedBitstreamUnitType*)decompose_unit_types;
+//    s->cbc->nb_decompose_unit_types = FF_ARRAY_ELEMS(decompose_unit_types);
+
+    /* extradata is a must */
+//    if (!ctx->par_in->extradata_size)
+//        return AVERROR(ENODEV);
+
+//    ret = ff_cbs_read_extradata(s->cbc, td, ctx->par_in);
+//    if (ret < 0) {
+//        av_log(ctx, AV_LOG_ERROR, "Failed to parse extradata.\n");
+//        return ret;
+//    }
+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_reset(td);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_reset(s->cbc, td);
+#else
+    ff_cbs_fragment_uninit(s->cbc, td);
+#endif
+
+    ni_bitstream_init(&s->stream);
+
+    return 0;
+}
+
+static void hevc_rawtotile_flush(AVBSFContext *ctx)
+{
+    HEVCFtoTileContext *s = ctx->priv_data;
+
+    ni_bitstream_reset(&s->stream);
+    av_packet_unref(s->buffer_pkt);
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_reset(&s->temporal_unit);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_reset(s->cbc, &s->temporal_unit);
+#else
+    ff_cbs_fragment_uninit(s->cbc, &s->temporal_unit);
+#endif
+}
+
+static void hevc_rawtotile_close(AVBSFContext *ctx)
+{
+    HEVCFtoTileContext *s = ctx->priv_data;
+
+    ni_bitstream_deinit(&s->stream);
+    av_packet_free(&s->buffer_pkt);
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    ff_cbs_fragment_free(&s->temporal_unit);
+#elif (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 54)
+    ff_cbs_fragment_free(s->cbc, &s->temporal_unit);
+#else
+    ff_cbs_fragment_uninit(s->cbc, &s->temporal_unit);
+#endif
+    ff_cbs_close(&s->cbc);
+}
+
+static const enum AVCodecID hevc_rawtotile_codec_ids[] = {
+        AV_CODEC_ID_HEVC, AV_CODEC_ID_NONE,
+};
+
+#define OFFSET(x) offsetof(HEVCFtoTileContext, x)
+
+static const AVOption options[] = {
+        {"width", NULL, OFFSET(width), AV_OPT_TYPE_INT, {.i64 = 1280}, 0, 8192, 0, 0},
+        {"height", NULL, OFFSET(height), AV_OPT_TYPE_INT, {.i64 = 720}, 0, 8192, 0, 0},
+        {"column", NULL, OFFSET(column), AV_OPT_TYPE_INT, {.i64 = 0}, 0, 128, 0, 0},  //support 128 columns max
+        {"row", NULL, OFFSET(row), AV_OPT_TYPE_INT, {.i64 = 0}, 0, 128, 0, 0},  //support 128 rows max
+        {"x", NULL, OFFSET(x), AV_OPT_TYPE_INT, {.i64 = 0}, 0, 8192, 0, 0},  //support 8192 columns max
+        {"y", NULL, OFFSET(y), AV_OPT_TYPE_INT, {.i64 = 0}, 0, 8192, 0, 0},  //support 8192 rows max
+        { NULL },
+};
+
+static const AVClass hevc_rawtotile_class = {
+        .class_name = "hevc_rawtotile",
+        .item_name  = av_default_item_name,
+        .option     = options,
+        .version    = LIBAVUTIL_VERSION_INT,
+};
+
+const AVBitStreamFilter ff_hevc_rawtotile_bsf = {
+        .name           = "hevc_rawtotile",
+        .priv_data_size = sizeof(HEVCFtoTileContext),
+        .priv_class     = &hevc_rawtotile_class,
+        .init           = hevc_rawtotile_init,
+        .flush          = hevc_rawtotile_flush,
+        .close          = hevc_rawtotile_close,
+        .filter         = hevc_rawtotile_filter,
+        .codec_ids      = hevc_rawtotile_codec_ids,
+};
diff --git a/libavcodec/ni_hevc_rbsp.c b/libavcodec/ni_hevc_rbsp.c
new file mode 100644
index 0000000000..7084ab3c06
--- /dev/null
+++ b/libavcodec/ni_hevc_rbsp.c
@@ -0,0 +1,948 @@
+/*
+ * NetInt HEVC RBSP parser common source code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ *
+ * An RBSP parser according to the HEVC syntax template. Using FFmpeg put_bits
+ * module for the bit write operations.
+ */
+
+#include <stdbool.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+// commented out since it's not used
+//#include <ni_rsrc_api.h>
+
+#include <libavcodec/avcodec.h>
+#include <libavcodec/cbs_h265.h>
+#include <libavcodec/h2645_parse.h>
+#include <libavcodec/hevc.h>
+#include <libavcodec/hevc_ps.h>
+#include <libavcodec/hevc_sei.h>
+#include <libavcodec/hevcdec.h>
+#include <libavutil/internal.h>
+
+#include "ni_hevc_rbsp.h"
+
+static const uint32_t ni_bit_set_mask[] = {
+    0x00000001, 0x00000002, 0x00000004, 0x00000008, 0x00000010, 0x00000020,
+    0x00000040, 0x00000080, 0x00000100, 0x00000200, 0x00000400, 0x00000800,
+    0x00001000, 0x00002000, 0x00004000, 0x00008000, 0x00010000, 0x00020000,
+    0x00040000, 0x00080000, 0x00100000, 0x00200000, 0x00400000, 0x00800000,
+    0x01000000, 0x02000000, 0x04000000, 0x08000000, 0x10000000, 0x20000000,
+    0x40000000, 0x80000000};
+
+int ni_bitstream_init(ni_bitstream_t *stream) {
+    memset(stream, 0, sizeof(ni_bitstream_t));
+    stream->pb_buf = av_mallocz(MAX_PUT_BUF_SIZE);
+    if (!stream->pb_buf) {
+        return AVERROR(ENOMEM);
+    }
+    init_put_bits(&stream->pbc, stream->pb_buf, MAX_PUT_BUF_SIZE);
+    return 0;
+}
+
+void ni_bitstream_deinit(ni_bitstream_t *stream) { av_freep(&stream->pb_buf); }
+
+void ni_bitstream_reset(ni_bitstream_t *stream) {
+    init_put_bits(&stream->pbc, stream->pb_buf, MAX_PUT_BUF_SIZE);
+}
+
+void ni_bitstream_fetch(const ni_bitstream_t *stream, uint8_t *buf,
+                        size_t size) {
+    memcpy(buf, stream->pb_buf, size);
+}
+
+int ni_bitstream_count(ni_bitstream_t *stream) {
+    return put_bits_count(&stream->pbc);
+}
+
+static uint8_t ni_bitstream_dump_last_byte(ni_bitstream_t *stream) {
+    return stream->pb_buf[ni_bitstream_count(stream) / 8 - 1];
+}
+
+/**
+ * \brief Write a byte to a byte aligned bitstream
+ * \param stream  stream the data is to be appended to
+ * \param data  input data
+ */
+static void ni_put_byte(ni_bitstream_t *stream, uint8_t data) {
+    const uint8_t emulation_prevention_three_byte = 0x03;
+    av_assert0(stream->cur_bits == 0);
+
+    if (stream->zero_cnt == 2 && data < 4) {
+        put_bits(&stream->pbc, 8, emulation_prevention_three_byte);
+        stream->zero_cnt = 0;
+    }
+    stream->zero_cnt = data == 0 ? stream->zero_cnt + 1 : 0;
+    put_bits(&stream->pbc, 8, data);
+    flush_put_bits(&stream->pbc);
+}
+
+/**
+ * \brief Write bits to bitstream
+ *        Buffers individual bits untill they make a full byte.
+ * \param stream  stream the data is to be appended to
+ * \param data  input data
+ * \param bits  number of bits to write from data to stream
+ */
+void ni_put_bits(ni_bitstream_t *stream, uint8_t bits, const uint32_t data) {
+    while (bits--) {
+        stream->cache <<= 1;
+
+        if (data & ni_bit_set_mask[bits]) {
+            stream->cache |= 1;
+        }
+        stream->cur_bits++;
+
+        // write byte to output
+        if (stream->cur_bits == 8) {
+            stream->cur_bits = 0;
+            ni_put_byte(stream, stream->cache);
+        }
+    }
+}
+
+static unsigned ni_math_floor_log2(unsigned value) {
+    unsigned result = 0;
+    av_assert0(value > 0);
+
+    for (int i = 4; i >= 0; --i) {
+        unsigned bits  = 1ull << i;
+        unsigned shift = value >= (1 << bits) ? bits : 0;
+        result += shift;
+        value >>= shift;
+    }
+
+    return result;
+}
+
+/**
+ * \brief Write unsigned Exp-Golomb bit string
+ */
+static void ni_set_ue_golomb(ni_bitstream_t *stream, uint32_t code_num) {
+    unsigned code_num_log2 = ni_math_floor_log2(code_num + 1);
+    unsigned prefix        = 1 << code_num_log2;
+    unsigned suffix        = code_num + 1 - prefix;
+    unsigned num_bits      = code_num_log2 * 2 + 1;
+    unsigned value         = prefix | suffix;
+
+    ni_put_bits(stream, num_bits, value);
+}
+
+/**
+ * \brief Write signed Exp-Golomb bit string
+ */
+static void ni_set_se_golomb(ni_bitstream_t *stream, int32_t data) {
+    // Map positive values to even and negative to odd values.
+    uint32_t code_num = data <= 0 ? (-data) << 1 : (data << 1) - 1;
+    ni_set_ue_golomb(stream, code_num);
+}
+
+/**
+ * \brief Add rbsp_trailing_bits syntax element, which aligns the bitstream.
+ */
+static void ni_rbsp_trailing_bits(ni_bitstream_t *stream) {
+    ni_put_bits(stream, 1, 1);
+    if ((stream->cur_bits & 7) != 0) {
+        ni_put_bits(stream, 8 - (stream->cur_bits & 7), 0);
+    }
+}
+
+/**
+ * \brief Add rbsp_trailing_bits syntax element, which aligns the bitstream.
+ */
+void ni_write_nal_header(ni_bitstream_t *bitstream, const uint8_t nal_type,
+                         const uint8_t temporal_id, const int long_start_code) {
+    uint8_t byte;
+
+    // Some useful constants
+    const uint8_t start_code_prefix_one_3bytes = 0x01;
+    const uint8_t zero                         = 0x00;
+
+    // zero_byte (0x00) shall be present in the byte stream NALU of VPS, SPS
+    // and PPS, or the first NALU of an access unit
+    if (long_start_code) {
+        put_bits(&bitstream->pbc, 8, zero);
+    }
+
+    // start_code_prefix_one_3bytes
+    put_bits(&bitstream->pbc, 8, zero);
+    put_bits(&bitstream->pbc, 8, zero);
+    ni_put_bits(bitstream, 8, start_code_prefix_one_3bytes);
+
+    // Handle header bits with full bytes instead of using bitstream
+    // forbidden_zero_flag(1) + nal_unit_type(6) + 1bit of nuh_layer_id
+    byte = nal_type << 1;
+    ni_put_bits(bitstream, 8, byte);
+
+    // 5bits of nuh_layer_id + nuh_temporal_id_plus1(3)
+    byte = (temporal_id + 1) & 7;
+    ni_put_bits(bitstream, 8, byte);
+}
+
+static void write_raw_ptl(ni_bitstream_t *pb,
+                          const H265RawProfileTierLevel *ptl,
+                          int max_sub_layers_minus1) {
+    int i;
+
+    ni_put_bits(pb, 2, ptl->general_profile_space);
+    ni_put_bits(pb, 1, ptl->general_tier_flag);
+    ni_put_bits(pb, 5, ptl->general_profile_idc);
+
+    // ni_put_bits(pb, 32, 3 << 29); only general_profile_compatibility_flag [1]
+    // and [2]
+    for (i = 0; i < 32; i++) {
+        ni_put_bits(pb, 1, ptl->general_profile_compatibility_flag[i]);
+    }
+
+    ni_put_bits(
+        pb, 1,
+        ptl->general_progressive_source_flag); // general_progressive_source_flag
+    ni_put_bits(
+        pb, 1,
+        ptl->general_interlaced_source_flag); // general_interlaced_source_flag
+    ni_put_bits(
+        pb, 1,
+        ptl->general_non_packed_constraint_flag); // general_non_packed_constraint_flag
+    ni_put_bits(
+        pb, 1,
+        ptl->general_frame_only_constraint_flag); // general_frame_only_constraint_flag
+
+    av_assert0(!ptl->general_one_picture_only_constraint_flag);
+    av_assert0(!ptl->general_inbld_flag);
+    ni_put_bits(pb, 7, 0);
+    ni_put_bits(pb, 1, ptl->general_one_picture_only_constraint_flag);
+    ni_put_bits(pb, 24, 0);
+    ni_put_bits(pb, 11, 0);
+    ni_put_bits(pb, 1, ptl->general_inbld_flag);
+
+    // end Profile Tier
+
+    ni_put_bits(pb, 8, ptl->general_level_idc); // general_level_idc
+
+    if (max_sub_layers_minus1 > 1) {
+        // TODO(tyroun) sub layers support
+        printf("not support sub layers yet\n");
+        av_assert0(0);
+        return;
+    }
+
+    for (i = 0; i < max_sub_layers_minus1; i++) {
+        ni_put_bits(pb, 1, ptl->sub_layer_profile_present_flag[i]);
+        ni_put_bits(pb, 1, ptl->sub_layer_level_present_flag[i]);
+    }
+
+    if (max_sub_layers_minus1 > 0) {
+        for (i = max_sub_layers_minus1; i < 8; i++) {
+            ni_put_bits(pb, 2, 0); // reserved_zero_2bits
+        }
+    }
+}
+
+int ni_hevc_encode_nal_vps(ni_bitstream_t *pb, const H265RawVPS *vps) {
+    int i;
+
+    ni_put_bits(pb, 4, vps->vps_video_parameter_set_id);
+    ni_put_bits(pb, 2, 3);
+    ni_put_bits(pb, 6, vps->vps_max_layers_minus1);
+    ni_put_bits(pb, 3, vps->vps_max_sub_layers_minus1);
+    ni_put_bits(pb, 1, vps->vps_temporal_id_nesting_flag);
+    ni_put_bits(pb, 16, 0xffff);
+
+    write_raw_ptl(pb, &vps->profile_tier_level, vps->vps_max_sub_layers_minus1);
+
+    ni_put_bits(pb, 1, vps->vps_sub_layer_ordering_info_present_flag);
+    for (i = vps->vps_sub_layer_ordering_info_present_flag
+                 ? 0
+                 : vps->vps_max_sub_layers_minus1;
+         i <= vps->vps_max_sub_layers_minus1; i++) {
+        ni_set_ue_golomb(pb, vps->vps_max_dec_pic_buffering_minus1[i]);
+        ni_set_ue_golomb(pb, vps->vps_max_num_reorder_pics[i]);
+        ni_set_ue_golomb(pb, vps->vps_max_latency_increase_plus1[i]);
+    }
+
+    ni_put_bits(pb, 6, vps->vps_max_layer_id);
+    ni_set_ue_golomb(pb, vps->vps_num_layer_sets_minus1);
+
+    if (vps->vps_num_layer_sets_minus1 > 0) {
+        avpriv_report_missing_feature(NULL, "Writing layer_id_included_flag");
+        return AVERROR_PATCHWELCOME;
+    }
+
+    ni_put_bits(pb, 1, vps->vps_timing_info_present_flag);
+    if (vps->vps_timing_info_present_flag) {
+        ni_put_bits(pb, 32, vps->vps_num_units_in_tick);
+        ni_put_bits(pb, 32, vps->vps_time_scale);
+        ni_put_bits(pb, 1, vps->vps_poc_proportional_to_timing_flag);
+        if (vps->vps_poc_proportional_to_timing_flag)
+            ni_set_ue_golomb(pb, vps->vps_num_ticks_poc_diff_one_minus1);
+
+        ni_set_ue_golomb(pb, vps->vps_num_hrd_parameters);
+        if (vps->vps_num_hrd_parameters) {
+            avpriv_report_missing_feature(NULL, "Writing HRD parameters");
+            return AVERROR_PATCHWELCOME;
+        }
+    }
+
+    ni_put_bits(pb, 1, 0); // extension flag
+
+    ni_rbsp_trailing_bits(pb);
+
+    return 0;
+}
+
+static void write_raw_scaling_list(ni_bitstream_t *pb,
+                                   const H265RawScalingList *sl) {
+    unsigned int size_id, matrix_id;
+    int i, coef_num;
+
+    for (size_id = 0; size_id < 4; size_id++) {
+        for (matrix_id = 0; matrix_id < 6;
+             matrix_id += ((size_id == 3) ? 3 : 1)) {
+            ni_put_bits(pb, 1,
+                        sl->scaling_list_pred_mode_flag[size_id][matrix_id]);
+
+            if (!sl->scaling_list_pred_mode_flag[size_id][matrix_id]) {
+                ni_set_ue_golomb(
+                    pb,
+                    sl->scaling_list_pred_matrix_id_delta[size_id][matrix_id]);
+            } else {
+                coef_num = FFMIN(64, 1 << (4 + (size_id << 1)));
+                if (size_id > 1) {
+                    ni_set_se_golomb(
+                        pb,
+                        sl->scaling_list_dc_coef_minus8[size_id][matrix_id]);
+                }
+                for (i = 0; i < coef_num; i++) {
+                    ni_set_se_golomb(
+                        pb,
+                        sl->scaling_list_delta_coeff[size_id][matrix_id][i]);
+                }
+            }
+        }
+    }
+}
+
+static void write_raw_VUI(ni_bitstream_t *pb, const H265RawSPS *sps) {
+    const H265RawVUI *vui = &sps->vui;
+
+    if (vui->aspect_ratio_info_present_flag) {
+        ni_put_bits(pb, 1, vui->aspect_ratio_info_present_flag);
+        ni_put_bits(pb, 8, vui->aspect_ratio_idc);
+        if (vui->aspect_ratio_idc == 255) {
+            ni_put_bits(pb, 16, vui->sar_width);
+            ni_put_bits(pb, 16, vui->sar_height);
+        }
+    } else
+        ni_put_bits(pb, 1, vui->aspect_ratio_info_present_flag);
+
+    // IF aspect ratio info
+    // ENDIF
+
+    if (vui->overscan_info_present_flag) {
+        ni_put_bits(pb, 1, vui->overscan_info_present_flag);
+        ni_put_bits(pb, 1, vui->overscan_appropriate_flag);
+    } else
+        ni_put_bits(pb, 1, vui->overscan_info_present_flag);
+
+    // IF overscan info
+    // ENDIF
+
+    if (vui->video_signal_type_present_flag) {
+        ni_put_bits(pb, 1, vui->video_signal_type_present_flag);
+        ni_put_bits(pb, 3, vui->video_format);
+        ni_put_bits(pb, 1, vui->video_full_range_flag);
+
+        if (vui->colour_description_present_flag) {
+            ni_put_bits(pb, 1, vui->colour_description_present_flag);
+            ni_put_bits(pb, 8, vui->colour_primaries);
+            ni_put_bits(pb, 8, vui->transfer_characteristics);
+            ni_put_bits(pb, 8, vui->matrix_coefficients);
+        } else
+            ni_put_bits(pb, 1, vui->colour_description_present_flag);
+    } else
+        ni_put_bits(pb, 1, vui->video_signal_type_present_flag);
+
+    // IF video type
+    // ENDIF
+
+    if (vui->chroma_loc_info_present_flag) {
+        ni_put_bits(pb, 1, vui->chroma_loc_info_present_flag);
+        ni_set_ue_golomb(pb, vui->chroma_sample_loc_type_top_field);
+        ni_set_ue_golomb(pb, vui->chroma_sample_loc_type_bottom_field);
+    } else
+        ni_put_bits(pb, 1, vui->chroma_loc_info_present_flag);
+
+    // IF chroma loc info
+    // ENDIF
+
+    ni_put_bits(pb, 1, vui->neutral_chroma_indication_flag);
+    ni_put_bits(pb, 1, vui->field_seq_flag); // 0: frames, 1: fields
+    ni_put_bits(pb, 1, vui->frame_field_info_present_flag);
+    ni_put_bits(pb, 1, vui->default_display_window_flag);
+
+    // IF default display window
+    // ENDIF
+
+    ni_put_bits(pb, 1, vui->vui_timing_info_present_flag);
+    if (vui->vui_timing_info_present_flag) {
+        ni_put_bits(pb, 32, vui->vui_num_units_in_tick);
+        ni_put_bits(pb, 32, vui->vui_time_scale);
+
+        ni_put_bits(pb, 1, vui->vui_poc_proportional_to_timing_flag);
+        ni_put_bits(pb, 1, vui->vui_hrd_parameters_present_flag);
+    }
+
+    ni_put_bits(pb, 1, vui->bitstream_restriction_flag);
+    if (vui->bitstream_restriction_flag) {
+        ni_put_bits(pb, 1, vui->tiles_fixed_structure_flag);
+        ni_put_bits(pb, 1, vui->motion_vectors_over_pic_boundaries_flag);
+        ni_put_bits(pb, 1, vui->restricted_ref_pic_lists_flag);
+        ni_set_ue_golomb(pb, vui->min_spatial_segmentation_idc);//  0, 4095);
+        ni_set_ue_golomb(pb, vui->max_bytes_per_pic_denom);//       0, 16);
+        ni_set_ue_golomb(pb, vui->max_bits_per_min_cu_denom);//     0, 16);
+        ni_set_ue_golomb(pb, vui->log2_max_mv_length_horizontal);// 0, 16);
+        ni_set_ue_golomb(pb, vui->log2_max_mv_length_vertical);//   0, 16);
+    }
+
+    // IF bitstream restriction
+    // ENDIF
+}
+
+static void short_term_ref_pic_set(ni_bitstream_t *pb,
+                                   const H265RawSTRefPicSet *p_st_rps,
+                                   int st_rps_idx, const H265RawSPS *sps) {
+    int i, ref_rps_idx, num_delta_pocs;
+    const H265RawSTRefPicSet *ref;
+
+    if (st_rps_idx > 0) {
+        ni_put_bits(pb, 1, p_st_rps->inter_ref_pic_set_prediction_flag);
+    }
+
+    if (p_st_rps->inter_ref_pic_set_prediction_flag) {
+        if (st_rps_idx == sps->num_short_term_ref_pic_sets) {
+            ni_set_ue_golomb(pb, p_st_rps->delta_idx_minus1);
+        }
+
+        ref_rps_idx    = st_rps_idx - (p_st_rps->delta_idx_minus1 + 1);
+        ref            = &sps->st_ref_pic_set[ref_rps_idx];
+        num_delta_pocs = ref->num_negative_pics + ref->num_positive_pics;
+
+        ni_put_bits(pb, 1, p_st_rps->delta_rps_sign);
+        ni_set_ue_golomb(pb, p_st_rps->abs_delta_rps_minus1);
+
+        for (i = 0; i <= num_delta_pocs; i++) {
+            ni_put_bits(pb, 1, p_st_rps->used_by_curr_pic_flag[i]);
+            if (!p_st_rps->used_by_curr_pic_flag[i]) {
+                ni_put_bits(pb, 1, p_st_rps->use_delta_flag[i]);
+            }
+        }
+    } else {
+        ni_set_ue_golomb(pb, p_st_rps->num_negative_pics);
+        ni_set_ue_golomb(pb, p_st_rps->num_positive_pics);
+
+        for (i = 0; i < p_st_rps->num_negative_pics; i++) {
+            ni_set_ue_golomb(pb, p_st_rps->delta_poc_s0_minus1[i]);
+            ni_put_bits(pb, 1, p_st_rps->used_by_curr_pic_s0_flag[i]);
+        }
+
+        for (i = 0; i < p_st_rps->num_positive_pics; i++) {
+            ni_set_ue_golomb(pb, p_st_rps->delta_poc_s1_minus1[i]);
+            ni_put_bits(pb, 1, p_st_rps->used_by_curr_pic_s1_flag[i]);
+        }
+    }
+}
+
+static void write_raw_sps_extension(ni_bitstream_t *pb, const H265RawSPS *sps) {
+    ni_put_bits(pb, 1, sps->sps_extension_present_flag);
+    if (sps->sps_extension_present_flag) {
+        ni_put_bits(pb, 1, sps->sps_range_extension_flag);
+        ni_put_bits(pb, 1, sps->sps_multilayer_extension_flag);
+        ni_put_bits(pb, 1, sps->sps_3d_extension_flag);
+        ni_put_bits(pb, 1, sps->sps_scc_extension_flag);
+        ni_put_bits(pb, 4, sps->sps_extension_4bits);
+    }
+
+    if (sps->sps_range_extension_flag) {
+        ni_put_bits(pb, 1, sps->transform_skip_rotation_enabled_flag);
+        ni_put_bits(pb, 1, sps->transform_skip_context_enabled_flag);
+        ni_put_bits(pb, 1, sps->implicit_rdpcm_enabled_flag);
+        ni_put_bits(pb, 1, sps->explicit_rdpcm_enabled_flag);
+        ni_put_bits(pb, 1, sps->extended_precision_processing_flag);
+        ni_put_bits(pb, 1, sps->intra_smoothing_disabled_flag);
+        ni_put_bits(pb, 1, sps->high_precision_offsets_enabled_flag);
+        ni_put_bits(pb, 1, sps->persistent_rice_adaptation_enabled_flag);
+        ni_put_bits(pb, 1, sps->cabac_bypass_alignment_enabled_flag);
+    }
+    av_assert0(!sps->sps_multilayer_extension_flag);
+    av_assert0(!sps->sps_3d_extension_flag);
+    av_assert0(!sps->sps_scc_extension_flag);
+    av_assert0(!sps->sps_extension_4bits);
+}
+
+int ni_hevc_encode_nal_sps(ni_bitstream_t *pb, const H265RawSPS *sps, int width,
+                           int height) {
+    int i, start;
+
+    if (!sps) {
+        return AVERROR(EINVAL);
+    }
+
+    // TODO: profile IDC and level IDC should be defined later on
+    ni_put_bits(pb, 4, sps->sps_video_parameter_set_id);
+    ni_put_bits(pb, 3, sps->sps_max_sub_layers_minus1);
+    ni_put_bits(pb, 1, sps->sps_temporal_id_nesting_flag);
+
+    write_raw_ptl(pb, &sps->profile_tier_level, sps->sps_max_sub_layers_minus1);
+
+    ni_set_ue_golomb(pb, sps->sps_seq_parameter_set_id);
+    ni_set_ue_golomb(pb, sps->chroma_format_idc);
+
+    if (sps->chroma_format_idc == 3) {
+        ni_put_bits(pb, 1, sps->separate_colour_plane_flag);
+    }
+
+    ni_set_ue_golomb(pb, width);
+    ni_set_ue_golomb(pb, height);
+
+    ni_put_bits(pb, 1, sps->conformance_window_flag);
+    if (sps->conformance_window_flag) {
+        ni_set_ue_golomb(pb, sps->conf_win_left_offset);
+        ni_set_ue_golomb(pb, sps->conf_win_right_offset);
+        ni_set_ue_golomb(pb, sps->conf_win_top_offset);
+        ni_set_ue_golomb(pb, sps->conf_win_bottom_offset);
+    }
+
+    ni_set_ue_golomb(pb, sps->bit_depth_luma_minus8);
+    ni_set_ue_golomb(pb, sps->bit_depth_chroma_minus8);
+    ni_set_ue_golomb(pb, sps->log2_max_pic_order_cnt_lsb_minus4);
+
+    ni_put_bits(pb, 1, sps->sps_sub_layer_ordering_info_present_flag);
+
+    // for each layer
+    start = sps->sps_sub_layer_ordering_info_present_flag
+                ? 0
+                : sps->sps_max_sub_layers_minus1;
+    for (i = start; i < sps->sps_max_sub_layers_minus1 + 1; i++) {
+        ni_set_ue_golomb(pb, sps->sps_max_dec_pic_buffering_minus1[i]);
+        ni_set_ue_golomb(pb, sps->sps_max_num_reorder_pics[i]);
+        ni_set_ue_golomb(pb, sps->sps_max_latency_increase_plus1[i]);
+    }
+    // end for
+
+    ni_set_ue_golomb(pb, sps->log2_min_luma_coding_block_size_minus3);
+    ni_set_ue_golomb(pb, sps->log2_diff_max_min_luma_coding_block_size);
+    ni_set_ue_golomb(pb, sps->log2_min_luma_transform_block_size_minus2);
+    ni_set_ue_golomb(pb, sps->log2_diff_max_min_luma_transform_block_size);
+    ni_set_ue_golomb(pb, sps->max_transform_hierarchy_depth_inter);
+    ni_set_ue_golomb(pb, sps->max_transform_hierarchy_depth_intra);
+
+    // scaling list
+    ni_put_bits(pb, 1, sps->scaling_list_enabled_flag);
+    if (sps->scaling_list_enabled_flag) {
+        // Signal scaling list data for custom lists
+        ni_put_bits(pb, 1, sps->sps_scaling_list_data_present_flag);
+        if (sps->sps_scaling_list_data_present_flag) {
+            write_raw_scaling_list(pb, &sps->scaling_list);
+        }
+    }
+
+    ni_put_bits(pb, 1, sps->amp_enabled_flag);
+    ni_put_bits(pb, 1, sps->sample_adaptive_offset_enabled_flag);
+    ni_put_bits(pb, 1, sps->pcm_enabled_flag);
+    if (sps->pcm_enabled_flag) {
+        ni_put_bits(pb, 4, sps->pcm_sample_bit_depth_luma_minus1);
+        ni_put_bits(pb, 4, sps->pcm_sample_bit_depth_chroma_minus1);
+        ni_set_ue_golomb(pb, sps->log2_min_pcm_luma_coding_block_size_minus3);
+        ni_set_ue_golomb(pb, sps->log2_diff_max_min_pcm_luma_coding_block_size);
+        ni_put_bits(pb, 1, sps->pcm_loop_filter_disabled_flag);
+    }
+
+    ni_set_ue_golomb(pb, sps->num_short_term_ref_pic_sets);
+    for (i = 0; i < sps->num_short_term_ref_pic_sets; i++) {
+        short_term_ref_pic_set(pb, &sps->st_ref_pic_set[i], i, sps);
+    }
+
+    // IF num short term ref pic sets
+    // ENDIF
+
+    av_assert0(!sps->long_term_ref_pics_present_flag);
+    ni_put_bits(
+        pb, 1,
+        sps->long_term_ref_pics_present_flag); // long_term_ref_pics_present_flag
+                                               // TODO(tyroun): netint not
+                                               // encode long term ref yet
+
+    // IF long_term_ref_pics_present
+    // ENDIF
+
+    ni_put_bits(pb, 1, sps->sps_temporal_mvp_enabled_flag);
+    ni_put_bits(pb, 1, sps->strong_intra_smoothing_enabled_flag);
+    ni_put_bits(pb, 1, sps->vui_parameters_present_flag);
+
+    if (sps->vui_parameters_present_flag) {
+        write_raw_VUI(pb, sps);
+    }
+
+    write_raw_sps_extension(pb, sps);
+
+    ni_rbsp_trailing_bits(pb);
+
+    return 0;
+}
+
+static void write_raw_pps_extension(ni_bitstream_t *pb, const H265RawPPS *pps) {
+    ni_put_bits(pb, 1, pps->pps_extension_present_flag);
+    if (pps->pps_extension_present_flag) {
+        ni_put_bits(pb, 1, pps->pps_range_extension_flag);
+        ni_put_bits(pb, 1, pps->pps_multilayer_extension_flag);
+        ni_put_bits(pb, 1, pps->pps_3d_extension_flag);
+        ni_put_bits(pb, 1, pps->pps_scc_extension_flag);
+        ni_put_bits(pb, 4, pps->pps_extension_4bits);
+    }
+    av_assert0(!pps->pps_range_extension_flag);
+    av_assert0(!pps->pps_multilayer_extension_flag);
+    av_assert0(!pps->pps_3d_extension_flag);
+    av_assert0(!pps->pps_scc_extension_flag);
+    av_assert0(!pps->pps_extension_4bits);
+}
+
+/*
+ * force_tile: 0 for ignoring this flag. 1 for disabling tile. 2 for enabling
+ * tile.
+ * */
+int ni_hevc_encode_nal_pps(ni_bitstream_t *pb, const H265RawPPS *pps,
+                           uint8_t force_tile, int columns, int rows) {
+    int i;
+
+    if (!pps || force_tile > 2) {
+        return AVERROR(EINVAL);
+    }
+
+    ni_set_ue_golomb(pb, pps->pps_pic_parameter_set_id);
+    ni_set_ue_golomb(pb, pps->pps_seq_parameter_set_id);
+    ni_put_bits(pb, 1, 0); /* dependent_slice_segments_enabled_flag */
+    ni_put_bits(pb, 1, pps->output_flag_present_flag);
+    ni_put_bits(pb, 3, pps->num_extra_slice_header_bits);
+    ni_put_bits(pb, 1, pps->sign_data_hiding_enabled_flag);
+    ni_put_bits(pb, 1, pps->cabac_init_present_flag);
+
+    ni_set_ue_golomb(pb, pps->num_ref_idx_l0_default_active_minus1);
+    ni_set_ue_golomb(pb, pps->num_ref_idx_l1_default_active_minus1);
+
+    // If tiles and slices = tiles is enabled, signal QP in the slice header.
+    // Keeping the PPS constant for OMAF etc Keep QP constant here also if it
+    // will be only set at CU level.
+    ni_set_se_golomb(pb, pps->init_qp_minus26);
+
+    ni_put_bits(pb, 1, pps->constrained_intra_pred_flag);
+    ni_put_bits(pb, 1, pps->transform_skip_enabled_flag);
+    ni_put_bits(pb, 1, pps->cu_qp_delta_enabled_flag);
+
+    if (pps->cu_qp_delta_enabled_flag) {
+        // Use separate QP for each LCU when rate control is enabled.
+        ni_set_ue_golomb(pb, pps->diff_cu_qp_delta_depth);
+    }
+
+    ni_set_se_golomb(pb, pps->pps_cb_qp_offset);
+    ni_set_se_golomb(pb, pps->pps_cr_qp_offset);
+    ni_put_bits(pb, 1, pps->pps_slice_chroma_qp_offsets_present_flag);
+    ni_put_bits(pb, 1, pps->weighted_pred_flag);
+    ni_put_bits(pb, 1, pps->weighted_bipred_flag);
+    ni_put_bits(pb, 1, pps->transquant_bypass_enabled_flag);
+    ni_put_bits(
+        pb, 1,
+        !!((force_tile == 2) || (!force_tile && pps->tiles_enabled_flag)));
+    // wavefronts
+    ni_put_bits(pb, 1, pps->entropy_coding_sync_enabled_flag);
+
+    if (force_tile) {
+        if (force_tile == 2) {
+            ni_set_ue_golomb(pb, columns - 1);
+            ni_set_ue_golomb(pb, rows - 1);
+
+            ni_put_bits(pb, 1, 1); // uniform_spacing_flag must be 1
+            // loop_filter_across_tiles_enabled_flag must be 0
+            ni_put_bits(pb, 1, pps->loop_filter_across_tiles_enabled_flag);
+        }
+    } else if (pps->tiles_enabled_flag) {
+        ni_set_ue_golomb(pb, columns - 1);
+        ni_set_ue_golomb(pb, rows - 1);
+
+        ni_put_bits(pb, 1, pps->uniform_spacing_flag);
+
+        if (!pps->uniform_spacing_flag) {
+            for (i = 0; i < pps->num_tile_columns_minus1; ++i) {
+                ni_set_ue_golomb(pb, pps->column_width_minus1[i]);
+            }
+            for (i = 0; i < pps->num_tile_rows_minus1; ++i) {
+                ni_set_ue_golomb(pb, pps->row_height_minus1[i]);
+            }
+        }
+        ni_put_bits(pb, 1, pps->loop_filter_across_tiles_enabled_flag);
+    }
+
+    ni_put_bits(pb, 1, pps->pps_loop_filter_across_slices_enabled_flag);
+    ni_put_bits(pb, 1, pps->deblocking_filter_control_present_flag);
+
+    if (pps->deblocking_filter_control_present_flag) {
+        ni_put_bits(pb, 1, pps->deblocking_filter_override_enabled_flag);
+        ni_put_bits(pb, 1, pps->pps_deblocking_filter_disabled_flag);
+        if (!pps->pps_deblocking_filter_disabled_flag) {
+            ni_set_se_golomb(pb, pps->pps_beta_offset_div2);
+            ni_set_se_golomb(pb, pps->pps_tc_offset_div2);
+        }
+    }
+
+    ni_put_bits(pb, 1, pps->pps_scaling_list_data_present_flag);
+    av_assert0(!pps->pps_scaling_list_data_present_flag);
+
+    ni_put_bits(pb, 1, pps->lists_modification_present_flag);
+    ni_set_ue_golomb(pb, pps->log2_parallel_merge_level_minus2);
+    ni_put_bits(pb, 1, pps->slice_segment_header_extension_present_flag);
+
+    write_raw_pps_extension(pb, pps);
+
+    ni_rbsp_trailing_bits(pb);
+
+    return 0;
+}
+
+static void write_raw_slice_header_independent(ni_bitstream_t *pb,
+                                               H265RawSliceHeader *slice,
+                                               const H265RawSPS *sps,
+                                               const H265RawPPS *pps) {
+    const H265RawSTRefPicSet *rps;
+    int i, idx_size, entry_size, num_pic_total_curr = 0;
+
+    for (i = 0; i < pps->num_extra_slice_header_bits; i++) {
+        ni_put_bits(pb, 1, 0); // slice_reserved_undetermined_flag
+    }
+    ni_set_ue_golomb(pb, slice->slice_type);
+
+    av_assert0(!pps->output_flag_present_flag);
+    av_assert0(!sps->separate_colour_plane_flag);
+
+    if (slice->nal_unit_header.nal_unit_type != HEVC_NAL_IDR_W_RADL &&
+        slice->nal_unit_header.nal_unit_type != HEVC_NAL_IDR_N_LP) {
+        ni_put_bits(pb, sps->log2_max_pic_order_cnt_lsb_minus4 + 4,
+                    slice->slice_pic_order_cnt_lsb);
+
+        ni_put_bits(pb, 1, slice->short_term_ref_pic_set_sps_flag);
+        if (!slice->short_term_ref_pic_set_sps_flag) {
+            rps = &slice->short_term_ref_pic_set;
+            short_term_ref_pic_set(pb, rps, sps->num_short_term_ref_pic_sets,
+                                   sps);
+        } else if (slice->short_term_ref_pic_set_sps_flag &&
+                   sps->num_short_term_ref_pic_sets > 1) {
+            idx_size = av_log2(sps->num_short_term_ref_pic_sets - 1) + 1;
+            ni_put_bits(pb, idx_size, slice->short_term_ref_pic_set_idx);
+            rps = &sps->st_ref_pic_set[slice->short_term_ref_pic_set_idx];
+        } else {
+            rps = &sps->st_ref_pic_set[0];
+        }
+
+        for (i = 0; i < rps->num_negative_pics; i++)
+            if (rps->used_by_curr_pic_s0_flag[i])
+                ++num_pic_total_curr;
+
+        for (i = 0; i < rps->num_positive_pics; i++)
+            if (rps->used_by_curr_pic_s1_flag[i])
+                ++num_pic_total_curr;
+
+        if (sps->sps_temporal_mvp_enabled_flag) {
+            ni_put_bits(pb, 1, slice->slice_temporal_mvp_enabled_flag);
+        }
+
+        if (pps->pps_curr_pic_ref_enabled_flag) {
+            ++num_pic_total_curr;
+        }
+    }
+
+    if (sps->sample_adaptive_offset_enabled_flag) {
+        ni_put_bits(pb, 1, slice->slice_sao_luma_flag);
+        if (!sps->separate_colour_plane_flag && sps->chroma_format_idc > 0) {
+            ni_put_bits(pb, 1, slice->slice_sao_chroma_flag);
+        }
+    }
+
+    if (slice->slice_type == HEVC_SLICE_P ||
+        slice->slice_type == HEVC_SLICE_B) {
+        ni_put_bits(pb, 1, slice->num_ref_idx_active_override_flag);
+        if (slice->num_ref_idx_active_override_flag) {
+            ni_set_ue_golomb(pb, slice->num_ref_idx_l0_active_minus1);
+            if (slice->slice_type == HEVC_SLICE_B) {
+                ni_set_ue_golomb(pb, slice->num_ref_idx_l1_active_minus1);
+            }
+        }
+
+        if (pps->lists_modification_present_flag && num_pic_total_curr > 1) {
+            entry_size = av_log2(num_pic_total_curr - 1) + 1;
+
+            ni_put_bits(pb, 1, slice->ref_pic_list_modification_flag_l0);
+            if (slice->ref_pic_list_modification_flag_l0) {
+                for (i = 0; i <= slice->num_ref_idx_l0_active_minus1; i++) {
+                    ni_put_bits(pb, entry_size, slice->list_entry_l0[i]);
+                }
+            }
+
+            if (slice->slice_type == HEVC_SLICE_B) {
+                ni_put_bits(pb, 1, slice->ref_pic_list_modification_flag_l1);
+                if (slice->ref_pic_list_modification_flag_l1) {
+                    for (i = 0; i <= slice->num_ref_idx_l1_active_minus1; i++) {
+                        ni_put_bits(pb, entry_size, slice->list_entry_l1[i]);
+                    }
+                }
+            }
+        }
+
+        if (slice->slice_type == HEVC_SLICE_B) {
+            ni_put_bits(pb, 1, slice->mvd_l1_zero_flag);
+        }
+
+        if (pps->cabac_init_present_flag) {
+            ni_put_bits(pb, 1, slice->cabac_init_flag);
+        }
+
+        // Temporal Motion Vector Prediction flags
+        if (slice->slice_temporal_mvp_enabled_flag) {
+            if (slice->slice_type == HEVC_SLICE_B) {
+                // Always use L0 for prediction
+                ni_put_bits(pb, 1, slice->collocated_from_l0_flag);
+            }
+
+            if (slice->collocated_from_l0_flag) {
+                if (slice->num_ref_idx_l0_active_minus1 > 0) {
+                    ni_set_ue_golomb(pb, slice->collocated_ref_idx);
+                }
+            } else {
+                if (slice->num_ref_idx_l1_active_minus1 > 0) {
+                    ni_set_ue_golomb(pb, slice->collocated_ref_idx);
+                }
+            }
+        }
+
+        av_assert0(!pps->weighted_pred_flag);
+        av_assert0(!pps->weighted_bipred_flag);
+
+        ni_set_ue_golomb(pb, slice->five_minus_max_num_merge_cand);
+
+        if (sps->motion_vector_resolution_control_idc == 2) {
+            ni_put_bits(pb, 1, slice->use_integer_mv_flag);
+        }
+    }
+
+    ni_set_se_golomb(pb, slice->slice_qp_delta);
+
+    av_assert0(!pps->pps_slice_chroma_qp_offsets_present_flag);
+    av_assert0(!pps->pps_slice_act_qp_offsets_present_flag);
+    av_assert0(!pps->chroma_qp_offset_list_enabled_flag);
+    av_assert0(!pps->deblocking_filter_override_enabled_flag);
+    av_assert0(!slice->deblocking_filter_override_flag);
+
+    if (pps->pps_loop_filter_across_slices_enabled_flag &&
+        (slice->slice_sao_chroma_flag || slice->slice_sao_luma_flag ||
+         !slice->slice_deblocking_filter_disabled_flag)) {
+        ni_put_bits(pb, 1, slice->slice_loop_filter_across_slices_enabled_flag);
+    }
+}
+
+/*
+ * force_tile: 0 for ignoring this flag. 1 for disabling tile. 2 for enabling
+ * tile.
+ * */
+int ni_hevc_encode_nal_slice_header(ni_bitstream_t *pb,
+                                    H265RawSliceHeader *slice,
+                                    const H265RawSPS *sps,
+                                    const H265RawPPS *pps, int width,
+                                    int height, uint8_t force_tile, int x,
+                                    int y, int independent) {
+    int i;
+    int first_slice_segment_in_pic;
+    int slice_segment_addr, len;
+    int MinCbLog2SizeY;
+    int CtbLog2SizeY;
+    int CtbSizeY;
+    int PicWidthInCtbsY;
+    int PicHeightInCtbsY;
+    int PicSizeInCtbsY;
+
+    if (!pps || !slice || force_tile > 2) {
+        return AVERROR(EINVAL);
+    }
+
+    first_slice_segment_in_pic = (x == 0 && y == 0) ? 1 : 0;
+    ni_put_bits(pb, 1, first_slice_segment_in_pic);
+
+    if (slice->nal_unit_header.nal_unit_type >= 16 &&
+        slice->nal_unit_header.nal_unit_type <= 23) {
+        ni_put_bits(pb, 1, slice->no_output_of_prior_pics_flag);
+    }
+
+    ni_set_ue_golomb(pb, slice->slice_pic_parameter_set_id);
+
+    if (!first_slice_segment_in_pic) {
+        MinCbLog2SizeY = sps->log2_min_luma_coding_block_size_minus3 + 3;
+        CtbLog2SizeY =
+            MinCbLog2SizeY + sps->log2_diff_max_min_luma_coding_block_size;
+        //    int MinCbSizeY = 1 << MinCbLog2SizeY;
+        CtbSizeY = 1 << CtbLog2SizeY;
+        //    int PicWidthInMinCbsY = width / MinCbSizeY;
+        PicWidthInCtbsY = (width + CtbSizeY - 1) / CtbSizeY;
+        //    int PicHeightInMinCbsY = height / MinCbSizeY;
+        PicHeightInCtbsY = (height + CtbSizeY - 1) / CtbSizeY;
+        //    int PicSizeInMinCbsY = PicWidthInMinCbsY * PicHeightInMinCbsY;
+        PicSizeInCtbsY = PicWidthInCtbsY * PicHeightInCtbsY;
+        len            = av_ceil_log2_c(PicSizeInCtbsY);
+
+        slice_segment_addr = y / CtbSizeY * PicWidthInCtbsY + x / CtbSizeY;
+
+        ni_put_bits(pb, len, slice_segment_addr);
+    }
+
+    if (independent) {
+        write_raw_slice_header_independent(pb, slice, sps, pps);
+    }
+
+    //if (pps->tiles_enabled_flag || pps->entropy_coding_sync_enabled_flag)
+    //{
+    if (force_tile == 2 || (!force_tile && pps->tiles_enabled_flag) ||
+        pps->entropy_coding_sync_enabled_flag) {
+        ni_set_ue_golomb(pb, slice->num_entry_point_offsets);
+        if (slice->num_entry_point_offsets > 0) {
+            ni_set_ue_golomb(pb, slice->offset_len_minus1);
+            for (i = 0; i < slice->num_entry_point_offsets; i++) {
+                ni_put_bits(pb, slice->offset_len_minus1 + 1,
+                            slice->entry_point_offset_minus1[i]);
+            }
+        }
+    }
+
+    av_assert0(!pps->slice_segment_header_extension_present_flag);
+
+    ni_rbsp_trailing_bits(pb);
+
+    return 0;
+}
diff --git a/libavcodec/ni_hevc_rbsp.h b/libavcodec/ni_hevc_rbsp.h
new file mode 100644
index 0000000000..eb841e913b
--- /dev/null
+++ b/libavcodec/ni_hevc_rbsp.h
@@ -0,0 +1,60 @@
+/*
+ * NetInt HEVC RBSP parser common code header
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef ENCODER_STATE_BITSTREAM_H_
+#define ENCODER_STATE_BITSTREAM_H_
+
+#include "put_bits.h"
+
+#define MAX_PUT_BUF_SIZE (2 * 1024 * 1024)
+
+typedef struct ni_bitstream_t {
+    PutBitContext pbc;
+    uint8_t *pb_buf;
+    uint8_t cache;
+    uint8_t cur_bits;
+    uint8_t zero_cnt;
+} ni_bitstream_t;
+
+extern int ni_bitstream_init(ni_bitstream_t *stream);
+extern void ni_bitstream_deinit(ni_bitstream_t *stream);
+extern void ni_bitstream_reset(ni_bitstream_t *stream);
+extern void ni_bitstream_fetch(const ni_bitstream_t *stream, uint8_t *buf,
+                               size_t size);
+extern int ni_bitstream_count(ni_bitstream_t *stream);
+extern void ni_put_bits(ni_bitstream_t *stream, uint8_t bits,
+                        const uint32_t data);
+extern void ni_write_nal_header(ni_bitstream_t *stream, const uint8_t nal_type,
+                                const uint8_t temporal_id,
+                                const int long_start_code);
+extern int ni_hevc_encode_nal_slice_header(ni_bitstream_t *stream,
+                                           H265RawSliceHeader *slice,
+                                           const H265RawSPS *sps,
+                                           const H265RawPPS *pps, int width,
+                                           int height, uint8_t force_tile,
+                                           int x, int y, int independent);
+extern int ni_hevc_encode_nal_vps(ni_bitstream_t *stream,
+                                  const H265RawVPS *vps);
+extern int ni_hevc_encode_nal_sps(ni_bitstream_t *stream, const H265RawSPS *sps,
+                                  int width, int height);
+extern int ni_hevc_encode_nal_pps(ni_bitstream_t *stream, const H265RawPPS *pps,
+                                  uint8_t force_tile, int columns, int rows);
+#endif // ENCODER_STATE_BITSTREAM_H_
diff --git a/libavcodec/ni_hevc_tile_repack_bsf.c b/libavcodec/ni_hevc_tile_repack_bsf.c
new file mode 100644
index 0000000000..f7d4c4492c
--- /dev/null
+++ b/libavcodec/ni_hevc_tile_repack_bsf.c
@@ -0,0 +1,325 @@
+/*
+ * NetInt HEVC tile repack BSF common source code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ *
+ * This bitstream filter repacks HEVC tiles into one packet containing
+ * just one frame.
+ */
+
+#include "libavutil/avassert.h"
+#include "libavutil/opt.h"
+
+#include "avcodec.h"
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 91)
+#include "bsf_internal.h"
+#else
+#include "bsf.h"
+#endif
+#include "hevc.h"
+#include "internal.h"
+
+typedef struct HEVCRepackContext {
+    AVPacket *buffer_pkt;
+    AVPacket **tile_pkt;
+    int tile_pos;
+    int tile_num;
+} HEVCRepackContext;
+
+static int hevc_tile_repack_filter(AVBSFContext *ctx, AVPacket *out) {
+    HEVCRepackContext *s = ctx->priv_data;
+    int ret;
+    int tile_idx;
+    int *side_data;
+    int i;
+
+    av_log(ctx, AV_LOG_DEBUG, "tile_pos %d, tile_num %d\n", s->tile_pos,
+           s->tile_num);
+
+    if (s->tile_pos < s->tile_num) {
+        if (!s->buffer_pkt->data) {
+            ret = ff_bsf_get_packet_ref(ctx, s->buffer_pkt);
+            if (ret < 0) {
+                av_log(ctx, AV_LOG_INFO, "failed to get packet ref: 0x%x\n",
+                       ret);
+                return ret;
+            }
+        }
+
+        side_data = (int *)av_packet_get_side_data(
+            s->buffer_pkt, AV_PKT_DATA_SLICE_ADDR, NULL);
+        if (!side_data) {
+            av_log(ctx, AV_LOG_ERROR, "failed to get packet side data\n");
+            return AVERROR(EINVAL);
+        }
+
+        tile_idx = *side_data;
+        if (tile_idx >= s->tile_num) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "tile index %d exceeds maximum tile number %d\n", tile_idx,
+                   s->tile_num);
+            return AVERROR(EINVAL);
+        }
+
+        if (s->tile_pkt[tile_idx]->buf) {
+            av_log(ctx, AV_LOG_ERROR, "duplicated tile index %d\n", tile_idx);
+            return AVERROR(EINVAL);
+        }
+
+        s->tile_pkt[tile_idx]->buf = av_buffer_ref(s->buffer_pkt->buf);
+        if (!s->tile_pkt[tile_idx]->buf) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "failed to get buffer for tile index %d\n", tile_idx);
+            return AVERROR(ENOMEM);
+        }
+        s->tile_pkt[tile_idx]->data = s->buffer_pkt->data;
+        s->tile_pkt[tile_idx]->size = s->buffer_pkt->size;
+
+        av_log(ctx, AV_LOG_DEBUG, "tile %d, data actual size %d\n", tile_idx,
+               s->buffer_pkt->size);
+
+        if (s->tile_pos == 0) {
+            s->tile_pkt[0]->pts          = s->buffer_pkt->pts;
+            s->tile_pkt[0]->dts          = s->buffer_pkt->dts;
+            s->tile_pkt[0]->pos          = s->buffer_pkt->pos;
+            s->tile_pkt[0]->flags        = s->buffer_pkt->flags;
+            s->tile_pkt[0]->stream_index = s->buffer_pkt->stream_index;
+
+            s->tile_pkt[0]->side_data       = NULL;
+            s->tile_pkt[0]->side_data_elems = 0;
+
+            for (i = 0; i < s->tile_pkt[0]->side_data_elems; i++) {
+                enum AVPacketSideDataType type =
+                    s->buffer_pkt->side_data[i].type;
+                if (type != AV_PKT_DATA_SLICE_ADDR) {
+                    int size          = s->buffer_pkt->side_data[i].size;
+                    uint8_t *src_data = s->buffer_pkt->side_data[i].data;
+                    uint8_t *dst_data =
+                        av_packet_new_side_data(s->tile_pkt[0], type, size);
+
+                    if (!dst_data) {
+                        av_packet_free_side_data(s->tile_pkt[0]);
+                        return AVERROR(ENOMEM);
+                    }
+
+                    memcpy(dst_data, src_data, size);
+                }
+            }
+        } else {
+            if (s->buffer_pkt->pts != s->tile_pkt[0]->pts ||
+                s->buffer_pkt->dts != s->tile_pkt[0]->dts ||
+                s->buffer_pkt->flags != s->tile_pkt[0]->flags ||
+                s->buffer_pkt->stream_index != s->tile_pkt[0]->stream_index) {
+                av_log(ctx, AV_LOG_ERROR, "packet metadata does not match\n");
+                return AVERROR(EINVAL);
+            }
+        }
+        s->tile_pos++;
+        av_packet_unref(s->buffer_pkt);
+    }
+
+    if (s->tile_pos == s->tile_num) {
+        int new_size = 0;
+        int found;
+        const uint8_t *ptr;
+        const uint8_t *end;
+        const uint8_t *p_offset;
+        uint32_t nalu_type;
+        uint32_t stc;
+        uint8_t *data;
+        AVBufferRef *buf;
+
+        /* max payload size */
+        for (i = 0; i < s->tile_num; i++) {
+            new_size += s->tile_pkt[i]->size;
+        }
+
+        buf = av_buffer_alloc(new_size);
+        if (!buf) {
+            av_log(ctx, AV_LOG_ERROR, "failed to allocate new packet data\n");
+            return AVERROR(ENOMEM);
+        }
+
+        data = buf->data;
+        memcpy(data, s->tile_pkt[0]->data, s->tile_pkt[0]->size);
+        new_size = s->tile_pkt[0]->size;
+        av_log(ctx, AV_LOG_DEBUG, "tile %d size %d\n", 0, new_size);
+        av_buffer_unref(&s->tile_pkt[0]->buf);
+        s->tile_pkt[0]->buf = NULL;
+
+        for (i = 1; i < s->tile_num; i++) {
+            ptr = s->tile_pkt[i]->data;
+            end = s->tile_pkt[i]->data + s->tile_pkt[i]->size;
+
+            stc   = -1;
+            found = 0;
+            ptr   = avpriv_find_start_code(ptr, end, &stc);
+            while (ptr < end) {
+                av_log(ctx, AV_LOG_DEBUG, "tile %d, %02x %02x %02x %02x %02x\n",
+                       i, *(ptr - 4), *(ptr - 3), *(ptr - 2), *(ptr - 1), *ptr);
+
+                if (found) {
+                    memcpy(data + new_size, p_offset, ptr - 4 - p_offset);
+                    new_size += (int)(ptr - 4 - p_offset);
+                    found = 0;
+                }
+
+                nalu_type = (stc >> 1) & 0x3F;
+                if (nalu_type <= HEVC_NAL_RSV_VCL31) {
+                    p_offset = ptr - 4;
+                    found    = 1;
+                }
+
+                stc = -1;
+                ptr = avpriv_find_start_code(ptr, end, &stc);
+            }
+
+            if (found) {
+                memcpy(data + new_size, p_offset, end - p_offset);
+                new_size += (int)(end - p_offset);
+                av_log(ctx, AV_LOG_DEBUG, "tile %d size %d\n", i,
+                       (int)(end - p_offset));
+            }
+            av_buffer_unref(&s->tile_pkt[i]->buf);
+            s->tile_pkt[i]->buf = NULL;
+        }
+
+        out->buf  = buf;
+        out->data = data;
+        out->size = new_size;
+
+        av_log(ctx, AV_LOG_DEBUG, "repacket new size %d\n", new_size);
+
+        s->tile_pos = 0;
+        return 0;
+    } else {
+        return AVERROR(EAGAIN);
+    }
+}
+
+static int hevc_tile_repack_init(AVBSFContext *ctx) {
+    HEVCRepackContext *s = ctx->priv_data;
+    int ret;
+    int i;
+
+    av_log(ctx, AV_LOG_INFO, "number of tiles %d\n", s->tile_num);
+    if (s->tile_num <= 0) {
+        return AVERROR(EINVAL);
+    }
+
+    s->buffer_pkt = av_packet_alloc();
+    if (!s->buffer_pkt) {
+        return AVERROR(ENOMEM);
+    }
+
+    s->tile_pkt = av_malloc(sizeof(AVPacket *) * s->tile_num);
+    if (!s->tile_pkt) {
+        ret = AVERROR(ENOMEM);
+        goto fail_alloc_tile_pkt;
+    }
+    memset(s->tile_pkt, 0, sizeof(AVPacket *) * s->tile_num);
+
+    for (i = 0; i < s->tile_num; i++) {
+        s->tile_pkt[i] = av_packet_alloc();
+        if (!s->tile_pkt[i]) {
+            ret = AVERROR(ENOMEM);
+            goto fail_alloc_pkts;
+        }
+    }
+
+    return 0;
+
+fail_alloc_pkts:
+    for (i -= 1; i >= 0; i--) {
+        av_packet_free(&s->tile_pkt[i]);
+    }
+    free(s->tile_pkt);
+    s->tile_pkt = NULL;
+
+fail_alloc_tile_pkt:
+    av_packet_free(&s->buffer_pkt);
+    s->buffer_pkt = NULL;
+
+    return ret;
+}
+
+static void hevc_tile_repack_flush(AVBSFContext *ctx) {
+    HEVCRepackContext *s = ctx->priv_data;
+    int i;
+
+    av_packet_unref(s->buffer_pkt);
+
+    for (i = 0; i < s->tile_num; i++) {
+        av_packet_unref(s->tile_pkt[i]);
+    }
+}
+
+static void hevc_tile_repack_close(AVBSFContext *ctx) {
+    HEVCRepackContext *s = ctx->priv_data;
+    int i;
+
+    av_packet_free(&s->buffer_pkt);
+    s->buffer_pkt = NULL;
+
+    for (i = 0; i < s->tile_num; i++) {
+        av_packet_free(&s->tile_pkt[i]);
+    }
+    free(s->tile_pkt);
+    s->tile_pkt = NULL;
+}
+
+static const enum AVCodecID hevc_tile_repack_codec_ids[] = {
+    AV_CODEC_ID_HEVC,
+    AV_CODEC_ID_NONE,
+};
+
+#define OFFSET(x) offsetof(HEVCRepackContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_BSF_PARAM)
+static const AVOption options[] = {
+    {"tile_num",
+     "specify number of tiles",
+     OFFSET(tile_num),
+     AV_OPT_TYPE_INT,
+     {.i64 = 0},
+     0,
+     INT_MAX,
+     FLAGS},
+    {NULL},
+};
+
+static const AVClass tile_repack_class = {
+    .class_name = "hevc_tile_repack_bsf",
+    .item_name  = av_default_item_name,
+    .option     = options,
+    .version    = LIBAVUTIL_VERSION_INT,
+};
+
+const AVBitStreamFilter ff_hevc_tile_repack_bsf = {
+    .name           = "hevc_tile_repack",
+    .priv_data_size = sizeof(HEVCRepackContext),
+    .priv_class     = &tile_repack_class,
+    .init           = hevc_tile_repack_init,
+    .flush          = hevc_tile_repack_flush,
+    .close          = hevc_tile_repack_close,
+    .filter         = hevc_tile_repack_filter,
+    .codec_ids      = hevc_tile_repack_codec_ids,
+};
diff --git a/libavcodec/nicodec.c b/libavcodec/nicodec.c
new file mode 100644
index 0000000000..23dd111a77
--- /dev/null
+++ b/libavcodec/nicodec.c
@@ -0,0 +1,1758 @@
+/*
+ * XCoder Codec Lib Wrapper
+ * Copyright (c) 2018 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder codec lib wrapper.
+ */
+
+#include "nicodec.h"
+#include "get_bits.h"
+#include "internal.h"
+#include "libavcodec/h264.h"
+#include "libavcodec/h264_sei.h"
+#include "libavcodec/hevc.h"
+#include "libavcodec/hevc_sei.h"
+#include "libavcodec/startcode.h"
+#include "libavcodec/bsf.h"
+#include "libavutil/eval.h"
+#include "libavutil/hdr_dynamic_metadata.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/hwcontext_ni_quad.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/intreadwrite.h"
+#include "libavutil/mastering_display_metadata.h"
+#include "libavutil/pixdesc.h"
+#include "nidec.h"
+
+#include <math.h>
+#include <ni_av_codec.h>
+#include <ni_rsrc_api.h>
+
+#define NAL_264(X) ((X) & (0x1F))
+#define NAL_265(X) (((X)&0x7E) >> 1)
+#define MAX_HEADERS_SIZE 1000
+
+static const char *const var_names[] = {
+  "in_w", "iw",   ///< width  of the input video
+  "in_h", "ih",   ///< height of the input video
+  "out_w", "ow",  ///< width  of the cropped video
+  "out_h", "oh",  ///< height of the cropped video
+  "x",
+  "y",
+  NULL
+};
+
+enum var_name {
+  VAR_IN_W, VAR_IW,
+  VAR_IN_H, VAR_IH,
+  VAR_OUT_W, VAR_OW,
+  VAR_OUT_H, VAR_OH,
+  VAR_X,
+  VAR_Y,
+  VAR_VARS_NB
+};
+
+static inline void ni_align_free(void *opaque, uint8_t *data)
+{
+  ni_buf_t *buf = (ni_buf_t *)opaque;
+  if (buf)
+  {
+    ni_decoder_frame_buffer_pool_return_buf(buf, (ni_buf_pool_t *)buf->pool);
+  }
+}
+
+static inline void ni_align_free_nop(void *opaque, uint8_t *data)
+{
+}
+
+static inline void ni_free(void *opaque, uint8_t *data)
+{
+  free(data);
+}
+
+static inline void ni_frame_free(void *opaque, uint8_t *data)
+{
+  if (data)
+  {
+    niFrameSurface1_t* p_data3 = (niFrameSurface1_t*)data;
+    int ret;
+    if (p_data3->ui16FrameIdx != 0)
+    {
+      av_log(NULL, AV_LOG_DEBUG, "Recycle trace ui16FrameIdx = [%d] DevHandle %d\n", p_data3->ui16FrameIdx, p_data3->device_handle);
+      ret = ni_hwframe_buffer_recycle(p_data3, p_data3->device_handle);
+      if (ret != NI_RETCODE_SUCCESS)
+      {
+        av_log(NULL, AV_LOG_ERROR, "ERROR Failed to recycle trace ui16frameidx = [%d] DevHandle %d\n", p_data3->ui16FrameIdx, p_data3->device_handle);
+      }
+    }
+    free(data);
+  }
+}
+
+static enum AVPixelFormat ni_supported_pixel_formats[] =
+{
+  AV_PIX_FMT_YUV420P,
+  AV_PIX_FMT_YUV420P10LE,
+  AV_PIX_FMT_NV12,
+  AV_PIX_FMT_P010LE,
+  AV_PIX_FMT_NONE, //convert RGB to unused 
+  AV_PIX_FMT_NONE,
+};
+
+static inline int ni_pix_fmt_2_ff_pix_fmt(ni_pix_fmt_t pix_fmt)
+{
+  return ni_supported_pixel_formats[pix_fmt];
+}
+
+int parse_symbolic_decoder_param(XCoderH264DecContext *s)
+{
+  ni_decoder_input_params_t *pdec_param = &s->api_param.dec_input_params;
+  int i, ret;
+  double res;
+  double var_values[VAR_VARS_NB];
+
+  if (pdec_param == NULL)
+  {
+    return AVERROR_INVALIDDATA;
+  }
+  
+  for (i = 0; i < NI_MAX_NUM_OF_DECODER_OUTPUTS; i++)
+  {
+    /*Set output width and height*/
+    var_values[VAR_IN_W] = var_values[VAR_IW] = pdec_param->crop_whxy[i][0];
+    var_values[VAR_IN_H] = var_values[VAR_IH] = pdec_param->crop_whxy[i][1];
+    var_values[VAR_OUT_W] = var_values[VAR_OW] = pdec_param->crop_whxy[i][0];
+    var_values[VAR_OUT_H] = var_values[VAR_OH] = pdec_param->crop_whxy[i][1];
+    if (pdec_param->cr_expr[i][0][0] && pdec_param->cr_expr[i][1][0])
+    {
+        if (av_expr_parse_and_eval(&res, pdec_param->cr_expr[i][0], var_names,
+                                   var_values, NULL, NULL, NULL, NULL, NULL, 0,
+                                   s) < 0) {
+            return AVERROR_INVALIDDATA;
+        }
+        var_values[VAR_OUT_W] = var_values[VAR_OW] = (double)floor(res);
+        if (av_expr_parse_and_eval(&res, pdec_param->cr_expr[i][1], var_names,
+                                   var_values, NULL, NULL, NULL, NULL, NULL, 0,
+                                   s) < 0) {
+            return AVERROR_INVALIDDATA;
+        }
+        var_values[VAR_OUT_H] = var_values[VAR_OH] = (double)floor(res);
+        /* evaluate again ow as it may depend on oh */
+        ret = av_expr_parse_and_eval(&res, pdec_param->cr_expr[i][0], var_names,
+                                     var_values, NULL, NULL, NULL, NULL, NULL,
+                                     0, s);
+        if (ret < 0) {
+            return AVERROR_INVALIDDATA;
+        }
+        var_values[VAR_OUT_W] = var_values[VAR_OW] = (double)floor(res);
+        pdec_param->crop_whxy[i][0]                = (int)var_values[VAR_OUT_W];
+        pdec_param->crop_whxy[i][1]                = (int)var_values[VAR_OUT_H];
+    } 
+    /*Set output crop offset X,Y*/
+    if (pdec_param->cr_expr[i][2][0])
+    {
+        ret = av_expr_parse_and_eval(&res, pdec_param->cr_expr[i][2], var_names,
+                                     var_values, NULL, NULL, NULL, NULL, NULL,
+                                     0, s);
+        if (ret < 0) {
+            return AVERROR_INVALIDDATA;
+        }
+      var_values[VAR_X] = res;
+      pdec_param->crop_whxy[i][2] = floor(var_values[VAR_X]);
+    }
+    if (pdec_param->cr_expr[i][3][0])
+    {
+        ret = av_expr_parse_and_eval(&res, pdec_param->cr_expr[i][3], var_names,
+                                     var_values, NULL, NULL, NULL, NULL, NULL,
+                                     0, s);
+        if (ret < 0) {
+            return AVERROR_INVALIDDATA;
+        }
+      var_values[VAR_Y] = res;
+      pdec_param->crop_whxy[i][3] = floor(var_values[VAR_Y]);
+    }
+    /*Set output Scale*/
+    /*Reset OW and OH to next lower even number*/
+    var_values[VAR_OUT_W] = var_values[VAR_OW] =
+        (double)(pdec_param->crop_whxy[i][0] -
+                 (pdec_param->crop_whxy[i][0] % 2));
+    var_values[VAR_OUT_H] = var_values[VAR_OH] =
+        (double)(pdec_param->crop_whxy[i][1] -
+                 (pdec_param->crop_whxy[i][1] % 2));
+    if (pdec_param->sc_expr[i][0][0] && pdec_param->sc_expr[i][1][0])
+    {
+        if (av_expr_parse_and_eval(&res, pdec_param->sc_expr[i][0], var_names,
+                                   var_values, NULL, NULL, NULL, NULL, NULL, 0,
+                                   s) < 0) {
+            return AVERROR_INVALIDDATA;
+        }
+        pdec_param->scale_wh[i][0] = ceil(res);
+        ret = av_expr_parse_and_eval(&res, pdec_param->sc_expr[i][1], var_names,
+                                     var_values, NULL, NULL, NULL, NULL, NULL,
+                                     0, s);
+        if (ret < 0) {
+            return AVERROR_INVALIDDATA;
+        }
+        pdec_param->scale_wh[i][1] = ceil(res);
+    }
+  }
+  return 0;
+}
+
+int ff_xcoder_dec_init(AVCodecContext *avctx, XCoderH264DecContext *s)
+{
+  /* ToDo: call xcode_dec_open to open a decoder instance */
+  int ret = 0;
+  ni_xcoder_params_t *p_param = &s->api_param;
+
+  s->api_ctx.hw_id = s->dev_dec_idx;
+  s->api_ctx.decoder_low_delay = 0;
+
+  ret = ni_device_session_open(&s->api_ctx, NI_DEVICE_TYPE_DECODER);
+  if (ret != 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to open decoder (status = %d), "
+           "resource unavailable\n", ret);
+    ret = AVERROR_EXTERNAL;
+    ff_xcoder_dec_close(avctx, s);
+  }
+  else
+  {
+      s->dev_xcoder_name = s->api_ctx.dev_xcoder_name;
+      s->blk_xcoder_name = s->api_ctx.blk_xcoder_name;
+      s->dev_dec_idx     = s->api_ctx.hw_id;
+      av_log(avctx, AV_LOG_VERBOSE,
+             "XCoder %s.%d (inst: %d) opened successfully\n",
+             s->dev_xcoder_name, s->dev_dec_idx, s->api_ctx.session_id);
+
+      if (p_param->dec_input_params.hwframes) {
+          if (!avctx->hw_device_ctx) // avctx->hw_frames_ctx)
+          {
+              char buf[64] = {0};
+              av_log(avctx, AV_LOG_DEBUG,
+                     "nicodec.c:ff_xcoder_dec_init() hwdevice_ctx_create\n");
+              av_hwdevice_ctx_create(&avctx->hw_device_ctx, AV_HWDEVICE_TYPE_NI_QUADRA,
+                                     buf, NULL, 0); // create with null device
+          }
+          if (!avctx->hw_frames_ctx) {
+              avctx->hw_frames_ctx = av_hwframe_ctx_alloc(avctx->hw_device_ctx);
+
+              if (!avctx->hw_frames_ctx) {
+                  ret = AVERROR(ENOMEM);
+                  return ret;
+              }
+          }
+          s->frames = (AVHWFramesContext *)avctx->hw_frames_ctx->data;
+
+          s->frames->format = AV_PIX_FMT_NI_QUAD;
+          s->frames->width  = avctx->width;
+          s->frames->height = avctx->height;
+
+          s->frames->sw_format = avctx->sw_pix_fmt;
+          // Decoder has its own dedicated pool
+          s->frames->initial_pool_size = -1;
+
+          ret = av_hwframe_ctx_init(avctx->hw_frames_ctx);
+
+          avctx->pix_fmt       = AV_PIX_FMT_NI_QUAD;
+          s->api_ctx.hw_action = NI_CODEC_HW_ENABLE;
+      } else {
+          // reassign in case above conditions alter value
+          avctx->pix_fmt       = avctx->sw_pix_fmt;
+          s->api_ctx.hw_action = NI_CODEC_HW_NONE;
+      }
+  }
+
+  return ret;
+}
+
+int ff_xcoder_dec_close(AVCodecContext *avctx, XCoderH264DecContext *s)
+{
+    ni_session_context_t *p_ctx = &s->api_ctx;
+
+    if (p_ctx)
+    {
+        // dec params in union with enc params struct
+        ni_retcode_t ret;
+        ni_xcoder_params_t *p_param = &s->api_param; 
+
+        ret = ni_device_session_close(p_ctx, s->eos, NI_DEVICE_TYPE_DECODER);
+        if (NI_RETCODE_SUCCESS != ret)
+        {
+            av_log(avctx, AV_LOG_ERROR,
+                   "Failed to close Decode Session (status = %d)\n", ret);
+        }
+        ni_device_session_context_clear(p_ctx);
+
+        if (p_param->dec_input_params.hwframes)
+        {
+            av_log(avctx, AV_LOG_ERROR,
+                   "File BLK handle %d close suspended to frames Uninit\n",
+                   p_ctx->blk_io_handle); // suspended_device_handle
+            if (avctx->hw_frames_ctx)
+            {
+                AVHWFramesContext *ctx =
+                    (AVHWFramesContext *)avctx->hw_frames_ctx->data;
+                if (ctx)
+                {
+                    NIFramesContext *dst_ctx = ctx->internal->priv;
+                    if (dst_ctx)
+                    {
+                        dst_ctx->suspended_device_handle = p_ctx->blk_io_handle;
+                    }
+                }
+            }
+#ifdef __linux__
+            ni_device_close(p_ctx->device_handle);
+#endif
+        }
+        else
+        {
+#ifdef _WIN32
+            ni_device_close(p_ctx->device_handle);
+#elif __linux__
+            ni_device_close(p_ctx->device_handle);
+            ni_device_close(p_ctx->blk_io_handle);
+#endif
+        }
+        p_ctx->device_handle = NI_INVALID_DEVICE_HANDLE;
+        p_ctx->blk_io_handle = NI_INVALID_DEVICE_HANDLE;
+    }
+
+    return 0;
+}
+
+/*!******************************************************************************
+ * \brief  Extract custom sei payload data from AVPacket,
+ *  and save it to ni_packet_t
+ *
+ * \param AVCodecContext *avctx - avcodec context
+ * \param AVPacket *pkt - AVPacket
+ * \param long index - pkt data index of custom sei first byte after SEI type
+ * \param ni_packet_t *p_packet - netint internal packet
+ * \param uint8_t sei_type - type of SEI
+ * \param int vcl_found - whether got vcl in the pkt data, 1 means got
+ *
+ * \return - 0 on success, non-0 on failure
+ ********************************************************************************/
+static int ff_xcoder_extract_custom_sei(AVCodecContext *avctx, AVPacket *pkt, long index,
+                                        ni_packet_t *p_packet, uint8_t sei_type, int vcl_found)
+{
+  int i, len;
+  uint8_t *udata;
+  uint8_t *sei_data;
+  int sei_size;
+  int sei_index;
+  ni_custom_sei_t *p_custom_sei;
+
+  av_log(avctx, AV_LOG_TRACE, "%s() enter\n", __FUNCTION__);
+
+  if (p_packet->p_custom_sei_set == NULL)
+  {
+    /* max size */
+    p_packet->p_custom_sei_set = (ni_custom_sei_set_t *)malloc(sizeof(ni_custom_sei_set_t));
+    if (p_packet->p_custom_sei_set == NULL)
+    {
+      av_log(avctx, AV_LOG_ERROR, "failed to allocate all custom sei buffer.\n");
+      return AVERROR(ENOMEM);
+    }
+    memset(p_packet->p_custom_sei_set, 0, sizeof(ni_custom_sei_set_t));
+  }
+
+  sei_index = p_packet->p_custom_sei_set->count;
+  p_custom_sei = &p_packet->p_custom_sei_set->custom_sei[sei_index];
+  if (sei_index >= NI_MAX_CUSTOM_SEI_CNT)
+  {
+    av_log(avctx, AV_LOG_WARNING, "number of custom sei in current frame is out of limit(%d).\n",
+           NI_MAX_CUSTOM_SEI_CNT);
+    return AVERROR(EINVAL);
+  }
+  sei_data = &p_custom_sei->data[0];
+
+  /*! extract SEI payload size.
+   *  the first byte after SEI type is the SEI payload size.
+   *  if the first byte is 255(0xFF), it means the SEI payload size is more than 255.
+   *  in this case, to get the SEI payload size is to do a summation.
+   *  the end of SEI size is the first non-0xFF value.
+   *  for example, 0xFF 0xFF 0x08, the SEI payload size equals to (0xFF+0xFF+0x08).
+   */
+  sei_size = 0;
+  while (index < pkt->size && pkt->data[index] == 0xff)
+  {
+    sei_size += pkt->data[index++];
+  }
+
+  if (index >= pkt->size)
+  {
+    av_log(avctx, AV_LOG_WARNING, "custom sei corrupted: length truncated.\n");
+    return AVERROR(EINVAL);
+  }
+  sei_size += pkt->data[index++];
+
+  if (sei_size > NI_MAX_SEI_DATA)
+  {
+    av_log(avctx, AV_LOG_WARNING, "custom sei corrupted: size(%d) out of limit(%d).\n",
+           sei_size, NI_MAX_SEI_DATA);
+    return AVERROR(EINVAL);
+  }
+
+  udata = &pkt->data[index];
+
+  /* set SEI payload type at the first byte */
+  sei_data[0] = sei_type;
+
+  /* extract SEI payload data
+   * SEI payload data in NAL is EBSP(Encapsulated Byte Sequence Payload), 
+   * need change EBSP to RBSP(Raw Byte Sequence Payload) for exact size
+   */
+  for (i = 0, len = 0; (i < pkt->size - index) && len < sei_size; i++, len++)
+  {
+    /* if the latest 3-byte data pattern matchs '00 00 03' which means udata[i] is an escaping byte,
+     * discard udata[i]. */
+    if (i >= 2 && udata[i - 2] == 0 && udata[i - 1] == 0 && udata[i] == 3)
+    {
+        len--;
+        continue;
+    }
+    sei_data[len] = udata[i];
+  }
+
+  if (len != sei_size)
+  {
+    av_log(avctx, AV_LOG_WARNING, "custom sei corrupted: data truncated, "
+           "required size:%d, actual size:%d.\n", sei_size, len);
+    return AVERROR(EINVAL);
+  }
+
+  p_custom_sei->type = sei_type;
+  p_custom_sei->size = sei_size;
+  p_custom_sei->location = vcl_found ? NI_CUSTOM_SEI_LOC_AFTER_VCL : NI_CUSTOM_SEI_LOC_BEFORE_VCL;
+
+  p_packet->p_custom_sei_set->count++;
+
+  av_log(avctx, AV_LOG_TRACE, "%s() exit, custom sei size=%d type=%d\n",
+         __FUNCTION__, sei_size, sei_type);
+
+  return 0;
+}
+
+int ff_xcoder_add_headers(AVCodecContext *avctx, AVPacket *pkt,
+                          uint8_t *extradata, int extradata_size)
+{
+    int ret = 0;
+    uint32_t pkt_idx = 0;
+    uint32_t remaining_size = pkt->size;
+    uint8_t nalu_type;
+    static int got_first_key_frame = 0;
+    static uint8_t* p_headers[MAX_HEADERS_SIZE];
+    int headersCopyDone = 0;
+
+    if (!pkt->data || !avctx)
+    {
+      return ret;
+    }
+
+    while (pkt_idx < pkt->size)
+    {
+        if (((remaining_size >= 3) && (AV_RB24(pkt->data + pkt_idx) == 1)) || ((remaining_size >= 4) && (AV_RB32(pkt->data + pkt_idx) == 1))) // if start code
+        {
+            if ((remaining_size >= 3) && (AV_RB24(pkt->data + pkt_idx) == 1)){
+                pkt_idx += 3;
+                remaining_size -= 3;
+            }
+            if ((remaining_size >= 4) && (AV_RB32(pkt->data + pkt_idx) == 1)){
+                pkt_idx += 4;
+                remaining_size -= 4;
+            }
+
+            if (avctx->codec_id == AV_CODEC_ID_H264){ // if h264
+                nalu_type = NAL_264(pkt->data[pkt_idx]);
+                if (got_first_key_frame && pkt->flags & AV_PKT_FLAG_KEY)// not first IDR
+                {
+                    //check if headers have changed
+                    if (headersCopyDone &&
+                        (memcmp(p_headers, extradata, extradata_size) != 0)) {
+                        av_log(avctx, AV_LOG_INFO, "New headers !!\n");
+                        got_first_key_frame = 0;
+                        headersCopyDone = 0;
+                    }
+                }
+                if (!got_first_key_frame && pkt->flags & AV_PKT_FLAG_KEY)// first IDR
+                {
+                    //save headers
+                    memcpy(p_headers, extradata, extradata_size);
+                    got_first_key_frame = 1;
+                    ret = 1;
+                    break;
+                }
+                if ((nalu_type == 7)||(nalu_type == 8)) //found sps or pps
+                {
+                    got_first_key_frame = 1; //assuming headers always come with an IDR
+                    ret = 0;
+                    break;
+                }
+                if ((nalu_type >= 1)&&(nalu_type <= 5)) //found VCL
+                {
+                    ret = 0;
+                    break;
+                }
+
+            }
+            if (avctx->codec_id == AV_CODEC_ID_HEVC){ //if hevc
+                nalu_type = NAL_265(pkt->data[pkt_idx]);
+                if (got_first_key_frame && pkt->flags & AV_PKT_FLAG_KEY)// not first IDR
+                {
+                    //check if headers have changed
+                    if (headersCopyDone &&
+                        (memcmp(p_headers, extradata, extradata_size) != 0)) {
+                        av_log(avctx, AV_LOG_INFO, "New headers !!\n");
+                        got_first_key_frame = 0;
+                        headersCopyDone = 0;
+                    }
+                }
+                if (!got_first_key_frame && pkt->flags & AV_PKT_FLAG_KEY)// first IRAP/IDR
+                {
+                    //save headers
+                    memcpy(p_headers, extradata, extradata_size);
+                    got_first_key_frame = 1;
+                    ret = 1;
+                    break;
+                }
+                if ((nalu_type == 32)||(nalu_type == 33)||(nalu_type == 34)) //found vps or sps or pps may be we need to add nals prefix 39 and suffix 40
+                {
+                    got_first_key_frame = 1;//assuming headers always come with an IDR
+                    ret = 0;
+                    break;
+                }
+                if (nalu_type <= 31) // found VCL
+                {
+                    ret = 0;
+                    break;
+                }
+            }
+        }
+        pkt_idx += 1;
+        remaining_size -= 1;
+    }
+    return ret;
+}
+
+// check if the packet is SEI only and also, check if getting the header of
+// streams in decoder low delay mode, and update its value
+static int xcoder_packet_parse(AVCodecContext *avctx, XCoderH264DecContext *s,
+                               AVPacket *pkt, ni_packet_t *p_packet)
+{
+    ni_xcoder_params_t *p_param = &s->api_param;
+    int pkt_sei_alone = 1;
+    int vcl_found = 0;
+    int low_delay      = (s->api_ctx.decoder_low_delay == -1) ? 0 : s->low_delay;
+    int pkt_nal_bitmap = s->pkt_nal_bitmap;
+    const uint8_t *ptr = pkt->data;
+    const uint8_t *end = pkt->data + pkt->size;
+    uint32_t stc;
+    uint8_t nalu_type;
+    uint8_t sei_type = 0;
+    int ret = 0, nalu_count = 0;
+
+    if (pkt_nal_bitmap & NI_GENERATE_ALL_NAL_HEADER_BIT) {
+        av_log(avctx, AV_LOG_TRACE,
+               "xcoder_packet_parse(): already find the header of streams.\n");
+        low_delay = 0;
+    }
+
+    while (((s->custom_sei_type != NI_INVALID_SEI_TYPE || p_param->dec_input_params.custom_sei_passthru != NI_INVALID_SEI_TYPE) || low_delay) && ptr < end)
+    {
+        stc = -1;
+        ptr = avpriv_find_start_code(ptr, end, &stc);
+        if (ptr == end)
+        {
+            if (0 == nalu_count)
+            {
+                pkt_sei_alone = 0;
+                av_log(avctx, AV_LOG_TRACE, "%s(): no NAL found in pkt.\n",
+                       __FUNCTION__);
+            }
+            break;
+        }
+        nalu_count++;
+
+        if (AV_CODEC_ID_H264 == avctx->codec_id)
+        {
+            nalu_type = stc & 0x1f;
+
+            //check whether the packet is sei alone
+            pkt_sei_alone = (pkt_sei_alone && H264_NAL_SEI == nalu_type);
+
+            // Enable decoder low delay mode on sequence change
+            if (low_delay) {
+                switch (nalu_type) {
+                case H264_NAL_SPS:
+                    pkt_nal_bitmap |= NI_NAL_SPS_BIT;
+                    break;
+                case H264_NAL_PPS:
+                    pkt_nal_bitmap |= NI_NAL_PPS_BIT;
+                    break;
+                default:
+                    break;
+                }
+
+                if (pkt_nal_bitmap == (NI_NAL_SPS_BIT | NI_NAL_PPS_BIT)) {
+                    // Packets before the header is sent cannot be decoded.
+                    // So set packet num to zero here.
+                    av_log(avctx, AV_LOG_TRACE,
+                           "xcoder_packet_parse(): Detect SPS, PPS and IDR, "
+                           "enable decoder low delay mode.\n");
+                    s->api_ctx.decoder_low_delay = low_delay;
+                    pkt_nal_bitmap |= NI_GENERATE_ALL_NAL_HEADER_BIT;
+                    low_delay = 0;
+                }
+            }
+
+            // check whether the packet contains SEI NAL after VCL units
+            if (H264_NAL_SEI == nalu_type)
+            {
+                sei_type = *ptr;
+                if (vcl_found || (s->custom_sei_type == sei_type || p_param->dec_input_params.custom_sei_passthru == sei_type))
+                {
+                    // SEI after VCL found or SEI type indicated then extract the SEI unit;
+                    ret = ff_xcoder_extract_custom_sei(avctx, pkt, ptr + 1 - pkt->data, p_packet, sei_type, vcl_found);
+                    if (ret != 0)
+                    {
+                        return ret;
+                    }
+                }
+            }
+            else if ((nalu_type >= H264_NAL_SLICE) && (nalu_type <= H264_NAL_IDR_SLICE))
+            {
+                vcl_found = 1;
+            }
+        } else if (AV_CODEC_ID_HEVC == avctx->codec_id) {
+            nalu_type = (stc >> 1) & 0x3F;
+
+            // check whether the packet is sei alone
+            pkt_sei_alone = (pkt_sei_alone && (HEVC_NAL_SEI_PREFIX == nalu_type || HEVC_NAL_SEI_SUFFIX == nalu_type));
+
+            // Enable decoder low delay mode on sequence change
+            if (low_delay) {
+                switch (nalu_type) {
+                case HEVC_NAL_VPS:
+                    pkt_nal_bitmap |= NI_NAL_VPS_BIT;
+                    break;
+                case HEVC_NAL_SPS:
+                    pkt_nal_bitmap |= NI_NAL_SPS_BIT;
+                    break;
+                case HEVC_NAL_PPS:
+                    pkt_nal_bitmap |= NI_NAL_PPS_BIT;
+                    break;
+                default:
+                    break;
+                }
+
+                if (pkt_nal_bitmap == (NI_NAL_VPS_BIT | NI_NAL_SPS_BIT | NI_NAL_PPS_BIT)) {
+                    av_log(avctx, AV_LOG_TRACE,
+                           "xcoder_packet_parse(): Detect VPS, SPS, PPS and IDR, "
+                           "enable decoder low delay mode.\n");
+                    s->api_ctx.decoder_low_delay = low_delay;
+                    pkt_nal_bitmap |= NI_GENERATE_ALL_NAL_HEADER_BIT;
+                    low_delay = 0;
+                }
+            }
+
+            // check whether the packet contains SEI NAL after VCL units
+            if (HEVC_NAL_SEI_PREFIX == nalu_type || HEVC_NAL_SEI_SUFFIX == nalu_type)
+            {
+                sei_type = *(ptr + 1);
+                if (vcl_found || (s->custom_sei_type == sei_type || p_param->dec_input_params.custom_sei_passthru == sei_type))
+                {
+                    // SEI after VCL found or SEI type indicated then extract the SEI unit;
+                    ret = ff_xcoder_extract_custom_sei(avctx, pkt, ptr + 2 - pkt->data, p_packet, sei_type, vcl_found);
+                    if (ret != 0)
+                    {
+                        return ret;
+                    }
+                }
+            }
+            else if ((nalu_type >= HEVC_NAL_TRAIL_N) && (nalu_type <= HEVC_NAL_RSV_VCL31))
+            {
+              vcl_found = 1;
+            }
+        } 
+        else
+        {
+            av_log(avctx, AV_LOG_DEBUG, "%s() wrong codec %d !\n",
+                   __FUNCTION__, avctx->codec_id);
+            pkt_sei_alone = 0;
+            break;
+        }
+    }
+
+    if (nalu_count > 0)
+    {
+      s->is_lone_sei_pkt = pkt_sei_alone;
+    }
+
+    return 0;
+}
+
+int ff_xcoder_dec_send(AVCodecContext *avctx, XCoderH264DecContext *s, AVPacket *pkt)
+{
+  /* call ni_decoder_session_write to send compressed video packet to the decoder
+     instance */
+  int need_draining = 0;
+  size_t size;
+  ni_packet_t *xpkt = &(s->api_pkt.data.packet);
+  int ret;
+  int sent;
+  int send_size = 0;
+  int new_packet = 0;
+  int extra_prev_size = 0;
+
+  size = pkt->size;
+
+  if (s->flushing)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Decoder is flushing and cannot accept new "
+                                "buffer until all output buffers have been released\n");
+    return AVERROR_EXTERNAL;
+  }
+
+  if (pkt->size == 0)
+  {
+    need_draining = 1;
+  }
+
+  if (s->draining && s->eos)
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "Decoder is draining, eos\n");
+    return AVERROR_EOF;
+  }
+
+  if (xpkt->data_len == 0)
+  {
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 91)
+    AVBSFContext *bsf = avctx->internal->bsf;
+#else
+    AVBSFContext *bsf = avctx->internal->filter.bsfs[0];
+#endif
+    uint8_t *extradata = bsf ? bsf->par_out->extradata : avctx->extradata;
+    int extradata_size = bsf ? bsf->par_out->extradata_size : avctx->extradata_size;
+
+    memset(xpkt, 0, sizeof(ni_packet_t));
+    xpkt->pts = pkt->pts;
+    xpkt->dts = pkt->dts;
+    xpkt->flags        = pkt->flags;
+    xpkt->video_width = avctx->width;
+    xpkt->video_height = avctx->height;
+    xpkt->p_data = NULL;
+    xpkt->data_len = pkt->size;
+
+    if (extradata_size > 0 &&
+        extradata_size != s->extradata_size &&
+        ff_xcoder_add_headers(avctx, pkt, extradata, extradata_size))
+    {
+      s->extradata_size = extradata_size;
+      if (extradata_size > s->api_ctx.max_nvme_io_size * 2)
+      {
+        av_log(avctx, AV_LOG_ERROR, "ff_xcoder_dec_send extradata_size %d "
+               "exceeding max size supported: %d\n", extradata_size,
+               s->api_ctx.max_nvme_io_size * 2);
+      }
+      else
+      {
+        av_log(avctx, AV_LOG_VERBOSE, "ff_xcoder_dec_send extradata_size %d "
+               "copied to pkt start.\n", extradata_size);
+
+        s->api_ctx.prev_size = extradata_size;
+        memcpy(s->api_ctx.p_leftover, extradata, extradata_size);
+      }
+    }
+
+    // If there was lone custom sei in the last packet and the firmware would
+    // fail to recoginze it. So passthrough the custom sei here.
+    if (s->lone_sei_pkt.size > 0)
+    {
+      // No need to check the return value here because the lone_sei_pkt was
+      // parsed before. Here it is only to extract the SEI data.
+      xcoder_packet_parse(avctx, s, &s->lone_sei_pkt, xpkt);
+    }
+
+    ret = xcoder_packet_parse(avctx, s, pkt, xpkt);
+    if (ret != 0)
+    {
+      goto fail;
+    }
+
+    // If the current packet is a lone SEI, save it to be sent with the next
+    // packet. And also check if getting the first packet containing key frame
+    // in decoder low delay mode.
+    if (s->is_lone_sei_pkt)
+    {
+        av_packet_ref(&s->lone_sei_pkt, pkt);
+        xpkt->data_len = 0;
+        free(xpkt->p_custom_sei_set);
+        xpkt->p_custom_sei_set = NULL;
+        if (s->low_delay && s->got_first_key_frame &&
+            !(s->pkt_nal_bitmap & NI_GENERATE_ALL_NAL_HEADER_BIT)) {
+            // Packets before the IDR is sent cannot be decoded. So
+            // set packet num to zero here.
+            s->api_ctx.decoder_low_delay = s->low_delay;
+            s->api_ctx.pkt_num = 0;
+            s->pkt_nal_bitmap |= NI_GENERATE_ALL_NAL_HEADER_BIT;
+            av_log(avctx, AV_LOG_TRACE,
+                   "ff_xcoder_dec_send got first IDR in decoder low delay "
+                   "mode, "
+                   "delay time %dms, pkt_nal_bitmap %d\n",
+                   s->low_delay, s->pkt_nal_bitmap);
+        }
+        av_log(avctx, AV_LOG_TRACE, "ff_xcoder_dec_send pkt lone SEI, saved, "
+               "and return %d\n", pkt->size);
+        return pkt->size;
+    }
+
+    // Send the previous saved lone SEI packet to the decoder
+    if (s->lone_sei_pkt.size > 0)
+    {
+        av_log(avctx, AV_LOG_TRACE, "ff_xcoder_dec_send copy over lone SEI "
+               "data size: %d\n", s->lone_sei_pkt.size);
+        memcpy(s->api_ctx.p_leftover + s->api_ctx.prev_size,
+               s->lone_sei_pkt.data, s->lone_sei_pkt.size);
+        s->api_ctx.prev_size += s->lone_sei_pkt.size;
+        av_packet_unref(&s->lone_sei_pkt);
+    }
+
+    if (pkt->size + s->api_ctx.prev_size > 0)
+    {
+      ni_packet_buffer_alloc(xpkt, (pkt->size + s->api_ctx.prev_size));
+      if (!xpkt->p_data)
+      {
+        ret = AVERROR(ENOMEM);
+        goto fail;
+      }
+    }
+    new_packet = 1;
+  }
+  else
+  {
+    send_size = xpkt->data_len;
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "ff_xcoder_dec_send: pkt->size=%d\n", pkt->size);
+
+  if (s->started == 0)
+  {
+    xpkt->start_of_stream = 1;
+    s->started = 1;
+  }
+
+  if (need_draining && !s->draining)
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "Sending End Of Stream signal\n");
+    xpkt->end_of_stream = 1;
+    xpkt->data_len = 0;
+
+    av_log(avctx, AV_LOG_TRACE, "ni_packet_copy before: size=%d, s->prev_size=%d, send_size=%d (end of stream)\n", pkt->size, s->api_ctx.prev_size, send_size);
+    if (new_packet)
+    {
+      extra_prev_size = s->api_ctx.prev_size;
+      send_size = ni_packet_copy(xpkt->p_data, pkt->data, pkt->size, s->api_ctx.p_leftover, &s->api_ctx.prev_size);
+      // increment offset of data sent to decoder and save it
+      xpkt->pos = (long long)s->offset;
+      s->offset += pkt->size + extra_prev_size;
+    }
+    av_log(avctx, AV_LOG_TRACE, "ni_packet_copy after: size=%d, s->prev_size=%d, send_size=%d, xpkt->data_len=%d (end of stream)\n", pkt->size, s->api_ctx.prev_size, send_size, xpkt->data_len);
+
+    if (send_size < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to copy pkt (status = "
+                                  "%d)\n",
+             send_size);
+      ret = AVERROR_EXTERNAL;
+      goto fail;
+    }
+    xpkt->data_len += extra_prev_size;
+
+    sent = 0;
+    if (xpkt->data_len > 0)
+    {
+      sent = ni_device_session_write(&(s->api_ctx), &(s->api_pkt), NI_DEVICE_TYPE_DECODER);
+    }
+    if (sent < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to send eos signal (status = %d)\n",
+             sent);
+      if (NI_RETCODE_ERROR_VPU_RECOVERY == sent)
+      {
+        ret = xcoder_decode_reset(avctx);
+        if (0 == ret)
+        {
+          ret = AVERROR(EAGAIN);
+        }
+      }
+      else
+      {
+        ret = AVERROR(EIO);
+      }
+      goto fail;
+    }
+    av_log(avctx, AV_LOG_VERBOSE, "Queued eos (status = %d) ts=%llu\n",
+           sent, xpkt->pts);
+    s->draining = 1;
+
+    ni_device_session_flush(&(s->api_ctx), NI_DEVICE_TYPE_DECODER);
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_TRACE, "ni_packet_copy before: size=%d, s->prev_size=%d, send_size=%d\n", pkt->size, s->api_ctx.prev_size, send_size);
+    if (new_packet)
+    {
+      extra_prev_size = s->api_ctx.prev_size;
+      send_size = ni_packet_copy(xpkt->p_data, pkt->data, pkt->size, s->api_ctx.p_leftover, &s->api_ctx.prev_size);
+      // increment offset of data sent to decoder and save it
+      xpkt->pos = (long long)s->offset;
+      s->offset += pkt->size + extra_prev_size;
+    }
+    av_log(avctx, AV_LOG_TRACE, "ni_packet_copy after: size=%d, s->prev_size=%d, send_size=%d, xpkt->data_len=%d\n", pkt->size, s->api_ctx.prev_size, send_size, xpkt->data_len);
+
+    if (send_size < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to copy pkt (status = "
+                                  "%d)\n",
+             send_size);
+      ret = AVERROR_EXTERNAL;
+      goto fail;
+    }
+    xpkt->data_len += extra_prev_size;
+
+    sent = 0;
+    if (xpkt->data_len > 0)
+    {
+      sent = ni_device_session_write(&s->api_ctx, &(s->api_pkt), NI_DEVICE_TYPE_DECODER);
+      av_log(avctx, AV_LOG_VERBOSE, "ff_xcoder_dec_send pts=%" PRIi64 ", dts=%" PRIi64 ", pos=%" PRIi64 ", sent=%d\n", pkt->pts, pkt->dts, pkt->pos, sent);
+    }
+    if (sent < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to send compressed pkt (status = "
+                                  "%d)\n",
+             sent);
+      if (NI_RETCODE_ERROR_VPU_RECOVERY == sent)
+      {
+        ret = xcoder_decode_reset(avctx);
+        if (0 == ret)
+        {
+          ret = AVERROR(EAGAIN);
+        }
+      }
+      else
+      {
+        ret = AVERROR(EIO);
+      }
+      goto fail;
+    }
+    else if (sent == 0)
+    {
+      av_log(avctx, AV_LOG_VERBOSE, "Queued input buffer size=0\n");
+    }
+    else if (sent < size)
+    { /* partial sent; keep trying */
+      av_log(avctx, AV_LOG_VERBOSE, "Queued input buffer size=%d\n", sent);
+    }
+  }
+
+  if (xpkt->data_len == 0)
+  {
+    /* if this packet is done sending, free any sei buffer. */
+    free(xpkt->p_custom_sei_set);
+    xpkt->p_custom_sei_set = NULL;
+  }
+
+  if (sent != 0)
+  {
+    //keep the current pkt to resend next time
+    ni_packet_buffer_free(xpkt);
+    return sent;
+  }
+  else
+  {
+    return AVERROR(EAGAIN);
+  }
+  
+
+fail:
+  ni_packet_buffer_free(xpkt);
+  free(xpkt->p_custom_sei_set);
+  xpkt->p_custom_sei_set = NULL;
+  s->draining = 1;
+  s->eos = 1;
+
+  return ret;
+}
+
+int retrieve_frame(AVCodecContext *avctx, AVFrame *data, int *got_frame,
+                   ni_frame_t *xfme)
+{
+  XCoderH264DecContext *s = avctx->priv_data;
+  ni_xcoder_params_t *p_param =
+      &s->api_param; // dec params in union with enc params struct
+  int num_extra_outputs = (p_param->dec_input_params.enable_out1 > 0) + (p_param->dec_input_params.enable_out2 > 0);
+  uint32_t buf_size = xfme->data_len[0] + xfme->data_len[1] +
+                      xfme->data_len[2] + xfme->data_len[3];
+  uint8_t *buf = xfme->p_data[0];
+  uint8_t *buf1, *buf2;
+  bool is_hw, isnv12frame;
+  int stride = 0;
+  int res = 0;
+  AVHWFramesContext *ctx   = NULL;
+  NIFramesContext *dst_ctx = NULL;
+  AVFrame *frame = data;
+  ni_aux_data_t *aux_data       = NULL;
+  AVFrameSideData *av_side_data = NULL;
+  ni_session_data_io_t session_io_data1;
+  ni_session_data_io_t session_io_data2;
+  ni_session_data_io_t * p_session_data1 = &session_io_data1;
+  ni_session_data_io_t * p_session_data2 = &session_io_data2;
+  niFrameSurface1_t* p_data3;
+  niFrameSurface1_t* p_data3_1;
+  niFrameSurface1_t* p_data3_2;
+
+  av_log(avctx, AV_LOG_TRACE,
+         "retrieve_frame: buf %p data_len [%d %d %d %d] buf_size %u\n", buf,
+         xfme->data_len[0], xfme->data_len[1], xfme->data_len[2],
+         xfme->data_len[3], buf_size);
+
+  memset(p_session_data1, 0, sizeof(ni_session_data_io_t));
+  memset(p_session_data2, 0, sizeof(ni_session_data_io_t));
+
+  isnv12frame = (avctx->sw_pix_fmt == AV_PIX_FMT_NV12 ||
+                 avctx->sw_pix_fmt == AV_PIX_FMT_P010LE);
+
+  if (num_extra_outputs)
+  {
+      ni_frame_buffer_alloc(&(p_session_data1->data.frame), 1,
+                            1, // width height does not matter//codec id does
+                               // not matter//no metadata
+                            1, 0, 1, 1, !isnv12frame);
+      buf1 = p_session_data1->data.frame.p_data[0];
+      if (num_extra_outputs > 1) {
+          ni_frame_buffer_alloc(&(p_session_data2->data.frame), 1,
+                                1, // width height does not matter
+                                1, 0, 1, 1, !isnv12frame);
+          buf2 = p_session_data2->data.frame.p_data[0];
+    }
+  }
+
+  is_hw = xfme->data_len[3] > 0;
+  if (is_hw)
+  {
+    if (frame->hw_frames_ctx) 
+    {
+      ctx = (AVHWFramesContext*)frame->hw_frames_ctx->data;
+      dst_ctx = ctx->internal->priv;
+    }
+    if (s->api_ctx.frame_num == 1)
+    {
+      if (frame->hw_frames_ctx) 
+      {
+        av_log(avctx, AV_LOG_ERROR, "First frame, set hw_frame_context to copy decode sessions threads\n");
+        res = ni_device_session_copy(&s->api_ctx, &dst_ctx->api_ctx);
+        if (NI_RETCODE_SUCCESS != res)
+        {
+          return res;
+        }
+        av_log(avctx, AV_LOG_VERBOSE, "retrieve_frame: blk_io_handle %d device_handle %d\n", s->api_ctx.blk_io_handle, s->api_ctx.device_handle);
+      }
+    }
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "decoding %" PRId64 " frame ...\n", s->api_ctx.frame_num);
+
+  if (avctx->width <= 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "width is not set\n");
+    return AVERROR_INVALIDDATA;
+  }
+  if (avctx->height <= 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "height is not set\n");
+    return AVERROR_INVALIDDATA;
+  }
+
+  stride = s->api_ctx.active_video_width;
+
+  av_log(avctx, AV_LOG_VERBOSE, "XFRAME SIZE: %d, STRIDE: %d\n", buf_size, stride);
+
+  if (!is_hw && (stride == 0 || buf_size < stride * avctx->height))
+  {
+    av_log(avctx, AV_LOG_ERROR, "Packet too small (%d)\n", buf_size);
+    return AVERROR_INVALIDDATA;
+  }
+
+  frame->key_frame = 0;
+  if(xfme->ni_pict_type & 0x10) //key frame marker
+  	frame->key_frame = 1;
+  switch (xfme->ni_pict_type & 0xF)
+  {
+  case DECODER_PIC_TYPE_IDR:
+      frame->key_frame = 1;
+  case PIC_TYPE_I:
+      frame->pict_type = AV_PICTURE_TYPE_I;
+      break;
+  case PIC_TYPE_P:
+      frame->pict_type = AV_PICTURE_TYPE_P;
+      break;
+  case PIC_TYPE_B:
+      frame->pict_type = AV_PICTURE_TYPE_B;
+      break;
+  default:
+      frame->pict_type = AV_PICTURE_TYPE_NONE;
+  }
+
+  res = ff_decode_frame_props(avctx, frame);
+  if (res < 0)
+    return res;
+
+  frame->pkt_pos = avctx->internal->last_pkt_props->pos;
+  frame->pkt_duration = avctx->internal->last_pkt_props->duration;
+
+  if ((res = av_image_check_size(xfme->video_width, xfme->video_height, 0, avctx)) < 0)
+    return res;
+
+  if (is_hw)
+  {
+    frame->buf[0] = av_buffer_create(buf, buf_size, ni_frame_free, NULL, 0);
+    if (num_extra_outputs)
+    {
+        frame->buf[1] =
+            av_buffer_create(buf1, (int)(buf_size / 3), ni_frame_free, NULL, 0);
+        buf1 = frame->buf[1]->data;
+        memcpy(buf1, buf + sizeof(niFrameSurface1_t),
+               sizeof(niFrameSurface1_t)); // copy hwdesc to new buffer
+        if (num_extra_outputs > 1) {
+            frame->buf[2] = av_buffer_create(buf2, (int)(buf_size / 3),
+                                             ni_frame_free, NULL, 0);
+            buf2          = frame->buf[2]->data;
+            memcpy(buf2, buf + 2 * sizeof(niFrameSurface1_t),
+                   sizeof(niFrameSurface1_t));
+        }
+    }
+  }
+  else
+  {
+    frame->buf[0] = av_buffer_create(buf, buf_size, ni_align_free, xfme->dec_buf, 0);
+  }
+  av_log(avctx, AV_LOG_TRACE,
+         "retrieve_frame: is_hw %d frame->buf[0] %p buf %p buf_size %u "
+         "num_extra_outputs %d pkt_duration %ld\n",
+         is_hw, frame->buf[0], buf, buf_size, num_extra_outputs,
+         frame->pkt_duration);
+
+  buf = frame->buf[0]->data;
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+  int i;
+  // retrieve the GStreamer data based on frame's packet offset
+  if (0 == s->api_ctx.frame_pkt_offset)
+  {
+      frame->opaque = s->gs_data[0].opaque;
+      frame->buf[3] = s->gs_data[0].buf0;
+      s->gs_data[0].opaque = NULL;
+      s->gs_data[0].buf0 = NULL;
+
+      av_log(avctx, AV_LOG_DEBUG, "pos 0 pkt opaque %p buf0 %p retrieved\n",
+             frame->opaque, frame->buf[1]);
+  }
+  else
+  {
+      for (i = 0; i < NI_FIFO_SZ; i++)
+      {
+          if (s->api_ctx.frame_pkt_offset >= s->gs_opaque_offsets_index_min[i]
+              && s->api_ctx.frame_pkt_offset < s->gs_opaque_offsets_index[i])
+          {
+              frame->opaque = s->gs_data[i].opaque;
+              frame->buf[3] = s->gs_data[i].buf0;
+              s->gs_data[i].opaque = NULL;
+              s->gs_data[i].buf0 = NULL;
+
+              av_log(avctx, AV_LOG_DEBUG, "pos %d pkt opaque %p buf0 %p retrieved\n",
+                     i, frame->opaque, frame->buf[1]);
+              break;
+          }
+          if (i == NI_FIFO_SZ -1 )
+          {
+              av_log(avctx, AV_LOG_ERROR, "ERROR: NO GS opaque found, consider "
+                                   "increasing NI_FIFO_SZ (%d)!\n", NI_FIFO_SZ);
+          }
+      }
+  }
+#endif
+
+  // retrieve side data if available
+  ni_dec_retrieve_aux_data(xfme);
+
+  // User Data Unregistered SEI if available
+  av_log(avctx, AV_LOG_VERBOSE, "#SEI# UDU (offset=%u len=%u)\n",
+         xfme->sei_user_data_unreg_offset, xfme->sei_user_data_unreg_len);
+  if (xfme->sei_user_data_unreg_offset)
+  {
+      if ((aux_data = ni_frame_get_aux_data(xfme, NI_FRAME_AUX_DATA_UDU_SEI))) {
+          av_side_data = av_frame_new_side_data(
+              frame, AV_FRAME_DATA_NETINT_UDU_SEI, aux_data->size);
+
+          if (!av_side_data) {
+              return AVERROR(ENOMEM);
+          } else {
+              memcpy(av_side_data->data, aux_data->data, aux_data->size);
+          }
+          av_log(avctx, AV_LOG_VERBOSE, "UDU SEI added (len=%d type=5)\n",
+                 xfme->sei_user_data_unreg_len);
+      } else {
+          av_log(avctx, AV_LOG_ERROR, "UDU SEI dropped! (len=%d type=5)\n",
+                 xfme->sei_user_data_unreg_len);
+      }
+  }
+
+  // close caption data if available
+  av_log(avctx, AV_LOG_VERBOSE, "#SEI# CC (offset=%u len=%u)\n",
+         xfme->sei_cc_offset, xfme->sei_cc_len);
+  if ((aux_data = ni_frame_get_aux_data(xfme, NI_FRAME_AUX_DATA_A53_CC))) {
+      av_side_data =
+          av_frame_new_side_data(frame, AV_FRAME_DATA_A53_CC, aux_data->size);
+
+      if (!av_side_data) {
+          return AVERROR(ENOMEM);
+      } else {
+          memcpy(av_side_data->data, aux_data->data, aux_data->size);
+      }
+  }
+
+  // hdr10 sei data if available
+  av_log(avctx, AV_LOG_VERBOSE, "#SEI# MDCV (offset=%u len=%u)\n",
+         xfme->sei_hdr_mastering_display_color_vol_offset,
+         xfme->sei_hdr_mastering_display_color_vol_len);
+  if ((aux_data = ni_frame_get_aux_data(
+           xfme, NI_FRAME_AUX_DATA_MASTERING_DISPLAY_METADATA))) {
+      AVMasteringDisplayMetadata *mdm =
+          av_mastering_display_metadata_create_side_data(frame);
+      if (!mdm) {
+          return AVERROR(ENOMEM);
+      } else {
+          memcpy(mdm, aux_data->data, aux_data->size);
+      }
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "#SEI# CLL (offset=%u len=%u)\n",
+         xfme->sei_hdr_content_light_level_info_offset,
+         xfme->sei_hdr_content_light_level_info_len);
+  if ((aux_data = ni_frame_get_aux_data(
+           xfme, NI_FRAME_AUX_DATA_CONTENT_LIGHT_LEVEL))) {
+      AVContentLightMetadata *clm =
+          av_content_light_metadata_create_side_data(frame);
+      if (!clm) {
+          return AVERROR(ENOMEM);
+      } else {
+          memcpy(clm, aux_data->data, aux_data->size);
+      }
+  }
+
+  // hdr10+ sei data if available
+  av_log(avctx, AV_LOG_VERBOSE, "#SEI# HDR10+ (offset=%u len=%u)\n",
+         xfme->sei_hdr_plus_offset, xfme->sei_hdr_plus_len);
+  if ((aux_data = ni_frame_get_aux_data(xfme, NI_FRAME_AUX_DATA_HDR_PLUS))) {
+      AVDynamicHDRPlus *hdrp = av_dynamic_hdr_plus_create_side_data(frame);
+
+      if (!hdrp) {
+          return AVERROR(ENOMEM);
+      } else {
+          memcpy(hdrp, aux_data->data, aux_data->size);
+      }
+  } // hdr10+ sei
+
+  // remember to clean up auxiliary data of ni_frame after their use
+  ni_frame_wipe_aux_data(xfme);
+
+  if (xfme->p_custom_sei_set)
+  {
+    AVBufferRef *sei_ref = av_buffer_create((uint8_t *)xfme->p_custom_sei_set,
+                                            sizeof(ni_custom_sei_set_t),
+                                            ni_free, NULL, 0);
+    if (! sei_ref ||
+        ! av_frame_new_side_data_from_buf(frame, AV_FRAME_DATA_NETINT_CUSTOM_SEI,
+                                    sei_ref))
+    {
+        return AVERROR(ENOMEM);
+    }
+    xfme->p_custom_sei_set = NULL;
+  }
+
+  frame->pkt_dts = xfme->dts;
+  frame->pts     = xfme->pts;
+  if (xfme->pts != NI_NOPTS_VALUE)
+  {
+      s->current_pts = frame->pts;
+  }
+  else
+  {
+      if (!s->api_ctx.ready_to_close) {
+          s->current_pts += frame->pkt_duration;
+          frame->pts = s->current_pts;
+      }
+  }
+
+  if (is_hw)
+  {
+    p_data3 = (niFrameSurface1_t*)(xfme->p_buffer + xfme->data_len[0] + xfme->data_len[1] + xfme->data_len[2]);
+    frame->data[3] = xfme->p_buffer + xfme->data_len[0] + xfme->data_len[1] + xfme->data_len[2];
+
+    av_log(avctx, AV_LOG_DEBUG, "retrieve_frame: OUT0 data[3] trace ui16FrameIdx = [%d], device_handle=%d bitdep=%d, WxH %d x %d\n",
+           p_data3->ui16FrameIdx,
+           p_data3->device_handle,
+           p_data3->bit_depth,
+           p_data3->ui16width,
+           p_data3->ui16height);
+    
+    if (num_extra_outputs)
+    {
+      p_data3_1 = (niFrameSurface1_t*)buf1;
+      //p_data3_1->bit_depth = (p_data3_1->bit_depth == 1) ? 1 : (int8_t)s->api_ctx.bit_depth_factor;
+      //p_data3_1->no_crop = 1;
+      av_log(avctx, AV_LOG_DEBUG, "retrieve_frame: OUT1 data[3] trace ui16FrameIdx = [%d], device_handle=%d bitdep=%d, WxH %d x %d\n",
+             p_data3_1->ui16FrameIdx,
+             p_data3_1->device_handle,
+             p_data3_1->bit_depth,
+             p_data3_1->ui16width,
+             p_data3_1->ui16height);
+      if (num_extra_outputs > 1)
+      {
+        p_data3_2 = (niFrameSurface1_t*)buf2;
+        //p_data3_2->bit_depth = (p_data3_2->bit_depth == 1) ? 1 : (int8_t)s->api_ctx.bit_depth_factor;
+        //p_data3_2->no_crop = 1;
+        av_log(avctx, AV_LOG_DEBUG, "retrieve_frame: OUT2 data[3] trace ui16FrameIdx = [%d], device_handle=%d bitdep=%d, WxH %d x %d\n",
+               p_data3_2->ui16FrameIdx,
+               p_data3_2->device_handle,
+               p_data3_2->bit_depth,
+               p_data3_2->ui16width,
+               p_data3_2->ui16height);
+      }
+    }
+  }
+  av_log(avctx, AV_LOG_VERBOSE, "retrieve_frame: frame->buf[0]=%p, frame->data=%p, frame->pts=%" PRId64 ", frame size=%d, s->current_pts=%" PRId64 ", frame->pkt_pos=%" PRId64 ", frame->pkt_duration=%" PRId64 " sei size %d offset %u\n",
+         frame->buf[0], frame->data, frame->pts,
+         buf_size, s->current_pts, frame->pkt_pos,
+         frame->pkt_duration, xfme->sei_cc_len, xfme->sei_cc_offset);
+
+  /* av_buffer_ref(avpkt->buf); */
+  if (!frame->buf[0])
+    return AVERROR(ENOMEM);
+
+  if (!is_hw &&
+      ((res = av_image_fill_arrays(
+            frame->data, frame->linesize, buf, avctx->sw_pix_fmt,
+            (int)(s->api_ctx.active_video_width / s->api_ctx.bit_depth_factor),
+            s->api_ctx.active_video_height, 1)) < 0)) {
+      av_buffer_unref(&frame->buf[0]);
+      return res;
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "retrieve_frame: success av_image_fill_arrays "
+         "return %d\n", res);
+  if (QUADRA)
+  {
+    if (!is_hw)
+    {
+      //frame->linesize[0] = (((frame->width * s->api_ctx.bit_depth_factor) + 63) / 64) * 64;
+      //frame->linesize[1] = frame->linesize[2] = (((frame->width / 2 * s->api_ctx.bit_depth_factor) + 63) / 64) * 64;
+      frame->linesize[1] = frame->linesize[2] = (((frame->width / ((isnv12frame) ? 1 : 2) * s->api_ctx.bit_depth_factor) + 127) / 128) * 128;
+      frame->linesize[2] = (isnv12frame) ? 0 : frame->linesize[1];
+      //frame->data[1] = frame->data[0] + (frame->linesize[0] * frame->height);
+      frame->data[2] = (isnv12frame) ? 0 : frame->data[1] + (frame->linesize[1] * frame->height / 2);
+    }
+  }
+  else
+  {
+    frame->width = s->api_ctx.active_video_width;
+    frame->height = s->api_ctx.active_video_height;
+  }  
+
+  frame->crop_top = xfme->crop_top;
+  if (QUADRA)
+  {
+    frame->crop_bottom = frame->height - xfme->crop_bottom; // ppu auto crop should have cropped out padding, crop_bottom should be 0
+  }
+  else
+  {
+    frame->crop_bottom = s->api_ctx.active_video_height - xfme->crop_bottom;  
+  }
+  frame->crop_left = xfme->crop_left;
+  if (QUADRA)
+  {
+    frame->crop_right = frame->width - xfme->crop_right; // ppu auto crop should have cropped out padding, crop_right should be 0
+  }
+  else
+  {
+    frame->crop_right = s->api_ctx.active_video_width - xfme->crop_right;
+  }
+
+#if 0
+  frame->crop_top =    0;//PPu autocrops out padding //xfme->crop_top;
+  frame->crop_bottom = 0;//PPu autocrops out padding //s->api_ctx.active_video_height - xfme->crop_bottom;
+  frame->crop_left =   0;//PPu autocrops out padding //xfme->crop_left;
+  frame->crop_right =  0;//PPu autocrops out padding //s->api_ctx.active_video_width - xfme->crop_right;
+#endif
+
+#if 0
+  if(p_param->dec_input_params.crop_mode[0] != NI_DEC_CROP_MODE_DISABLE && p_param->dec_input_params.scale_wh[0][0])
+  {
+    frame->crop_top = frame->crop_left = frame->crop_right = frame->crop_bottom = 0; //Quadra has already cropped
+  }
+#endif
+
+  if (is_hw && frame->hw_frames_ctx && dst_ctx != NULL) {
+      av_log(avctx, AV_LOG_TRACE,
+             "retrieve_frame: hw_frames_ctx av_buffer_get_ref_count=%d\n",
+             av_buffer_get_ref_count(frame->hw_frames_ctx));
+      // dst_ctx->pc_height = frame->height;
+      // dst_ctx->pc_crop_bottom = frame->crop_bottom;
+      // dst_ctx->pc_width = frame->width;
+      // dst_ctx->pc_crop_right = frame->crop_right;
+      dst_ctx->split_ctx.enabled = (num_extra_outputs >= 1) ? 1 : 0;
+      dst_ctx->split_ctx.w[0]    = p_data3->ui16width;
+      dst_ctx->split_ctx.h[0]    = p_data3->ui16height;
+      dst_ctx->split_ctx.f[0]    = (int)p_data3->encoding_type;
+      dst_ctx->split_ctx.f8b[0]  = (int)p_data3->bit_depth;
+      dst_ctx->split_ctx.w[1] =
+          (num_extra_outputs >= 1) ? p_data3_1->ui16width : 0;
+      dst_ctx->split_ctx.h[1] =
+          (num_extra_outputs >= 1) ? p_data3_1->ui16height : 0;
+      dst_ctx->split_ctx.f[1] =
+          (num_extra_outputs >= 1) ? p_data3_1->encoding_type : 0;
+      dst_ctx->split_ctx.f8b[1] =
+          (num_extra_outputs >= 1) ? p_data3_1->bit_depth : 0;
+      dst_ctx->split_ctx.w[2] =
+          (num_extra_outputs == 2) ? p_data3_2->ui16width : 0;
+      dst_ctx->split_ctx.h[2] =
+          (num_extra_outputs == 2) ? p_data3_2->ui16height : 0;
+      dst_ctx->split_ctx.f[2] =
+          (num_extra_outputs == 2) ? p_data3_2->encoding_type : 0;
+      dst_ctx->split_ctx.f8b[2] =
+          (num_extra_outputs == 2) ? p_data3_2->bit_depth : 0;
+#if 0
+    for (i = 0; i < num_extra_outputs + 1; i++)
+    {
+      bool use_out_2 = (p_param->dec_input_params.enable_out1 == 0 && i == 1); //in case out0 enabled, out1 disabled out2 enabled
+      if (p_param->dec_input_params.crop_mode[i + use_out_2] == NI_DEC_CROP_MODE_DISABLE && 
+          p_param->dec_input_params.scale_wh[i + use_out_2][0] == 0)
+      {//crop info from decoded headers
+        dst_ctx->split_ctx.crop_meta_data_rltb[i][0] = frame->crop_right;
+        dst_ctx->split_ctx.crop_meta_data_rltb[i][1] = frame->crop_left;
+        dst_ctx->split_ctx.crop_meta_data_rltb[i][2] = frame->crop_top;
+        dst_ctx->split_ctx.crop_meta_data_rltb[i][3] = frame->crop_bottom; 
+      }
+      else
+      {//crop has been applied, no offsets, just the window
+        dst_ctx->split_ctx.crop_meta_data_rltb[i][0] = 0;
+        dst_ctx->split_ctx.crop_meta_data_rltb[i][1] = 0;
+        dst_ctx->split_ctx.crop_meta_data_rltb[i][2] = 0;
+        dst_ctx->split_ctx.crop_meta_data_rltb[i][3] = 0;
+      }
+    }
+#endif
+  }
+  *got_frame = 1;
+  return buf_size;
+}
+
+int ff_xcoder_dec_receive(AVCodecContext *avctx, XCoderH264DecContext *s,
+                          AVFrame *frame, bool wait)
+{
+  /* call xcode_dec_receive to get a decoded YUV frame from the decoder
+     instance */
+  int ret = 0;
+  int got_frame = 0;
+  ni_session_data_io_t session_io_data;
+  ni_session_data_io_t * p_session_data = &session_io_data;
+  int alloc_mem, height, actual_width, cropped_width, cropped_height;
+  bool isnv12frame, bSequenceChange = 0;
+
+  if (s->draining && s->eos)
+  {
+    return AVERROR_EOF;
+  }
+  // TODO: 
+  // (HW_FRAME_EXTRA_BUFFER-1) where FW defines HW_FRAME_EXTRA_BUFFER 3
+  // may need to pass it as a param to FW
+  memset(p_session_data, 0, sizeof(ni_session_data_io_t));
+  if (s->api_ctx.frame_num % 2 == 0)
+  {
+    s->api_ctx.burst_control = (s->api_ctx.burst_control == 0 ? 1 : 0); //toggle
+  }
+  if (s->api_ctx.burst_control)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "ff_xcoder_dec_receive burst return%" PRId64 " frame\n", s->api_ctx.frame_num);
+    return AVERROR(EAGAIN);
+  }
+
+  // if active video resolution has been obtained we just use it as it's the
+  // exact size of frame to be returned, otherwise we use what we are told by
+  // upper stream as the initial setting and it will be adjusted.
+  // width = s->api_ctx.active_video_width > 0 ? s->api_ctx.active_video_width :
+  // avctx->width;
+  height =
+      (int)(s->api_ctx.active_video_height > 0 ? s->api_ctx.active_video_height
+                                               : avctx->height);
+  actual_width =
+      (int)(s->api_ctx.actual_video_width > 0 ? s->api_ctx.actual_video_width
+                                              : avctx->width);
+
+  // allocate memory only after resolution is known (buffer pool set up)
+  alloc_mem = (s->api_ctx.active_video_width > 0 && 
+               s->api_ctx.active_video_height > 0 ? 1 : 0);
+  isnv12frame = (avctx->sw_pix_fmt == AV_PIX_FMT_NV12 ||
+                 avctx->sw_pix_fmt == AV_PIX_FMT_P010LE);
+  if (avctx->pix_fmt != AV_PIX_FMT_NI_QUAD)
+  {
+      ret = ni_decoder_frame_buffer_alloc(
+          s->api_ctx.dec_fme_buf_pool, &(p_session_data->data.frame), alloc_mem,
+          actual_width, height, (avctx->codec_id == AV_CODEC_ID_H264),
+          s->api_ctx.bit_depth_factor, !isnv12frame);
+  }
+  else
+  {
+      ret = ni_frame_buffer_alloc(&(p_session_data->data.frame), actual_width,
+                                  height, (avctx->codec_id == AV_CODEC_ID_H264),
+                                  1, s->api_ctx.bit_depth_factor, 3,
+                                  !isnv12frame);
+  }
+
+  if (NI_RETCODE_SUCCESS != ret)
+  {
+    return AVERROR_EXTERNAL;
+  }
+
+  if (avctx->pix_fmt != AV_PIX_FMT_NI_QUAD)
+  {
+    ret = ni_device_session_read(&s->api_ctx, p_session_data, NI_DEVICE_TYPE_DECODER);
+  }
+  else
+  {
+    ret = ni_device_session_read_hwdesc(&s->api_ctx, p_session_data, NI_DEVICE_TYPE_DECODER);
+  }
+
+  if (ret == 0)
+  {
+
+    s->eos = p_session_data->data.frame.end_of_stream;
+    if (avctx->pix_fmt != AV_PIX_FMT_NI_QUAD) {
+        ni_decoder_frame_buffer_free(&(p_session_data->data.frame));
+    } else {
+        ni_frame_buffer_free(&(p_session_data->data.frame));
+    }
+    return AVERROR(EAGAIN);
+
+  }
+  else if (ret > 0)
+  {
+    int dec_ff_pix_fmt;
+
+    if (p_session_data->data.frame.flags & AV_PKT_FLAG_DISCARD) {
+        av_log(avctx, AV_LOG_DEBUG,
+               "Current frame is dropped when AV_PKT_FLAG_DISCARD is set\n");
+        if (avctx->pix_fmt != AV_PIX_FMT_NI_QUAD) {
+            ni_decoder_frame_buffer_free(&(p_session_data->data.frame));
+        } else {
+            ni_frame_free(NULL, p_session_data->data.frame.p_data[0]); // recycle frame mem bin buffer & free p_buffer
+        }
+        return AVERROR(EAGAIN);
+    }
+
+    av_log(avctx, AV_LOG_VERBOSE, "Got output buffer pts=%lld "
+                                  "dts=%lld eos=%d sos=%d\n",
+           p_session_data->data.frame.pts, p_session_data->data.frame.dts,
+           p_session_data->data.frame.end_of_stream, p_session_data->data.frame.start_of_stream);
+
+    s->eos = p_session_data->data.frame.end_of_stream;
+
+    // update ctxt resolution if change has been detected
+    frame->width = cropped_width = p_session_data->data.frame.video_width; // ppu auto crop reports wdith as cropped width
+    frame->height = cropped_height = p_session_data->data.frame.video_height; // ppu auto crop reports heigth as cropped height
+    //cropped_width = p_session_data->data.frame.crop_right;
+    //cropped_height = p_session_data->data.frame.crop_bottom;
+
+    if (cropped_width != avctx->width || cropped_height != avctx->height)
+
+    {
+      av_log(avctx, AV_LOG_WARNING, "ff_xcoder_dec_receive: resolution "
+             "changed: %dx%d to %dx%d\n", avctx->width, avctx->height,
+            cropped_width, cropped_height);
+      avctx->width = cropped_width;
+      avctx->height = cropped_height;
+      bSequenceChange = 1;
+    }
+
+    dec_ff_pix_fmt = ni_pix_fmt_2_ff_pix_fmt(s->api_ctx.pixel_format);
+
+    // If the codec is Jpeg or color range detected is a full range,
+    // yuv420p from xxx_ni_quadra_dec means a full range.
+    // Change it to yuvj420p so that FFmpeg can process it as a full range.
+    if ((avctx->pix_fmt != AV_PIX_FMT_NI_QUAD) &&
+        (dec_ff_pix_fmt == AV_PIX_FMT_YUV420P) &&
+        ((avctx->codec_id == AV_CODEC_ID_MJPEG) ||
+         (avctx->color_range == AVCOL_RANGE_JPEG)))
+    {
+       avctx->sw_pix_fmt = avctx->pix_fmt = dec_ff_pix_fmt = AV_PIX_FMT_YUVJ420P;
+       avctx->color_range = AVCOL_RANGE_JPEG;
+    }
+
+    if (avctx->sw_pix_fmt != dec_ff_pix_fmt)
+    {
+      av_log(avctx, AV_LOG_VERBOSE, "update sw_pix_fmt from %d to %d\n",
+             avctx->sw_pix_fmt, dec_ff_pix_fmt);
+      avctx->sw_pix_fmt = dec_ff_pix_fmt;
+      if (avctx->pix_fmt != AV_PIX_FMT_NI_QUAD)
+      {
+        avctx->pix_fmt = avctx->sw_pix_fmt;
+      }
+      bSequenceChange = 1;
+    }
+    
+    frame->format = avctx->pix_fmt; 
+
+    av_log(avctx, AV_LOG_VERBOSE, "ff_xcoder_dec_receive: frame->format %d, sw_pix_fmt = %d\n", frame->format, avctx->sw_pix_fmt);
+
+    if (avctx->pix_fmt == AV_PIX_FMT_NI_QUAD)
+    {
+      if (bSequenceChange)
+      {
+        AVHWFramesContext *ctx;
+        NIFramesContext *dst_ctx;
+
+        av_buffer_unref(&avctx->hw_frames_ctx);
+        avctx->hw_frames_ctx = av_hwframe_ctx_alloc(avctx->hw_device_ctx);
+        if (!avctx->hw_frames_ctx) 
+        {
+          ret = AVERROR(ENOMEM);
+          return ret;
+        }
+        
+        s->frames = (AVHWFramesContext*)avctx->hw_frames_ctx->data;
+        s->frames->format = AV_PIX_FMT_NI_QUAD;
+        s->frames->width     = avctx->width;
+        s->frames->height     = avctx->height;
+        s->frames->sw_format = avctx->sw_pix_fmt;
+        s->frames->initial_pool_size = -1; //Decoder has its own dedicated pool
+        ret = av_hwframe_ctx_init(avctx->hw_frames_ctx);
+        if (ret < 0)
+        {
+          return ret;
+        }
+        
+        ctx = (AVHWFramesContext*)avctx->hw_frames_ctx->data;
+        dst_ctx = ctx->internal->priv;
+        av_log(avctx, AV_LOG_ERROR, "ff_xcoder_dec_receive: sequence change, set hw_frame_context to copy decode sessions threads\n");
+        ret = ni_device_session_copy(&s->api_ctx, &dst_ctx->api_ctx);
+        if (NI_RETCODE_SUCCESS != ret)
+        {
+          return ret;
+        }
+      }
+      frame->hw_frames_ctx = av_buffer_ref(avctx->hw_frames_ctx);
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+      /* Set the hw_id/card number */
+      AVHWFramesContext *hwframes = (AVHWFramesContext*)avctx->hw_frames_ctx->data;
+      AVNIFramesContext *ni_hw_ctx;
+      ni_hw_ctx = (AVNIFramesContext *)hwframes->hwctx;
+      ni_hw_ctx->dev_dec_idx = s->dev_dec_idx;
+#else
+      /* Set the hw_id/card number in the opaque field */
+      // NOLINTNEXTLINE(clang-diagnostic-int-to-void-pointer-cast)
+      frame->opaque = (void *)s->dev_dec_idx;
+#endif
+    }
+    if (s->api_ctx.frame_num == 1)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "NI:%splanar:out\n", (isnv12frame) ? "semi" : "");
+    }
+    retrieve_frame(avctx, frame, &got_frame, &(p_session_data->data.frame));
+    
+    av_log(avctx, AV_LOG_VERBOSE, "ff_xcoder_dec_receive: got_frame=%d, frame->width=%d, frame->height=%d, crop top %" SIZE_SPECIFIER " bottom %" SIZE_SPECIFIER " left %" SIZE_SPECIFIER " right %" SIZE_SPECIFIER ", frame->format=%d, frame->linesize=%d/%d/%d\n", got_frame, frame->width, frame->height, frame->crop_top, frame->crop_bottom, frame->crop_left, frame->crop_right, frame->format, frame->linesize[0], frame->linesize[1], frame->linesize[2]);
+
+#if FF_API_PKT_PTS
+    FF_DISABLE_DEPRECATION_WARNINGS
+    frame->pkt_pts = frame->pts;
+    FF_ENABLE_DEPRECATION_WARNINGS
+#endif
+    frame->best_effort_timestamp = frame->pts;
+
+    av_log(avctx, AV_LOG_VERBOSE, "ff_xcoder_dec_receive: pkt_timebase= %d/%d, frame_rate=%d/%d, frame->pts=%" PRId64 ", frame->pkt_dts=%" PRId64 "\n", avctx->pkt_timebase.num, avctx->pkt_timebase.den, avctx->framerate.num, avctx->framerate.den, frame->pts, frame->pkt_dts);
+
+    // release buffer ownership and let frame owner return frame buffer to 
+    // buffer pool later
+    p_session_data->data.frame.dec_buf = NULL;
+
+    free(p_session_data->data.frame.p_custom_sei_set);
+    p_session_data->data.frame.p_custom_sei_set = NULL;
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to get output buffer (status = %d)\n",
+           ret);
+    
+    if (NI_RETCODE_ERROR_VPU_RECOVERY == ret)
+    {
+      av_log(avctx, AV_LOG_WARNING, "ff_xcoder_dec_receive VPU recovery, need to reset ..\n");
+      ni_decoder_frame_buffer_free(&(p_session_data->data.frame));
+      return ret;
+    }
+
+    return AVERROR(EIO);
+  }
+
+  ret = 0;
+
+  return ret;
+}
+
+int ff_xcoder_dec_is_flushing(AVCodecContext *avctx, XCoderH264DecContext *s)
+{
+  return s->flushing;
+}
+
+int ff_xcoder_dec_flush(AVCodecContext *avctx, XCoderH264DecContext *s)
+{
+  s->draining = 0;
+  s->flushing = 0;
+  s->eos = 0;
+
+#if 0
+  int ret;
+  ret = ni_device_session_flush(s, NI_DEVICE_TYPE_DECODER);
+  if (ret < 0) {
+    av_log(avctx, AV_LOG_ERROR, "Failed to flush decoder (status = %d)\n", ret);
+    return AVERROR_EXTERNAL;
+  }
+#endif
+
+  /* Future: for now, always return 1 to indicate the codec has been flushed
+     and it leaves the flushing state and can process again ! will consider
+     case of user retaining frames in HW "surface" usage */
+  return 1;
+}
diff --git a/libavcodec/nicodec.h b/libavcodec/nicodec.h
new file mode 100644
index 0000000000..f8742f8f0f
--- /dev/null
+++ b/libavcodec/nicodec.h
@@ -0,0 +1,195 @@
+/*
+ * XCoder Codec Lib Wrapper
+ * Copyright (c) 2018 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder codec lib wrapper header.
+ */
+
+#ifndef AVCODEC_NICODEC_H
+#define AVCODEC_NICODEC_H
+
+#include <stdbool.h>
+#include <time.h>
+#include "avcodec.h"
+#include "libavutil/fifo.h"
+
+#include <ni_device_api.h>
+#include "libavutil/hwcontext_ni_quad.h"
+#include "libavutil/hwcontext.h"
+
+#define NI_NAL_VPS_BIT (0x01)
+#define NI_NAL_SPS_BIT (0x01 << 1)
+#define NI_NAL_PPS_BIT (0x01 << 2)
+#define NI_GENERATE_ALL_NAL_HEADER_BIT (0x01 << 3)
+
+/* enum for specifying xcoder device/coder index; can be specified in either
+   decoder or encoder options. */
+enum {
+  BEST_DEVICE_INST = -2,
+  BEST_DEVICE_LOAD = -1
+};
+
+enum {
+    HW_FRAMES_OFF = 0,
+    HW_FRAMES_ON = 1
+};
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+typedef struct _GsData {
+    void *opaque;
+    AVBufferRef *buf0;
+} GsData;
+#endif
+
+typedef struct XCoderH264DecContext {
+  AVClass *avclass;
+
+  char *dev_xcoder_name;          /* dev name of the xcoder card to use */
+  char *blk_xcoder_name;          /* blk name of the xcoder card to use */
+  int dev_dec_idx;                /* index of the decoder on the xcoder card */
+  int keep_alive_timeout;         /* keep alive timeout setting */
+  ni_device_context_t *rsrc_ctx;  /* resource management context */
+
+  ni_session_context_t api_ctx;
+  ni_xcoder_params_t api_param;
+  ni_session_data_io_t api_pkt;
+
+  AVPacket buffered_pkt;
+  AVPacket lone_sei_pkt;
+  
+  // stream header copied/saved from AVCodecContext.extradata
+  int got_first_key_frame;
+  uint8_t *extradata;
+  int extradata_size;
+
+  int64_t current_pts;
+  unsigned long long offset;
+
+  int started;
+  int draining;
+  int flushing;
+  int is_lone_sei_pkt;
+  int eos;
+  AVHWFramesContext    *frames;
+
+  /* below are all command line options */
+  char *xcoder_opts;
+  int enable_user_data_sei_passthru;
+  int custom_sei_type;
+  int low_delay;
+  int pkt_nal_bitmap;
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+  // GStreamer support: use pkt offset to save/retrieve associated GS data
+  void *cur_gs_opaque;
+  AVBufferRef *cur_gs_buf0;
+  GsData gs_data[NI_FIFO_SZ];
+  uint64_t gs_opaque_offsets_index_min[NI_FIFO_SZ];
+  uint64_t gs_opaque_offsets_index[NI_FIFO_SZ];
+#endif
+} XCoderH264DecContext;
+
+typedef struct XCoderH265EncContext {
+  AVClass *avclass;
+
+  char *dev_xcoder_name;          /* dev name of the xcoder card to use */
+  char *blk_xcoder_name;          /* blk name of the xcoder card to use */
+  int dev_enc_idx;                /* index of the encoder on the xcoder card */
+  int nvme_io_size;               /* custom nvme io size */
+  int keep_alive_timeout;         /* keep alive timeout setting */
+  ni_device_context_t *rsrc_ctx;  /* resource management context */
+  unsigned long xcode_load_pixel; /* xcode load in pixels by this encode task */
+  
+  // frame fifo, to be used for sequence change frame buffering
+  AVFifoBuffer *fme_fifo;
+  int eos_fme_received;
+  AVFrame buffered_fme; // buffered frame for sequence change handling
+
+  ni_session_data_io_t  api_pkt; /* used for receiving bitstream from xcoder */
+  ni_session_data_io_t   api_fme; /* used for sending YUV data to xcoder */
+  ni_session_context_t api_ctx;
+  ni_xcoder_params_t api_param;
+
+  int started;
+  uint8_t *p_spsPpsHdr;
+  int spsPpsHdrLen;
+  int spsPpsArrived;
+  int firstPktArrived;
+  int64_t dtsOffset;
+  int gop_offset_count;/*this is a counter to guess the pts only dtsOffset times*/
+  uint64_t total_frames_received;
+  int64_t first_frame_pts;
+  int64_t latest_dts;
+
+  int encoder_flushing;
+  int encoder_eof;
+  
+  // ROI
+  int roi_side_data_size;
+  AVRegionOfInterest *av_rois;  // last passed in AVRegionOfInterest
+  int nb_rois;
+
+  /* backup copy of original values of -enc command line option */
+  int  orig_dev_enc_idx;
+
+  AVFrame *sframe_pool[MAX_NUM_FRAMEPOOL_HWAVFRAME];
+  int aFree_Avframes_list[MAX_NUM_FRAMEPOOL_HWAVFRAME + 1];
+  int freeHead;
+  int freeTail;
+
+  /* below are all command line options */
+  char *xcoder_opts;
+  char *xcoder_gop;
+
+  int reconfigCount;
+  // actual enc_change_params is in ni_session_context !
+
+} XCoderH265EncContext;
+
+int ff_xcoder_dec_close(AVCodecContext *avctx,
+                        XCoderH264DecContext *s);
+
+int ff_xcoder_dec_init(AVCodecContext *avctx,
+                       XCoderH264DecContext *s);
+
+int ff_xcoder_dec_send(AVCodecContext *avctx,
+                       XCoderH264DecContext *s,
+                       AVPacket *pkt);
+
+int ff_xcoder_dec_receive(AVCodecContext *avctx,
+                          XCoderH264DecContext *s,
+                          AVFrame *frame,
+                          bool wait);
+
+int ff_xcoder_dec_is_flushing(AVCodecContext *avctx,
+                              XCoderH264DecContext *s);
+
+int ff_xcoder_dec_flush(AVCodecContext *avctx,
+                        XCoderH264DecContext *s);
+
+int parse_symbolic_decoder_param(XCoderH264DecContext *s);
+
+int retrieve_frame(AVCodecContext *avctx, AVFrame *data, int *got_frame,
+                   ni_frame_t *xfme);
+int ff_xcoder_add_headers(AVCodecContext *avctx, AVPacket *pkt,
+                          uint8_t *extradata, int extradata_size);
+#endif /* AVCODEC_NICODEC_H */
diff --git a/libavcodec/nicodec_logan.c b/libavcodec/nicodec_logan.c
new file mode 100644
index 0000000000..42eff83077
--- /dev/null
+++ b/libavcodec/nicodec_logan.c
@@ -0,0 +1,1788 @@
+/*
+ * XCoder Codec Lib Wrapper
+ * Copyright (c) 2018 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder codec lib wrapper.
+ */
+
+#include <ni_rsrc_api_logan.h>
+#include <ni_av_codec_logan.h>
+#include "nicodec_logan.h"
+#include "nidec_logan.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/mastering_display_metadata.h"
+#include "libavutil/hdr_dynamic_metadata.h"
+#include "get_bits.h"
+#include "internal.h"
+#include "libavutil/intreadwrite.h"
+#include "libavcodec/hevc.h"
+#include "libavcodec/hevc_sei.h"
+#include "libavcodec/h264.h"
+#include "libavcodec/h264_sei.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/hwcontext_ni_logan.h"
+
+static inline void ni_logan_buf_pool_free(void *opaque, uint8_t *data)
+{
+  if (data)
+  {
+    ni_logan_buf_t *buf = (ni_logan_buf_t *)opaque;
+    ni_logan_decoder_frame_buffer_pool_return_buf(buf, (ni_logan_buf_pool_t *)buf->pool);
+  }
+}
+
+static void ni_logan_frame_free(void *opaque, uint8_t *data)
+{
+  if (data)
+  {
+    ni_event_handle_t event_handle = (ni_event_handle_t) opaque;
+    ni_logan_hwframe_surface_t* p_data3 = (ni_logan_hwframe_surface_t*) data; //for hwframes there is no data0,1,2
+    //TODO use int32t device_handle to kill the buffer!
+    if (p_data3->i8FrameIdx != NI_LOGAN_INVALID_HW_FRAME_IDX)
+    {
+#ifdef _WIN32
+      int64_t handle = (((int64_t) p_data3->device_handle_ext) << 32) | p_data3->device_handle;
+      ni_logan_decode_buffer_free(p_data3, (ni_device_handle_t) handle, event_handle);
+#else
+      ni_logan_decode_buffer_free(p_data3, p_data3->device_handle, event_handle);
+#endif
+    }
+    free(p_data3);
+  }
+}
+
+static inline void ni_logan_align_free_nop(void *opaque, uint8_t *data)
+{
+}
+
+static inline void ni_logan_free(void *opaque, uint8_t *data)
+{
+  free(data);
+}
+
+int ff_xcoder_logan_dec_init(AVCodecContext *avctx,
+                             XCoderLoganDecContext *s)
+{
+  /* ToDo: call xcode_dec_open to open a decoder instance */
+  int ret;
+  ni_logan_encoder_params_t *p_param = &s->api_param;
+
+  s->api_ctx.hw_id = s->dev_dec_idx;
+  if(s->low_delay)
+  {
+    s->api_ctx.decoder_low_delay = s->low_delay;
+  }
+  else
+  {
+    s->api_ctx.decoder_low_delay = s->api_param.dec_input_params.lowdelay;
+  }
+  strcpy(s->api_ctx.dev_xcoder, s->dev_xcoder);
+
+  ret = ni_logan_device_session_open(&s->api_ctx, NI_LOGAN_DEVICE_TYPE_DECODER);
+  if (ret != 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to open decoder (status = %d), "
+           "resource unavailable\n", ret);
+    ret = AVERROR_EXTERNAL;
+    ff_xcoder_logan_dec_close(avctx, s);
+  }
+  else
+  {
+    s->dev_xcoder_name = s->api_ctx.dev_xcoder_name;
+    s->blk_xcoder_name = s->api_ctx.blk_xcoder_name;
+    s->dev_dec_idx = s->api_ctx.hw_id;
+    av_log(avctx, AV_LOG_VERBOSE, "XCoder %s Index %d (inst: %d) opened successfully\n",
+           s->dev_xcoder_name, s->dev_dec_idx, s->api_ctx.session_id);
+
+    if (s->hwFrames || p_param->dec_input_params.hwframes)
+    {
+      if (!avctx->hw_device_ctx)
+      {
+        av_log(avctx, AV_LOG_DEBUG, "nicodec.c:ff_xcoder_logan_dec_init() hwdevice_ctx_create\n");
+
+        av_hwdevice_ctx_create(&avctx->hw_device_ctx, AV_HWDEVICE_TYPE_NI_LOGAN, NULL, NULL, 0); //create with null device
+      }
+      if (!avctx->hw_frames_ctx)
+      {
+        avctx->hw_frames_ctx = av_hwframe_ctx_alloc(avctx->hw_device_ctx);
+
+        if (!avctx->hw_frames_ctx)
+        {
+          ret = AVERROR(ENOMEM);
+          return ret;
+        }
+      }
+      s->hwfc = (AVHWFramesContext*)avctx->hw_frames_ctx->data;
+
+      s->hwfc->format = AV_PIX_FMT_NI_LOGAN;
+      s->hwfc->width = avctx->width;
+      s->hwfc->height = avctx->height;
+
+      s->hwfc->sw_format = avctx->sw_pix_fmt;
+      s->hwfc->initial_pool_size = -1; //Decoder has its own dedicated pool
+
+      ret = av_hwframe_ctx_init(avctx->hw_frames_ctx);
+      avctx->pix_fmt = AV_PIX_FMT_NI_LOGAN;
+      s->api_ctx.hw_action = NI_LOGAN_CODEC_HW_ENABLE;
+    }
+    else
+    {
+      avctx->pix_fmt = avctx->sw_pix_fmt; //reassign in case above conditions alter value
+      s->api_ctx.hw_action = NI_LOGAN_CODEC_HW_NONE;//
+    }
+  }
+
+  return ret;
+}
+
+int ff_xcoder_logan_dec_close(AVCodecContext *avctx, XCoderLoganDecContext *s)
+{
+  AVHWFramesContext *ctx;
+  NILOGANFramesContext *dst_ctx;
+  ni_logan_retcode_t ret = NI_LOGAN_RETCODE_FAILURE;
+  ni_logan_encoder_params_t *p_param = &s->api_param; //dec params in union with enc params struct
+
+  ret = ni_logan_device_session_close(&s->api_ctx, s->eos, NI_LOGAN_DEVICE_TYPE_DECODER);
+  if (NI_LOGAN_RETCODE_SUCCESS != ret)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to close Decode Session (status = %d)\n", ret);
+  }
+
+  if (p_param->dec_input_params.hwframes)
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "File BLK handle %d close suspended to frames Uninit\n", s->api_ctx.blk_io_handle); //suspended_device_handle
+    ctx = (AVHWFramesContext*)avctx->hw_frames_ctx->data;
+    dst_ctx = ctx->internal->priv;
+    dst_ctx->suspended_device_handle = s->api_ctx.blk_io_handle;
+
+#ifdef __linux__
+    ni_logan_device_close(s->api_ctx.device_handle);
+#endif
+  }
+  else
+  {
+#ifdef _WIN32
+    ni_logan_device_close(s->api_ctx.device_handle);
+#elif __linux__
+    ni_logan_device_close(s->api_ctx.device_handle);
+    ni_logan_device_close(s->api_ctx.blk_io_handle);
+#endif
+  }
+
+  s->api_ctx.device_handle = NI_INVALID_DEVICE_HANDLE;
+  s->api_ctx.blk_io_handle = NI_INVALID_DEVICE_HANDLE;
+
+  return 0;
+}
+
+/*!******************************************************************************
+ *  \brief  Extract custom sei payload data from AVPacket,
+ *  and save it to ni_logan_packet_t
+ *
+ *  \param AVCodecContext *avctx - avcodec context
+ *  \param AVPacket *pkt - AVPacket
+ *  \param int start_index - pkt data index of custom sei first byte after SEI type
+ *  \param ni_logan_packet_t *p_packet - netint internal packet
+ *  \param uint8_t sei_type - type of SEI
+ *  \param int got_slice - whether got vcl in the pkt data, 1 means got
+ *
+ *  \return - 0 on success, non-0 on failure
+ *******************************************************************************/
+static int ff_xcoder_logan_extract_custom_sei(AVCodecContext *avctx, AVPacket *pkt, int start_index,
+                                              ni_logan_packet_t *p_packet, uint8_t sei_type, int got_slice)
+{
+  int i;
+  uint8_t *udata;
+  uint8_t *sei_data;
+  int len = 0;
+  int sei_size = 0; // default is 0
+  int index = start_index;
+  int sei_index = 0;
+
+  av_log(avctx, AV_LOG_TRACE, "%s() enter\n", __FUNCTION__);
+  if (p_packet->p_all_custom_sei == NULL)
+  {
+    /* max size */
+    p_packet->p_all_custom_sei = (ni_logan_all_custom_sei_t *)malloc(sizeof(ni_logan_all_custom_sei_t));
+    if (p_packet->p_all_custom_sei == NULL)
+    {
+      av_log(avctx, AV_LOG_ERROR, "failed to allocate all custom sei buffer.\n");
+      return AVERROR(ENOMEM);
+    }
+    memset(p_packet->p_all_custom_sei, 0, sizeof(ni_logan_custom_sei_t));
+  }
+
+  sei_index = p_packet->p_all_custom_sei->custom_sei_cnt;
+  if (sei_index >= NI_LOGAN_MAX_CUSTOM_SEI_CNT)
+  {
+    av_log(avctx, AV_LOG_WARNING, "number of custom sei in current frame is out of limit(%d).\n",
+           NI_LOGAN_MAX_CUSTOM_SEI_CNT);
+    return AVERROR(EINVAL);
+  }
+  sei_data = p_packet->p_all_custom_sei->ni_custom_sei[sei_index].custom_sei_data;
+
+  /*! extract SEI payload size.
+   *  the first byte after SEI type is the SEI payload size.
+   *  if the first byte is 255(0xFF), it means the SEI payload size is more than 255.
+   *  in this case, to get the SEI payload size is to do a summation.
+   *  the end of SEI size is the first non-0xFF value.
+   *  for example, 0xFF 0xFF 0x08, the SEI payload size equals to (0xFF+0xFF+0x08).
+   */
+  while ((index < pkt->size) && (pkt->data[index] == 255))
+  {
+    sei_size += pkt->data[index++];
+  }
+
+  if (index >= pkt->size)
+  {
+    av_log(avctx, AV_LOG_WARNING, "custom sei corrupted: length truncated.\n");
+    return AVERROR(EINVAL);
+  }
+  sei_size += pkt->data[index++];
+
+  /* check sei size*/
+  if (sei_size > NI_LOGAN_MAX_CUSTOM_SEI_SZ)
+  {
+    av_log(avctx, AV_LOG_WARNING, "custom sei corrupted: size(%d) out of limit(%d).\n",
+           sei_size, NI_LOGAN_MAX_CUSTOM_SEI_SZ);
+    return AVERROR(EINVAL);
+  }
+
+  udata = &pkt->data[index];
+
+  /* extract SEI payload data
+   * SEI payload data in NAL is EBSP(Encapsulated Byte Sequence Payload),
+   * need change EBSP to RBSP(Raw Byte Sequence Payload) for exact size
+  */
+  for (i = 0; (i < (pkt->size - index)) && len < sei_size; i++)
+  {
+    /* if the latest 3-byte data pattern matchs '00 00 03' which means udata[i] is an escaping byte,
+     * discard udata[i]. */
+    if (i >= 2 && udata[i - 2] == 0 && udata[i - 1] == 0 && udata[i] == 3)
+    {
+      continue;
+    }
+    sei_data[len++] = udata[i];
+  }
+
+  if (len != sei_size)
+  {
+    av_log(avctx, AV_LOG_WARNING, "custom sei corrupted: data truncated, "
+           "requied size:%d, actual size:%d.\n", sei_size, len);
+    return AVERROR(EINVAL);
+  }
+
+  p_packet->p_all_custom_sei->ni_custom_sei[sei_index].custom_sei_size = sei_size;
+  p_packet->p_all_custom_sei->ni_custom_sei[sei_index].custom_sei_type = sei_type;
+  if (got_slice)
+  {
+    p_packet->p_all_custom_sei->ni_custom_sei[sei_index].custom_sei_loc = NI_LOGAN_CUSTOM_SEI_LOC_AFTER_VCL;
+  }
+  else
+  {
+    p_packet->p_all_custom_sei->ni_custom_sei[sei_index].custom_sei_loc = NI_LOGAN_CUSTOM_SEI_LOC_BEFORE_VCL;
+  }
+  p_packet->p_all_custom_sei->custom_sei_cnt ++;
+  av_log(avctx, AV_LOG_TRACE, "%s() exit, custom_sei_cnt=%d, size=%d type=%d\n",
+         __FUNCTION__, p_packet->p_all_custom_sei->custom_sei_cnt, sei_size, sei_type);
+
+  return 0;
+}
+
+/*!******************************************************************************
+ *  \brief  detect custom SEI payload data in AVPacket data,
+ *          custom SEI has two meanings:
+ *          a. the SEI type is not in the standard protocol, which is added by customes,
+ *             for example SEI type 100, note that custom SEI is not user data unregistered SEI.
+ *          b. the SEI NAL location does not conform to protocol. It's after VCL NALs.
+ *          So there are cases to handle here:
+ *          case a: enable custom_sei, detext custom SEIs before VCL.
+ *          case b: enable custom_sei and enable_check_packet, detect custom SEIs before VCL,
+ *                  and all SEIs after VCL.
+ *          all of these SEIs are passthroughed in the same places after encoding.
+ *
+ *  \param AVCodecContext *avctx - avcodec context
+ *  \param XCoderLoganDecContext *s - netint decoder context
+ *  \param AVPacket *pkt - AVPacket
+ *  \param int start_index - pkt data index of custom sei first byte after SEI type
+ *  \param ni_logan_packet_t *p_packet - netint internal packet
+ *
+ *  \return - 0 on success or not detect correct custom sei, non-0 on failure
+ *******************************************************************************/
+static int ff_xcoder_logan_detect_custom_sei(AVCodecContext *avctx, XCoderLoganDecContext *s,
+                                             AVPacket *pkt, ni_logan_packet_t *p_packet)
+{
+  int ret = 0;
+  const uint8_t *ptr = NULL;
+  const uint8_t *end = NULL;
+  uint8_t custom_sei_type = s->custom_sei;
+  uint8_t chk_pkt = s->enable_check_packet | s->api_param.dec_input_params.check_packet;
+  uint8_t nalu_type;
+  uint8_t sei_type;
+  uint32_t stc = -1;
+  int got_slice = 0;
+
+  if(s->api_param.dec_input_params.custom_sei_passthru != NI_LOGAN_INVALID_SEI_TYPE)
+  {
+    custom_sei_type = s->api_param.dec_input_params.custom_sei_passthru;
+  }
+  av_log(avctx, AV_LOG_TRACE, "%s(): custom SEI type %d\n", __FUNCTION__, custom_sei_type);
+
+  if (!pkt->data || !avctx)
+  {
+    return ret;
+  }
+
+  // search custom sei in the lone buffer
+  // if there is a custom sei in the lone sei, the firmware can't recoginze it.
+  // passthrough the custom sei here.
+  if (s->api_ctx.lone_sei_size)
+  {
+    av_log(avctx, AV_LOG_TRACE, "%s(): detect in lone SEI, size=%d\n",
+           __FUNCTION__, s->api_ctx.lone_sei_size);
+    ptr = s->api_ctx.buf_lone_sei;
+    end = s->api_ctx.buf_lone_sei + s->api_ctx.lone_sei_size;
+    stc = -1;
+    ptr = avpriv_find_start_code(ptr, end, &stc);
+    while (ptr < end)
+    {
+      if (avctx->codec_id == AV_CODEC_ID_H264)
+      { // if h264
+        nalu_type = stc & 0x1F;
+        sei_type = *ptr;
+        if ((nalu_type == H264_NAL_SEI) && (sei_type == custom_sei_type))
+        {
+          /* extract SEI payload, store in ni_logan_packet and pass to libxcoder. */
+          ret = ff_xcoder_logan_extract_custom_sei(avctx, pkt, ptr + 1 - pkt->data, p_packet, sei_type, got_slice);
+          if (ret == AVERROR(ENOMEM))
+          {
+            return ret;
+          }
+          else if (ret != 0)
+          {
+            if ((p_packet->p_all_custom_sei) && (p_packet->p_all_custom_sei->custom_sei_cnt == 0))
+            {
+              free(p_packet->p_all_custom_sei);
+              p_packet->p_all_custom_sei = NULL;
+            }
+            return 0;
+          }
+        }
+      }
+      else if (avctx->codec_id == AV_CODEC_ID_HEVC)
+      { //if hevc
+        nalu_type = (stc >> 1) & 0x3F;
+        sei_type = *(ptr + 1);
+        // check nalu_type, check nuh_temporal_id_plus1 = 1, check sei_pype
+        if ((nalu_type == HEVC_NAL_SEI_PREFIX) && (*ptr == 1) && (sei_type == custom_sei_type))
+        {
+          /* extract SEI payload, store in ni_logan_packet and pass to libxcoder. */
+          ret = ff_xcoder_logan_extract_custom_sei(avctx, pkt, ptr + 2 - pkt->data, p_packet, sei_type, got_slice);
+          if (ret == AVERROR(ENOMEM))
+          {
+            return ret;
+          }
+          else if (ret != 0)
+          {
+            if ((p_packet->p_all_custom_sei) && (p_packet->p_all_custom_sei->custom_sei_cnt == 0))
+            {
+              free(p_packet->p_all_custom_sei);
+              p_packet->p_all_custom_sei = NULL;
+            }
+            return 0;
+          }
+        }
+      }
+      else
+      {
+        av_log(avctx, AV_LOG_ERROR, "%s wrong codec %d !\n", __FUNCTION__,
+               avctx->codec_id);
+        break;
+      }
+
+      stc = -1;
+      ptr = avpriv_find_start_code(ptr, end, &stc);
+    }
+  }
+
+  // search custom sei in the packet
+  av_log(avctx, AV_LOG_TRACE, "%s(): detect in packet, size=%d\n",
+         __FUNCTION__, pkt->size);
+  ptr = pkt->data;
+  end = pkt->data + pkt->size;
+  stc = -1;
+  ptr = avpriv_find_start_code(ptr, end, &stc);
+  while (ptr < end)
+  {
+    if (avctx->codec_id == AV_CODEC_ID_H264)
+    { // if h264
+      nalu_type = stc & 0x1F;
+      sei_type = *ptr;
+      if ((nalu_type == H264_NAL_SEI) &&
+          ((sei_type == custom_sei_type) || (got_slice && chk_pkt)))
+      {
+        /* extract SEI payload, store in ni_logan_packet and pass to libxcoder. */
+        ret = ff_xcoder_logan_extract_custom_sei(avctx, pkt, ptr + 1 - pkt->data, p_packet, sei_type, got_slice);
+        if (ret == AVERROR(ENOMEM))
+        {
+          return ret;
+        }
+        else if (ret != 0)
+        {
+          return 0;
+        }
+      }
+      else if ((nalu_type >= H264_NAL_SLICE) && (nalu_type <= H264_NAL_IDR_SLICE)) //VCL
+      {
+        ret = 0;
+        got_slice = 1;
+        /* if disable check packet and VCL is found, then stop searching for SEI after VCL. */
+        if (!chk_pkt)
+        {
+          break;
+        }
+      }
+    }
+    else if (avctx->codec_id == AV_CODEC_ID_HEVC)
+    { //if hevc
+      nalu_type = (stc >> 1) & 0x3F;
+      sei_type = *(ptr + 1);
+      // check nalu_type, check nuh_temporal_id_plus1 = 1, check sei_pype
+      // if enable chk_pkt, continue search SEI after VCL
+      if ((nalu_type == HEVC_NAL_SEI_PREFIX) && (*ptr == 1) &&
+          ((sei_type == custom_sei_type) || (got_slice && chk_pkt)))
+      {
+        /* extract SEI payload, store in ni_logan_packet and pass to libxcoder. */
+        ret = ff_xcoder_logan_extract_custom_sei(avctx, pkt, ptr + 2 - pkt->data, p_packet, sei_type, got_slice);
+        if (ret == AVERROR(ENOMEM))
+        {
+          return ret;
+        }
+        else if (ret != 0)
+        {
+          return 0;
+        }
+      }
+      else if (nalu_type >= HEVC_NAL_TRAIL_N && nalu_type <= HEVC_NAL_RSV_VCL31) //found VCL
+      {
+        ret = 0;
+        got_slice = 1;
+        /* if disable check packet and VCL is found, then stop searching for SEI after VCL. */
+        if (!chk_pkt)
+        {
+          break;
+        }
+      }
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_ERROR, "%s wrong codec %d !\n", __FUNCTION__,
+             avctx->codec_id);
+      break;
+    }
+
+    stc = -1;
+    ptr = avpriv_find_start_code(ptr, end, &stc);
+  }
+
+  if (p_packet->p_all_custom_sei)
+  {
+    av_log(avctx, AV_LOG_TRACE, "%s(): total custom SEI number %d\n", __FUNCTION__,
+          p_packet->p_all_custom_sei->custom_sei_cnt);
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_TRACE, "%s(): no custom SEI detected\n", __FUNCTION__);
+  }
+
+  return ret;
+}
+
+// return 1 if need to prepend saved header to pkt data, 0 otherwise
+int ff_xcoder_logan_add_headers(AVCodecContext *avctx, AVPacket *pkt,
+                                uint8_t *extradata, int extradata_size)
+{
+  XCoderLoganDecContext *s = avctx->priv_data;
+  int ret = 0;
+  const uint8_t *ptr = pkt->data;
+  const uint8_t *end = pkt->data + pkt->size;
+  uint32_t stc = -1;
+  uint8_t nalu_type = 0;
+
+  if (!pkt->data || !avctx)
+  {
+    return ret;
+  }
+
+  while (ptr < end)
+  {
+    stc = -1;
+    ptr = avpriv_find_start_code(ptr, end, &stc);
+    if (ptr == end)
+    {
+      break;
+    }
+
+    if (AV_CODEC_ID_H264 == avctx->codec_id)
+    {
+      nalu_type = stc & 0x1f;
+
+      // update extra data on sequence change by resetting got_first_key_frame
+      if (s->got_first_key_frame && pkt->flags & AV_PKT_FLAG_KEY)
+      {
+        if (s->extradata_size != extradata_size ||
+            memcmp(s->extradata, extradata, s->extradata_size))
+        {
+          s->got_first_key_frame = 0;
+        }
+      }
+
+      // Find the first packet containing key frame
+      if (!s->got_first_key_frame && pkt->flags & AV_PKT_FLAG_KEY)
+      {
+        free(s->extradata);
+        s->extradata = malloc(extradata_size);
+        if (!s->extradata)
+        {
+          av_log(avctx, AV_LOG_ERROR, "%s memory allocation failed !\n", __FUNCTION__);
+          ret = 0;
+          break;
+        }
+        av_log(avctx, AV_LOG_TRACE, "%s size %d\n", __FUNCTION__, extradata_size);
+        memcpy(s->extradata, extradata, extradata_size);
+        s->extradata_size = extradata_size;
+        s->got_first_key_frame = 1;
+        ret = 1;
+        break;
+      }
+
+      // If SPS/PPS already exists, no need to prepend it again;
+      // we use one of the header info to simplify the checking.
+      if (H264_NAL_SPS == nalu_type || H264_NAL_PPS == nalu_type)
+      {
+        // save the header if not done yet for subsequent comparison
+        if (! s->extradata_size || ! s->extradata)
+        {
+          s->extradata = malloc(extradata_size);
+          if (! s->extradata)
+          {
+            av_log(avctx, AV_LOG_ERROR, "%s memory allocation failed !\n",
+                   __FUNCTION__);
+            ret = 0;
+            break;
+          }
+          av_log(avctx, AV_LOG_TRACE, "%s size %d\n", __FUNCTION__, extradata_size);
+          memcpy(s->extradata, extradata, extradata_size);
+          s->extradata_size = extradata_size;
+        }
+        s->got_first_key_frame = 1;
+        ret = 0;
+        break;
+      }
+      else if (nalu_type >= H264_NAL_SLICE && nalu_type <= H264_NAL_IDR_SLICE)
+      {
+        // VCL types results in no header inserted
+        ret = 0;
+        break;
+      }
+    }
+    else if (AV_CODEC_ID_HEVC == avctx->codec_id)
+    {
+      nalu_type = (stc >> 1) & 0x3F;
+
+      // IRAP picture types include: BLA, CRA, IDR and IRAP reserve types,
+      // 16-23, and insert header in front of IRAP at start or if header changes
+      if (s->got_first_key_frame && pkt->flags & AV_PKT_FLAG_KEY)
+      {
+        if (s->extradata_size != extradata_size ||
+            memcmp(s->extradata, extradata, s->extradata_size))
+        {
+          s->got_first_key_frame = 0;
+        }
+      }
+
+      if (! s->got_first_key_frame && pkt->flags & AV_PKT_FLAG_KEY)
+      {
+        free(s->extradata);
+        s->extradata = malloc(extradata_size);
+        if (! s->extradata)
+        {
+          av_log(avctx, AV_LOG_ERROR, "%s memory allocation failed !\n",
+                 __FUNCTION__);
+          ret = 0;
+          break;
+        }
+        av_log(avctx, AV_LOG_TRACE, "%s size %d\n", __FUNCTION__, extradata_size);
+        memcpy(s->extradata, extradata, extradata_size);
+        s->extradata_size = extradata_size;
+        s->got_first_key_frame = 1;
+        ret = 1;
+        break;
+      }
+
+      // when header (VPS/SPS/PPS) already exists, no need to prepend it again;
+      // we use one of the header info to simplify the checking.
+      if (HEVC_NAL_VPS == nalu_type || HEVC_NAL_SPS == nalu_type ||
+          HEVC_NAL_PPS == nalu_type)
+      {
+        // save the header if not done yet for subsequent comparison
+        if (! s->extradata_size || ! s->extradata)
+        {
+          s->extradata = malloc(extradata_size);
+          if (! s->extradata)
+          {
+            av_log(avctx, AV_LOG_ERROR, "%s memory allocation failed !\n",
+                   __FUNCTION__);
+            ret = 0;
+            break;
+          }
+          av_log(avctx, AV_LOG_TRACE, "%s size %d\n", __FUNCTION__, extradata_size);
+          memcpy(s->extradata, extradata, extradata_size);
+          s->extradata_size = extradata_size;
+        }
+        s->got_first_key_frame = 1;
+        ret = 0;
+        break;
+      }
+      else if (nalu_type >= HEVC_NAL_TRAIL_N && nalu_type <= HEVC_NAL_RSV_VCL31)
+      {
+        // VCL types results in no header inserted
+        ret = 0;
+        break;
+      }
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_ERROR, "%s wrong codec %d!\n", __FUNCTION__, avctx->codec_id);
+      break;
+    }
+  }
+
+  return ret;
+}
+
+// check if the packet is SEI only, 1 yes, 0 otherwise
+// check if getting the header of streams in decoder low delay mode, and update its value
+// check the new sequence headers and cache them.
+static int xcoder_logan_packet_parse(AVCodecContext *avctx, XCoderLoganDecContext *s,
+                                     AVPacket *pkt, ni_logan_packet_t *p_packet)
+{
+  int pkt_sei_alone = 0;
+  int got_slice = 0;
+  int low_delay = (s->low_delay == 0)?s->api_param.dec_input_params.lowdelay:s->low_delay;
+  int pkt_nal_bitmap = 0;
+  int chk_pkt = s->enable_check_packet | s->api_param.dec_input_params.check_packet;
+  const uint8_t *ptr = pkt->data;
+  const uint8_t *end = pkt->data + pkt->size;
+  uint32_t stc = -1;
+  uint8_t nalu_type = 0;
+  int nalu_count = 0;
+
+  if (!pkt->data || !pkt->size || !avctx)
+  {
+    return pkt_sei_alone;
+  }
+
+  if (s->pkt_nal_bitmap & NI_LOGAN_GENERATE_ALL_NAL_HEADER_BIT)
+  {
+    av_log(avctx, AV_LOG_TRACE, "%s(): already find the header of streams.\n", __FUNCTION__);
+    low_delay = 0;
+  }
+
+  pkt_sei_alone = 1;
+  while ((pkt_sei_alone || low_delay || chk_pkt) && (ptr < end))
+  {
+    stc = -1;
+    ptr = avpriv_find_start_code(ptr, end, &stc);
+    if (ptr == end)
+    {
+      if (0 == nalu_count)
+      {
+        pkt_sei_alone = 0;
+        av_log(avctx, AV_LOG_TRACE, "%s(): no NAL found in pkt.\n", __FUNCTION__);
+      }
+      break;
+    }
+    nalu_count++;
+
+    if (AV_CODEC_ID_H264 == avctx->codec_id)
+    {
+      nalu_type = stc & 0x1F;
+
+      //check whether the packet is sei alone
+      pkt_sei_alone = (pkt_sei_alone && H264_NAL_SEI == nalu_type);
+
+      //check whether the packet contains SEI NAL after VCL NAL units
+      if (got_slice && (H264_NAL_SEI == nalu_type))
+      {
+        chk_pkt = 0;
+        // 5 bytes = 3 bytes start code + 1 byte nal type(0x06) + 1 byte sei type
+        p_packet->len_of_sei_after_vcl = (end - ptr) + 5;
+        av_log(avctx, AV_LOG_TRACE, "%s(): found SEI NAL after VCL NAL, len = %d.\n",
+               __FUNCTION__, p_packet->len_of_sei_after_vcl);
+      }
+      else if ((nalu_type >= H264_NAL_SLICE) && (nalu_type <= H264_NAL_IDR_SLICE)) //VCL
+      {
+        got_slice = 1;
+      }
+
+      // FFmpeg stores a complete frame in each AVPacket. A complete frame
+      // includes headers, SEIs, video slices, and other NALs such as access
+      // unit delimiters, etc. The decoder expects a complete frame for each
+      // packet writing to the decoder. Otherwise it can cause all sorts of
+      // problems. However in some cases the first NALU could be unit delimiter
+      // so we need to force to check NALU split to collect all the headers in
+      // case of decoder VPU recovery.
+      switch (nalu_type)
+      {
+        case H264_NAL_SPS:
+          chk_pkt = 1;
+          pkt_nal_bitmap |= NI_LOGAN_NAL_SPS_BIT;
+          break;
+        case H264_NAL_PPS:
+          chk_pkt = 1;
+          pkt_nal_bitmap |= NI_LOGAN_NAL_PPS_BIT;
+          break;
+        case H264_NAL_SEI:
+        case H264_NAL_AUD:
+          chk_pkt = 1;
+          break;
+        default:
+          chk_pkt = s->enable_check_packet | s->api_param.dec_input_params.check_packet;
+          break;
+      }
+
+      // Set decoder low delay mode one-time.
+      if (pkt_nal_bitmap & (NI_LOGAN_NAL_SPS_BIT | NI_LOGAN_NAL_PPS_BIT))
+      {
+        av_log(avctx, AV_LOG_TRACE, "%s(): Detect SPS, PPS and IDR, enable "
+               "decoder low delay mode.\n", __FUNCTION__);
+        pkt_nal_bitmap |= NI_LOGAN_GENERATE_ALL_NAL_HEADER_BIT;
+        if (low_delay)
+        {
+          s->api_ctx.decoder_low_delay = low_delay;
+          low_delay = 0;
+        }
+
+        // Update cached packet including SPS+PPS+IDR slice. A complete frame
+        // needs to be cached for stream with intraPeriod = 0
+        if (pkt != &s->seq_hdr_pkt && got_slice)
+        {
+          av_packet_unref(&s->seq_hdr_pkt);
+          av_packet_ref(&s->seq_hdr_pkt, pkt);
+        }
+      }
+    }
+    else if (AV_CODEC_ID_HEVC == avctx->codec_id)
+    {
+      nalu_type = (stc >> 1) & 0x3F;
+
+      //check whether the packet is sei alone
+      pkt_sei_alone = (pkt_sei_alone && (HEVC_NAL_SEI_PREFIX == nalu_type ||
+                                         HEVC_NAL_SEI_SUFFIX == nalu_type));
+
+      //check whether the packet contains SEI NAL after VCL NAL units
+      if (got_slice && (HEVC_NAL_SEI_PREFIX == nalu_type || HEVC_NAL_SEI_SUFFIX == nalu_type))
+      {
+        chk_pkt = 0;
+        // 5 bytes = 3 bytes start code + 2 bytes nal type(0x4e 0x01)
+        p_packet->len_of_sei_after_vcl = (end - ptr) + 5;
+        av_log(avctx, AV_LOG_TRACE, "%s(): found SEI NAL after VCL NAL, len = %d.\n",
+               __FUNCTION__, p_packet->len_of_sei_after_vcl);
+      }
+      else if ((nalu_type >= HEVC_NAL_TRAIL_N) && (nalu_type <= HEVC_NAL_RSV_VCL31)) //VCL
+      {
+        got_slice = 1;
+      }
+
+      // FFmpeg stores a complete frame in each AVPacket. A complete frame
+      // includes headers, SEIs, video slices, and other NALs such as access
+      // unit delimiters, etc. The decoder expects a complete frame for each
+      // packet writing to the decoder. Otherwise it can cause all sorts of
+      // problems. However in some cases the first NALU could be unit delimiter
+      // so we need to force to check NALU split to collect all the headers in
+      // case of decoder VPU recovery.
+      switch (nalu_type)
+      {
+        case HEVC_NAL_VPS:
+          chk_pkt = 1;
+          pkt_nal_bitmap |= NI_LOGAN_NAL_VPS_BIT;
+          break;
+        case HEVC_NAL_SPS:
+          chk_pkt = 1;
+          pkt_nal_bitmap |= NI_LOGAN_NAL_SPS_BIT;
+          break;
+        case HEVC_NAL_PPS:
+          chk_pkt = 1;
+          pkt_nal_bitmap |= NI_LOGAN_NAL_PPS_BIT;
+          break;
+        case HEVC_NAL_AUD:
+        case HEVC_NAL_EOS_NUT:
+        case HEVC_NAL_EOB_NUT:
+        case HEVC_NAL_FD_NUT:
+        case HEVC_NAL_SEI_PREFIX:
+        case HEVC_NAL_SEI_SUFFIX:
+          chk_pkt = 1;
+          break;
+        default:
+          chk_pkt = s->enable_check_packet | s->api_param.dec_input_params.check_packet;
+          break;
+      }
+
+      // Set decoder low delay mode one-time.
+      if (pkt_nal_bitmap & (NI_LOGAN_NAL_VPS_BIT | NI_LOGAN_NAL_SPS_BIT | NI_LOGAN_NAL_PPS_BIT))
+      {
+        av_log(avctx, AV_LOG_TRACE, "%s(): Detect VPS, SPS, PPS and IDR, "
+               "enable decoder low delay mode.\n", __FUNCTION__);
+        pkt_nal_bitmap |= NI_LOGAN_GENERATE_ALL_NAL_HEADER_BIT;
+        if (low_delay)
+        {
+          s->api_ctx.decoder_low_delay = low_delay;
+          low_delay = 0;
+        }
+
+        // Update cached packet including VPS+SPS+PPS+IDR slice. A complete frame
+        // needs to be cached for stream with intraPeriod = 0
+        if (pkt != &s->seq_hdr_pkt && got_slice)
+        {
+          av_packet_unref(&s->seq_hdr_pkt);
+          av_packet_ref(&s->seq_hdr_pkt, pkt);
+        }
+      }
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_ERROR, "%s() wrong codec %d !\n",
+             __FUNCTION__, avctx->codec_id);
+      pkt_sei_alone = 0;
+      break;
+    }
+  }
+
+  s->pkt_nal_bitmap |= pkt_nal_bitmap;
+  return pkt_sei_alone;
+}
+
+int ff_xcoder_logan_dec_send(AVCodecContext *avctx,
+                             XCoderLoganDecContext *s,
+                             AVPacket *pkt)
+{
+  /* call ni_logan_decoder_session_write to send compressed video packet to the decoder
+     instance */
+  int need_draining = 0;
+  size_t size;
+  ni_logan_packet_t *xpkt = &s->api_pkt.data.packet;
+  int ret;
+  int sent;
+  int send_size = 0;
+  int new_packet = 0;
+  int extra_prev_size = 0;
+
+  size = pkt->size;
+
+  if (s->flushing)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Decoder is flushing and cannot accept new "
+           "buffer until all output buffers have been released\n");
+    return AVERROR_EXTERNAL;
+  }
+
+  if (pkt->size == 0)
+  {
+    need_draining = 1;
+    // Once VPU recovery, the s->draining may be lost during session reset. And
+    // so is the EOS indicator which might be lost in decoder session read. Here
+    // we try to recover EOS indicator so as to return EOF in this calling.
+    s->eos |= (s->draining && s->vpu_reset);
+  }
+
+  if (s->draining && s->eos)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "Decoder is draining, eos\n");
+    return AVERROR_EOF;
+  }
+
+  if (xpkt->data_len == 0)
+  {
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 91)
+    AVBSFContext *bsf = avctx->internal->bsf;
+#else
+    AVBSFContext *bsf = avctx->internal->filter.bsfs[0];
+#endif
+    uint8_t *extradata = bsf ? bsf->par_out->extradata : avctx->extradata;
+    int extradata_size = bsf ? bsf->par_out->extradata_size : avctx->extradata_size;
+
+    memset(xpkt, 0, sizeof(ni_logan_packet_t));
+    xpkt->pts = pkt->pts;
+    xpkt->dts = pkt->dts;
+    xpkt->flags = pkt->flags;
+    xpkt->video_width = avctx->width;
+    xpkt->video_height = avctx->height;
+    xpkt->p_data = NULL;
+    xpkt->data_len = pkt->size;
+    xpkt->p_all_custom_sei = NULL;
+    xpkt->len_of_sei_after_vcl = 0;
+
+    if (extradata_size > 0 && extradata_size != s->extradata_size &&
+        ff_xcoder_logan_add_headers(avctx, pkt, extradata, extradata_size))
+    {
+      if (avctx->extradata_size > s->api_ctx.max_nvme_io_size * 2)
+      {
+        av_log(avctx, AV_LOG_ERROR, "%s extradata_size %d exceeding max size "
+               "supported: %d\n", __FUNCTION__, s->extradata_size,
+               s->api_ctx.max_nvme_io_size * 2);
+      }
+      else
+      {
+        av_log(avctx, AV_LOG_DEBUG, "%s extradata_size %d copied to pkt start.\n",
+               __FUNCTION__, s->extradata_size);
+        s->api_ctx.prev_size = s->extradata_size;
+        memcpy(s->api_ctx.p_leftover, s->extradata, s->extradata_size);
+      }
+    }
+    if ((s->low_delay || s->api_param.dec_input_params.lowdelay) &&
+        s->got_first_key_frame && !(s->pkt_nal_bitmap & NI_LOGAN_GENERATE_ALL_NAL_HEADER_BIT))
+    {
+      if(s->low_delay)
+        s->api_ctx.decoder_low_delay = s->low_delay;
+      else
+        s->api_ctx.decoder_low_delay = s->api_param.dec_input_params.lowdelay;
+      s->pkt_nal_bitmap |= NI_LOGAN_GENERATE_ALL_NAL_HEADER_BIT;
+      av_log(avctx, AV_LOG_TRACE, "%s got first IDR in decoder low delay mode, "
+             "delay time %dms, pkt_nal_bitmap %d\n", __FUNCTION__, s->api_ctx.decoder_low_delay,
+             s->pkt_nal_bitmap);
+    }
+
+    // check if the packet is SEI only, save it to be sent with the next data frame.
+    // check if getting the header of streams in decoder low delay mode, and update its value.
+    // check if new sequence headers come and cache them.
+    if (xcoder_logan_packet_parse(avctx, s, pkt, xpkt))
+    {
+      // skip the packet if it's corrupted and/or exceeding lone SEI buf size
+      if (pkt->size + s->api_ctx.lone_sei_size <= NI_LOGAN_MAX_SEI_DATA)
+      {
+        memcpy(s->api_ctx.buf_lone_sei + s->api_ctx.lone_sei_size,
+               pkt->data, pkt->size);
+        s->api_ctx.lone_sei_size += pkt->size;
+        av_log(avctx, AV_LOG_TRACE, "%s pkt lone SEI, saved, and return %d\n",
+               __FUNCTION__, pkt->size);
+      }
+      else
+      {
+        av_log(avctx, AV_LOG_WARNING, "lone SEI size %d > buf size %ld, "
+               "corrupted? skipped ..\n", pkt->size, NI_LOGAN_MAX_SEI_DATA);
+      }
+
+      xpkt->data_len = 0;
+      return pkt->size;
+    }
+
+    if (s->custom_sei != NI_LOGAN_INVALID_SEI_TYPE || s->api_param.dec_input_params.custom_sei_passthru != NI_LOGAN_INVALID_SEI_TYPE)
+    {
+      ret = ff_xcoder_logan_detect_custom_sei(avctx, s, pkt, xpkt);
+      if (ret != 0)
+      {
+        goto fail;
+      }
+    }
+
+    // embed lone SEI saved previously (if any) to send to decoder
+    if (s->api_ctx.lone_sei_size)
+    {
+      av_log(avctx, AV_LOG_TRACE, "%s copy over lone SEI data size: %d\n",
+             __FUNCTION__, s->api_ctx.lone_sei_size);
+      memcpy((uint8_t *)s->api_ctx.p_leftover + s->api_ctx.prev_size,
+             s->api_ctx.buf_lone_sei, s->api_ctx.lone_sei_size);
+      s->api_ctx.prev_size += s->api_ctx.lone_sei_size;
+      s->api_ctx.lone_sei_size = 0;
+    }
+
+    if ((pkt->size + s->api_ctx.prev_size) > 0)
+    {
+      ni_logan_packet_buffer_alloc(xpkt, (pkt->size + s->api_ctx.prev_size - xpkt->len_of_sei_after_vcl));
+      if (!xpkt->p_data)
+      {
+        ret = AVERROR(ENOMEM);
+        goto fail;
+      }
+    }
+    new_packet = 1;
+  }
+  else
+  {
+    send_size = xpkt->data_len;
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "%s: pkt->size=%d\n", __FUNCTION__, pkt->size);
+
+  if (s->started == 0)
+  {
+    xpkt->start_of_stream = 1;
+    s->started = 1;
+  }
+
+  if (need_draining && !s->draining)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "Sending End Of Stream signal\n");
+    xpkt->end_of_stream = 1;
+    xpkt->data_len = 0;
+
+    av_log(avctx, AV_LOG_TRACE, "ni_logan_packet_copy before: size=%d, s->prev_size=%d, send_size=%d, "
+           "len_of_sei_after_slice=%d (end of stream)\n",
+           pkt->size, s->api_ctx.prev_size, send_size, xpkt->len_of_sei_after_vcl);
+    if (new_packet)
+    {
+      extra_prev_size = s->api_ctx.prev_size;
+      send_size = ni_logan_packet_copy(xpkt->p_data, pkt->data, (pkt->size - xpkt->len_of_sei_after_vcl),
+                                 s->api_ctx.p_leftover, &s->api_ctx.prev_size);
+      // increment offset of data sent to decoder and save it
+      xpkt->pos = s->offset;
+      if (s->api_ctx.is_dec_pkt_512_aligned)
+      {
+        s->offset += send_size;
+      }
+      else
+      {
+        s->offset += pkt->size + extra_prev_size;
+      }
+    }
+    av_log(avctx, AV_LOG_TRACE, "ni_logan_packet_copy after: size=%d, s->prev_size=%d, send_size=%d, "
+           "xpkt->data_len=%d, len_of_sei_after_slice=%d (end of stream)\n",
+           pkt->size, s->api_ctx.prev_size, send_size, xpkt->data_len, xpkt->len_of_sei_after_vcl);
+
+    if (send_size < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to copy pkt (status = %d)\n",
+             send_size);
+      ret = AVERROR_EXTERNAL;
+      goto fail;
+    }
+    if (s->api_ctx.is_dec_pkt_512_aligned)
+    {
+      xpkt->data_len = send_size;
+    }
+    else
+    {
+      xpkt->data_len += extra_prev_size;
+    }
+
+    sent = 0;
+    if (xpkt->data_len > 0)
+    {
+      sent = ni_logan_device_session_write(&(s->api_ctx), &(s->api_pkt), NI_LOGAN_DEVICE_TYPE_DECODER);
+    }
+    if (sent < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to send eos signal (status = %d)\n",
+             sent);
+      if (NI_LOGAN_RETCODE_ERROR_VPU_RECOVERY == sent)
+      {
+        ret = xcoder_logan_decode_reset(avctx);
+        if (0 == ret)
+        {
+          ret = AVERROR(EAGAIN);
+        }
+      }
+      else
+      {
+        ret = AVERROR_EOF;
+      }
+      goto fail;
+    }
+    av_log(avctx, AV_LOG_DEBUG, "Queued eos (status = %d) ts=%llu\n",
+           sent, xpkt->pts);
+    s->draining = 1;
+    s->vpu_reset = 0;
+
+    ni_logan_device_session_flush(&(s->api_ctx), NI_LOGAN_DEVICE_TYPE_DECODER);
+  }
+  else
+  {
+#if 0
+    if (pkt->pts == AV_NOPTS_VALUE)
+      av_log(avctx, AV_LOG_DEBUG, "DEC avpkt pts : NOPTS size %d  pos %lld \n",
+       pkt->size,  pkt->pos);
+    else
+      av_log(avctx, AV_LOG_DEBUG, "DEC avpkt pts : %lld  dts : %lld  size %d  pos %lld \n", pkt->pts, pkt->dts, pkt->size,
+       pkt->pos);
+#endif
+    av_log(avctx, AV_LOG_TRACE, "ni_logan_packet_copy before: size=%d, s->prev_size=%d, send_size=%d, len_of_sei_after_slice=%d\n",
+           pkt->size, s->api_ctx.prev_size, send_size, xpkt->len_of_sei_after_vcl);
+    if (new_packet)
+    {
+      extra_prev_size = s->api_ctx.prev_size;
+      send_size = ni_logan_packet_copy(xpkt->p_data, pkt->data, (pkt->size - xpkt->len_of_sei_after_vcl),
+                                 s->api_ctx.p_leftover, &s->api_ctx.prev_size);
+      // increment offset of data sent to decoder and save it
+      xpkt->pos = s->offset;
+      if (s->api_ctx.is_dec_pkt_512_aligned)
+      {
+        s->offset += send_size;
+      }
+      else
+      {
+        s->offset += pkt->size + extra_prev_size;
+      }
+    }
+    av_log(avctx, AV_LOG_TRACE, "ni_logan_packet_copy after: size=%d, s->prev_size=%d, send_size=%d, "
+           "xpkt->data_len=%d, len_of_sei_after_slice=%d\n",
+           pkt->size, s->api_ctx.prev_size, send_size, xpkt->data_len, xpkt->len_of_sei_after_vcl);
+
+    if (send_size < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to copy pkt (status = %d)\n",
+             send_size);
+      ret = AVERROR_EXTERNAL;
+      goto fail;
+    }
+    if (s->api_ctx.is_dec_pkt_512_aligned)
+    {
+      xpkt->data_len = send_size;
+    }
+    else
+    {
+      xpkt->data_len += extra_prev_size;
+    }
+
+    sent = 0;
+    if (xpkt->data_len > 0)
+    {
+      sent = ni_logan_device_session_write(&s->api_ctx, &(s->api_pkt), NI_LOGAN_DEVICE_TYPE_DECODER);
+      av_log(avctx, AV_LOG_DEBUG, "%s pts=%" PRIi64 ", dts=%" PRIi64 ", "
+             "pos=%" PRIi64 ", sent=%d\n", __FUNCTION__, pkt->pts, pkt->dts,
+             pkt->pos, sent);
+    }
+    if (sent < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to send compressed pkt (status = "
+                                  "%d)\n", sent);
+      if (NI_LOGAN_RETCODE_ERROR_VPU_RECOVERY == sent)
+      {
+        ret = xcoder_logan_decode_reset(avctx);
+        if (0 == ret)
+        {
+          ret = AVERROR(EAGAIN);
+        }
+      }
+      else
+      {
+        ret = AVERROR_EOF;
+      }
+      goto fail;
+    }
+    else if (sent == 0)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "Queued input buffer size=0\n");
+    }
+    else if (sent < size)
+    {
+      /* partial sent; keep trying */
+      av_log(avctx, AV_LOG_DEBUG, "Queued input buffer size=%d\n", sent);
+    }
+  }
+
+  if (sent != 0)
+  {
+    //keep the current pkt to resend next time
+    ni_logan_packet_buffer_free(xpkt);
+  }
+
+  if (xpkt->data_len == 0)
+  {
+    /* if this packet is done sending, free any sei buffer. */
+    free(xpkt->p_all_custom_sei);
+    xpkt->p_all_custom_sei = NULL;
+  }
+
+  if (sent == 0)
+  {
+    return AVERROR(EAGAIN);
+  }
+
+  return sent;
+
+fail:
+  ni_logan_packet_buffer_free(xpkt);
+  free(xpkt->p_all_custom_sei);
+  xpkt->p_all_custom_sei = NULL;
+
+  return ret;
+}
+
+int retrieve_logan_frame(AVCodecContext *avctx, AVFrame *data, int *got_frame,
+                         ni_logan_frame_t *xfme)
+{
+  XCoderLoganDecContext *s = avctx->priv_data;
+
+  int buf_size = xfme->data_len[0] + xfme->data_len[1] + xfme->data_len[2] + xfme->data_len[3];
+  uint8_t *buf = xfme->p_data[0];
+  int stride = 0;
+  int res = 0;
+  AVHWFramesContext *ctx;
+  NILOGANFramesContext *dst_ctx;
+  AVFrame *frame = data;
+  bool is_hw = xfme->data_len[3] > 0;
+  ni_aux_data_t *aux_data = NULL;
+  AVFrameSideData *av_side_data = NULL;
+
+  av_log(avctx, AV_LOG_TRACE, "%s: buf %p data_len [%d %d %d %d] buf_size %d\n",
+         __FUNCTION__, buf, xfme->data_len[0], xfme->data_len[1],
+         xfme->data_len[2], xfme->data_len[3], buf_size);
+
+  if(is_hw)
+  {
+    if (frame->hw_frames_ctx)
+    {
+      ctx = (AVHWFramesContext*)frame->hw_frames_ctx->data;
+      dst_ctx = ctx->internal->priv;
+    }
+    if (s->api_ctx.frame_num == 1)
+    {
+      if (frame->hw_frames_ctx)
+      {
+        av_log(avctx, AV_LOG_VERBOSE, "First frame, set hw_frame_context to copy decode sessions threads\n");
+        res = ni_logan_device_session_copy(&s->api_ctx, &dst_ctx->api_ctx);
+        if (NI_LOGAN_RETCODE_SUCCESS != res)
+        {
+          return res;
+        }
+        av_log(avctx, AV_LOG_VERBOSE, "%s: blk_io_handle %d device_handle %d\n",
+               __FUNCTION__, s->api_ctx.blk_io_handle, s->api_ctx.device_handle);
+      }
+    }
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "decoding %" PRId64 " frame ...\n", s->api_ctx.frame_num);
+
+  if (avctx->width <= 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "width is not set\n");
+    return AVERROR_INVALIDDATA;
+  }
+  if (avctx->height <= 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "height is not set\n");
+    return AVERROR_INVALIDDATA;
+  }
+
+  stride = s->api_ctx.active_video_width;
+
+  av_log(avctx, AV_LOG_DEBUG, "XFRAME SIZE: %d, STRIDE: %d\n", buf_size, stride);
+
+  if (!is_hw && (stride == 0 || buf_size < stride * avctx->height))
+  {
+    av_log(avctx, AV_LOG_ERROR, "Packet too small (%d)\n", buf_size);
+    return AVERROR_INVALIDDATA;
+  }
+
+  frame->key_frame = 0;
+  switch (xfme->ni_logan_pict_type)
+  {
+  case LOGAN_PIC_TYPE_I:
+    frame->pict_type = AV_PICTURE_TYPE_I;
+    break;
+  case LOGAN_PIC_TYPE_IDR:
+  case LOGAN_PIC_TYPE_CRA:
+    frame->pict_type = AV_PICTURE_TYPE_I;
+    frame->key_frame = 1;
+    break;
+  case LOGAN_PIC_TYPE_P:
+      frame->pict_type = AV_PICTURE_TYPE_P;
+      break;
+  case LOGAN_PIC_TYPE_B:
+      frame->pict_type = AV_PICTURE_TYPE_B;
+      break;
+  default:
+      frame->pict_type = AV_PICTURE_TYPE_NONE;
+  }
+
+  res = ff_decode_frame_props(avctx, frame);
+  if (res < 0)
+    return res;
+
+  frame->pkt_pos = avctx->internal->last_pkt_props->pos;
+  frame->pkt_duration = avctx->internal->last_pkt_props->duration;
+
+  if ((res = av_image_check_size(xfme->video_width, xfme->video_height, 0, avctx)) < 0)
+    return res;
+
+  if (is_hw)
+  {
+    frame->buf[0] = av_buffer_create(buf, buf_size, ni_logan_frame_free,
+                                     (void *) s->api_ctx.event_handle, 0);
+  }
+  else
+  {
+    frame->buf[0] = av_buffer_create(buf, buf_size, ni_logan_buf_pool_free, xfme->dec_buf, 0);
+  }
+
+  buf = frame->buf[0]->data;
+
+  // retrieve side data if available
+  ni_logan_dec_retrieve_aux_data(xfme);
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+  int i;
+  // retrieve the GStreamer data based on frame's packet offset
+  if (0 == s->api_ctx.frame_pkt_offset)
+  {
+      frame->opaque = s->gs_data[0].opaque;
+      frame->buf[1] = s->gs_data[0].buf0;
+      s->gs_data[0].opaque = NULL;
+      s->gs_data[0].buf0 = NULL;
+
+      av_log(avctx, AV_LOG_DEBUG, "pos 0 pkt opaque %p buf0 %p retrieved\n",
+             frame->opaque, frame->buf[1]);
+  }
+  else
+  {
+      for (i = 0; i < NI_LOGAN_FIFO_SZ; i++)
+      {
+          if (s->api_ctx.frame_pkt_offset >= s->gs_opaque_offsets_index_min[i]
+              && s->api_ctx.frame_pkt_offset < s->gs_opaque_offsets_index[i])
+          {
+              frame->opaque = s->gs_data[i].opaque;
+              frame->buf[1] = s->gs_data[i].buf0;
+              s->gs_data[i].opaque = NULL;
+              s->gs_data[i].buf0 = NULL;
+
+              av_log(avctx, AV_LOG_DEBUG, "pos %d pkt opaque %p buf0 %p retrieved\n",
+                     i, frame->opaque, frame->buf[1]);
+              break;
+          }
+          if (i == NI_LOGAN_FIFO_SZ -1 )
+          {
+              av_log(avctx, AV_LOG_ERROR, "ERROR: NO GS opaque found, consider "
+                     "increasing NI_LOGAN_FIFO_SZ (%d)!\n", NI_LOGAN_FIFO_SZ);
+          }
+      }
+  }
+#endif
+
+  // User Data Unregistered SEI if available
+  if ((s->enable_user_data_sei_passthru || s->api_param.dec_input_params.enable_user_data_sei_passthru) &&
+      (aux_data = ni_logan_frame_get_aux_data(xfme, NI_FRAME_AUX_DATA_UDU_SEI)))
+  {
+    av_side_data = av_frame_new_side_data(frame, AV_FRAME_DATA_NETINT_UDU_SEI,
+                                          aux_data->size);
+    if (! av_side_data)
+    {
+      return AVERROR(ENOMEM);
+    }
+    else
+    {
+      memcpy(av_side_data->data, aux_data->data, aux_data->size);
+    }
+  }
+  // close caption data if available
+  if (aux_data = ni_logan_frame_get_aux_data(xfme, NI_FRAME_AUX_DATA_A53_CC))
+  {
+    av_side_data = av_frame_new_side_data(frame, AV_FRAME_DATA_A53_CC,
+                                          aux_data->size);
+    if (! av_side_data)
+    {
+      return AVERROR(ENOMEM);
+    }
+    else
+    {
+      memcpy(av_side_data->data, aux_data->data, aux_data->size);
+    }
+  }
+
+  // hdr10 sei data if available
+  if (aux_data = ni_logan_frame_get_aux_data(
+        xfme, NI_FRAME_AUX_DATA_MASTERING_DISPLAY_METADATA))
+  {
+    AVMasteringDisplayMetadata *mdm =
+    av_mastering_display_metadata_create_side_data(frame);
+
+    if (! mdm)
+    {
+      return AVERROR(ENOMEM);
+    }
+    else
+    {
+      memcpy(mdm, aux_data->data, aux_data->size);
+    }
+  }
+
+  if (aux_data = ni_logan_frame_get_aux_data(xfme,
+                                       NI_FRAME_AUX_DATA_CONTENT_LIGHT_LEVEL))
+  {
+    AVContentLightMetadata *clm = 
+    av_content_light_metadata_create_side_data(frame);
+    if (! clm)
+    {
+      return AVERROR(ENOMEM);
+    }
+    else
+    {
+      memcpy(clm, aux_data->data, aux_data->size);
+    }
+  }
+
+  // hdr10+ sei data if available
+  if (aux_data = ni_logan_frame_get_aux_data(xfme, NI_FRAME_AUX_DATA_HDR_PLUS))
+  {
+    AVDynamicHDRPlus *hdrp = av_dynamic_hdr_plus_create_side_data(frame);
+    if (! hdrp)
+    {
+      return AVERROR(ENOMEM);
+    }
+    else
+    {
+      memcpy(hdrp, aux_data->data, aux_data->size);
+    }
+  } // hdr10+ sei
+
+  // remember to clean up auxiliary data of ni_logan_frame after their use
+  ni_logan_frame_wipe_aux_data(xfme);
+
+  if (xfme->p_custom_sei)
+  {
+    AVBufferRef *sei_ref = av_buffer_create(xfme->p_custom_sei,
+                                            sizeof(ni_logan_all_custom_sei_t),
+                                            ni_logan_free, NULL, 0);
+    if (! sei_ref ||
+        ! av_frame_new_side_data_from_buf(frame, AV_FRAME_DATA_NETINT_CUSTOM_SEI,
+                                    sei_ref))
+    {
+        return AVERROR(ENOMEM);
+    }
+    xfme->p_custom_sei = NULL;
+  }
+
+  frame->pkt_dts = xfme->dts;
+  frame->pts = xfme->pts;
+  if (xfme->pts != NI_LOGAN_NOPTS_VALUE)
+  {
+    s->current_pts = frame->pts;
+  }
+  else
+  {
+    if (!s->api_ctx.ready_to_close) {
+        s->current_pts += frame->pkt_duration;
+        frame->pts = s->current_pts;
+    }
+  }
+
+  if (is_hw)
+  {
+    ni_logan_hwframe_surface_t* p_data3;
+    p_data3 = (ni_logan_hwframe_surface_t*)((uint8_t*)xfme->p_buffer
+                                + xfme->data_len[0] + xfme->data_len[1]
+                                + xfme->data_len[2]);
+    frame->data[3] = (uint8_t*) p_data3;
+    av_log(avctx, AV_LOG_DEBUG, "%s: OUT0 data[3] i8FrameIdx=%d, device_handle=%ld"
+           " bitdep=%d, WxH %d x %d\n", __FUNCTION__,
+           p_data3->i8FrameIdx,
+           p_data3->device_handle,
+           p_data3->bit_depth,
+           p_data3->ui16width,
+           p_data3->ui16height);
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "%s: frame->buf[0]=%p, "
+         "frame->data=%p, frame->pts=%" PRId64 ", frame size=%d, "
+         "s->current_pts=%" PRId64 ", frame->pkt_pos=%" PRId64 ", "
+         "frame->pkt_duration=%" PRId64 " sei size %d offset %u\n",
+         __FUNCTION__, frame->buf[0], frame->data, frame->pts, buf_size,
+         s->current_pts, frame->pkt_pos, frame->pkt_duration,
+         xfme->sei_cc_len, xfme->sei_cc_offset);
+
+  /* av_buffer_ref(avpkt->buf); */
+  if (!frame->buf[0])
+  {
+    return AVERROR(ENOMEM);
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "%s: fill array, linesize[0]=%d, fmt=%d, width=%d"
+         ", height=%d\n", __FUNCTION__, frame->linesize[0], avctx->sw_pix_fmt,
+         s->api_ctx.active_video_width, s->api_ctx.active_video_height);
+  if (!is_hw && ((res = av_image_fill_arrays(frame->data, frame->linesize,
+                                              buf, avctx->sw_pix_fmt,
+                                              s->api_ctx.active_video_width,
+                                              s->api_ctx.active_video_height, 1)) < 0))
+  {
+    av_buffer_unref(&frame->buf[0]);
+    return res;
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "%s: success av_image_fill_arrays return %d\n",
+         __FUNCTION__, res);
+  frame->width = s->api_ctx.active_video_width;
+  frame->height = s->api_ctx.active_video_height;
+  frame->crop_top = xfme->crop_top;
+  frame->crop_bottom = s->api_ctx.active_video_height - xfme->crop_bottom;
+  frame->crop_left = xfme->crop_left;
+  frame->crop_right = s->api_ctx.active_video_width - xfme->crop_right;
+
+  if (is_hw)
+  {
+    av_log(avctx, AV_LOG_TRACE, "%s: hw frame av_buffer_get_ref_count=%d\n",
+           __FUNCTION__, av_buffer_get_ref_count(frame->buf[0]));
+    dst_ctx->pc_width = frame->width;
+    dst_ctx->pc_height = frame->height;
+    dst_ctx->pc_crop_bottom = frame->crop_bottom;
+    dst_ctx->pc_crop_right = frame->crop_right;
+  }
+
+  *got_frame = 1;
+  return buf_size;
+}
+
+static int decoder_logan_frame_alloc(AVCodecContext *avctx, XCoderLoganDecContext *s,
+                                     ni_logan_session_data_io_t *p_session_data)
+{
+  int ret, width, height, alloc_mem;
+
+  // If active video resolution has been obtained we just use it as it's the
+  // exact size of frame to be returned, otherwise we use what we are told by
+  // upper stream as the initial setting and it will be adjusted.
+  width = s->api_ctx.active_video_width > 0 ? s->api_ctx.active_video_width : avctx->width;
+  height = s->api_ctx.active_video_height > 0 ? s->api_ctx.active_video_height : avctx->height;
+
+  // allocate memory only after resolution is known (buffer pool set up)
+  alloc_mem = (s->api_ctx.active_video_width > 0) &&
+              (s->api_ctx.active_video_height > 0 ? 1 : 0);
+
+  // HW frame
+  if (avctx->pix_fmt == AV_PIX_FMT_NI_LOGAN)
+  {
+    ret = ni_logan_frame_buffer_alloc(&p_session_data->data.frame,
+                                width,
+                                height,
+                                avctx->codec_id == AV_CODEC_ID_H264,
+                                1,
+                                s->api_ctx.bit_depth_factor,
+                                1);
+  }
+  else
+  {
+    ret = ni_logan_decoder_frame_buffer_alloc(s->api_ctx.dec_fme_buf_pool,
+                                        &p_session_data->data.frame,
+                                        alloc_mem,
+                                        width,
+                                        height,
+                                        avctx->codec_id == AV_CODEC_ID_H264,
+                                        s->api_ctx.bit_depth_factor);
+  }
+
+  return ret;
+}
+
+static void decoder_logan_frame_free(AVCodecContext *avctx,
+                                     ni_logan_session_data_io_t *p_session_data)
+{
+  if (avctx->pix_fmt == AV_PIX_FMT_NI_LOGAN)
+  {
+    ni_logan_frame_buffer_free(&p_session_data->data.frame);
+  }
+  else
+  {
+    ni_logan_decoder_frame_buffer_free(&p_session_data->data.frame);
+  }
+}
+
+int ff_xcoder_logan_dec_receive(AVCodecContext *avctx, XCoderLoganDecContext *s,
+                                AVFrame *frame, bool wait)
+{
+  /* call xcode_dec_receive to get a decoded YUV frame from the decoder
+     instance */
+  int ret = 0;
+  int got_frame = 0;
+  ni_logan_session_data_io_t session_io_data;
+  ni_logan_session_data_io_t *p_session_data = &session_io_data;
+  int avctx_bit_depth = 0;
+  int is_hw_frm = (avctx->pix_fmt == AV_PIX_FMT_NI_LOGAN);
+
+  if (s->draining && s->eos)
+  {
+    return AVERROR_EOF;
+  }
+
+  memset(p_session_data, 0, sizeof(ni_logan_session_data_io_t));
+
+  ret = decoder_logan_frame_alloc(avctx, s, p_session_data);
+  if (NI_LOGAN_RETCODE_SUCCESS != ret)
+  {
+    return AVERROR_EXTERNAL;
+  }
+
+  if (is_hw_frm)
+  {
+    ret = ni_logan_device_session_read_hwdesc(&s->api_ctx, p_session_data);
+  }
+  else
+  {
+    ret = ni_logan_device_session_read(&s->api_ctx, p_session_data, NI_LOGAN_DEVICE_TYPE_DECODER);
+  }
+
+  if (ret == 0)
+  {
+    s->eos = p_session_data->data.frame.end_of_stream;
+    decoder_logan_frame_free(avctx, p_session_data);
+    return AVERROR(EAGAIN);
+  }
+  else if (ret > 0)
+  {
+    if (s->vpu_reset)
+    {
+      // On decoder VPU recovery the first received frame corresponding to the
+      // cached seq_hdr_pkt should be dropped since the data is outdated.
+      s->vpu_reset = 0;
+      decoder_logan_frame_free(avctx, p_session_data);
+      return AVERROR(EAGAIN);
+    }
+
+    if (p_session_data->data.frame.flags & AV_PKT_FLAG_DISCARD)
+    {
+      decoder_logan_frame_free(avctx, p_session_data);
+      return AVERROR(EAGAIN);
+    }
+
+    av_log(avctx, AV_LOG_DEBUG, "Got output buffer pts=%lld dts=%lld eos=%d sos=%d\n",
+           p_session_data->data.frame.pts, p_session_data->data.frame.dts,
+           p_session_data->data.frame.end_of_stream, p_session_data->data.frame.start_of_stream);
+
+    s->eos = p_session_data->data.frame.end_of_stream;
+
+    // update ctxt resolution if change has been detected
+    frame->width = p_session_data->data.frame.video_width;
+    frame->height = p_session_data->data.frame.video_height;
+
+    if (is_hw_frm)
+    {
+      avctx_bit_depth = p_session_data->data.frame.bit_depth;
+    }
+    else
+    {
+      avctx_bit_depth = (avctx->pix_fmt == AV_PIX_FMT_YUV420P10LE)?10:8;
+    }
+
+    if (frame->width != avctx->width || frame->height != avctx->height || avctx_bit_depth  != p_session_data->data.frame.bit_depth)
+    {
+      av_log(avctx, AV_LOG_WARNING, "%s: sequence changed: %dx%d %dbits to "
+             "%dx%d %dbits\n", __FUNCTION__, avctx->width, avctx->height,
+             avctx_bit_depth, frame->width, frame->height,
+             p_session_data->data.frame.bit_depth);
+      avctx->width = frame->width;
+      avctx->height = frame->height;
+
+      if (is_hw_frm)
+      {
+        s->hwfc->width = frame->width;
+        s->hwfc->height = frame->height;
+      }
+      else
+      {
+        avctx->sw_pix_fmt = (p_session_data->data.frame.bit_depth == 10)? AV_PIX_FMT_YUV420P10LE : AV_PIX_FMT_YUV420P;
+        avctx->pix_fmt = avctx->sw_pix_fmt;
+      }
+    }
+
+    frame->format = avctx->pix_fmt;
+
+    if (avctx->pix_fmt == AV_PIX_FMT_NI_LOGAN)
+    {
+      frame->hw_frames_ctx = av_buffer_ref(avctx->hw_frames_ctx);
+    }
+
+    retrieve_logan_frame(avctx, frame, &got_frame, &p_session_data->data.frame);
+
+    av_log(avctx, AV_LOG_DEBUG, "%s: got_frame=%d, frame->width=%d, frame->height=%d, "
+           "crop top %" SIZE_SPECIFIER " bottom %" SIZE_SPECIFIER " left "
+           "%" SIZE_SPECIFIER " right %" SIZE_SPECIFIER ", frame->format=%d, "
+           "frame->linesize=%d/%d/%d\n", __FUNCTION__, got_frame, frame->width,
+           frame->height, frame->crop_top, frame->crop_bottom, frame->crop_left,
+           frame->crop_right, frame->format,
+           frame->linesize[0], frame->linesize[1], frame->linesize[2]);
+
+#if FF_API_PKT_PTS
+    FF_DISABLE_DEPRECATION_WARNINGS
+    frame->pkt_pts = frame->pts;
+    FF_ENABLE_DEPRECATION_WARNINGS
+#endif
+    frame->best_effort_timestamp = frame->pts;
+#if 0
+    av_log(avctx, AV_LOG_DEBUG, "\n   NI dec out frame: pts  %lld  pkt_dts  %lld   pkt_pts  %lld \n\n", frame->pts, frame->pkt_dts,
+     frame->pkt_pts);
+#endif
+    av_log(avctx, AV_LOG_DEBUG, "%s: pkt_timebase= %d/%d, frame_rate=%d/%d, "
+           "frame->pts=%" PRId64 ", frame->pkt_dts=%" PRId64 "\n", __FUNCTION__,
+           avctx->pkt_timebase.num, avctx->pkt_timebase.den, avctx->framerate.num,
+           avctx->framerate.den, frame->pts, frame->pkt_dts);
+
+    // release buffer ownership and let frame owner return frame buffer to
+    // buffer pool later
+    p_session_data->data.frame.dec_buf = NULL;
+    free(p_session_data->data.frame.p_custom_sei);
+    p_session_data->data.frame.p_custom_sei = NULL;
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to get output buffer (status=%d)\n", ret);
+
+    if (NI_LOGAN_RETCODE_ERROR_VPU_RECOVERY == ret)
+    {
+      av_log(avctx, AV_LOG_WARNING, "%s VPU recovery, need to reset\n", __FUNCTION__);
+      decoder_logan_frame_free(avctx, p_session_data);
+      return ret;
+    }
+
+    return AVERROR_EOF;
+  }
+
+  ret = 0;
+
+  return ret;
+}
+
+int ff_xcoder_logan_dec_is_flushing(AVCodecContext *avctx,
+                                    XCoderLoganDecContext *s)
+{
+  return s->flushing;
+}
+
+int ff_xcoder_logan_dec_flush(AVCodecContext *avctx,
+                              XCoderLoganDecContext *s)
+{
+  s->draining = 0;
+  s->flushing = 0;
+  s->eos = 0;
+
+#if 0
+  int ret;
+  ret = ni_logan_device_session_flush(s, NI_LOGAN_DEVICE_TYPE_DECODER);
+  if (ret < 0) {
+    av_log(avctx, AV_LOG_ERROR, "Failed to flush decoder (status = %d)\n", ret);
+    return AVERROR_EXTERNAL;
+  }
+#endif
+
+  /* Future: for now, always return 1 to indicate the codec has been flushed
+     and it leaves the flushing state and can process again ! will consider
+     case of user retaining frames in HW "surface" usage */
+  return 1;
+}
diff --git a/libavcodec/nicodec_logan.h b/libavcodec/nicodec_logan.h
new file mode 100644
index 0000000000..e16524c7ce
--- /dev/null
+++ b/libavcodec/nicodec_logan.h
@@ -0,0 +1,221 @@
+/*
+ * XCoder Codec Lib Wrapper
+ * Copyright (c) 2018 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder codec lib wrapper header.
+ */
+
+#ifndef AVCODEC_NICODEC_LOGAN_H
+#define AVCODEC_NICODEC_LOGAN_H
+
+#include <stdbool.h>
+#include <time.h>
+#include "avcodec.h"
+#include "libavutil/fifo.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_ni_logan.h"
+
+#include <ni_device_api_logan.h>
+
+#define NI_LOGAN_NAL_VPS_BIT                  (0x01)
+#define NI_LOGAN_NAL_SPS_BIT                  (0x01<<1)
+#define NI_LOGAN_NAL_PPS_BIT                  (0x01<<2)
+#define NI_LOGAN_GENERATE_ALL_NAL_HEADER_BIT  (0x01<<3)
+
+/* enum for specifying xcoder device/coder index; can be specified in either
+   decoder or encoder options. */
+enum {
+  BEST_DEVICE_INST = -2,
+  BEST_DEVICE_LOAD = -1
+};
+
+/* enum for specifying hardware accelbrate index */
+enum {
+  HW_FRAMES_OFF = 0,
+  HW_FRAMES_ON = 1
+};
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+typedef struct _GsData {
+  void *opaque;
+  AVBufferRef *buf0;
+} GsData;
+#endif
+
+typedef struct XCoderLoganDecContext {
+  AVClass *avclass;
+
+  char *dev_xcoder;         /* from the user command, which device allocation method we use */
+  char *dev_xcoder_name;    /* dev name of the xcoder card to use */
+  char *blk_xcoder_name;    /* blk name of the xcoder card to use */
+  int  dev_dec_idx;         /* index of the decoder on the xcoder card */
+  int  keep_alive_timeout;    /* keep alive timeout setting */
+  int  set_high_priority;   /*set_high_priority*/
+  ni_logan_device_context_t *rsrc_ctx;  /* resource management context */
+
+  ni_logan_session_context_t api_ctx;
+  ni_logan_encoder_params_t  api_param;
+  ni_logan_session_data_io_t api_pkt;
+
+  AVPacket buffered_pkt;
+  AVPacket seq_hdr_pkt;
+
+  // stream header copied/saved from AVCodecContext.extradata
+  int got_first_key_frame;
+  uint8_t *extradata;
+  int extradata_size;
+
+  int64_t current_pts;
+  unsigned long long offset;
+
+  int started;
+  int draining;
+  int flushing;
+  int eos;
+  int vpu_reset;
+  AVHWFramesContext    *hwfc;
+
+  /* below are all command line options */
+  char *xcoder_opts;
+  int enable_user_data_sei_passthru;
+  int enable_check_packet;  // check source packet. Skip SEI payloads after VCL
+  int custom_sei;
+  int low_delay;
+  int pkt_nal_bitmap;
+  int hwFrames;
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+  // GStreamer support: use pkt offset to save/retrieve associated GS data
+  void *cur_gs_opaque;
+  AVBufferRef *cur_gs_buf0;
+  GsData gs_data[NI_LOGAN_FIFO_SZ];
+  uint64_t gs_opaque_offsets_index_min[NI_LOGAN_FIFO_SZ];
+  uint64_t gs_opaque_offsets_index[NI_LOGAN_FIFO_SZ];
+#endif
+
+} XCoderLoganDecContext;
+
+typedef struct XCoderLoganEncContext {
+  AVClass *avclass;
+
+  char *dev_xcoder;         /* from the user command, which device allocation method we use */
+  char *dev_xcoder_name;    /* dev name of the xcoder card to use */
+  char *blk_xcoder_name;    /* blk name of the xcoder card to use */
+  int  dev_enc_idx;         /* index of the encoder on the xcoder card */
+  uint8_t d_serial_number[20]; /*Serial number of card (dec) in use*/
+  uint8_t e_serial_number[20]; /*Serial number of card (enc) in use*/
+  int  keep_alive_timeout;    /* keep alive timeout setting */
+  int  set_high_priority;   /*set_high_priority*/
+  ni_logan_device_context_t *rsrc_ctx;  /* resource management context */
+  unsigned long xcode_load_pixel; /* xcode load in pixels by this encode task */
+
+  // frame fifo, to be used for sequence change frame buffering
+  AVFifoBuffer *fme_fifo;
+  int fme_fifo_capacity;
+  int eos_fme_received;
+  AVFrame buffered_fme;
+
+  ni_logan_session_data_io_t  api_pkt; /* used for receiving bitstream from xcoder */
+  ni_logan_session_data_io_t   api_fme; /* used for sending YUV data to xcoder */
+  ni_logan_session_context_t api_ctx;
+  ni_logan_encoder_params_t  api_param;
+
+  int started;
+  uint8_t *p_spsPpsHdr;
+  int spsPpsHdrLen;
+  int spsPpsArrived;
+  int firstPktArrived;
+  int dts_offset;
+  uint64_t total_frames_received;
+  int64_t first_frame_pts;
+  int64_t latest_dts;
+  int vpu_reset;
+  int encoder_flushing;
+  int encoder_eof;
+
+  // ROI
+  int roi_side_data_size;
+  AVRegionOfInterest *av_rois;  // last passed in AVRegionOfInterest
+  int nb_rois;
+  ni_logan_enc_avc_roi_custom_map_t *avc_roi_map; // actual AVC/HEVC map(s)
+  uint8_t *hevc_sub_ctu_roi_buf;
+  ni_logan_enc_hevc_roi_custom_map_t *hevc_roi_map;
+
+  /* backup copy of original values of -enc command line option */
+  int  orig_dev_enc_idx;
+
+  // for hw trancoding
+  // refer the hw frame when sending to encoder,
+  // unrefer the hw frame after received the encoded packet.
+  // Then it can recycle the HW frame buffer
+  AVFrame *sframe_pool[LOGAN_MAX_NUM_FRAMEPOOL_HWAVFRAME];
+  int aFree_Avframes_list[LOGAN_MAX_NUM_FRAMEPOOL_HWAVFRAME+1];
+  int freeHead;
+  int freeTail;
+
+ /* below are all command line options */
+  char *xcoder_opts;
+  char *xcoder_gop;
+
+  int reconfigCount;
+  // actual enc_change_params is in ni_logan_session_context !
+
+  // low delay mode flags
+  int gotPacket; /* used to stop receiving packets when a packet is already received */
+  int sentFrame; /* used to continue receiving packets when a frame is sent and a packet is not yet received */
+
+  // HRD parameters
+  uint32_t au_cpb_removal_delay_length_minus1;
+  uint32_t dpb_output_delay_length_minus1;
+  uint32_t initial_cpb_removal_delay_length_minus1;
+  int64_t bit_rate_unscale;
+  int64_t cpb_size_unscale;
+  uint32_t au_cpb_removal_delay_minus1;
+
+} XCoderLoganEncContext;
+
+int ff_xcoder_logan_dec_close(AVCodecContext *avctx,
+                              XCoderLoganDecContext *s);
+
+int ff_xcoder_logan_dec_init(AVCodecContext *avctx,
+                             XCoderLoganDecContext *s);
+
+int ff_xcoder_logan_dec_send(AVCodecContext *avctx,
+                             XCoderLoganDecContext *s,
+                             AVPacket *pkt);
+
+int ff_xcoder_logan_dec_receive(AVCodecContext *avctx,
+                                XCoderLoganDecContext *s,
+                                AVFrame *frame,
+                                bool wait);
+
+int ff_xcoder_logan_dec_is_flushing(AVCodecContext *avctx,
+                                    XCoderLoganDecContext *s);
+
+int ff_xcoder_logan_dec_flush(AVCodecContext *avctx,
+                              XCoderLoganDecContext *s);
+
+int retrieve_logan_frame(AVCodecContext *avctx, AVFrame *data, int *got_frame,
+                         ni_logan_frame_t *xfme);
+int ff_xcoder_logan_add_headers(AVCodecContext *avctx, AVPacket *pkt,
+                                uint8_t* extradata, int extradata_size);
+#endif /* AVCODEC_NICODEC_LOGAN_H */
diff --git a/libavcodec/nidec.c b/libavcodec/nidec.c
new file mode 100644
index 0000000000..c9508baf47
--- /dev/null
+++ b/libavcodec/nidec.c
@@ -0,0 +1,621 @@
+/*
+ * NetInt XCoder H.264/HEVC Decoder common code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder decoder.
+ */
+
+#include "nidec.h"
+#include "fftools/ffmpeg.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_ni_quad.h"
+
+#define USER_DATA_UNREGISTERED_SEI_PAYLOAD_TYPE 5
+
+int xcoder_decode_close(AVCodecContext *avctx)
+{
+  XCoderH264DecContext *s = avctx->priv_data;
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder decode close\n");
+
+  /* this call shall release resource based on s->api_ctx */
+  ff_xcoder_dec_close(avctx, s);
+
+  av_packet_unref(&s->buffered_pkt);
+  av_packet_unref(&s->lone_sei_pkt);
+
+  ni_rsrc_free_device_context(s->rsrc_ctx);
+  s->rsrc_ctx = NULL;
+
+  return 0;
+}
+
+static int xcoder_setup_decoder(AVCodecContext *avctx)
+{
+  XCoderH264DecContext *s = avctx->priv_data;
+  ni_xcoder_params_t *p_param =
+      &s->api_param; // dec params in union with enc params struct
+  int min_resolution_width, min_resolution_height;
+
+  int ret = 0;
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder setup device decoder\n");
+  //s->api_ctx.session_id = NI_INVALID_SESSION_ID;
+  if (ni_device_session_context_init(&(s->api_ctx)) < 0) {
+      av_log(avctx, AV_LOG_ERROR,
+             "Error XCoder init decoder context failure\n");
+      return AVERROR_EXTERNAL;
+  }
+
+  min_resolution_width  = NI_MIN_RESOLUTION_WIDTH;
+  min_resolution_height = NI_MIN_RESOLUTION_HEIGHT;
+
+  // Check codec id or format as well as profile idc.
+  switch (avctx->codec_id) {
+    case AV_CODEC_ID_HEVC:
+      s->api_ctx.codec_format = NI_CODEC_FORMAT_H265;
+      switch (avctx->profile)
+      {
+        case FF_PROFILE_HEVC_MAIN:
+        case FF_PROFILE_HEVC_MAIN_10:
+        case FF_PROFILE_HEVC_MAIN_STILL_PICTURE:
+          break;
+        default:
+          av_log(avctx, AV_LOG_ERROR, "Error: profile %d not supported.\n", avctx->profile);
+          return AVERROR_INVALIDDATA;
+      }
+      break;
+    case AV_CODEC_ID_VP9:
+      s->api_ctx.codec_format = NI_CODEC_FORMAT_VP9;
+      switch (avctx->profile)
+      {
+        case FF_PROFILE_VP9_0:
+        case FF_PROFILE_VP9_2:
+          break;
+        default:
+          av_log(avctx, AV_LOG_ERROR, "Error: profile %d not supported.\n", avctx->profile);
+          return AVERROR_INVALIDDATA;
+      }
+      break;
+    case AV_CODEC_ID_MJPEG:
+      s->api_ctx.codec_format = NI_CODEC_FORMAT_JPEG;
+      min_resolution_width    = NI_MIN_RESOLUTION_WIDTH_JPEG;
+      min_resolution_height   = NI_MIN_RESOLUTION_HEIGHT_JPEG;
+      switch (avctx->profile)
+      {
+        case FF_PROFILE_MJPEG_HUFFMAN_BASELINE_DCT:
+          break;
+        default:
+          av_log(avctx, AV_LOG_ERROR, "Error: profile %d not supported.\n", avctx->profile);
+          return AVERROR_INVALIDDATA;
+      }
+      break;
+    default:
+      s->api_ctx.codec_format = NI_CODEC_FORMAT_H264;
+      switch (avctx->profile)
+      {
+        case FF_PROFILE_H264_BASELINE:
+        case FF_PROFILE_H264_CONSTRAINED_BASELINE:
+        case FF_PROFILE_H264_MAIN:
+        case FF_PROFILE_H264_EXTENDED:
+        case FF_PROFILE_H264_HIGH:
+        case FF_PROFILE_H264_HIGH_10:
+          break;
+        default:
+          av_log(avctx, AV_LOG_ERROR, "Error: profile %d not supported.\n", avctx->profile);
+          return AVERROR_INVALIDDATA;
+      }
+      break;
+  }
+
+  if (avctx->width > NI_MAX_RESOLUTION_WIDTH ||
+      avctx->height > NI_MAX_RESOLUTION_HEIGHT ||
+      avctx->width * avctx->height > NI_MAX_RESOLUTION_AREA) {
+      av_log(avctx, AV_LOG_ERROR,
+             "Error XCoder resolution %dx%d not supported\n", avctx->width,
+             avctx->height);
+      av_log(avctx, AV_LOG_ERROR, "Max Supported Width: %d Height %d Area %d\n",
+             NI_MAX_RESOLUTION_WIDTH, NI_MAX_RESOLUTION_HEIGHT,
+             NI_MAX_RESOLUTION_AREA);
+      return AVERROR_EXTERNAL;
+  } else if (avctx->width < min_resolution_width ||
+             avctx->height < min_resolution_height) {
+      av_log(avctx, AV_LOG_ERROR,
+             "Error XCoder resolution %dx%d not supported\n", avctx->width,
+             avctx->height);
+      av_log(avctx, AV_LOG_ERROR, "Min Supported Width: %d Height %d\n",
+             min_resolution_width, min_resolution_height);
+      return AVERROR_EXTERNAL;
+  }
+
+  s->offset = 0LL;
+
+  s->draining = 0;
+
+  s->api_ctx.pic_reorder_delay = avctx->has_b_frames;
+  s->api_ctx.bit_depth_factor = 1;
+  if (AV_PIX_FMT_YUV420P10BE == avctx->pix_fmt ||
+      AV_PIX_FMT_YUV420P10LE == avctx->pix_fmt ||
+      AV_PIX_FMT_P010LE == avctx->pix_fmt)
+  {
+    s->api_ctx.bit_depth_factor = 2;
+  }
+   av_log(avctx, AV_LOG_VERBOSE, "xcoder_setup_decoder: pix_fmt %u bit_depth_factor %u\n", avctx->pix_fmt, s->api_ctx.bit_depth_factor);
+
+  //Xcoder User Configuration
+  if (ni_decoder_init_default_params(p_param, avctx->framerate.num, avctx->framerate.den, avctx->bit_rate, avctx->width, avctx->height) < 0)
+  {
+
+    av_log(avctx, AV_LOG_INFO, "Error setting params\n");
+
+    return AVERROR(EINVAL);
+  }
+
+  if (s->xcoder_opts)
+  {
+    AVDictionary *dict = NULL;
+    AVDictionaryEntry *en = NULL;
+
+    if (av_dict_parse_string(&dict, s->xcoder_opts, "=", ":", 0))
+    {
+      av_log(avctx, AV_LOG_ERROR, "Xcoder options provided contain error(s)\n");
+      return AVERROR_EXTERNAL;
+    }
+    else
+    {
+      while ((en = av_dict_get(dict, "", en, AV_DICT_IGNORE_SUFFIX)))
+      {
+        int parse_ret = ni_decoder_params_set_value(p_param, en->key, en->value);
+        switch (parse_ret)
+        {
+        case NI_RETCODE_PARAM_INVALID_NAME:
+          av_log(avctx, AV_LOG_ERROR, "Unknown option: %s.\n", en->key);
+          return AVERROR_EXTERNAL;
+        case NI_RETCODE_PARAM_ERROR_TOO_BIG:
+          av_log(avctx, AV_LOG_ERROR, "Invalid %s: too big, max char len = %d\n", en->key, NI_MAX_PPU_PARAM_EXPR_CHAR);
+          return AVERROR_EXTERNAL;
+        case NI_RETCODE_PARAM_ERROR_TOO_SMALL:
+          av_log(avctx, AV_LOG_ERROR, "Invalid %s: too small\n", en->key);
+          return AVERROR_EXTERNAL;
+        case NI_RETCODE_PARAM_ERROR_OOR:
+          av_log(avctx, AV_LOG_ERROR, "Invalid %s: out of range\n", en->key);
+          return AVERROR_EXTERNAL;
+        case NI_RETCODE_PARAM_ERROR_ZERO:
+          av_log(avctx, AV_LOG_ERROR, "Error setting option %s to value 0\n", en->key);
+          return AVERROR_EXTERNAL;
+        case NI_RETCODE_PARAM_INVALID_VALUE:
+          av_log(avctx, AV_LOG_ERROR, "Invalid value for %s: %s.\n", en->key, en->value);
+          return AVERROR_EXTERNAL;
+        case NI_RETCODE_PARAM_WARNING_DEPRECATED:
+          av_log(avctx, AV_LOG_WARNING, "Parameter %s is deprecated\n", en->key);
+          break;
+        default:
+          break;
+        }
+      }
+      av_dict_free(&dict);
+    }
+
+    for (size_t i = 0; i < NI_MAX_NUM_OF_DECODER_OUTPUTS; i++) {
+      if (p_param->dec_input_params.crop_mode[i] != NI_DEC_CROP_MODE_AUTO) {
+        continue;
+      }
+      for (size_t j = 0; j < 4; j++) {
+        if (strlen(p_param->dec_input_params.cr_expr[i][j])) {
+          av_log(avctx, AV_LOG_ERROR, "Setting crop parameters without setting crop mode to manual?\n");
+          return AVERROR_EXTERNAL;
+        }
+      }
+    }
+  }
+  parse_symbolic_decoder_param(s);
+  return 0;
+}
+
+int xcoder_decode_init(AVCodecContext *avctx)
+{
+  int ret = 0;
+  XCoderH264DecContext *s = avctx->priv_data;
+  const AVPixFmtDescriptor *desc;
+  ni_xcoder_params_t *p_param = &s->api_param;
+  ni_log_set_level(ff_to_ni_log_level(av_log_get_level()));
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder decode init\n");
+
+  avctx->sw_pix_fmt = avctx->pix_fmt;
+
+  //av_log(avctx, AV_LOG_VERBOSE, "XCoder setup device decoder: pix_fmt set to AV_PIX_FMT_NI_QUAD\n");//maybe later check for hwcontext first then apply this
+
+  desc = av_pix_fmt_desc_get(avctx->sw_pix_fmt);
+  av_log(avctx, AV_LOG_VERBOSE, "width: %d height: %d sw_pix_fmt: %s\n",
+         avctx->width, avctx->height, desc ? desc->name : "NONE");
+
+  if (0 == avctx->width || 0 == avctx->height)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Error probing input stream\n");
+    return AVERROR_INVALIDDATA;
+  }
+
+  switch (avctx->pix_fmt)
+  {
+    case AV_PIX_FMT_YUV420P:
+    case AV_PIX_FMT_YUV420P10BE:
+    case AV_PIX_FMT_YUV420P10LE:
+    case AV_PIX_FMT_YUVJ420P:
+    case AV_PIX_FMT_GRAY8:
+      break;
+    default:
+      av_log(avctx, AV_LOG_ERROR, "Error: pixel format %s not supported.\n",
+             desc ? desc->name : "NONE");
+      return AVERROR_INVALIDDATA;
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "(avctx->field_order = %d)\n", avctx->field_order);
+  if (avctx->field_order > AV_FIELD_PROGRESSIVE)
+  { //AVFieldOrder with bottom or top coding order represents interlaced video
+    av_log(avctx, AV_LOG_ERROR, "interlaced video not supported!\n");
+    return AVERROR_INVALIDDATA;
+  }
+
+  if ((ret = xcoder_setup_decoder(avctx)) < 0)
+  {
+    return ret;
+  }
+
+  //--------reassign pix format based on user param------------//
+  if (p_param->dec_input_params.semi_planar[0])
+  {
+    if (avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P10BE ||
+      avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P10LE ||
+      avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P)
+    {
+      av_log(avctx, AV_LOG_VERBOSE, "XCoder decode init: YV12 forced to NV12\n");
+      avctx->sw_pix_fmt = (avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P) ? AV_PIX_FMT_NV12 : AV_PIX_FMT_P010LE;
+    }
+  }
+  if (p_param->dec_input_params.force_8_bit[0])
+  {
+    if (avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P10BE ||
+      avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P10LE ||
+      avctx->sw_pix_fmt == AV_PIX_FMT_P010LE)
+    {
+      av_log(avctx, AV_LOG_VERBOSE, "XCoder decode init: 10Bit input forced to 8bit\n");
+      avctx->sw_pix_fmt = (avctx->sw_pix_fmt == AV_PIX_FMT_P010LE) ? AV_PIX_FMT_NV12 : AV_PIX_FMT_YUV420P;
+      s->api_ctx.bit_depth_factor = 1;
+    }
+  }
+  if (p_param->dec_input_params.hwframes)
+  { //need to set before open decoder
+    s->api_ctx.hw_action = NI_CODEC_HW_ENABLE;
+  }
+  else
+  {
+    s->api_ctx.hw_action = NI_CODEC_HW_NONE;
+  }
+  //------reassign pix format based on user param done--------//
+
+  if (s->custom_sei_type == USER_DATA_UNREGISTERED_SEI_PAYLOAD_TYPE ||
+      p_param->dec_input_params.custom_sei_passthru == USER_DATA_UNREGISTERED_SEI_PAYLOAD_TYPE)
+  {
+    // use SW passthru only
+    s->api_ctx.enable_user_data_sei_passthru = 0;
+  }
+  else
+  {
+    s->api_ctx.enable_user_data_sei_passthru = s->enable_user_data_sei_passthru;
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "api_ctx %p api_ctx/s: user_data_sei_passthru = %d/%d, custom_sei_type = %d\n", 
+         &s->api_ctx, s->api_ctx.enable_user_data_sei_passthru, s->enable_user_data_sei_passthru, s->custom_sei_type);
+
+  // reference h264_decode_init in h264dec.c
+  if (avctx->ticks_per_frame == 1)
+  {
+    if (avctx->time_base.den < INT_MAX / 2)
+    {
+      avctx->time_base.den *= 2;
+    }
+    else
+      avctx->time_base.num /= 2;
+  }
+
+  avctx->ticks_per_frame = 2;
+
+  s->started = 0;
+  memset(&s->api_pkt, 0, sizeof(ni_packet_t));
+  s->pkt_nal_bitmap = 0;
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+  s->cur_gs_opaque = NULL;
+  s->cur_gs_buf0 = NULL;
+  int i = 0;
+  for (i = 0; i < NI_FIFO_SZ; i++) {
+      s->gs_data[i].opaque = NULL;
+      s->gs_data[i].buf0 = NULL;
+  }
+#endif
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder decode init: time_base = %d/%d, frame rate = %d/%d, ticks_per_frame=%d\n", avctx->time_base.num, avctx->time_base.den, avctx->framerate.num, avctx->framerate.den, avctx->ticks_per_frame);
+
+  // overwrite keep alive timeout value here with a custom value if it was
+  // provided
+  // if xcoder option is set then overwrite the (legacy) decoder option
+  uint32_t xcoder_timeout = s->api_param.dec_input_params.keep_alive_timeout;
+  if (xcoder_timeout != NI_DEFAULT_KEEP_ALIVE_TIMEOUT) 
+  {
+      s->api_ctx.keep_alive_timeout = xcoder_timeout;
+  } 
+  else 
+  {
+      s->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+  }
+  av_log(avctx, AV_LOG_VERBOSE, "Custom NVME Keep Alive Timeout set to %d\n",
+         s->api_ctx.keep_alive_timeout);
+  s->api_ctx.decoder_low_delay = s->low_delay =
+      s->api_param.dec_input_params.decoder_low_delay;
+
+  s->api_ctx.p_session_config = &s->api_param;
+
+  if ((ret = ff_xcoder_dec_init(avctx, s)) < 0)
+  {
+    goto done;
+  }
+
+  s->current_pts = 0;
+
+done:
+  //if ( (NI_INVALID_DEVICE_HANDLE == s->api_ctx.blk_io_handle) || (NI_INVALID_DEVICE_HANDLE == s->api_ctx.device_handle) )
+  //{
+  //  xcoder_decode_close(avctx);
+  //}
+  return ret;
+}
+
+// reset and restart when xcoder decoder resets
+int xcoder_decode_reset(AVCodecContext *avctx)
+{
+  XCoderH264DecContext *s = avctx->priv_data;
+  ni_retcode_t ret = NI_RETCODE_FAILURE;
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder decode reset\n");
+
+  ni_device_session_close(&s->api_ctx, s->eos, NI_DEVICE_TYPE_DECODER);
+
+  ni_device_session_context_clear(&s->api_ctx);
+
+#ifdef _WIN32
+  ni_device_close(s->api_ctx.device_handle);
+#elif __linux__
+  ni_device_close(s->api_ctx.device_handle);
+  ni_device_close(s->api_ctx.blk_io_handle);
+#endif
+  s->api_ctx.device_handle = NI_INVALID_DEVICE_HANDLE;
+  s->api_ctx.blk_io_handle = NI_INVALID_DEVICE_HANDLE;
+
+  ni_packet_buffer_free(&(s->api_pkt.data.packet));
+  int64_t bcp_current_pts = s->current_pts;
+  ret = xcoder_decode_init(avctx);
+  s->current_pts = bcp_current_pts;
+  s->api_ctx.session_run_state = SESSION_RUN_STATE_RESETTING;
+  return ret;
+}
+
+static int xcoder_send_receive(AVCodecContext *avctx,
+                               XCoderH264DecContext *s,
+                               AVFrame *frame, bool wait)
+{
+  int ret;
+
+  /* send any pending data from buffered packet */
+  while (s->buffered_pkt.size)
+  {
+    ret = ff_xcoder_dec_send(avctx, s, &s->buffered_pkt);
+    if (ret == AVERROR(EAGAIN))
+      break;
+    else if (ret < 0)
+      return ret;
+    s->buffered_pkt.size -= ret;
+    s->buffered_pkt.data += ret;
+    if (s->buffered_pkt.size <= 0)
+    {
+    }
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+    // pkt is sent out, store GS data based on pkt offset so it can be
+    // retrieved when the decoded frame is returned
+    s->gs_data[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ].opaque =
+        s->cur_gs_opaque;
+    s->gs_data[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ].buf0 = s->cur_gs_buf0;
+    s->cur_gs_opaque = NULL;
+    s->cur_gs_buf0 = NULL;
+
+    s->gs_opaque_offsets_index_min[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ] =
+        s->api_ctx.pkt_offsets_index_min[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ];
+
+    s->gs_opaque_offsets_index[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ] =
+        s->api_ctx.pkt_offsets_index[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ];
+
+    av_log(avctx, AV_LOG_DEBUG, "pkt # %"PRIu64" bytes %d offset %"PRIu64" "
+                                "%"PRIu64" opaque %p buf0 %p\n", (s->api_ctx.pkt_index - 1) % NI_FIFO_SZ,
+           s->buffered_pkt.size,
+           s->gs_opaque_offsets_index_min[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ],
+           s->gs_opaque_offsets_index[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ],
+           s->gs_data[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ].opaque,
+           s->gs_data[(s->api_ctx.pkt_index - 1) % NI_FIFO_SZ].buf0);
+#endif
+
+    av_packet_unref(&s->buffered_pkt);
+  }
+
+  /* check for new frame */
+  return ff_xcoder_dec_receive(avctx, s, frame, wait);
+}
+
+int xcoder_receive_frame(AVCodecContext *avctx, AVFrame *frame)
+{
+  XCoderH264DecContext *s = avctx->priv_data;
+  int ret;
+
+  const AVPixFmtDescriptor *desc;
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder receive frame\n");
+
+  /* 
+   * reference mediacodec_receive_frame in mediacodec.c.
+   *
+   * After we have buffered an input packet, check if the codec is in the
+   * flushing state. If it is, we need to call ff_xcoder_dec_flush.
+   *
+   * ff_xcoder_dec_flush returns 0 if the flush cannot be performed on
+   * the codec (because the user retains frames). The codec stays in the
+   * flushing state.
+   *
+   * ff_xcoder_dec_flush returns 1 if the flush can actually be
+   * performed on the codec. The codec leaves the flushing state and can
+   * process again packets.
+   *
+   * ff_xcoder_dec_flush returns a negative value if an error has
+   * occurred.
+   *
+   * NetInt: for now we don't consider the case of user retaining the frame
+   *         (connected decoder-encoder case), so the return can only be 1
+   *         (flushed successfully), or < 0 (failure)
+   */
+  if (ff_xcoder_dec_is_flushing(avctx, s))
+  {
+    if (!ff_xcoder_dec_flush(avctx, s))
+    {
+      return AVERROR(EAGAIN);
+    }
+  }
+
+  // give priority to sending data to decoder
+  if (s->buffered_pkt.size == 0)
+  {
+    ret = ff_decode_get_packet(avctx, &s->buffered_pkt);
+    if (ret < 0) {
+        av_log(avctx, AV_LOG_VERBOSE, "ff_decode_get_packet 1 rc: %s\n",
+               av_err2str(ret));
+    }
+    else
+    {
+        av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 1 rc: Success\n");
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+            AVFrame frame_copy = {0};
+
+            /* avoid ff_get_buffer failure in case HW AVFrame */
+            AVBufferRef *tmp_ctx = avctx->hw_frames_ctx;
+            enum AVPixelFormat tmp_fmt = avctx->pix_fmt;
+            avctx->hw_frames_ctx = NULL;
+            avctx->pix_fmt = avctx->sw_pix_fmt;
+
+            av_frame_copy_props(&frame_copy, frame);
+
+            // retrieve/save from GStreamer info of this pkt which is the
+            // returned frame's opaque and buf[0]
+            ff_get_buffer(avctx, &frame_copy, 0);
+            s->cur_gs_opaque = frame_copy.opaque;
+            s->cur_gs_buf0 = frame_copy.buf[0];
+            frame_copy.opaque = NULL;
+            frame_copy.buf[0] = NULL;
+
+            avctx->hw_frames_ctx = tmp_ctx;
+            avctx->pix_fmt = tmp_fmt;
+            av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 1 rc: Success pkt size "
+                                        "= %d call ff_get_buffer return opaque %p buf0 %p\n",
+                   s->buffered_pkt.size, s->cur_gs_opaque, s->cur_gs_buf0);
+
+            av_frame_unref(&frame_copy);
+#endif
+    }
+  }
+
+  /* flush buffered packet and check for new frame */
+  ret = xcoder_send_receive(avctx, s, frame, false);
+  if (NI_RETCODE_ERROR_VPU_RECOVERY == ret)
+  {
+    ret = xcoder_decode_reset(avctx);
+    if (0 == ret)
+    {
+      return AVERROR(EAGAIN);
+    }
+    else
+    {
+      return ret;
+    }
+  }
+  else if (ret != AVERROR(EAGAIN))
+    return ret;
+
+  /* skip fetching new packet if we still have one buffered */
+  if (s->buffered_pkt.size > 0)
+    return xcoder_send_receive(avctx, s, frame, true);
+
+  /* fetch new packet or eof */
+  ret = ff_decode_get_packet(avctx, &s->buffered_pkt);
+  if (ret < 0) {
+      av_log(avctx, AV_LOG_VERBOSE, "ff_decode_get_packet 2 rc: %s\n",
+             av_err2str(ret));
+  }
+  else
+  {
+      av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 2 rc: Success\n");
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+        AVFrame frame_copy = {0};
+        av_frame_copy_props(&frame_copy, frame);
+
+        ff_get_buffer(avctx, &frame_copy, 0);
+        s->cur_gs_opaque = frame_copy.opaque;
+        s->cur_gs_buf0 = frame_copy.buf[0];
+        frame_copy.opaque = NULL;
+        frame_copy.buf[0] = NULL;
+
+        av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 2 rc: Success pkt size = "
+                                    "%d call ff_get_buffer return opaque %p buf0 %p\n",
+               s->buffered_pkt.size, s->cur_gs_opaque, s->cur_gs_buf0);
+
+        av_frame_unref(&frame_copy);
+#endif
+    }
+
+  if (ret == AVERROR_EOF)
+  {
+    AVPacket null_pkt = {0};
+    ret = ff_xcoder_dec_send(avctx, s, &null_pkt);
+
+    /* ToDelete: mark end of stream; this should be signalled by Lib 
+       s->eos = 1; */
+
+    if (ret < 0)
+      return ret;
+  }
+  else if (ret < 0)
+    return ret;
+  else
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "width: %d  height: %d\n", avctx->width, avctx->height);
+    desc = av_pix_fmt_desc_get(avctx->pix_fmt);
+    av_log(avctx, AV_LOG_VERBOSE, "pix_fmt: %s\n", desc ? desc->name : "NONE");
+  }
+
+  /* crank decoder with new packet */
+  return xcoder_send_receive(avctx, s, frame, true);
+}
diff --git a/libavcodec/nidec.h b/libavcodec/nidec.h
new file mode 100644
index 0000000000..e27576e77f
--- /dev/null
+++ b/libavcodec/nidec.h
@@ -0,0 +1,53 @@
+/*
+ * NetInt XCoder H.264/HEVC Decoder common code header
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_NIDEC_H
+#define AVCODEC_NIDEC_H
+
+#include <stdbool.h>
+#include <ni_rsrc_api.h>
+#include <ni_device_api.h>
+#include <ni_util.h>
+
+#include "avcodec.h"
+#include "decode.h"
+#include "internal.h"
+
+#include "libavutil/internal.h"
+#include "libavutil/frame.h"
+#include "libavutil/buffer.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/opt.h"
+
+#include "nicodec.h"
+
+#define OFFSETDEC(x) offsetof(XCoderH264DecContext, x)
+#define VD           AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_DECODING_PARAM
+
+int xcoder_decode_close(AVCodecContext *avctx);
+
+int xcoder_decode_init(AVCodecContext *avctx);
+
+int xcoder_decode_reset(AVCodecContext *avctx);
+
+int xcoder_receive_frame(AVCodecContext *avctx, AVFrame *frame);
+
+#endif /* AVCODEC_NIDEC_H */
diff --git a/libavcodec/nidec_h264.c b/libavcodec/nidec_h264.c
new file mode 100644
index 0000000000..c0bd9e3254
--- /dev/null
+++ b/libavcodec/nidec_h264.c
@@ -0,0 +1,140 @@
+/*
+ * XCoder H.264 Decoder
+ * Copyright (c) 2018 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder decoder.
+ */
+
+#include "nidec.h"
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+#include "hwconfig.h"
+#else
+#include "hwaccel.h"
+#endif
+
+#include "codec_internal.h"
+
+static const AVCodecHWConfigInternal *ff_ni_quad_hw_configs[] = {
+  &(const AVCodecHWConfigInternal) {
+  .public = {
+    .pix_fmt = AV_PIX_FMT_NI_QUAD,
+    .methods = AV_CODEC_HW_CONFIG_METHOD_HW_FRAMES_CTX |
+    AV_CODEC_HW_CONFIG_METHOD_AD_HOC | AV_CODEC_HW_CONFIG_METHOD_HW_DEVICE_CTX,
+    .device_type = AV_HWDEVICE_TYPE_NI_QUADRA,
+  },
+    .hwaccel = NULL,
+},
+NULL
+};
+
+static const AVOption dec_options[] = {
+    {"dec",
+     "Select which decoder to use by index. First is 0, second is 1, and so "
+     "on.",
+     OFFSETDEC(dev_dec_idx),
+     AV_OPT_TYPE_INT,
+     {.i64 = BEST_DEVICE_LOAD},
+     -1,
+     INT_MAX,
+     VD,
+     "dec"},
+
+    {"user_data_sei_passthru",
+     "Enable user data unregistered SEI passthrough.",
+     OFFSETDEC(enable_user_data_sei_passthru),
+     AV_OPT_TYPE_BOOL,
+     {.i64 = 0},
+     0,
+     1,
+     VD,
+     "user_data_sei_passthru"},
+
+    {"custom_sei_passthru",
+     "Specify a custom SEI type to passthrough.",
+     OFFSETDEC(custom_sei_type),
+     AV_OPT_TYPE_INT,
+     {.i64 = -1},
+     -1,
+     254,
+     VD,
+     "custom_sei_passthru"},
+
+    {"xcoder-params",
+     "Set the XCoder configuration using a :-separated list of key=value "
+     "parameters",
+     OFFSETDEC(xcoder_opts),
+     AV_OPT_TYPE_STRING,
+     {0},
+     0,
+     0,
+     VD},
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSETDEC(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     VD,
+     "keep_alive_timeout"},
+
+    {"low_delay",
+     "Enable low delay decoding mode for 1 in, 1 out decoding sequence. set 1 "
+     "to enable low delay mode. Should be used only for streams that are in "
+     "sequence.",
+     OFFSETDEC(low_delay),
+     AV_OPT_TYPE_INT,
+     {.i64 = 0},
+     0,
+     1,
+     VD,
+     "low_delay"},
+
+    {NULL}};
+
+static const AVClass h264_xcoderdec_class = {
+  .class_name = "h264_ni_quadra_dec",
+  .item_name = av_default_item_name,
+  .option = dec_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+const FFCodec ff_h264_ni_quadra_decoder = {
+  .p.name           = "h264_ni_quadra_dec",
+  .p.long_name      = NULL_IF_CONFIG_SMALL("H.264 NetInt Quadra decoder v" NI_XCODER_REVISION),
+  .p.type           = AVMEDIA_TYPE_VIDEO,
+  .p.id             = AV_CODEC_ID_H264,
+  .priv_data_size = sizeof(XCoderH264DecContext),
+  .p.priv_class     = &h264_xcoderdec_class,
+  .init           = xcoder_decode_init,
+  FF_CODEC_RECEIVE_FRAME_CB(xcoder_receive_frame),
+  .close          = xcoder_decode_close,
+  .hw_configs     = ff_ni_quad_hw_configs,
+  .caps_internal  = FF_CODEC_CAP_SETS_PKT_DTS,
+  .p.capabilities   = AV_CODEC_CAP_AVOID_PROBING | AV_CODEC_CAP_DELAY | AV_CODEC_CAP_HARDWARE,
+  .p.pix_fmts       = (const enum AVPixelFormat[]){ AV_PIX_FMT_YUV420P, AV_PIX_FMT_NV12,
+                                                  AV_PIX_FMT_YUV420P10LE, AV_PIX_FMT_P010LE,
+                                                  AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE },
+  .bsfs           = "h264_mp4toannexb",
+};
diff --git a/libavcodec/nidec_h264_logan.c b/libavcodec/nidec_h264_logan.c
new file mode 100644
index 0000000000..1784cbe7d0
--- /dev/null
+++ b/libavcodec/nidec_h264_logan.c
@@ -0,0 +1,101 @@
+/*
+ * XCoder H.264 Decoder
+ * Copyright (c) 2018 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder decoder.
+ */
+
+#include "nidec_logan.h"
+
+
+#define OFFSETDEC(x) offsetof(XCoderLoganDecContext, x)
+#define VD AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_DECODING_PARAM
+static const AVOption dec_options[] = {
+  { "xcoder",    "Select which XCoder card to use.",  OFFSETDEC(dev_xcoder),
+    AV_OPT_TYPE_STRING, { .str = "bestload" }, CHAR_MIN, CHAR_MAX, VD, "xcoder" },
+
+  { "bestload",      "Pick the least loaded XCoder/decoder available.", 0, AV_OPT_TYPE_CONST,
+    { .str = "bestload" }, 0, 0, VD, "xcoder" },
+
+  { "bestinst",      "Pick the XCoder/decoder with the least number of running decoding instances.", 0, AV_OPT_TYPE_CONST,
+    { .str = "bestinst" }, 0, 0, VD, "xcoder" },
+
+  { "list",      "List the available XCoder cards.", 0, AV_OPT_TYPE_CONST,
+    { .str = "list" }, 0, 0, VD, "xcoder" },
+
+  { "dec",       "Select which decoder to use by index. First is 0, second is 1, and so on.", OFFSETDEC(dev_dec_idx),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VD, "dec" },
+
+  { "keep_alive_timeout",       "Specify a custom session keep alive timeout in seconds.", OFFSETDEC(keep_alive_timeout),
+    AV_OPT_TYPE_INT, { .i64 = NI_LOGAN_DEFAULT_KEEP_ALIVE_TIMEOUT }, NI_LOGAN_MIN_KEEP_ALIVE_TIMEOUT, NI_LOGAN_MAX_KEEP_ALIVE_TIMEOUT, VD, "keep_alive_timeout" },
+
+  { "user_data_sei_passthru",       "Enable user data unregistered SEI passthrough.", OFFSETDEC(enable_user_data_sei_passthru),
+    AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VD, "user_data_sei_passthru" },
+
+  { "check_packet",       "Enable checking source packets. Skip SEI payloads after SLICE", OFFSETDEC(enable_check_packet),
+    AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VD, "check_packet" },
+
+  { "custom_sei_passthru",       "Specify a custom SEI type to passthrough.", OFFSETDEC(custom_sei),
+    AV_OPT_TYPE_INT, { .i64 = -1 }, -1, 254, VD, "custom_sei_passthru" },
+
+  { "low_delay",       "Specify a decode timeout value (in milliseconds, recommended value is 600) "
+    "to enable low delay mode. Should be used only for streams that are in sequence.", OFFSETDEC(low_delay),
+    AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 10000, VD, "low_delay" },
+
+  { "hwframes",       "Use hwframes to reduce YUV buffer traffic.", OFFSETDEC(hwFrames),
+    AV_OPT_TYPE_INT,{ .i64 = HW_FRAMES_OFF }, 0, INT_MAX, VD, "hwFrames" },
+
+  { "xcoder-params", "Set the XCoder configuration using a :-separated list of key=value parameters", OFFSETDEC(xcoder_opts),
+    AV_OPT_TYPE_STRING,{ 0 }, 0, 0, VD },
+
+  { "set_high_priority",       "Specify a custom session set high priority in 0 or 1.", OFFSETDEC(set_high_priority),
+    AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 1, VD, "set_high_priority" },
+
+  { NULL }
+};
+
+static const AVClass h264_xcoderdec_class = {
+  .class_name = "h264_ni_logan_dec",
+  .item_name = av_default_item_name,
+  .option = dec_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+AVCodec ff_h264_ni_logan_decoder = {
+  .name           = "h264_ni_logan_dec",
+  .long_name      = NULL_IF_CONFIG_SMALL("H.264 NetInt Logan decoder v" NI_LOGAN_XCODER_REVISION),
+  .type           = AVMEDIA_TYPE_VIDEO,
+  .id             = AV_CODEC_ID_H264,
+  .priv_data_size = sizeof(XCoderLoganDecContext),
+  .priv_class     = &h264_xcoderdec_class,
+  .init           = xcoder_logan_decode_init,
+  .receive_frame  = xcoder_logan_receive_frame,
+  .close          = xcoder_logan_decode_close,
+  .caps_internal  = FF_CODEC_CAP_SETS_PKT_DTS,
+  .capabilities   = AV_CODEC_CAP_AVOID_PROBING | AV_CODEC_CAP_DELAY
+#ifndef NI_DEC_GSTREAMER_SUPPORT
+  | AV_CODEC_CAP_HARDWARE
+#endif
+,
+  .pix_fmts       = (const enum AVPixelFormat[]){ AV_PIX_FMT_YUV420P, AV_PIX_FMT_YUV420P10BE, AV_PIX_FMT_YUV420P10LE, AV_PIX_FMT_NONE },
+  .bsfs           = "h264_mp4toannexb",
+};
diff --git a/libavcodec/nidec_hevc.c b/libavcodec/nidec_hevc.c
new file mode 100644
index 0000000000..024cf5ff58
--- /dev/null
+++ b/libavcodec/nidec_hevc.c
@@ -0,0 +1,140 @@
+/*
+ * XCoder HEVC Decoder
+ * Copyright (c) 2018 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder decoder.
+ */
+
+#include "nidec.h"
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+#include "hwconfig.h"
+#else
+#include "hwaccel.h"
+#endif
+
+#include "codec_internal.h"
+
+static const AVCodecHWConfigInternal *ff_ni_quad_hw_configs[] = {
+  &(const AVCodecHWConfigInternal) {
+  .public = {
+    .pix_fmt = AV_PIX_FMT_NI_QUAD,
+    .methods = AV_CODEC_HW_CONFIG_METHOD_HW_FRAMES_CTX |
+    AV_CODEC_HW_CONFIG_METHOD_AD_HOC | AV_CODEC_HW_CONFIG_METHOD_HW_DEVICE_CTX,
+    .device_type = AV_HWDEVICE_TYPE_NI_QUADRA,
+  },
+    .hwaccel = NULL,
+},
+NULL
+};
+
+static const AVOption dec_options[] = {
+    {"dec",
+     "Select which decoder to use by index. First is 0, second is 1, and so "
+     "on.",
+     OFFSETDEC(dev_dec_idx),
+     AV_OPT_TYPE_INT,
+     {.i64 = BEST_DEVICE_LOAD},
+     -1,
+     INT_MAX,
+     VD,
+     "dec"},
+
+    {"user_data_sei_passthru",
+     "Enable user data unregistered SEI passthrough.",
+     OFFSETDEC(enable_user_data_sei_passthru),
+     AV_OPT_TYPE_BOOL,
+     {.i64 = 0},
+     0,
+     1,
+     VD,
+     "user_data_sei_passthru"},
+
+    {"custom_sei_passthru",
+     "Specify a custom SEI type to passthrough.",
+     OFFSETDEC(custom_sei_type),
+     AV_OPT_TYPE_INT,
+     {.i64 = -1},
+     -1,
+     254,
+     VD,
+     "custom_sei_passthru"},
+
+    {"xcoder-params",
+     "Set the XCoder configuration using a :-separated list of key=value "
+     "parameters",
+     OFFSETDEC(xcoder_opts),
+     AV_OPT_TYPE_STRING,
+     {0},
+     0,
+     0,
+     VD},
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSETDEC(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     VD,
+     "keep_alive_timeout"},
+
+    {"low_delay",
+     "Enable low delay decoding mode for 1 in, 1 out decoding sequence. set 1 "
+     "to enable low delay mode. Should be used only for streams that are in "
+     "sequence.",
+     OFFSETDEC(low_delay),
+     AV_OPT_TYPE_INT,
+     {.i64 = 0},
+     0,
+     1,
+     VD,
+     "low_delay"},
+
+    {NULL}};
+
+static const AVClass h265_xcoderdec_class = {
+  .class_name = "h265_ni_quadra_dec",
+  .item_name = av_default_item_name,
+  .option = dec_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+const FFCodec ff_h265_ni_quadra_decoder = {
+  .p.name           = "h265_ni_quadra_dec",
+  .p.long_name      = NULL_IF_CONFIG_SMALL("H.265 NetInt Quadra decoder v" NI_XCODER_REVISION),
+  .p.type           = AVMEDIA_TYPE_VIDEO,
+  .p.id             = AV_CODEC_ID_HEVC,
+  .priv_data_size = sizeof(XCoderH264DecContext),
+  .p.priv_class     = &h265_xcoderdec_class,
+  .init           = xcoder_decode_init,
+  FF_CODEC_RECEIVE_FRAME_CB(xcoder_receive_frame),
+  .close          = xcoder_decode_close,
+  .hw_configs     = ff_ni_quad_hw_configs,
+  .caps_internal  = FF_CODEC_CAP_SETS_PKT_DTS,
+  .p.capabilities   = AV_CODEC_CAP_AVOID_PROBING | AV_CODEC_CAP_DELAY | AV_CODEC_CAP_HARDWARE,
+  .p.pix_fmts       = (const enum AVPixelFormat[]){ AV_PIX_FMT_YUV420P, AV_PIX_FMT_NV12,
+                                                  AV_PIX_FMT_YUV420P10LE, AV_PIX_FMT_P010LE,
+                                                  AV_PIX_FMT_NONE },
+  .bsfs           = "hevc_mp4toannexb",
+};
diff --git a/libavcodec/nidec_hevc_logan.c b/libavcodec/nidec_hevc_logan.c
new file mode 100644
index 0000000000..79ba668488
--- /dev/null
+++ b/libavcodec/nidec_hevc_logan.c
@@ -0,0 +1,101 @@
+/*
+ * XCoder HEVC Decoder
+ * Copyright (c) 2018 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder decoder.
+ */
+
+#include "nidec_logan.h"
+
+
+#define OFFSETDEC(x) offsetof(XCoderLoganDecContext, x)
+#define VD AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_DECODING_PARAM
+static const AVOption dec_options[] = {
+  { "xcoder",    "Select which XCoder card to use.",  OFFSETDEC(dev_xcoder),
+    AV_OPT_TYPE_STRING, { .str = "bestload" }, CHAR_MIN, CHAR_MAX, VD, "xcoder" },
+
+  { "bestload",      "Pick the least loaded XCoder/decoder available.", 0, AV_OPT_TYPE_CONST,
+    { .str = "bestload" }, 0, 0, VD, "xcoder" },
+
+  { "bestinst",      "Pick the XCoder/decoder with the least number of running decoding instances.", 0, AV_OPT_TYPE_CONST,
+    { .str = "bestinst" }, 0, 0, VD, "xcoder" },
+
+  { "list",      "List the available XCoder cards.", 0, AV_OPT_TYPE_CONST,
+    { .str = "list" }, 0, 0, VD, "xcoder" },
+
+  { "dec",       "Select which decoder to use by index. First is 0, second is 1, and so on.", OFFSETDEC(dev_dec_idx),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VD, "dec" },
+
+  { "keep_alive_timeout",       "Specify a custom session keep alive timeout in seconds.", OFFSETDEC(keep_alive_timeout),
+    AV_OPT_TYPE_INT, { .i64 = NI_LOGAN_DEFAULT_KEEP_ALIVE_TIMEOUT }, NI_LOGAN_MIN_KEEP_ALIVE_TIMEOUT, NI_LOGAN_MAX_KEEP_ALIVE_TIMEOUT, VD, "keep_alive_timeout" },
+
+  { "user_data_sei_passthru",       "Enable user data unregistered SEI passthrough.", OFFSETDEC(enable_user_data_sei_passthru),
+    AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VD, "user_data_sei_passthru" },
+
+  { "check_packet",       "Enable checking source packets. Skip SEI payloads after SLICE", OFFSETDEC(enable_check_packet),
+    AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VD, "check_packet" },
+
+  { "custom_sei_passthru",       "Specify a custom SEI type to passthrough.", OFFSETDEC(custom_sei),
+    AV_OPT_TYPE_INT, { .i64 = -1 }, -1, 254, VD, "custom_sei_passthru" },
+
+  { "low_delay",       "Specify a decode timeout value (in milliseconds, recommended value is 600) "
+    "to enable low delay mode. Should be used only for streams that are in sequence.", OFFSETDEC(low_delay),
+    AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 10000, VD, "low_delay" },
+
+  { "hwframes",       "Use hwframes to reduce YUV buffer traffic.", OFFSETDEC(hwFrames),
+    AV_OPT_TYPE_INT,{ .i64 = HW_FRAMES_OFF }, 0, INT_MAX, VD, "hwFrames" },
+
+  { "xcoder-params", "Set the XCoder configuration using a :-separated list of key=value parameters", OFFSETDEC(xcoder_opts),
+    AV_OPT_TYPE_STRING,{ 0 }, 0, 0, VD },
+
+  { "set_high_priority",       "Specify a custom session set high priority in 0 or 1", OFFSETDEC(set_high_priority),
+    AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 1, VD, "set_high_priority" },
+
+  { NULL }
+};
+
+static const AVClass h265_xcoderdec_class = {
+  .class_name = "h265_ni_logan_dec",
+  .item_name = av_default_item_name,
+  .option = dec_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+AVCodec ff_h265_ni_logan_decoder = {
+  .name           = "h265_ni_logan_dec",
+  .long_name      = NULL_IF_CONFIG_SMALL("H.265 NetInt Logan decoder v" NI_LOGAN_XCODER_REVISION),
+  .type           = AVMEDIA_TYPE_VIDEO,
+  .id             = AV_CODEC_ID_HEVC,
+  .priv_data_size = sizeof(XCoderLoganDecContext),
+  .priv_class     = &h265_xcoderdec_class,
+  .init           = xcoder_logan_decode_init,
+  .receive_frame  = xcoder_logan_receive_frame,
+  .close          = xcoder_logan_decode_close,
+  .caps_internal  = FF_CODEC_CAP_SETS_PKT_DTS,
+  .capabilities   = AV_CODEC_CAP_AVOID_PROBING | AV_CODEC_CAP_DELAY
+#ifndef NI_DEC_GSTREAMER_SUPPORT
+  | AV_CODEC_CAP_HARDWARE
+#endif
+,
+  .pix_fmts       = (const enum AVPixelFormat[]){ AV_PIX_FMT_YUV420P, AV_PIX_FMT_YUV420P10BE, AV_PIX_FMT_YUV420P10LE, AV_PIX_FMT_NONE },
+  .bsfs           = "hevc_mp4toannexb",
+};
diff --git a/libavcodec/nidec_jpeg.c b/libavcodec/nidec_jpeg.c
new file mode 100644
index 0000000000..c6d6ca9673
--- /dev/null
+++ b/libavcodec/nidec_jpeg.c
@@ -0,0 +1,102 @@
+/*
+ * XCoder JPEG Decoder
+ * Copyright (c) 2021 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "nidec.h"
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+#include "hwconfig.h"
+#else
+#include "hwaccel.h"
+#endif
+#include "profiles.h"
+
+#include "codec_internal.h"
+
+static const AVCodecHWConfigInternal *ff_ni_quad_hw_configs[] = {
+  &(const AVCodecHWConfigInternal) {
+  .public = {
+    .pix_fmt = AV_PIX_FMT_NI_QUAD,
+    .methods = AV_CODEC_HW_CONFIG_METHOD_HW_FRAMES_CTX |
+    AV_CODEC_HW_CONFIG_METHOD_AD_HOC | AV_CODEC_HW_CONFIG_METHOD_HW_DEVICE_CTX,
+    .device_type = AV_HWDEVICE_TYPE_NI_QUADRA,
+  },
+    .hwaccel = NULL,
+},
+NULL
+};
+
+static const AVOption dec_options[] = {
+    {"dec",
+     "Select which decoder to use by index. First is 0, second is 1, and so "
+     "on.",
+     OFFSETDEC(dev_dec_idx),
+     AV_OPT_TYPE_INT,
+     {.i64 = BEST_DEVICE_LOAD},
+     -1,
+     INT_MAX,
+     VD,
+     "dec"},
+    {"xcoder-params",
+     "Set the XCoder configuration using a :-separated list of key=value "
+     "parameters",
+     OFFSETDEC(xcoder_opts),
+     AV_OPT_TYPE_STRING,
+     {0},
+     0,
+     0,
+     VD},
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSETDEC(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     VD,
+     "keep_alive_timeout"},
+    {NULL},
+};
+
+#define JPEG_NI_QUADRA_DEC "jpeg_ni_quadra_dec"
+
+static const AVClass jpeg_xcoderdec_class = {
+    .class_name = JPEG_NI_QUADRA_DEC,
+    .item_name  = av_default_item_name,
+    .option     = dec_options,
+    .version    = LIBAVUTIL_VERSION_INT,
+};
+
+const FFCodec ff_jpeg_ni_quadra_decoder = {
+    .p.name           = JPEG_NI_QUADRA_DEC,
+    .p.long_name      = NULL_IF_CONFIG_SMALL("JPEG NetInt Quadra decoder v" NI_XCODER_REVISION),
+    .p.type           = AVMEDIA_TYPE_VIDEO,
+    .p.id             = AV_CODEC_ID_MJPEG,
+    .hw_configs     = ff_ni_quad_hw_configs,
+    .caps_internal  = FF_CODEC_CAP_SETS_PKT_DTS,
+    .p.capabilities   = AV_CODEC_CAP_AVOID_PROBING | AV_CODEC_CAP_DELAY | AV_CODEC_CAP_HARDWARE,
+    .init           = xcoder_decode_init,
+    .close          = xcoder_decode_close,
+    FF_CODEC_RECEIVE_FRAME_CB(xcoder_receive_frame),
+    .p.priv_class     = &jpeg_xcoderdec_class,
+    .priv_data_size = sizeof(XCoderH264DecContext),
+    .p.pix_fmts       = (const enum AVPixelFormat[]){ AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_NI_QUAD,
+                                                    AV_PIX_FMT_NONE },
+};
diff --git a/libavcodec/nidec_logan.c b/libavcodec/nidec_logan.c
new file mode 100644
index 0000000000..5acf747c76
--- /dev/null
+++ b/libavcodec/nidec_logan.c
@@ -0,0 +1,557 @@
+/*
+ * NetInt XCoder H.264/HEVC Decoder common code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder decoder.
+ */
+
+#include "nidec_logan.h"
+
+int xcoder_logan_decode_close(AVCodecContext *avctx)
+{
+  XCoderLoganDecContext *s = avctx->priv_data;
+  av_log(avctx, AV_LOG_DEBUG, "XCoder decode close\n");
+
+  /* this call shall release resource based on s->api_ctx */
+  ff_xcoder_logan_dec_close(avctx, s);
+
+  ni_logan_packet_buffer_free(&s->api_pkt.data.packet);
+  ni_logan_device_session_context_clear(&s->api_ctx);
+
+  av_packet_unref(&s->buffered_pkt);
+  av_packet_unref(&s->seq_hdr_pkt);
+
+  free(s->extradata);
+  s->extradata = NULL;
+  s->extradata_size = 0;
+  s->got_first_key_frame = 0;
+
+  ni_logan_rsrc_free_device_context(s->rsrc_ctx);
+  s->rsrc_ctx = NULL;
+
+  return 0;
+}
+
+static int xcoder_logan_setup_decoder(AVCodecContext *avctx)
+{
+  XCoderLoganDecContext *s = avctx->priv_data;
+
+  av_log(avctx, AV_LOG_DEBUG, "XCoder setup device decoder\n");
+
+  ni_logan_device_session_context_init(&(s->api_ctx));
+
+  s->api_ctx.codec_format = NI_LOGAN_CODEC_FORMAT_H264;
+  if (avctx->codec_id == AV_CODEC_ID_HEVC)
+  {
+    s->api_ctx.codec_format = NI_LOGAN_CODEC_FORMAT_H265;
+  }
+
+  if (0 == strcmp(s->dev_xcoder, LIST_DEVICES_STR))
+  {
+    av_log(avctx, AV_LOG_DEBUG, "XCoder: printing out all xcoder devices and their load, and exit ...\n");
+    ni_logan_rsrc_print_all_devices_capability();
+    return AVERROR_EXIT;
+  }
+  else if (avctx->width > NI_LOGAN_MAX_RESOLUTION_WIDTH ||
+           avctx->height > NI_LOGAN_MAX_RESOLUTION_HEIGHT ||
+           avctx->width * avctx->height > NI_LOGAN_MAX_RESOLUTION_AREA)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Error XCoder resolution %dx%d not supported\n",
+           avctx->width, avctx->height);
+    av_log(avctx, AV_LOG_ERROR, "Max Supported Width: %d Height %d Area %d\n",
+           NI_LOGAN_MAX_RESOLUTION_WIDTH, NI_LOGAN_MAX_RESOLUTION_HEIGHT, NI_LOGAN_MAX_RESOLUTION_AREA);
+    return AVERROR_EXTERNAL;
+  }
+
+  s->offset = 0LL;
+  s->draining = 0;
+  s->api_ctx.pic_reorder_delay = avctx->has_b_frames;
+  s->api_ctx.bit_depth_factor = 1;
+
+  if (AV_PIX_FMT_YUV420P10BE == avctx->sw_pix_fmt ||
+      AV_PIX_FMT_YUV420P10LE == avctx->sw_pix_fmt)
+  {
+    s->api_ctx.bit_depth_factor = 2;
+  }
+
+  return 0;
+}
+
+int xcoder_logan_decode_init(AVCodecContext *avctx)
+{
+  int ret = 0;
+  XCoderLoganDecContext *s = avctx->priv_data;
+  const AVPixFmtDescriptor *desc;
+  ni_logan_encoder_params_t *p_param = &s->api_param;
+
+  ni_log_set_level(ff_to_ni_log_level(av_log_get_level()));
+
+  av_log(avctx, AV_LOG_DEBUG, "XCoder decode init pix_fmt %d\n",
+         avctx->pix_fmt);
+
+  if (s->dev_xcoder == NULL)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Error: XCoder decode options dev_xcoder is null\n");
+    return AVERROR_INVALIDDATA;
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "XCoder options: dev_xcoder: %s dev_dec_idx %d\n",
+           s->dev_xcoder, s->dev_dec_idx);
+  }
+
+  avctx->sw_pix_fmt = avctx->pix_fmt;
+
+  desc = av_pix_fmt_desc_get(avctx->sw_pix_fmt);
+  av_log(avctx, AV_LOG_VERBOSE, "width: %d height: %d sw_pix_fmt: %s\n",
+         avctx->width, avctx->height, desc ? desc->name : "NONE");
+
+  if (0 == avctx->width || 0 == avctx->height)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Error probing input stream\n");
+    return AVERROR_INVALIDDATA;
+  }
+
+  switch (avctx->pix_fmt)
+  {
+    case AV_PIX_FMT_YUV420P:
+    case AV_PIX_FMT_YUV420P10BE:
+    case AV_PIX_FMT_YUV420P10LE:
+    case AV_PIX_FMT_YUVJ420P:
+      break;
+    default:
+      av_log(avctx, AV_LOG_ERROR, "Error: pixel format %s not supported.\n",
+             desc ? desc->name : "NONE");
+      return AVERROR_INVALIDDATA;
+  }
+
+  // Check profile idc
+  if (avctx->codec_id == AV_CODEC_ID_H264)
+  {
+    switch (avctx->profile)
+    {
+      case FF_PROFILE_H264_BASELINE:
+      case FF_PROFILE_H264_CONSTRAINED_BASELINE:
+      case FF_PROFILE_H264_MAIN:
+      case FF_PROFILE_H264_EXTENDED:
+      case FF_PROFILE_H264_HIGH:
+      case FF_PROFILE_H264_HIGH_10:
+        break;
+      default:
+        av_log(avctx, AV_LOG_ERROR, "Error: profile %d not supported.\n", avctx->profile);
+        return AVERROR_INVALIDDATA;
+    }
+  }
+  else if (avctx->codec_id == AV_CODEC_ID_HEVC)
+  {
+    switch (avctx->profile)
+    {
+      case FF_PROFILE_HEVC_MAIN:
+      case FF_PROFILE_HEVC_MAIN_10:
+      case FF_PROFILE_HEVC_MAIN_STILL_PICTURE:
+        break;
+      default:
+        av_log(avctx, AV_LOG_ERROR, "Error: profile %d not supported.\n", avctx->profile);
+        return AVERROR_INVALIDDATA;
+    }
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "(avctx->field_order = %d)\n", avctx->field_order);
+  if (avctx->field_order > AV_FIELD_PROGRESSIVE)
+  { //AVFieldOrder with bottom or top coding order represents interlaced video
+    av_log(avctx, AV_LOG_ERROR, "interlaced video not supported!\n");
+    return AVERROR_INVALIDDATA;
+  }
+
+  if ((ret = xcoder_logan_setup_decoder(avctx)) < 0)
+  {
+    return ret;
+  }
+
+  // reference h264_decode_init in h264dec.c
+  if (avctx->ticks_per_frame == 1)
+  {
+    if (avctx->time_base.den < INT_MAX / 2)
+    {
+      avctx->time_base.den *= 2;
+    }
+    else
+    {
+      avctx->time_base.num /= 2;
+    }
+  }
+
+  avctx->ticks_per_frame = 2;
+
+  s->started = 0;
+  memset(&s->api_pkt, 0, sizeof(ni_logan_packet_t));
+  s->got_first_key_frame = 0;
+  s->pkt_nal_bitmap = 0;
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+  s->cur_gs_opaque = NULL;
+  s->cur_gs_buf0 = NULL;
+  int i = 0;
+  for (i = 0; i < NI_LOGAN_FIFO_SZ; i++) {
+    s->gs_data[i].opaque = NULL;
+    s->gs_data[i].buf0 = NULL;
+  }
+#endif
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder decode init: time_base = %d/%d, "
+         "frame rate = %d/%d, ticks_per_frame=%d\n", avctx->time_base.num,
+         avctx->time_base.den, avctx->framerate.num, avctx->framerate.den,
+         avctx->ticks_per_frame);
+
+  //Xcoder User Configuration
+
+  if (ni_logan_decoder_init_default_params(p_param, avctx->framerate.num,
+      avctx->framerate.den, avctx->bit_rate, avctx->width, avctx->height) < 0)
+  {
+    av_log(avctx, AV_LOG_INFO, "Error setting params\n");
+    return AVERROR(EINVAL);
+  }
+
+  if (s->xcoder_opts)
+  {
+    AVDictionary *dict = NULL;
+    AVDictionaryEntry *dec = NULL;
+
+    if (! av_dict_parse_string(&dict, s->xcoder_opts, "=", ":", 0))
+    {
+      while ((dec = av_dict_get(dict, "", dec, AV_DICT_IGNORE_SUFFIX)))
+      {
+        int parse_ret = ni_logan_decoder_params_set_value(p_param, dec->key,
+                                                    dec->value);
+        if (NI_LOGAN_RETCODE_SUCCESS != parse_ret)
+        {
+          av_log(avctx, AV_LOG_ERROR, "Error parsing xcoder-params: %d\n",
+                 parse_ret);
+          return AVERROR_EXTERNAL;
+        }
+      }
+      av_dict_free(&dict);
+    }
+  }
+
+  // overwrite keep alive timeout value here with a custom value if it was
+  // provided
+  // if xcoder option is set then overwrite the (legacy) decoder option
+  uint32_t xcoder_timeout = s->api_param.dec_input_params.keep_alive_timeout;
+  if (xcoder_timeout != NI_LOGAN_DEFAULT_KEEP_ALIVE_TIMEOUT)
+  {
+    s->api_ctx.keep_alive_timeout = xcoder_timeout;
+  }
+  else
+  {
+    s->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+  }
+  av_log(avctx, AV_LOG_VERBOSE, "Custom NVME Keep Alive Timeout set to %d\n",
+         s->api_ctx.keep_alive_timeout);
+
+  //overwrite set_high_priority value here with a custom value if it was provided
+  uint32_t xcoder_high_priority = s->api_param.dec_input_params.set_high_priority;
+  if(xcoder_high_priority != 0)
+  {
+    s->api_ctx.set_high_priority = xcoder_high_priority;
+  }
+  else
+  {
+    s->api_ctx.set_high_priority = s->set_high_priority;
+  }
+  av_log(avctx, AV_LOG_VERBOSE, "Custom NVMe set_high_priority set to = %d\n",
+         s->api_ctx.set_high_priority);
+  //hwframes can be set from 'out=hw' or 'hwframes 1' param
+  p_param->dec_input_params.hwframes = s->hwFrames | p_param->dec_input_params.hwframes;
+
+  if (s->hwFrames || p_param->dec_input_params.hwframes)
+  {
+    s->api_ctx.hw_action = NI_LOGAN_CODEC_HW_ENABLE;
+    av_log(avctx, AV_LOG_TRACE, "xcoder_logan_decode_init: enable hw codec\n");
+  }
+  else
+  {
+    s->api_ctx.hw_action = NI_LOGAN_CODEC_HW_NONE;//
+  }
+
+  s->api_ctx.p_session_config = &s->api_param;
+
+  if ((ret = ff_xcoder_logan_dec_init(avctx, s)) < 0)
+  {
+    goto done;
+  }
+
+  s->current_pts = 0;
+
+done:
+  if (NI_INVALID_DEVICE_HANDLE == s->api_ctx.blk_io_handle ||
+      NI_INVALID_DEVICE_HANDLE == s->api_ctx.device_handle)
+  {
+    xcoder_logan_decode_close(avctx);
+  }
+
+  return ret;
+}
+
+// reset and restart when xcoder decoder resets
+int xcoder_logan_decode_reset(AVCodecContext *avctx)
+{
+  XCoderLoganDecContext *s = avctx->priv_data;
+  ni_logan_retcode_t ret = NI_LOGAN_RETCODE_FAILURE;
+  int64_t bcp_current_pts = s->current_pts;
+  int draining = s->draining;
+
+  av_log(avctx, AV_LOG_WARNING, "XCoder decode reset\n");
+
+  s->vpu_reset = 1;
+
+  ret = ni_logan_device_session_close(&s->api_ctx, s->eos, NI_LOGAN_DEVICE_TYPE_DECODER);
+
+#ifdef _WIN32
+  ni_logan_device_close(s->api_ctx.device_handle);
+#elif __linux__
+  ni_logan_device_close(s->api_ctx.device_handle);
+  ni_logan_device_close(s->api_ctx.blk_io_handle);
+#endif
+  s->api_ctx.device_handle = NI_INVALID_DEVICE_HANDLE;
+  s->api_ctx.blk_io_handle = NI_INVALID_DEVICE_HANDLE;
+
+  ni_logan_packet_buffer_free(&s->api_pkt.data.packet);
+  ni_logan_device_session_context_clear(&s->api_ctx);
+
+  ret = xcoder_logan_decode_init(avctx);
+  s->draining = draining;  // recover the draining state when resetting.
+  s->current_pts = bcp_current_pts;
+  s->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_RESETTING;
+
+  // On VPU recovery send the cached sequence headers with IDR first.
+  if (s->seq_hdr_pkt.size > 0 &&
+      ff_xcoder_logan_dec_send(avctx, s, &s->seq_hdr_pkt) < 0)
+  {
+    s->vpu_reset = 0;
+    return AVERROR_EXTERNAL;
+  }
+
+  return ret;
+}
+
+static int xcoder_logan_send_receive(AVCodecContext *avctx,
+                                     XCoderLoganDecContext *s,
+                                     AVFrame *frame, bool wait)
+{
+  int ret;
+
+  if (s->buffered_pkt.size > 0)
+  {
+    ret = ff_xcoder_logan_dec_send(avctx, s, &s->buffered_pkt);
+    if (ret == AVERROR(EAGAIN))
+    {
+      av_log(avctx, AV_LOG_DEBUG, "ff_xcoder_logan_dec_send() return eagain\n");
+    }
+    else if (ret < 0)
+    {
+      return ret;
+    }
+    else
+    {
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+    // pkt is sent out, store GS data based on pkt offset so it can be
+    // retrieved when the decoded frame is returned
+    s->gs_data[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ].opaque =
+    s->cur_gs_opaque;
+    s->gs_data[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ].buf0 = s->cur_gs_buf0;
+    s->cur_gs_opaque = NULL;
+    s->cur_gs_buf0 = NULL;
+
+    s->gs_opaque_offsets_index_min[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ] =
+    s->api_ctx.pkt_offsets_index_min[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ];
+
+    s->gs_opaque_offsets_index[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ] =
+    s->api_ctx.pkt_offsets_index[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ];
+
+    av_log(avctx, AV_LOG_DEBUG, "pkt # %"PRIu64" bytes %d offset %"PRIu64" "
+           "%"PRIu64" opaque %p buf0 %p\n", (s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ,
+           s->buffered_pkt.size,
+           s->gs_opaque_offsets_index_min[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ],
+           s->gs_opaque_offsets_index[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ],
+           s->gs_data[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ].opaque,
+	   s->gs_data[(s->api_ctx.pkt_index - 1) % NI_LOGAN_FIFO_SZ].buf0);
+#endif
+
+      av_packet_unref(&s->buffered_pkt);
+    }
+  }
+
+  /* check for new frame */
+  ret = ff_xcoder_logan_dec_receive(avctx, s, frame, wait);
+  if (NI_LOGAN_RETCODE_ERROR_VPU_RECOVERY == ret)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to receive frame because of VPU recovery\n");
+    ret = xcoder_logan_decode_reset(avctx);
+    if (0 == ret)
+    {
+      return AVERROR(EAGAIN);
+    }
+  }
+
+  return ret;
+}
+
+int xcoder_logan_receive_frame(AVCodecContext *avctx, AVFrame *frame)
+{
+  XCoderLoganDecContext *s = avctx->priv_data;
+  int ret;
+  const AVPixFmtDescriptor *desc;
+
+  av_log(avctx, AV_LOG_DEBUG, "XCoder receive frame\n");
+
+  /*
+   * reference mediacodec_receive_frame in mediacodec.c.
+   *
+   * After we have buffered an input packet, check if the codec is in the
+   * flushing state. If it is, we need to call ff_xcoder_logan_dec_flush.
+   *
+   * ff_xcoder_logan_dec_flush returns 0 if the flush cannot be performed on
+   * the codec (because the user retains frames). The codec stays in the
+   * flushing state.
+   *
+   * ff_xcoder_logan_dec_flush returns 1 if the flush can actually be
+   * performed on the codec. The codec leaves the flushing state and can
+   * process again packets.
+   *
+   * ff_xcoder_logan_dec_flush returns a negative value if an error has
+   * occurred.
+   *
+   * NetInt: for now we don't consider the case of user retaining the frame
+   *         (connected decoder-encoder case), so the return can only be 1
+   *         (flushed successfully), or < 0 (failure)
+   */
+  if (ff_xcoder_logan_dec_is_flushing(avctx, s))
+  {
+    if (!ff_xcoder_logan_dec_flush(avctx, s))
+    {
+      return AVERROR(EAGAIN);
+    }
+  }
+
+  // give priority to sending data to decoder
+  if (s->buffered_pkt.size == 0)
+  {
+    ret = ff_decode_get_packet(avctx, &s->buffered_pkt);
+    if (ret < 0)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 1 rc: %s\n", av_err2str(ret));
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 1 rc: Success\n");
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+      AVFrame frame_copy = {0};
+      av_frame_copy_props(&frame_copy, frame);
+
+      // retrieve/save from GStreamer info of this pkt which is the
+      // returned frame's opaque and buf[0]
+      ff_get_buffer(avctx, &frame_copy, 0);
+      s->cur_gs_opaque = frame_copy.opaque;
+      s->cur_gs_buf0 = frame_copy.buf[0];
+      frame_copy.opaque = NULL;
+      frame_copy.buf[0] = NULL;
+
+      av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 1 rc: Success pkt size "
+             "= %d call ff_get_buffer return opaque %p buf0 %p\n",
+	     s->buffered_pkt.size, s->cur_gs_opaque, s->cur_gs_buf0);
+
+      av_frame_unref(&frame_copy);
+#endif
+    }
+  }
+
+  /* flush buffered packet and check for new frame */
+  ret = xcoder_logan_send_receive(avctx, s, frame, false);
+  if (ret != AVERROR(EAGAIN))
+  {
+    return ret;
+  }
+
+  /* skip fetching new packet if we still have one buffered */
+  if (s->buffered_pkt.size > 0)
+  {
+    return xcoder_logan_send_receive(avctx, s, frame, true);
+  }
+
+  /* fetch new packet or eof */
+  ret = ff_decode_get_packet(avctx, &s->buffered_pkt);
+  if (ret < 0)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 2 rc: %s\n", av_err2str(ret));
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 2 rc: Success\n");
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+    AVFrame frame_copy = {0};
+    av_frame_copy_props(&frame_copy, frame);
+
+    ff_get_buffer(avctx, &frame_copy, 0);
+    s->cur_gs_opaque = frame_copy.opaque;
+    s->cur_gs_buf0 = frame_copy.buf[0];
+    frame_copy.opaque = NULL;
+    frame_copy.buf[0] = NULL;
+
+    av_log(avctx, AV_LOG_DEBUG, "ff_decode_get_packet 2 rc: Success pkt size = "
+           "%d call ff_get_buffer return opaque %p buf0 %p\n",
+	   s->buffered_pkt.size, s->cur_gs_opaque, s->cur_gs_buf0);
+
+    av_frame_unref(&frame_copy);
+#endif
+  }
+
+  if (ret == AVERROR_EOF)
+  {
+    AVPacket null_pkt = {0};
+    ret = ff_xcoder_logan_dec_send(avctx, s, &null_pkt);
+
+    av_log(avctx, AV_LOG_DEBUG, "AVERROR_EOF, done ff_xcoder_logan_dec_send, "
+           "ret = %d\n", ret);
+
+    if (ret < 0)
+    {
+      return ret;
+    }
+  }
+  else if (ret < 0)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "ret < 0 but NOT AVERROR_EOF %d!\n", ret);
+    return ret;
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_DEBUG, "width: %d  height: %d\n", avctx->width, avctx->height);
+    desc = av_pix_fmt_desc_get(avctx->pix_fmt);
+    av_log(avctx, AV_LOG_DEBUG, "pix_fmt: %s\n", desc ? desc->name : "NONE");
+  }
+
+  /* crank decoder with new packet */
+  return xcoder_logan_send_receive(avctx, s, frame, true);
+}
diff --git a/libavcodec/nidec_logan.h b/libavcodec/nidec_logan.h
new file mode 100644
index 0000000000..31cb38cbe6
--- /dev/null
+++ b/libavcodec/nidec_logan.h
@@ -0,0 +1,51 @@
+/*
+ * NetInt XCoder H.264/HEVC Decoder common code header
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_NIDEC_LOGAN_H
+#define AVCODEC_NIDEC_LOGAN_H
+
+#include <stdbool.h>
+#include <ni_rsrc_api_logan.h>
+#include <ni_util_logan.h>
+#include <ni_device_api_logan.h>
+
+#include "avcodec.h"
+#include "decode.h"
+#include "internal.h"
+
+#include "libavutil/internal.h"
+#include "libavutil/frame.h"
+#include "libavutil/buffer.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/opt.h"
+
+#include "nicodec_logan.h"
+
+int xcoder_logan_decode_close(AVCodecContext *avctx);
+
+int xcoder_logan_decode_init (AVCodecContext *avctx);
+
+int xcoder_logan_decode_reset(AVCodecContext *avctx);
+
+int xcoder_logan_receive_frame(AVCodecContext *avctx, AVFrame *frame);
+
+
+#endif /* AVCODEC_NIDEC_LOGAN_H */
diff --git a/libavcodec/nidec_vp9.c b/libavcodec/nidec_vp9.c
new file mode 100644
index 0000000000..ac210fe055
--- /dev/null
+++ b/libavcodec/nidec_vp9.c
@@ -0,0 +1,89 @@
+/*
+ * XCoder VP9 Decoder
+ * Copyright (c) 2020 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder decoder.
+ */
+
+#include "nidec.h"
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+#include "hwconfig.h"
+#else
+#include "hwaccel.h"
+#endif
+#include "profiles.h"
+
+#include "codec_internal.h"
+
+static const AVCodecHWConfigInternal *ff_ni_quad_hw_configs[] = {
+  &(const AVCodecHWConfigInternal) {
+  .public = {
+    .pix_fmt = AV_PIX_FMT_NI_QUAD,
+    .methods = AV_CODEC_HW_CONFIG_METHOD_HW_FRAMES_CTX |
+    AV_CODEC_HW_CONFIG_METHOD_AD_HOC | AV_CODEC_HW_CONFIG_METHOD_HW_DEVICE_CTX,
+    .device_type = AV_HWDEVICE_TYPE_NI_QUADRA,
+  },
+    .hwaccel = NULL,
+},
+NULL
+};
+
+static const AVOption dec_options[] = {
+  { "dec",       "Select which decoder to use by index. First is 0, second is 1, and so on.", OFFSETDEC(dev_dec_idx),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VD, "dec" },
+
+  { "xcoder-params", "Set the XCoder configuration using a :-separated list of key=value parameters", OFFSETDEC(xcoder_opts),
+    AV_OPT_TYPE_STRING,{ 0 }, 0, 0, VD },
+    
+  { "keep_alive_timeout",       "Specify a custom session keep alive timeout in seconds.", OFFSETDEC(keep_alive_timeout),
+    AV_OPT_TYPE_INT, { .i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT }, NI_MIN_KEEP_ALIVE_TIMEOUT, NI_MAX_KEEP_ALIVE_TIMEOUT, VD, "keep_alive_timeout" },
+    
+
+  { NULL }
+};
+
+static const AVClass vp9_xcoderdec_class = {
+  .class_name = "vp9_ni_quadra_dec",
+  .item_name = av_default_item_name,
+  .option = dec_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+const FFCodec ff_vp9_ni_quadra_decoder = {
+  .p.name           = "vp9_ni_quadra_dec",
+  .p.long_name      = NULL_IF_CONFIG_SMALL("VP9 NetInt Quadra decoder v" NI_XCODER_REVISION),
+  .p.type           = AVMEDIA_TYPE_VIDEO,
+  .p.id             = AV_CODEC_ID_VP9,
+  .priv_data_size = sizeof(XCoderH264DecContext),//?
+  .p.priv_class     = &vp9_xcoderdec_class,
+  .init           = xcoder_decode_init,
+  FF_CODEC_RECEIVE_FRAME_CB(xcoder_receive_frame),
+  .close          = xcoder_decode_close,
+  .hw_configs     = ff_ni_quad_hw_configs,
+  .caps_internal  = FF_CODEC_CAP_SETS_PKT_DTS,
+  .p.capabilities   = AV_CODEC_CAP_AVOID_PROBING | AV_CODEC_CAP_DELAY | AV_CODEC_CAP_HARDWARE,
+  .p.pix_fmts       = (const enum AVPixelFormat[]){ AV_PIX_FMT_YUV420P, AV_PIX_FMT_NV12,
+                                                  AV_PIX_FMT_YUV420P10LE, AV_PIX_FMT_P010LE,
+                                                  AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE },
+  .p.profiles       = NULL_IF_CONFIG_SMALL(ff_vp9_profiles),
+};
diff --git a/libavcodec/nienc.c b/libavcodec/nienc.c
new file mode 100644
index 0000000000..3f1e889da2
--- /dev/null
+++ b/libavcodec/nienc.c
@@ -0,0 +1,4256 @@
+/*
+ * NetInt XCoder H.264/HEVC Encoder common code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+#include "nienc.h"
+#include "bytestream.h"
+#include "libavcodec/h264.h"
+#include "libavcodec/h264_sei.h"
+#include "libavcodec/hevc.h"
+#include "libavcodec/hevc_sei.h"
+#include "libavcodec/put_bits.h"
+#include "libavutil/hdr_dynamic_metadata.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/hwcontext_ni_quad.h"
+#include "libavutil/mastering_display_metadata.h"
+#include "ni_av_codec.h"
+#include "ni_util.h"
+#include "put_bits.h"
+
+#include <unistd.h>
+#if ((LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134))
+#include "encode.h"
+#endif
+
+#include "startcode.h"
+
+static ni_enc_avc_roi_custom_map_t *g_avc_roi_map = NULL;
+// H.265 test roi buffer for up to 8k resolution H.265 - 32 x 32 sub CTUs
+static uint8_t *g_hevc_sub_ctu_roi_buf = NULL;
+// H.265 custom map buffer for up to 8k resolution  - 64x64 CTU Regions
+static ni_enc_hevc_roi_custom_map_t *g_hevc_roi_map = NULL;
+// actual ROI map is stored in individual session context !
+
+//extern const char * const g_xcoder_preset_names[3];
+//extern const char * const g_xcoder_log_names[7];
+
+// sequence change not working with MULTI_THREAD
+// disable MULTI_THREAD because this is only needed for 8K parallel encode on 2 cards (4K each)
+//#define MULTI_THREAD
+#ifdef MULTI_THREAD
+#undef MULIT_THREAD
+#endif
+
+#ifdef MULTI_THREAD
+typedef struct _write_thread_arg_struct_t
+{
+  uint32_t session_id;
+  pthread_t thread;
+  XCoderH265EncContext *ctx;
+  ni_retcode_t ret;
+}write_thread_arg_struct_t;
+
+static write_thread_arg_struct_t write_thread_args[NI_MAX_NUM_SESSIONS];
+
+static uint8_t find_session_idx(uint32_t sid)
+{
+  uint8_t i;
+  for (i = 0; i < NI_MAX_NUM_SESSIONS; i++)
+  {
+    if (write_thread_args[i].session_id == sid)
+    {
+      break;
+    }
+  }
+  return i;
+}
+#endif
+
+static int expand_ni_frame(AVCodecContext *avctx, ni_frame_t *dst,
+                           const ni_frame_t *src, const int dst_stride[],
+                           int raw_width, int raw_height,
+                           enum AVPixelFormat format) {
+    int i, j, h, nb_planes, tenBit;
+    int vpad[3], hpad[3], src_height[3], src_width[3], src_stride[3];
+    uint8_t *src_line, *dst_line, *sample, *dest, YUVsample;
+    uint16_t lastidx;
+
+    nb_planes = av_pix_fmt_count_planes(format);
+
+    switch (format) {
+    case AV_PIX_FMT_YUV420P:
+        /* width of source frame for each plane in pixels */
+        src_width[0] = FFALIGN(raw_width, 2);
+        src_width[1] = FFALIGN(raw_width, 2) / 2;
+        src_width[2] = FFALIGN(raw_width, 2) / 2;
+
+        /* height of source frame for each plane in pixels */
+        src_height[0] = FFALIGN(raw_height, 2);
+        src_height[1] = FFALIGN(raw_height, 2) / 2;
+        src_height[2] = FFALIGN(raw_height, 2) / 2;
+
+        /* stride of source frame for each plane in bytes */
+        src_stride[0] = FFALIGN(src_width[0], 128);
+        src_stride[1] = FFALIGN(src_width[1], 128);
+        src_stride[2] = FFALIGN(src_width[2], 128);
+
+        tenBit = 0;
+
+        /* horizontal padding needed for each plane in bytes */
+        hpad[0] = dst_stride[0] - src_width[0];
+        hpad[1] = dst_stride[1] - src_width[1];
+        hpad[2] = dst_stride[2] - src_width[2];
+
+        /* vertical padding needed for each plane in pixels */
+        vpad[0] = NI_MIN_HEIGHT - src_height[0];
+        vpad[1] = NI_MIN_HEIGHT / 2 - src_height[1];
+        vpad[2] = NI_MIN_HEIGHT / 2 - src_height[2];
+
+        break;
+
+    case AV_PIX_FMT_YUV420P10LE:
+        /* width of source frame for each plane in pixels */
+        src_width[0] = FFALIGN(raw_width, 2);
+        src_width[1] = FFALIGN(raw_width, 2) / 2;
+        src_width[2] = FFALIGN(raw_width, 2) / 2;
+
+        /* height of source frame for each plane in pixels */
+        src_height[0] = FFALIGN(raw_height, 2);
+        src_height[1] = FFALIGN(raw_height, 2) / 2;
+        src_height[2] = FFALIGN(raw_height, 2) / 2;
+
+        /* stride of source frame for each plane in bytes */
+        src_stride[0] = FFALIGN(src_width[0] * 2, 128);
+        src_stride[1] = FFALIGN(src_width[1] * 2, 128);
+        src_stride[2] = FFALIGN(src_width[2] * 2, 128);
+
+        tenBit = 1;
+
+        /* horizontal padding needed for each plane in bytes */
+        hpad[0] = dst_stride[0] - src_width[0] * 2;
+        hpad[1] = dst_stride[1] - src_width[1] * 2;
+        hpad[2] = dst_stride[2] - src_width[2] * 2;
+
+        /* vertical padding needed for each plane in pixels */
+        vpad[0] = NI_MIN_HEIGHT - src_height[0];
+        vpad[1] = NI_MIN_HEIGHT / 2 - src_height[1];
+        vpad[2] = NI_MIN_HEIGHT / 2 - src_height[2];
+
+        break;
+
+    case AV_PIX_FMT_NV12:
+        /* width of source frame for each plane in pixels */
+        src_width[0] = FFALIGN(raw_width, 2);
+        src_width[1] = FFALIGN(raw_width, 2);
+        src_width[2] = 0;
+
+        /* height of source frame for each plane in pixels */
+        src_height[0] = FFALIGN(raw_height, 2);
+        src_height[1] = FFALIGN(raw_height, 2) / 2;
+        src_height[2] = 0;
+
+        /* stride of source frame for each plane in bytes */
+        src_stride[0] = FFALIGN(src_width[0], 128);
+        src_stride[1] = FFALIGN(src_width[1], 128);
+        src_stride[2] = 0;
+
+        tenBit = 0;
+
+        /* horizontal padding needed for each plane in bytes */
+        hpad[0] = dst_stride[0] - src_width[0];
+        hpad[1] = dst_stride[1] - src_width[1];
+        hpad[2] = 0;
+
+        /* vertical padding for each plane in pixels */
+        vpad[0] = NI_MIN_HEIGHT - src_height[0];
+        vpad[1] = NI_MIN_HEIGHT / 2 - src_height[1];
+        vpad[2] = 0;
+
+        break;
+
+    case AV_PIX_FMT_P010LE:
+        /* width of source frame for each plane in pixels */
+        src_width[0] = FFALIGN(raw_width, 2);
+        src_width[1] = FFALIGN(raw_width, 2);
+        src_width[2] = 0;
+
+        /* height of source frame for each plane in pixels */
+        src_height[0] = FFALIGN(raw_height, 2);
+        src_height[1] = FFALIGN(raw_height, 2) / 2;
+        src_height[2] = 0;
+
+        /* stride of source frame for each plane in bytes */
+        src_stride[0] = FFALIGN(src_width[0] * 2, 128);
+        src_stride[1] = FFALIGN(src_width[1] * 2, 128);
+        src_stride[2] = 0;
+
+        tenBit = 1;
+
+        /* horizontal padding needed for each plane in bytes */
+        hpad[0] = dst_stride[0] - src_width[0] * 2;
+        hpad[1] = dst_stride[1] - src_width[1] * 2;
+        hpad[2] = 0;
+
+        /* vertical padding for each plane in pixels */
+        vpad[0] = NI_MIN_HEIGHT - src_height[0];
+        vpad[1] = NI_MIN_HEIGHT / 2 - src_height[1];
+        vpad[2] = 0;
+
+        break;
+
+    default:
+        av_log(avctx, AV_LOG_ERROR, "Invalid pixel format %s\n",
+               av_get_pix_fmt_name(format));
+        return -1;
+    }
+
+    for (i = 0; i < nb_planes; i++) {
+        dst_line = dst->p_data[i];
+        src_line = src->p_data[i];
+
+        for (h = 0; h < src_height[i]; h++) {
+            memcpy(dst_line, src_line, src_width[i] * (tenBit + 1));
+
+            /* Add horizontal padding */
+            if (hpad[i]) {
+                lastidx = src_width[i];
+
+                if (tenBit) {
+                    sample = &src_line[(lastidx - 1) * 2];
+                    dest   = &dst_line[lastidx * 2];
+
+                    /* two bytes per sample */
+                    for (j = 0; j < hpad[i] / 2; j++) {
+                        memcpy(dest, sample, 2);
+                        dest += 2;
+                    }
+                } else {
+                    YUVsample = dst_line[lastidx - 1];
+                    memset(&dst_line[lastidx], YUVsample, hpad[i]);
+                }
+            }
+
+            src_line += src_stride[i];
+            dst_line += dst_stride[i];
+        }
+
+        /* Pad the height by duplicating the last line */
+        src_line = dst_line - dst_stride[i];
+
+        for (h = 0; h < vpad[i]; h++) {
+            memcpy(dst_line, src_line, dst_stride[i]);
+            dst_line += dst_stride[i];
+        }
+    }
+
+    return 0;
+}
+#if 0
+// convert FFmpeg ROIs to NetInt ROI map
+static int set_roi_map(AVCodecContext *avctx, const AVFrameSideData *sd,
+                       int nb_roi, int width, int height, int customMapSize)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  int i, j, k, m, r;
+  const AVRegionOfInterest *roi = (const AVRegionOfInterest*)sd->data;
+  uint32_t self_size = roi->self_size;
+  int32_t set_qp = 0;
+  uint32_t sumQp = 0;
+
+  uint32_t max_cu_size = (avctx->codec_id == AV_CODEC_ID_H264) ? 16 : 64;
+
+  // for H.264, select ROI Map Block Unit Size: 16x16
+  // for H.265, select ROI Map Block Unit Size: 64x64
+  uint32_t roiMapBlockUnitSize = (avctx->codec_id == AV_CODEC_ID_H264) ? 16 : 64;
+  uint32_t mbWidth = ((width+max_cu_size-1)& (~(max_cu_size - 1))) / roiMapBlockUnitSize;
+  uint32_t mbHeight = ((height+max_cu_size-1)& (~(max_cu_size - 1))) / roiMapBlockUnitSize;
+  uint32_t numMbs = mbWidth * mbHeight;
+  uint32_t subMbWidth = roiMapBlockUnitSize / 8;
+  uint32_t subMbHeight = subMbWidth;
+  uint32_t subNumMbs = subMbWidth * subMbHeight;
+
+  // init ipcm_flag to 0, roiAbsQp_falg to 0 (qp delta), and qp_info to 0
+  memset(g_quad_roi_map, 0, customMapSize);
+#if 0
+  for (i = 0; i < numMbs; i++)
+  {
+    for (j = 0; j < subNumMbs; j++)
+    {
+      g_quad_roi_map[i*subNumMbs+j].field.qp_info = NI_DEFAULT_INTRA_QP;
+    }
+  }
+#endif
+
+  // iterate ROI list from the last as regions are defined in order of
+  // decreasing importance.
+  for (r = nb_roi - 1; r >= 0; r--)
+  {
+    roi = (const AVRegionOfInterest*)(sd->data + self_size * r);
+    if (! roi->qoffset.den)
+    {
+      av_log(avctx, AV_LOG_ERROR, "AVRegionOfInterest.qoffset.den "
+             "must not be zero.\n");
+      continue;
+    }
+
+    set_qp = (int32_t)((float)roi->qoffset.num * 1.0f /
+                       (float)roi->qoffset.den * NI_INTRA_QP_RANGE);
+    set_qp = av_clip(set_qp, NI_MIN_QP_DELTA, NI_MAX_QP_DELTA);
+    // Adjust qp delta range (-25 to 25) to (0 to 63): 0 to 0, -1 to 1, -2 to 2 ... 1 to 63, 2 to 62 ...
+    // Theoretically the possible qp delta range is (-32 to 31)
+    set_qp = (NI_MAX_QP_INFO + 1 - set_qp ) % (NI_MAX_QP_INFO + 1);
+
+    av_log(avctx, AV_LOG_INFO, "set_roi_map: left %d right %d top %d bottom %d num %d den %d set_qp %d\n",
+            roi->left, roi->right, roi->top, roi->bottom, roi->qoffset.num, roi->qoffset.den, set_qp);
+
+    // copy ROI MBs QPs into custom map
+    for (j = 0; j < mbHeight; j++) {
+        for (i = 0; i < mbWidth; i++) {
+            k = j * (int)mbWidth + i;
+
+            for (m = 0; m < subNumMbs; m++) {
+                if (((int)(i % mbWidth) >=
+                     (int)((roi->left + roiMapBlockUnitSize - 1) /
+                           roiMapBlockUnitSize) -
+                         1) &&
+                    ((int)(i % mbWidth) <=
+                     (int)((roi->right + roiMapBlockUnitSize - 1) /
+                           roiMapBlockUnitSize) -
+                         1) &&
+                    ((int)(j % mbHeight) >=
+                     (int)((roi->top + roiMapBlockUnitSize - 1) /
+                           roiMapBlockUnitSize) -
+                         1) &&
+                    ((int)(j % mbHeight) <=
+                     (int)((roi->bottom + roiMapBlockUnitSize - 1) /
+                           roiMapBlockUnitSize) -
+                         1)) {
+                    g_quad_roi_map[k * subNumMbs + m].field.ipcm_flag =
+                        0; // don't force skip mode
+                    g_quad_roi_map[k * subNumMbs + m].field.roiAbsQp_flag =
+                        0; // delta QP
+                    g_quad_roi_map[k * subNumMbs + m].field.qp_info = set_qp;
+                    // av_log(avctx, AV_LOG_INFO, "## x %d y %d index %d\n", i,
+                    // j, k*subNumMbs+m);
+                }
+            }
+            sumQp += g_quad_roi_map[k * subNumMbs].field.qp_info;
+        }
+    }
+  }
+
+  ctx->api_ctx.roi_len = customMapSize;
+  ctx->api_ctx.roi_avg_qp = (sumQp + (numMbs>>1)) / numMbs + NI_DEFAULT_INTRA_QP; // round off
+
+  return 0;
+}
+#endif
+static int xcoder_encoder_headers(AVCodecContext *avctx)
+{
+  // use a copy of encoder context, take care to restore original config
+  // cropping setting
+  XCoderH265EncContext ctx;
+  ni_xcoder_params_t *p_param;
+  ni_packet_t *xpkt;
+  int orig_conf_win_right;
+  int orig_conf_win_bottom;
+  int linesize_aligned, height_aligned;
+  int ret, recv;
+
+  memcpy(&ctx, (XCoderH265EncContext *)(avctx->priv_data),
+         sizeof(XCoderH265EncContext));
+
+  p_param = (ni_xcoder_params_t *)ctx.api_ctx.p_session_config;
+
+  orig_conf_win_right  = p_param->cfg_enc_params.conf_win_right;
+  orig_conf_win_bottom = p_param->cfg_enc_params.conf_win_bottom;
+
+  linesize_aligned = avctx->width;
+  if (linesize_aligned < NI_MIN_WIDTH)
+  {
+      p_param->cfg_enc_params.conf_win_right +=
+          (NI_MIN_WIDTH - avctx->width) / 2 * 2;
+      linesize_aligned = NI_MIN_WIDTH;
+  }
+  else
+  {
+    linesize_aligned = (((avctx->width + 1) / 2) * 2);
+    p_param->cfg_enc_params.conf_win_right +=
+        (linesize_aligned - avctx->width) / 2 * 2;
+  }
+  p_param->source_width = linesize_aligned;
+
+  height_aligned = avctx->height;
+  if (height_aligned < NI_MIN_HEIGHT)
+  {
+      p_param->cfg_enc_params.conf_win_bottom +=
+          (NI_MIN_HEIGHT - avctx->height) / 2 * 2;
+      height_aligned = NI_MIN_HEIGHT;
+  }
+  else
+  {
+      height_aligned = FFALIGN(avctx->height, 2);
+      p_param->cfg_enc_params.conf_win_bottom +=
+          (height_aligned - avctx->height) / 2 * 2;
+  }
+  p_param->source_height = height_aligned;
+
+  ctx.api_ctx.hw_id = ctx.dev_enc_idx;
+  ret = ni_device_session_open(&ctx.api_ctx, NI_DEVICE_TYPE_ENCODER);
+
+  ctx.dev_xcoder_name = ctx.api_ctx.dev_xcoder_name;
+  ctx.blk_xcoder_name = ctx.api_ctx.blk_xcoder_name;
+  ctx.dev_enc_idx = ctx.api_ctx.hw_id;
+
+  if (ret != 0)
+  {
+      av_log(avctx, AV_LOG_ERROR,
+             "Failed to open encoder (status = %d), "
+             "resource unavailable\n",
+             ret);
+      ret = AVERROR_EXTERNAL;
+      goto end;
+  } else {
+      av_log(avctx, AV_LOG_VERBOSE,
+             "XCoder %s.%d (inst: %d) opened successfully\n",
+             ctx.dev_xcoder_name, ctx.dev_enc_idx, ctx.api_ctx.session_id);
+  }
+
+  xpkt = &ctx.api_pkt.data.packet;
+  ni_packet_buffer_alloc(xpkt, NI_MAX_TX_SZ);
+
+  while (1)
+  {
+    recv = ni_device_session_read(&ctx.api_ctx, &(ctx.api_pkt),
+                                  NI_DEVICE_TYPE_ENCODER);
+
+    if (recv > 0)
+    {
+        av_freep(&avctx->extradata);
+        avctx->extradata_size = recv - (int)ctx.api_ctx.meta_size;
+        avctx->extradata =
+            av_mallocz(avctx->extradata_size + AV_INPUT_BUFFER_PADDING_SIZE);
+        memcpy(avctx->extradata,
+               (uint8_t *)xpkt->p_data + ctx.api_ctx.meta_size,
+               avctx->extradata_size);
+        av_log(avctx, AV_LOG_VERBOSE, "Xcoder encoder headers len: %d\n",
+               avctx->extradata_size);
+        break;
+    }
+  }
+
+end:
+
+    // close and clean up the temporary session
+    if (ret != 0)
+    {
+        ni_device_session_close(&ctx.api_ctx, ctx.encoder_eof,
+                                NI_DEVICE_TYPE_ENCODER);
+    }
+    else
+    {
+        ret = ni_device_session_close(&ctx.api_ctx, ctx.encoder_eof,
+                                      NI_DEVICE_TYPE_ENCODER);
+    }
+#ifdef _WIN32
+  ni_device_close(ctx.api_ctx.device_handle);
+#elif __linux__
+  ni_device_close(ctx.api_ctx.device_handle);
+  ni_device_close(ctx.api_ctx.blk_io_handle);
+#endif
+  ctx.api_ctx.device_handle = NI_INVALID_DEVICE_HANDLE;
+  ctx.api_ctx.blk_io_handle = NI_INVALID_DEVICE_HANDLE;
+
+  ni_packet_buffer_free( &(ctx.api_pkt.data.packet) );
+
+  ni_rsrc_free_device_context(ctx.rsrc_ctx);
+  ctx.rsrc_ctx = NULL;
+
+  p_param->cfg_enc_params.conf_win_right  = orig_conf_win_right;
+  p_param->cfg_enc_params.conf_win_bottom = orig_conf_win_bottom;
+
+  return ret;
+}
+
+static int xcoder_encoder_header_check_set(AVCodecContext *avctx)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  ni_xcoder_params_t *p_param;
+  // set color metrics
+  enum AVColorPrimaries color_primaries = avctx->color_primaries;
+  enum AVColorTransferCharacteristic color_trc = avctx->color_trc;
+  enum AVColorSpace color_space = avctx->colorspace;
+
+  p_param = (ni_xcoder_params_t *)ctx->api_ctx.p_session_config;
+  p_param->cfg_enc_params.videoFullRange       = 0;
+
+  // DolbyVision support
+  if (5 == p_param->dolby_vision_profile && AV_CODEC_ID_HEVC == avctx->codec_id)
+  {
+      color_primaries                    = AVCOL_PRI_UNSPECIFIED;
+      color_trc                          = AVCOL_TRC_UNSPECIFIED;
+      color_space                        = AVCOL_SPC_UNSPECIFIED;
+      p_param->cfg_enc_params.hrdEnable = p_param->cfg_enc_params.EnableAUD = 1;
+      p_param->cfg_enc_params.forced_header_enable                          = 1;
+      p_param->cfg_enc_params.videoFullRange                                = 1;
+  }
+
+  if ((5 == p_param->dolby_vision_profile &&
+       AV_CODEC_ID_HEVC == avctx->codec_id) ||
+      color_primaries != AVCOL_PRI_UNSPECIFIED ||
+      color_trc != AVCOL_TRC_UNSPECIFIED ||
+      color_space != AVCOL_SPC_UNSPECIFIED) {
+      p_param->cfg_enc_params.colorDescPresent = 1;
+      p_param->cfg_enc_params.colorPrimaries   = color_primaries;
+      p_param->cfg_enc_params.colorTrc         = color_trc;
+      p_param->cfg_enc_params.colorSpace       = color_space;
+
+      av_log(avctx, AV_LOG_VERBOSE,
+             "XCoder HDR color info color_primaries: %d "
+             "color_trc: %d  color_space %d\n",
+             color_primaries, color_trc, color_space);
+  }
+  if (avctx->color_range == AVCOL_RANGE_JPEG ||
+      AV_PIX_FMT_YUVJ420P == avctx->pix_fmt ||
+      AV_PIX_FMT_YUVJ420P == avctx->sw_pix_fmt) {
+      p_param->cfg_enc_params.videoFullRange = 1;
+  }
+
+  return 0;
+}
+
+static int xcoder_setup_encoder(AVCodecContext *avctx)
+{
+  XCoderH265EncContext *s = avctx->priv_data;
+  int i, ret = 0;
+  uint32_t  xcoder_timeout;
+  ni_xcoder_params_t *p_param       = &s->api_param;
+  ni_xcoder_params_t *pparams       = NULL;
+  ni_session_run_state_t prev_state = s->api_ctx.session_run_state;
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder setup device encoder\n");
+  //s->api_ctx.session_id = NI_INVALID_SESSION_ID;
+  if (ni_device_session_context_init(&(s->api_ctx)) < 0)
+  {
+      av_log(avctx, AV_LOG_ERROR,
+             "Error XCoder init encoder context failure\n");
+      return AVERROR_EXTERNAL;
+  }
+
+  switch (avctx->codec_id)
+  {
+  case AV_CODEC_ID_HEVC:
+      s->api_ctx.codec_format = NI_CODEC_FORMAT_H265;
+      break;
+  case AV_CODEC_ID_AV1:
+      s->api_ctx.codec_format = NI_CODEC_FORMAT_AV1;
+      break;
+  case AV_CODEC_ID_MJPEG:
+      s->api_ctx.codec_format = NI_CODEC_FORMAT_JPEG;
+      break;
+  default:
+      s->api_ctx.codec_format = NI_CODEC_FORMAT_H264;
+      break;
+  }
+
+  s->api_ctx.session_run_state = prev_state;
+  s->av_rois = NULL;
+  s->firstPktArrived = 0;
+  s->spsPpsArrived = 0;
+  s->spsPpsHdrLen = 0;
+  s->p_spsPpsHdr = NULL;
+  s->xcode_load_pixel = 0;
+  s->reconfigCount = 0;
+  s->latest_dts = 0;
+  s->first_frame_pts = INT_MIN;
+
+  if (SESSION_RUN_STATE_SEQ_CHANGE_DRAINING != s->api_ctx.session_run_state)
+  {
+      av_log(avctx, AV_LOG_INFO, "Session state: %d allocate frame fifo.\n",
+          s->api_ctx.session_run_state);
+      s->fme_fifo = av_fifo_alloc(sizeof(AVFrame));
+  }
+  else
+  {
+      av_log(avctx, AV_LOG_INFO, "Session seq change, fifo size: %lu.\n",
+          av_fifo_size(s->fme_fifo) / sizeof(AVFrame));
+  }
+
+  if (!s->fme_fifo)
+  {
+      return AVERROR(ENOMEM);
+  }
+  s->eos_fme_received = 0;
+
+  //Xcoder User Configuration
+  ret = ni_encoder_init_default_params(
+      p_param, avctx->framerate.num,
+      avctx->framerate.den, avctx->bit_rate,
+      avctx->width, avctx->height, s->api_ctx.codec_format);
+  switch (ret)
+  {
+  case NI_RETCODE_PARAM_ERROR_WIDTH_TOO_BIG:
+    if (avctx->codec_id == AV_CODEC_ID_AV1)
+        av_log(avctx, AV_LOG_ERROR, "Invalid Picture Width: exceeds %d\n",
+               NI_PARAM_AV1_MAX_WIDTH);
+    else
+        av_log(avctx, AV_LOG_ERROR, "Invalid Picture Width: too big\n");
+    return AVERROR_EXTERNAL;
+  case NI_RETCODE_PARAM_ERROR_WIDTH_TOO_SMALL:
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Width: too small\n");
+    return AVERROR_EXTERNAL;
+  case NI_RETCODE_PARAM_ERROR_HEIGHT_TOO_BIG:
+    if (avctx->codec_id == AV_CODEC_ID_AV1)
+        av_log(avctx, AV_LOG_ERROR, "Invalid Picture Height: exceeds %d\n",
+               NI_PARAM_AV1_MAX_HEIGHT);
+    else
+        av_log(avctx, AV_LOG_ERROR, "Invalid Picture Height: too big\n");
+    return AVERROR_EXTERNAL;
+  case NI_RETCODE_PARAM_ERROR_HEIGHT_TOO_SMALL:
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Height: too small\n");
+    return AVERROR_EXTERNAL;
+  case NI_RETCODE_PARAM_ERROR_AREA_TOO_BIG:
+    if (avctx->codec_id == AV_CODEC_ID_AV1)
+        av_log(avctx, AV_LOG_ERROR,
+               "Invalid Picture Width x Height: exceeds %d\n",
+               NI_PARAM_AV1_MAX_AREA);
+    else
+        av_log(avctx, AV_LOG_ERROR,
+               "Invalid Picture Width x Height: exceeds %d\n",
+               NI_MAX_RESOLUTION_AREA);
+    return AVERROR_EXTERNAL;
+  case NI_RETCODE_PARAM_ERROR_PIC_WIDTH:
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Width\n");
+    return AVERROR_EXTERNAL;
+  case NI_RETCODE_PARAM_ERROR_PIC_HEIGHT:
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Height\n");
+    return AVERROR_EXTERNAL;
+  default:
+    if (ret < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Error setting preset or log.\n");
+      av_log(avctx, AV_LOG_INFO, "Possible presets:");
+      for (i = 0; g_xcoder_preset_names[i]; i++)
+        av_log(avctx, AV_LOG_INFO, " %s", g_xcoder_preset_names[i]);
+      av_log(avctx, AV_LOG_INFO, "\n");
+
+      av_log(avctx, AV_LOG_INFO, "Possible log:");
+      for (i = 0; g_xcoder_log_names[i]; i++)
+        av_log(avctx, AV_LOG_INFO, " %s", g_xcoder_log_names[i]);
+      av_log(avctx, AV_LOG_INFO, "\n");
+
+      return AVERROR(EINVAL);
+    }
+    break;
+  }
+
+  av_log(avctx, AV_LOG_INFO, "pix_fmt is %d, sw_pix_fmt is %d\n", avctx->pix_fmt, avctx->sw_pix_fmt);
+  if (avctx->pix_fmt != AV_PIX_FMT_NI_QUAD)
+  {
+    av_log(avctx, AV_LOG_INFO, "sw_pix_fmt assigned to pix_fmt was %d, is now %d\n", avctx->pix_fmt, avctx->sw_pix_fmt);
+    avctx->sw_pix_fmt = avctx->pix_fmt;
+  }
+  else
+  {
+      if ((avctx->height >= NI_MIN_HEIGHT) && (avctx->width >= NI_MIN_WIDTH)) {
+          p_param->hwframes = 1;
+      }
+  }
+
+  switch (avctx->sw_pix_fmt)
+  {
+  case AV_PIX_FMT_YUV420P:
+  case AV_PIX_FMT_YUVJ420P:
+  case AV_PIX_FMT_YUV420P10LE:
+  case AV_PIX_FMT_NV12:
+  case AV_PIX_FMT_P010LE:
+    break;
+  default:
+    av_log(avctx, AV_LOG_ERROR, "Pixfmt %s not supported in Quadra encoder\n",
+           av_get_pix_fmt_name(avctx->sw_pix_fmt));
+    return AVERROR_INVALIDDATA;
+  }
+
+  if (s->xcoder_opts)
+  {
+    AVDictionary *dict = NULL;
+    AVDictionaryEntry *en = NULL;
+
+    if (!av_dict_parse_string(&dict, s->xcoder_opts, "=", ":", 0))
+    {
+      while ((en = av_dict_get(dict, "", en, AV_DICT_IGNORE_SUFFIX)))
+      {
+        int parse_ret = ni_encoder_params_set_value(p_param, en->key, en->value);
+
+        switch (parse_ret)
+        {
+          case NI_RETCODE_PARAM_INVALID_NAME:
+            av_log(avctx, AV_LOG_ERROR,
+                   "Unknown option: %s.\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_ERROR_TOO_BIG:
+            av_log(avctx, AV_LOG_ERROR, "Invalid %s: too big\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_ERROR_TOO_SMALL:
+            av_log(avctx, AV_LOG_ERROR, "Invalid %s: too small\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_ERROR_OOR:
+            av_log(avctx, AV_LOG_ERROR, "Invalid %s: out of range\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_ERROR_ZERO:
+            av_log(avctx, AV_LOG_ERROR, "Error setting option %s to value 0\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_INVALID_VALUE:
+            av_log(avctx, AV_LOG_ERROR,
+                   "Invalid value for %s: %s.\n", en->key, en->value);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_WARNING_DEPRECATED:
+            av_log(avctx, AV_LOG_WARNING, "Parameter %s is deprecated\n", en->key);
+            break;
+          default:
+            break;
+        }
+      }
+      av_dict_free(&dict);
+    }
+  }
+
+  if (p_param->enable_vfr) {
+      // in the vfr mode, we use the default framerate
+      // using the time_base to initial timing info
+      p_param->cfg_enc_params.frame_rate = 30;
+      s->api_ctx.prev_fps                = 30;
+      s->api_ctx.last_change_framenum    = 0;
+      s->api_ctx.fps_change_detect_count = 0;
+  }
+
+  av_log(avctx, AV_LOG_ERROR, "p_param->hwframes = %d\n", p_param->hwframes);
+  if (s->xcoder_gop)
+  {
+    AVDictionary *dict = NULL;
+    AVDictionaryEntry *en = NULL;
+
+    if (!av_dict_parse_string(&dict, s->xcoder_gop, "=", ":", 0))
+    {
+      while ((en = av_dict_get(dict, "", en, AV_DICT_IGNORE_SUFFIX)))
+      {
+        int parse_ret = ni_encoder_gop_params_set_value(p_param, en->key, en->value);
+
+        switch (parse_ret)
+        {
+          case NI_RETCODE_PARAM_INVALID_NAME:
+            av_log(avctx, AV_LOG_ERROR,
+                   "Unknown option: %s.\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_ERROR_TOO_BIG:
+            av_log(avctx, AV_LOG_ERROR, "Invalid custom GOP parameters: %s too big\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_ERROR_TOO_SMALL:
+            av_log(avctx, AV_LOG_ERROR, "Invalid custom GOP parameters: %s too small\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_ERROR_OOR:
+            av_log(avctx, AV_LOG_ERROR, "Invalid custom GOP parameters: %s out of range \n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_ERROR_ZERO:
+             av_log(avctx, AV_LOG_ERROR, "Invalid custom GOP paramaters: Error setting option %s to value 0 \n", en->key);
+             return AVERROR_EXTERNAL;
+          case NI_RETCODE_PARAM_INVALID_VALUE:
+            av_log(avctx, AV_LOG_ERROR,
+                   "Invalid value for GOP param %s: %s.\n", en->key, en->value);
+            return AVERROR_EXTERNAL;
+          default:
+            break;
+        }
+      }
+      av_dict_free(&dict);
+    }
+  }
+  if (s->nvme_io_size > 0 && s->nvme_io_size % 4096 != 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Error XCoder iosize is not 4KB aligned!\n");
+    return AVERROR_EXTERNAL;
+  }
+
+  s->api_ctx.p_session_config = &s->api_param;
+  pparams = (ni_xcoder_params_t *)s->api_ctx.p_session_config;
+  if (QUADRA)
+  {
+      switch (pparams->cfg_enc_params.gopSize) {
+      /* dtsOffset is the max number of non-reference frames in a GOP
+       * (derived from x264/5 algo) In case of IBBBP the first dts of the I frame should be input_pts-(3*ticks_per_frame)
+       * In case of IBP the first dts of the I frame should be input_pts-(1*ticks_per_frame)
+       * thus we ensure pts>dts in all cases
+       * */
+      case 1:
+      case 2:
+      case 3:
+      case 4:
+      case 5:
+      case 6:
+      case 7:
+      case 8:
+          s->dtsOffset = pparams->cfg_enc_params.gopSize - 1;
+          break;
+      default: // gopSize 0 (adaptive GOP)
+        s->dtsOffset = 7;
+        av_log(avctx, AV_LOG_VERBOSE, "dts offset default to 7\n");
+        break;
+      }
+      if (pparams->cfg_enc_params.custom_gop_params.custom_gop_size)
+          s->dtsOffset =
+              pparams->cfg_enc_params.custom_gop_params.custom_gop_size - 1;
+  }
+  else
+  {
+      switch (pparams->cfg_enc_params.gop_preset_index) {
+      /* dtsOffset is the max number of non-reference frames in a GOP
+       * (derived from x264/5 algo) In case of IBBBP the first dts of the I frame should be input_pts-(3*ticks_per_frame)
+       * In case of IBP the first dts of the I frame should be input_pts-(1*ticks_per_frame)
+       * thus we ensure pts>dts in all cases
+       * */
+      case 1 /*PRESET_IDX_ALL_I*/:
+      case 2 /*PRESET_IDX_IPP*/:
+      case 3 /*PRESET_IDX_IBBB*/:
+      case 6 /*PRESET_IDX_IPPPP*/:
+      case 7 /*PRESET_IDX_IBBBB*/:
+      case 9 /*PRESET_IDX_SP*/:
+        s->dtsOffset = 0;
+        break;
+      case 4 /*PRESET_IDX_IBPBP*/:
+        s->dtsOffset = 1;
+        break;
+      case 5 /*PRESET_IDX_IBBBP*/:
+        s->dtsOffset = 2;
+        break;
+      case 8 /*PRESET_IDX_RA_IB*/:
+        s->dtsOffset = 3;
+        break;
+      default:
+        // TBD need user to specify offset
+        s->dtsOffset = 7;
+        av_log(avctx, AV_LOG_VERBOSE, "dts offset default to 7, TBD\n");
+        break;
+      }
+    if (1 == pparams->force_frame_type)
+    {
+      s->dtsOffset = 7;
+    }
+    //printf("dts offset: %lld \n", s->dtsOffset);
+  }
+
+  s->total_frames_received = 0;
+  s->gop_offset_count = 0;
+  av_log(avctx, AV_LOG_INFO, "dts offset: %ld, gop_offset_count: %d\n",
+         s->dtsOffset, s->gop_offset_count);
+
+  //overwrite the nvme io size here with a custom value if it was provided
+  if (s->nvme_io_size > 0)
+  {
+    s->api_ctx.max_nvme_io_size = s->nvme_io_size;
+    av_log(avctx, AV_LOG_VERBOSE, "Custom NVME IO Size set to = %u\n",
+           s->api_ctx.max_nvme_io_size);
+    printf("Encoder user specified NVMe IO Size set to: %u\n",
+           s->api_ctx.max_nvme_io_size);
+  }
+
+  // overwrite keep alive timeout value here with a custom value if it was
+  // provided
+  // if xcoder option is set then overwrite the (legacy) decoder option
+  xcoder_timeout = s->api_param.cfg_enc_params.keep_alive_timeout;
+  if (xcoder_timeout != NI_DEFAULT_KEEP_ALIVE_TIMEOUT)
+  {
+      s->api_ctx.keep_alive_timeout = xcoder_timeout;
+  }
+  else
+  {
+      s->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+  }
+  av_log(avctx, AV_LOG_VERBOSE, "Custom NVME Keep Alive Timeout set to = %d\n",
+         s->api_ctx.keep_alive_timeout);
+
+  s->encoder_eof = 0;
+  avctx->bit_rate = pparams->bitrate;
+
+  s->api_ctx.src_bit_depth = 8;
+  s->api_ctx.src_endian = NI_FRAME_LITTLE_ENDIAN;
+  s->api_ctx.roi_len = 0;
+  s->api_ctx.roi_avg_qp = 0;
+  s->api_ctx.bit_depth_factor = 1;
+  if (AV_PIX_FMT_YUV420P10BE == avctx->sw_pix_fmt ||
+      AV_PIX_FMT_YUV420P10LE == avctx->sw_pix_fmt ||
+      AV_PIX_FMT_P010LE == avctx->sw_pix_fmt)
+  {
+    s->api_ctx.bit_depth_factor = 2;
+    s->api_ctx.src_bit_depth = 10;
+    if (AV_PIX_FMT_YUV420P10BE == avctx->sw_pix_fmt)
+    {
+        s->api_ctx.src_endian = NI_FRAME_BIG_ENDIAN;
+    }
+  }
+  pparams->cfg_enc_params.planar = (avctx->sw_pix_fmt == AV_PIX_FMT_NV12 ||
+                                    avctx->sw_pix_fmt == AV_PIX_FMT_P010LE)
+                                       ? 0
+                                       : 1;
+  if (1)
+  {
+    s->freeHead = 0;
+    s->freeTail = 0;
+    for (i = 0; i < MAX_NUM_FRAMEPOOL_HWAVFRAME; i++)
+    {
+      s->sframe_pool[i] = av_frame_alloc();
+      if (!s->sframe_pool[i]) {
+        return AVERROR(ENOMEM);
+      }
+      s->aFree_Avframes_list[i] = i;
+      s->freeTail++;
+    }
+    s->aFree_Avframes_list[i] = -1;
+  }
+
+  // init HDR SEI stuff
+  s->api_ctx.sei_hdr_content_light_level_info_len =
+      s->api_ctx.light_level_data_len =
+      s->api_ctx.sei_hdr_mastering_display_color_vol_len =
+      s->api_ctx.mdcv_max_min_lum_data_len = 0;
+  s->api_ctx.p_master_display_meta_data = NULL;
+
+  memset( &(s->api_fme), 0, sizeof(ni_session_data_io_t) );
+  memset( &(s->api_pkt), 0, sizeof(ni_session_data_io_t) );
+
+  s->api_pkt.data.packet.av1_buffer_index = 0;
+
+  if (avctx->width > 0 && avctx->height > 0)
+  {
+      bool isnv12frame = (avctx->sw_pix_fmt == AV_PIX_FMT_NV12 ||
+                          avctx->sw_pix_fmt == AV_PIX_FMT_P010LE);
+      ni_frame_buffer_alloc(&(s->api_fme.data.frame), avctx->width,
+                            avctx->height, 0, 0, s->api_ctx.bit_depth_factor, 1,
+                            !isnv12frame);
+  }
+
+  //validate encoded bitstream headers struct for encoder open
+  xcoder_encoder_header_check_set(avctx);
+
+  // aspect ratio
+  p_param->cfg_enc_params.aspectRatioWidth  = avctx->sample_aspect_ratio.num;
+  p_param->cfg_enc_params.aspectRatioHeight = avctx->sample_aspect_ratio.den;
+
+  // generate encoded bitstream headers in advance if configured to do so
+  if ((pparams->generate_enc_hdrs) && (avctx->codec_id != AV_CODEC_ID_MJPEG)) {
+      ret = xcoder_encoder_headers(avctx);
+  }
+
+  // original resolution this stream started with, this is used by encoder sequence change
+  s->api_ctx.ori_width = avctx->width;
+  s->api_ctx.ori_height = avctx->height;
+  s->api_ctx.ori_bit_depth_factor = s->api_ctx.bit_depth_factor;
+  s->api_ctx.ori_pix_fmt = avctx->sw_pix_fmt;
+
+  return ret;
+}
+
+av_cold int xcoder_encode_init(AVCodecContext *avctx)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  AVHWFramesContext *avhwf_ctx;
+  int ret;
+  ni_log_set_level(ff_to_ni_log_level(av_log_get_level()));
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder encode init\n");
+
+   if (ctx->api_ctx.session_run_state == SESSION_RUN_STATE_SEQ_CHANGE_DRAINING)
+  {
+    ctx->dev_enc_idx = ctx->orig_dev_enc_idx;
+  }
+  else
+  {
+    ctx->orig_dev_enc_idx = ctx->dev_enc_idx;
+  }
+
+#ifdef MULTI_THREAD
+  static int initData = 0;
+  int i;
+  if (initData == 0)
+  {
+    for (i = 0; i < NI_MAX_NUM_SESSIONS; i++)
+    {
+      write_thread_args[i].session_id = NI_INVALID_SESSION_ID;
+    }
+    initData = 1;
+  }
+#endif
+
+  if ((ret = xcoder_setup_encoder(avctx)) < 0)
+  {
+      xcoder_encode_close(avctx);
+      return ret;
+  }
+
+  if (!avctx->hw_device_ctx) {
+      if (avctx->hw_frames_ctx) {
+          avhwf_ctx = (AVHWFramesContext *)avctx->hw_frames_ctx->data;
+          avctx->hw_device_ctx = av_buffer_ref(avhwf_ctx->device_ref);
+      }
+  }
+
+  return 0;
+}
+
+#ifdef MULTI_THREAD
+static void* write_frame_thread(void* arg)
+{
+  write_thread_arg_struct_t *args = (write_thread_arg_struct_t *) arg;
+  XCoderH265EncContext *ctx = args->ctx;
+  int ret;
+  int sent;
+
+  av_log(ctx, AV_LOG_DEBUG, "write_frame_thread: session_id %d\n", args->session_id);
+
+  av_log(ctx, AV_LOG_DEBUG, "write_frame_thread: ctx %p\n", ctx);
+
+  sent = ni_device_session_write(&ctx->api_ctx, &ctx->api_fme, NI_DEVICE_TYPE_ENCODER);
+
+  av_log(ctx, AV_LOG_DEBUG, "write_frame_thread: size %d sent to xcoder\n", sent);
+
+  if (NI_RETCODE_ERROR_RESOURCE_UNAVAILABLE == sent)
+  {
+    av_log(ctx, AV_LOG_DEBUG, "write_frame_thread(): Sequence Change in progress, returning EAGAIN\n");
+    ret = AVERROR(EAGAIN);
+  }
+  else
+  {
+    if (sent == -1)
+    {
+      ret = AVERROR(EAGAIN);
+    }
+    else
+    {
+      //pushing input pts in circular FIFO
+      ctx->api_ctx.enc_pts_list[ctx->api_ctx.enc_pts_w_idx % NI_FIFO_SZ] = ctx->api_fme.data.frame.pts;
+      ctx->api_ctx.enc_pts_w_idx++;
+      ret = 0;
+    }
+  }
+  args->ret = ret;
+  av_log(ctx, AV_LOG_DEBUG, "write_frame_thread: ret %d\n", ret);
+  pthread_exit(NULL);
+}
+#endif
+
+int xcoder_encode_close(AVCodecContext *avctx)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  ni_retcode_t ret = NI_RETCODE_FAILURE;
+  int i;
+
+#ifdef MULTI_THREAD
+  uint8_t id = find_session_idx(ctx->api_ctx.session_id);
+  if (id < NI_MAX_NUM_SESSIONS)
+  {
+    pthread_cancel(write_thread_args[id].thread);
+    write_thread_args[id].session_id = NI_INVALID_SESSION_ID;
+  }
+#endif
+
+  for (i = 0; i < MAX_NUM_FRAMEPOOL_HWAVFRAME; i++)
+  {
+    AVFrame *frame = NULL;
+    frame = ctx->sframe_pool[i];
+    if (frame && frame->data[3])
+    {
+        ((niFrameSurface1_t *)((uint8_t *)frame->data[3]))->device_handle =
+            (int32_t)((int64_t)(ctx->api_ctx.blk_io_handle) &
+                      0xFFFFFFFF); // update handle to most recent alive
+    }
+    av_frame_free(&(ctx->sframe_pool[i])); //any remaining stored AVframes that have not been unref will die here
+    ctx->sframe_pool[i] = NULL;
+  }
+
+  ret = ni_device_session_close(&ctx->api_ctx, ctx->encoder_eof,
+                                NI_DEVICE_TYPE_ENCODER);
+  if (NI_RETCODE_SUCCESS != ret)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to close Encoder Session (status = %d)\n", ret);
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder encode close: session_run_state %d\n", ctx->api_ctx.session_run_state);
+  if (ctx->api_ctx.session_run_state != SESSION_RUN_STATE_SEQ_CHANGE_DRAINING)
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "XCoder encode close: close blk_io_handle %d device_handle %d\n", ctx->api_ctx.blk_io_handle, ctx->api_ctx.device_handle);
+#ifdef _WIN32
+    ni_device_close(ctx->api_ctx.device_handle);
+#elif __linux__
+    ni_device_close(ctx->api_ctx.device_handle);
+    ni_device_close(ctx->api_ctx.blk_io_handle);
+#endif
+    ctx->api_ctx.device_handle = NI_INVALID_DEVICE_HANDLE;
+    ctx->api_ctx.blk_io_handle = NI_INVALID_DEVICE_HANDLE;
+    ctx->api_ctx.auto_dl_handle = NI_INVALID_DEVICE_HANDLE;
+    ctx->api_ctx.sender_handle = NI_INVALID_DEVICE_HANDLE;
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder encode close (status = %d)\n", ret);
+  
+  if (ctx->api_fme.data.frame.buffer_size)
+  {
+      ni_frame_buffer_free(&(ctx->api_fme.data.frame));
+  }
+  ni_packet_buffer_free( &(ctx->api_pkt.data.packet) );
+  if (AV_CODEC_ID_AV1 == avctx->codec_id &&
+      ctx->api_pkt.data.packet.av1_buffer_index)
+      ni_packet_buffer_free_av1(&(ctx->api_pkt.data.packet));
+
+  av_log(avctx, AV_LOG_DEBUG, "fifo size: %lu\n",
+         av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame));
+  if (ctx->api_ctx.session_run_state != SESSION_RUN_STATE_SEQ_CHANGE_DRAINING)
+  {
+    av_fifo_free(ctx->fme_fifo);
+    av_log(avctx, AV_LOG_DEBUG, " , freed.\n");
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_DEBUG, " , kept.\n");
+  }
+
+  ni_device_session_context_clear(&ctx->api_ctx);
+
+  ni_rsrc_free_device_context(ctx->rsrc_ctx);
+  ctx->rsrc_ctx = NULL;
+
+  free(ctx->av_rois);
+  ctx->av_rois = NULL;
+  if (ctx->p_spsPpsHdr) {
+      free(ctx->p_spsPpsHdr);
+      ctx->p_spsPpsHdr = NULL;
+  }
+
+  if (avctx->hw_device_ctx) {
+      av_buffer_unref(&avctx->hw_device_ctx);
+  }
+  ctx->started = 0;
+
+  return 0;
+}
+
+
+int xcoder_encode_sequence_change(AVCodecContext *avctx, int width, int height, int bit_depth_factor)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  ni_retcode_t ret = NI_RETCODE_FAILURE;
+  ni_xcoder_params_t *p_param       = &ctx->api_param;
+  ni_xcoder_params_t *pparams = (ni_xcoder_params_t *)ctx->api_ctx.p_session_config;
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder encode sequence change: session_run_state %d\n", ctx->api_ctx.session_run_state);
+
+  ret = ni_device_session_sequence_change(&ctx->api_ctx, width, height, bit_depth_factor, NI_DEVICE_TYPE_ENCODER);
+
+  if (NI_RETCODE_SUCCESS != ret)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to send Sequence Change to Encoder Session (status = %d)\n", ret);
+    return ret;
+  }
+
+  // update AvCodecContext
+  if (avctx->pix_fmt != AV_PIX_FMT_NI_QUAD)
+  {
+    av_log(avctx, AV_LOG_INFO, "sw_pix_fmt assigned to pix_fmt was %d, is now %d\n", avctx->pix_fmt, avctx->sw_pix_fmt);
+    avctx->sw_pix_fmt = avctx->pix_fmt;
+  }
+  else
+  {
+      if ((avctx->height >= NI_MIN_HEIGHT) && (avctx->width >= NI_MIN_WIDTH)) {
+          p_param->hwframes = 1;
+      }
+  }
+
+  switch (avctx->sw_pix_fmt)
+  {
+      case AV_PIX_FMT_YUV420P:
+      case AV_PIX_FMT_YUV420P10:
+      case AV_PIX_FMT_YUV420P12:
+      case AV_PIX_FMT_NV12:
+      case AV_PIX_FMT_P010LE:
+        break;
+      case AV_PIX_FMT_RGBA:
+        av_log(avctx, AV_LOG_ERROR, "AV_PIX_FMT_RGBA format not supported for encoding\n");
+      case AV_PIX_FMT_YUV422P:
+      case AV_PIX_FMT_YUV422P10:
+      case AV_PIX_FMT_YUV422P12:
+      case AV_PIX_FMT_GBRP:
+      case AV_PIX_FMT_GBRP10:
+      case AV_PIX_FMT_GBRP12:
+      case AV_PIX_FMT_YUV444P:
+      case AV_PIX_FMT_YUV444P10:
+      case AV_PIX_FMT_YUV444P12:
+      case AV_PIX_FMT_GRAY8:
+      case AV_PIX_FMT_GRAY10:
+      case AV_PIX_FMT_GRAY12:
+        return AVERROR_INVALIDDATA;
+      default:
+        break;
+  }
+
+  // update session context
+  ctx->api_ctx.bit_depth_factor = bit_depth_factor;
+  ctx->api_ctx.src_bit_depth = (bit_depth_factor == 1) ? 8 : 10;
+  ctx->api_ctx.src_endian = (AV_PIX_FMT_YUV420P10BE == avctx->sw_pix_fmt) ? NI_FRAME_BIG_ENDIAN : NI_FRAME_LITTLE_ENDIAN;
+  ctx->api_ctx.ready_to_close = 0;
+  ctx->api_ctx.frame_num = 0; // need to reset frame_num because pkt_num is set to 1 when header received after sequnce change, and low delay mode compares frame_num and pkt_num
+  ctx->api_ctx.pkt_num = 0; // also need to reset pkt_num because before header received, pkt_num > frame_num will also cause low delay mode stuck
+  ctx->api_pkt.data.packet.end_of_stream = 0;
+
+  pparams->cfg_enc_params.planar = (avctx->sw_pix_fmt == AV_PIX_FMT_NV12 ||
+                                    avctx->sw_pix_fmt == AV_PIX_FMT_P010LE)
+                                       ? 0
+                                       : 1;
+  return ret;
+}
+
+static int xcoder_encode_reset(AVCodecContext *avctx)
+{
+  av_log(avctx, AV_LOG_WARNING, "XCoder encode reset\n");
+  xcoder_encode_close(avctx);
+  return xcoder_encode_init(avctx);
+}
+
+// frame fifo operations
+static int is_input_fifo_empty(XCoderH265EncContext *s)
+{
+  return av_fifo_size(s->fme_fifo) < sizeof(AVFrame);
+}
+
+static int enqueue_frame(AVCodecContext *avctx, const AVFrame *inframe)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  int ret;
+
+  // expand frame buffer fifo if not enough space
+  if (av_fifo_space(ctx->fme_fifo) < sizeof(AVFrame))
+  {
+    ret = av_fifo_realloc2(ctx->fme_fifo,
+                           av_fifo_size(ctx->fme_fifo) + sizeof(AVFrame));
+    if (ret < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Enc av_fifo_realloc2 NO MEMORY !!!\n");
+      return ret;
+    }
+    if (((av_fifo_size(ctx->fme_fifo)+av_fifo_space(ctx->fme_fifo))  / sizeof(AVFrame) % 100) == 0)
+    {
+      av_log(avctx, AV_LOG_INFO, "Enc fifo being extended to: %lu\n",
+             (av_fifo_size(ctx->fme_fifo)+av_fifo_space(ctx->fme_fifo)) / sizeof(AVFrame));
+    }
+    av_assert0(0 == (av_fifo_size(ctx->fme_fifo)+av_fifo_space(ctx->fme_fifo)) % sizeof(AVFrame));
+  }
+
+  if (inframe == &ctx->buffered_fme)
+  {
+    // For FFmpeg-n4.4+ receive_packet interface the buffered_fme is fetched from
+    // ff_alloc_get_frame rather than passed as function argument. So we need to
+    // judge whether they are the same object. If they are the same NO need to do
+    // any reference before queue operation.
+    av_fifo_generic_write(ctx->fme_fifo, (void *)inframe, sizeof(*inframe), NULL);
+  }
+  else
+  {
+    AVFrame temp_frame;
+    memset(&temp_frame, 0, sizeof(AVFrame));
+    // In case double free for external input frame and our buffered frame.
+    av_frame_ref(&temp_frame, inframe);
+    av_fifo_generic_write(ctx->fme_fifo, &temp_frame, sizeof(*inframe), NULL);
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "fme queued, fifo size: %lu\n",
+         av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame));
+
+  return 0;
+}
+
+int xcoder_send_frame(AVCodecContext *avctx, const AVFrame *frame)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  bool ishwframe;
+  bool isnv12frame;
+  bool alignment_2pass_wa;
+  int format_in_use;
+  int ret = 0;
+  int sent;
+  int i, j;
+  int orig_avctx_width = avctx->width;
+  int orig_avctx_height = avctx->height;
+  ni_xcoder_params_t *p_param;
+  int need_to_copy = 1;
+  AVHWFramesContext *avhwf_ctx;
+  NIFramesContext *nif_src_ctx;
+  AVFrameSideData *side_data;
+  const AVFrame *first_frame = NULL;
+  // employ a ni_frame_t as a data holder to convert/prepare for side data
+  // of the passed in frame
+  ni_frame_t dec_frame    = {0};
+  ni_aux_data_t *aux_data = NULL;
+  // data buffer for various SEI: HDR mastering display color volume, HDR
+  // content light level, close caption, User data unregistered, HDR10+ etc.
+  int send_sei_with_idr;
+  uint8_t mdcv_data[NI_MAX_SEI_DATA];
+  uint8_t cll_data[NI_MAX_SEI_DATA];
+  uint8_t cc_data[NI_MAX_SEI_DATA];
+  uint8_t udu_data[NI_MAX_SEI_DATA];
+  uint8_t hdrp_data[NI_MAX_SEI_DATA];
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder send frame\n");
+
+#ifdef MULTI_THREAD
+  if (ctx->api_ctx.session_id!=NI_INVALID_SESSION_ID)
+  {
+    uint8_t id = find_session_idx(ctx->api_ctx.session_id);
+    if (id < NI_MAX_NUM_SESSIONS)
+    { //find the previous session
+      av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame previous: find_session_idx %d\n", id);
+      av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame previous: ctx->api_ctx.session_id %d\n", ctx->api_ctx.session_id);
+      if (write_thread_args[id].thread)
+      {
+        av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame: pthread_join start thread %d\n", write_thread_args[id].thread);
+        pthread_join(write_thread_args[id].thread, NULL);
+        av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame: pthread_join end write_thread_args[id].ret %d\n", write_thread_args[id].ret);
+        write_thread_args[id].thread = 0;
+        write_thread_args[id].session_id = NI_INVALID_SESSION_ID;
+        if (write_thread_args[id].ret == AVERROR(EAGAIN))
+        {
+          av_log(avctx, AV_LOG_ERROR, "xcoder_send_frame: ret %d\n", write_thread_args[id].ret);
+          return AVERROR(EAGAIN);
+        }
+        else
+        {
+          /*
+          // only if it's NOT sequence change flushing (in which case only the eos
+          // was sent and not the first sc pkt) AND
+          // only after successful sending will it be removed from fifo
+          if (SESSION_RUN_STATE_SEQ_CHANGE_DRAINING !=
+              ctx->api_ctx.session_run_state)
+          {
+            if (! is_input_fifo_empty(ctx))
+            {
+              av_fifo_drain(ctx->fme_fifo, sizeof(AVFrame));
+              av_log(avctx, AV_LOG_DEBUG, "fme popped, fifo size: %lu\n",
+                     av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame));
+            }
+            av_frame_unref(&ctx->buffered_fme);
+          }
+          else
+          {
+            av_log(avctx, AV_LOG_TRACE, "XCoder frame(eos) sent, sequence changing!"
+                   " NO fifo pop !\n");
+          }
+          */
+        }
+      }
+    }
+  }
+#endif
+
+  // NETINT_INTERNAL - currently only for internal testing
+  p_param = (ni_xcoder_params_t *) ctx->api_ctx.p_session_config;
+  alignment_2pass_wa = (p_param->cfg_enc_params.lookAheadDepth &&
+                       ((avctx->codec_id == AV_CODEC_ID_HEVC) ||
+                        (avctx->codec_id == AV_CODEC_ID_AV1)));
+
+  // leave encoder instance open to when the first frame buffer arrives so that
+  // its stride size is known and handled accordingly.
+  if (ctx->started == 0)
+  {
+    if (!is_input_fifo_empty(ctx))
+    {
+      av_log(avctx, AV_LOG_VERBOSE, "first frame: use fme from av_fifo_generic_peek\n");
+      av_fifo_generic_peek(ctx->fme_fifo, &ctx->buffered_fme,
+                           sizeof(AVFrame), NULL);
+      ctx->buffered_fme.extended_data = ctx->buffered_fme.data;
+      first_frame = &ctx->buffered_fme;
+
+    }
+    else if (frame)
+    {
+      av_log(avctx, AV_LOG_VERBOSE, "first frame: use input frame\n");
+      first_frame = frame;
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_ERROR, "first frame: NULL is unexpected!\n");
+    }
+  }
+  else if (ctx->api_ctx.session_run_state == SESSION_RUN_STATE_SEQ_CHANGE_OPENING)
+  {
+    if (!is_input_fifo_empty(ctx))
+    {
+      av_log(avctx, AV_LOG_VERBOSE, "first frame: use fme from av_fifo_generic_peek\n");
+      av_fifo_generic_peek(ctx->fme_fifo, &ctx->buffered_fme,
+                           sizeof(AVFrame), NULL);
+      ctx->buffered_fme.extended_data = ctx->buffered_fme.data;
+      first_frame = &ctx->buffered_fme;
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_ERROR, "No buffered frame - Sequence Change Fail");
+      ret = AVERROR_EXTERNAL;
+      // xcoder_encode_close(avctx); will be called at codec close
+      return ret;
+    }
+  }
+
+  /*
+  if ((SESSION_RUN_STATE_SEQ_CHANGE_OPENING == ctx->api_ctx.session_run_state) &&
+      first_frame->hw_frames_ctx)
+  {
+    av_buffer_unref(avctx->hw_frames_ctx);
+    avctx->hw_frames_ctx = av_buffer_ref(first_frame->hw_frames_ctx);
+  }
+  */
+  if (first_frame && ctx->started == 0)
+  {
+      // if frame stride size is not as we expect it,
+      // adjust using xcoder-params conf_win_right
+      int linesize_aligned = first_frame->width;
+      int height_aligned = first_frame->height;
+      ishwframe = first_frame->format == AV_PIX_FMT_NI_QUAD;
+      if (QUADRA) {
+          if (linesize_aligned < NI_MIN_WIDTH) {
+              p_param->cfg_enc_params.conf_win_right +=
+                  (NI_MIN_WIDTH - first_frame->width) / 2 * 2;
+              linesize_aligned = NI_MIN_WIDTH;
+          } else {
+              linesize_aligned = (((first_frame->width + 1) / 2) * 2);
+              p_param->cfg_enc_params.conf_win_right +=
+                  (linesize_aligned - first_frame->width) / 2 * 2;
+              // linesize_aligned = ((((frame->width *
+              // ctx->api_ctx.bit_depth_factor) + 15) / 16) * 16) /
+              // ctx->api_ctx.bit_depth_factor; av_log(avctx, AV_LOG_DEBUG,
+              // "xcoder_send_frame frame->width %d
+              // ctx->api_ctx.bit_depth_factor %d linesize_aligned %d\n",
+              // frame->width, ctx->api_ctx.bit_depth_factor, linesize_aligned);
+          }
+      } else {
+          linesize_aligned = ((first_frame->width + 31) / 32) * 32;
+          if (linesize_aligned < NI_MIN_WIDTH) {
+              p_param->cfg_enc_params.conf_win_right +=
+                  NI_MIN_WIDTH - first_frame->width;
+              linesize_aligned = NI_MIN_WIDTH;
+          } else if (linesize_aligned > first_frame->width) {
+              p_param->cfg_enc_params.conf_win_right +=
+                  linesize_aligned - first_frame->width;
+          }
+      }
+    p_param->source_width = linesize_aligned;
+
+    if (QUADRA)
+    {
+      if (height_aligned < NI_MIN_HEIGHT)
+      {
+          p_param->cfg_enc_params.conf_win_bottom +=
+              (NI_MIN_HEIGHT - first_frame->height) / 2 * 2;
+          height_aligned = NI_MIN_HEIGHT;
+      }
+      else
+      {
+          height_aligned = FFALIGN(first_frame->height, 2);
+          p_param->cfg_enc_params.conf_win_bottom +=
+              (height_aligned - first_frame->height) / 2 * 2;
+      }
+      p_param->source_height = height_aligned;
+    }
+    else
+    {
+      height_aligned = ((first_frame->height + 7) / 8) * 8;
+      if (avctx->codec_id == AV_CODEC_ID_H264) {
+        height_aligned = ((first_frame->height + 15) / 16) * 16;
+      }
+      if (height_aligned < NI_MIN_HEIGHT)
+      {
+          p_param->cfg_enc_params.conf_win_bottom +=
+              NI_MIN_HEIGHT - first_frame->height;
+          p_param->source_height = NI_MIN_HEIGHT;
+          height_aligned         = NI_MIN_HEIGHT;
+      }
+      else if (height_aligned > first_frame->height)
+      {
+          p_param->cfg_enc_params.conf_win_bottom +=
+              height_aligned - first_frame->height;
+          if (avctx->codec_id != AV_CODEC_ID_H264) {
+              p_param->source_height = height_aligned;
+          }
+      }
+    }
+
+    if (avctx->color_primaries == AVCOL_PRI_UNSPECIFIED) {
+        avctx->color_primaries = first_frame->color_primaries;
+    }
+    if (avctx->color_trc == AVCOL_TRC_UNSPECIFIED) {
+        avctx->color_trc = first_frame->color_trc;
+    }
+    if (avctx->colorspace == AVCOL_SPC_UNSPECIFIED) {
+        avctx->colorspace = first_frame->colorspace;
+    }
+    avctx->color_range = first_frame->color_range;
+
+    xcoder_encoder_header_check_set(avctx);
+
+    av_log(avctx, AV_LOG_VERBOSE,
+           "XCoder frame->linesize: %d/%d/%d frame width/height %dx%d"
+           " conf_win_right %d  conf_win_bottom %d , color primaries %u trc %u "
+           "space %u\n",
+           first_frame->linesize[0], first_frame->linesize[1],
+           first_frame->linesize[2], first_frame->width, first_frame->height,
+           p_param->cfg_enc_params.conf_win_right,
+           p_param->cfg_enc_params.conf_win_bottom,
+           first_frame->color_primaries, first_frame->color_trc,
+           first_frame->colorspace);
+
+    if (SESSION_RUN_STATE_SEQ_CHANGE_OPENING != ctx->api_ctx.session_run_state)
+    {
+      // sequence change backup / restore encoder device handles and hw_id, so no need to overwrite hw_id to user set value, which could be -1
+      ctx->api_ctx.hw_id = ctx->dev_enc_idx;
+    }
+
+    p_param->rootBufId = (ishwframe) ? ((niFrameSurface1_t*)((uint8_t*)first_frame->data[3]))->ui16FrameIdx : 0;
+    if (ishwframe)
+    {
+      ctx->api_ctx.hw_action = NI_CODEC_HW_ENABLE;
+      ctx->api_ctx.sender_handle = (ni_device_handle_t)(
+          (int64_t)(((niFrameSurface1_t *)((uint8_t *)first_frame->data[3]))
+                        ->device_handle));
+    }
+
+    if (first_frame->hw_frames_ctx && ctx->api_ctx.hw_id == -1) {
+        ctx->api_ctx.hw_id = ni_get_cardno(first_frame);
+        av_log(avctx, AV_LOG_VERBOSE,
+               "xcoder_send_frame: hw_id -1 collocated to %d\n",
+               ctx->api_ctx.hw_id);
+    }
+
+    // AUD insertion has to be handled differently in the firmware
+    // if it is global header
+    if (p_param->cfg_enc_params.EnableAUD) {
+        if (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {
+            p_param->cfg_enc_params.EnableAUD = NI_ENABLE_AUD_FOR_GLOBAL_HEADER;
+        }
+
+        av_log(avctx, AV_LOG_VERBOSE,
+               "%s: EnableAUD %d global header flag %d\n", __FUNCTION__,
+               (p_param->cfg_enc_params.EnableAUD),
+               (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) ? 1 : 0);
+    }
+
+    ret = ni_device_session_open(&ctx->api_ctx, NI_DEVICE_TYPE_ENCODER);
+
+    // // As the file handle may change we need to assign back
+    ctx->dev_xcoder_name = ctx->api_ctx.dev_xcoder_name;
+    ctx->blk_xcoder_name = ctx->api_ctx.blk_xcoder_name;
+    ctx->dev_enc_idx = ctx->api_ctx.hw_id;
+
+    if (ret == NI_RETCODE_INVALID_PARAM)
+    {
+      av_log(avctx, AV_LOG_ERROR, "%s\n", ctx->api_ctx.param_err_msg);
+    }
+    if (ret != 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Failed to open encoder (status = %d), "
+             "resource unavailable\n", ret);
+      ret = AVERROR_EXTERNAL;
+      // xcoder_encode_close(avctx); will be called at codec close
+      return ret;
+    }
+    else
+    {
+        av_log(avctx, AV_LOG_VERBOSE,
+               "XCoder %s.%d (inst: %d) opened successfully\n",
+               ctx->dev_xcoder_name, ctx->dev_enc_idx, ctx->api_ctx.session_id);
+    }
+
+    // set up ROI map if in ROI demo mode
+    // Note: this is for demo purpose, and its direct access to QP map in
+    //       session context is not the usual way to do ROI; the normal way is
+    //       through side data of AVFrame in libavcodec, or aux data of ni_frame
+    //       in libxcoder
+    if (p_param->cfg_enc_params.roi_enable &&
+        (1 == p_param->roi_demo_mode || 2 == p_param->roi_demo_mode)) {
+        int sumQp = 0, ctu;
+        // mode 1: Set QP for center 1/3 of picture to highest - lowest quality
+        // the rest to lowest - highest quality;
+        // mode non-1: reverse of mode 1
+        int importanceLevelCentre = p_param->roi_demo_mode == 1 ? 40 : 10;
+        int importanceLevelRest   = p_param->roi_demo_mode == 1 ? 10 : 40;
+        if (QUADRA) {
+            uint32_t block_size, max_cu_size, customMapSize;
+            uint32_t mbWidth;
+            uint32_t mbHeight;
+            uint32_t numMbs;
+            uint32_t roiMapBlockUnitSize;
+            uint32_t entryPerMb;
+
+            max_cu_size = avctx->codec_id == AV_CODEC_ID_H264 ? 16: 64;
+            
+            // AV1 non-8x8-aligned resolution is implicitly cropped due to Quadra HW limitation
+            if (AV_CODEC_ID_AV1 == avctx->codec_id)
+            {
+                linesize_aligned = (linesize_aligned / 8) * 8;
+                height_aligned = (height_aligned / 8) * 8;
+            }
+            
+            // (ROI map version >= 1) each QP info takes 8-bit, represent 8 x 8
+            // pixel block
+            block_size =
+                ((linesize_aligned + max_cu_size - 1) & (~(max_cu_size - 1))) *
+                ((height_aligned + max_cu_size - 1) & (~(max_cu_size - 1))) /
+                (8 * 8);
+
+            // need to align to 64 bytes
+            customMapSize = ((block_size + 63) & (~63));
+            if (!ctx->api_ctx.roi_map) {
+                ctx->api_ctx.roi_map =
+                    (ni_enc_quad_roi_custom_map *)calloc(1, customMapSize);
+            }
+            if (!ctx->api_ctx.roi_map) {
+                return AVERROR(ENOMEM);
+            }
+
+            // for H.264, select ROI Map Block Unit Size: 16x16
+            // for H.265, select ROI Map Block Unit Size: 64x64
+            roiMapBlockUnitSize = avctx->codec_id == AV_CODEC_ID_H264 ? 16 : 64;
+
+            mbWidth =
+                ((linesize_aligned + max_cu_size - 1) & (~(max_cu_size - 1))) /
+                roiMapBlockUnitSize;
+            mbHeight =
+                ((height_aligned + max_cu_size - 1) & (~(max_cu_size - 1))) /
+                roiMapBlockUnitSize;
+            numMbs = mbWidth * mbHeight;
+
+            // copy roi MBs QPs into custom map
+            // number of qp info (8x8) per mb or ctb
+            entryPerMb = (roiMapBlockUnitSize / 8) * (roiMapBlockUnitSize / 8);
+
+            for (i = 0; i < numMbs; i++) {
+                bool bIsCenter = (i % mbWidth > mbWidth / 3) && (i % mbWidth < mbWidth * 2 / 3);
+                for (j = 0; j < entryPerMb; j++) {
+                    /*
+                    g_quad_roi_map[i*4+j].field.skip_flag = 0; // don't force
+                    skip mode g_quad_roi_map[i*4+j].field.roiAbsQp_flag = 1; //
+                    absolute QP g_quad_roi_map[i*4+j].field.qp_info = bIsCenter
+                    ? importanceLevelCentre : importanceLevelRest;
+                    */
+                    ctx->api_ctx.roi_map[i * entryPerMb + j].field.ipcm_flag =
+                        0; // don't force skip mode
+                    ctx->api_ctx.roi_map[i * entryPerMb + j]
+                        .field.roiAbsQp_flag = 1; // absolute QP
+                    ctx->api_ctx.roi_map[i * entryPerMb + j].field.qp_info =
+                        bIsCenter ? importanceLevelCentre : importanceLevelRest;
+                }
+                sumQp += ctx->api_ctx.roi_map[i * entryPerMb].field.qp_info;
+            }
+            ctx->api_ctx.roi_len = customMapSize;
+            ctx->api_ctx.roi_avg_qp =
+                // NOLINTNEXTLINE(clang-analyzer-core.DivideZero)
+                (sumQp + (numMbs >> 1)) / numMbs; // round off
+        } else if (avctx->codec_id == AV_CODEC_ID_H264) {
+            // roi for H.264 is specified for 16x16 pixel macroblocks - 1 MB
+            // is stored in each custom map entry
+
+            // number of MBs in each row
+            uint32_t mbWidth = (linesize_aligned + 16 - 1) >> 4;
+            // number of MBs in each column
+            uint32_t mbHeight = (height_aligned + 16 - 1) >> 4;
+            uint32_t numMbs   = mbWidth * mbHeight;
+            uint32_t customMapSize =
+                sizeof(ni_enc_avc_roi_custom_map_t) * numMbs;
+            g_avc_roi_map =
+                (ni_enc_avc_roi_custom_map_t *)calloc(1, customMapSize);
+            if (!g_avc_roi_map) {
+                return AVERROR(ENOMEM);
+            }
+
+            // copy roi MBs QPs into custom map
+            for (i = 0; i < numMbs; i++) {
+                if ((i % mbWidth > mbWidth / 3) &&
+                    (i % mbWidth < mbWidth * 2 / 3)) {
+                    g_avc_roi_map[i].field.mb_qp = importanceLevelCentre;
+                } else {
+                    g_avc_roi_map[i].field.mb_qp = importanceLevelRest;
+                }
+                sumQp += g_avc_roi_map[i].field.mb_qp;
+            }
+            ctx->api_ctx.roi_len = customMapSize;
+            ctx->api_ctx.roi_avg_qp =
+                (sumQp + (numMbs >> 1)) / numMbs; // round off
+        } else if (avctx->codec_id == AV_CODEC_ID_HEVC) {
+            // roi for H.265 is specified for 32x32 pixel subCTU blocks - 4
+            // subCTU QPs are stored in each custom CTU map entry
+
+            // number of CTUs in each row
+            uint32_t ctuWidth = (linesize_aligned + 64 - 1) >> 6;
+            // number of CTUs in each column
+            uint32_t ctuHeight = (height_aligned + 64 - 1) >> 6;
+            // number of sub CTUs in each row
+            uint32_t subCtuWidth = ctuWidth * 2;
+            // number of CTUs in each column
+            uint32_t subCtuHeight = ctuHeight * 2;
+            uint32_t numSubCtus   = subCtuWidth * subCtuHeight;
+
+            g_hevc_sub_ctu_roi_buf = (uint8_t *)malloc(numSubCtus);
+            if (!g_hevc_sub_ctu_roi_buf) {
+                return AVERROR(ENOMEM);
+            }
+            for (i = 0; i < numSubCtus; i++) {
+                if ((i % subCtuWidth > subCtuWidth / 3) &&
+                    (i % subCtuWidth < subCtuWidth * 2 / 3)) {
+                    g_hevc_sub_ctu_roi_buf[i] = importanceLevelCentre;
+                } else {
+                    g_hevc_sub_ctu_roi_buf[i] = importanceLevelRest;
+                }
+            }
+            g_hevc_roi_map = (ni_enc_hevc_roi_custom_map_t *)calloc(
+                1, sizeof(ni_enc_hevc_roi_custom_map_t) * ctuWidth * ctuHeight);
+            if (!g_hevc_roi_map) {
+                return AVERROR(ENOMEM);
+            }
+
+            for (i = 0; i < ctuHeight; i++) {
+                uint8_t *ptr = &g_hevc_sub_ctu_roi_buf[subCtuWidth * i * 2];
+                for (j = 0; j < ctuWidth; j++, ptr += 2) {
+                    ctu = (int)(i * ctuWidth + j);
+                    g_hevc_roi_map[ctu].field.sub_ctu_qp_0 = *ptr;
+                    g_hevc_roi_map[ctu].field.sub_ctu_qp_1 = *(ptr + 1);
+                    g_hevc_roi_map[ctu].field.sub_ctu_qp_2 =
+                        *(ptr + subCtuWidth);
+                    g_hevc_roi_map[ctu].field.sub_ctu_qp_3 =
+                        *(ptr + subCtuWidth + 1);
+                    sumQp += (g_hevc_roi_map[ctu].field.sub_ctu_qp_0 +
+                              g_hevc_roi_map[ctu].field.sub_ctu_qp_1 +
+                              g_hevc_roi_map[ctu].field.sub_ctu_qp_2 +
+                              g_hevc_roi_map[ctu].field.sub_ctu_qp_3);
+                }
+            }
+            ctx->api_ctx.roi_len =
+                ctuWidth * ctuHeight * sizeof(ni_enc_hevc_roi_custom_map_t);
+            ctx->api_ctx.roi_avg_qp =
+                (sumQp + (numSubCtus >> 1)) / numSubCtus; // round off.
+        }
+    }
+  } //end if(first_frame && ctx->started == 0)
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder send frame, pkt_size %d\n",
+         frame ? frame->pkt_size : -1);
+#if 0
+  if (frame)
+    printf("*** NI enc In avframe pts: %lld  pkt_dts : %lld  best_effort : %lld \n", frame->pts, frame->pkt_dts, frame->best_effort_timestamp);
+#endif
+
+  if (ctx->encoder_flushing)
+  {
+     if (! frame && is_input_fifo_empty(ctx))
+    {
+      av_log(avctx, AV_LOG_DEBUG, "XCoder EOF: null frame && fifo empty\n");
+      return AVERROR_EOF;
+    }
+  }
+
+  if (! frame)
+  {
+    if (is_input_fifo_empty(ctx))
+    {
+      ctx->eos_fme_received = 1;
+      av_log(avctx, AV_LOG_DEBUG, "null frame, eos_fme_received = 1\n");
+    }
+    else
+    {
+      avctx->internal->draining = 0;
+      av_log(avctx, AV_LOG_DEBUG, "null frame, but fifo not empty, clear draining = 0\n");
+    }
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_DEBUG, "XCoder send frame #%"PRIu64"\n",
+           ctx->api_ctx.frame_num);
+
+    // queue up the frame if fifo is NOT empty, or: sequence change ongoing !
+    if (! is_input_fifo_empty(ctx) ||
+        SESSION_RUN_STATE_SEQ_CHANGE_DRAINING == ctx->api_ctx.session_run_state)
+    {
+      enqueue_frame(avctx, frame);
+
+      if (SESSION_RUN_STATE_SEQ_CHANGE_DRAINING ==
+          ctx->api_ctx.session_run_state)
+      {
+        av_log(avctx, AV_LOG_TRACE, "XCoder doing sequence change, frame #%"PRIu64" "
+               "queued and return 0 !\n", ctx->api_ctx.frame_num);
+        return 0;
+      }
+    }
+    else if (frame != &ctx->buffered_fme)
+    {
+      // For FFmpeg-n4.4+ receive_packet interface the buffered_fme is fetched from
+      // ff_alloc_get_frame rather than passed as function argument. So we need to
+      // judge whether they are the same object. If they are the same NO need to
+      // reference.
+      ret = av_frame_ref(&ctx->buffered_fme, frame);
+    }
+  }
+
+  if (ctx->started == 0)
+  {
+    ctx->api_fme.data.frame.start_of_stream = 1;
+    ctx->started = 1;
+    //#ifdef QUADRA
+    #if 0
+    // reconfig self-test code
+    ctx->reconfigCount = 0;
+    if (p_param->reconf_demo_mode == XCODER_TEST_RECONF_BR)
+    {
+        p_param->reconf_hash[ctx->reconfigCount][0] =  25;
+        //p_param->reconf_hash[ctx->reconfigCount][1] =  100000000;
+        p_param->reconf_hash[ctx->reconfigCount][1] =  5000000;
+        p_param->reconf_hash[ctx->reconfigCount+1][0] =  -1;
+    }
+    /*
+    else if (p_param->reconf_demo_mode == XCODER_TEST_RECONF_INTRAPRD)
+    {
+        p_param->reconf_hash[ctx->reconfigCount][0] =  25;
+        p_param->reconf_hash[ctx->reconfigCount][1] =  10; // intraQP
+        p_param->reconf_hash[ctx->reconfigCount][2] =  5;  // intraPeriod
+        p_param->reconf_hash[ctx->reconfigCount][3] =  1;  // repeatHeaders
+        p_param->reconf_hash[ctx->reconfigCount+1][0] =  -1;
+    }
+    */
+    else if (p_param->reconf_demo_mode == XCODER_TEST_RECONF_VUI_HRD)
+    {
+        p_param->reconf_hash[ctx->reconfigCount][0] =  25;
+        p_param->reconf_hash[ctx->reconfigCount][1] =  1;   // colorDescPresent
+        p_param->reconf_hash[ctx->reconfigCount][2] =  255; // colorPrimaries
+        p_param->reconf_hash[ctx->reconfigCount][3] =  255; // colorTrc
+        p_param->reconf_hash[ctx->reconfigCount][4] =  255; // colorSpace
+        p_param->reconf_hash[ctx->reconfigCount][5] =  16;  // aspectRatioWidth
+        p_param->reconf_hash[ctx->reconfigCount][6] =  11;  // aspectRatioHeight
+        p_param->reconf_hash[ctx->reconfigCount][7] =  1;   // videoFullRange
+        p_param->reconf_hash[ctx->reconfigCount+1][0] =  -1;
+    }
+    else if (p_param->reconf_demo_mode == XCODER_TEST_RECONF_LONG_TERM_REF)
+    {
+        p_param->reconf_hash[ctx->reconfigCount][0] =  10;  // long term ref frame poc
+        p_param->reconf_hash[ctx->reconfigCount][1] =  1; // set LTR
+        p_param->reconf_hash[ctx->reconfigCount][2] =  0; // ref LTR is not supported
+        p_param->reconf_hash[ctx->reconfigCount+1][0] =  13;  // long term ref frame poc
+        p_param->reconf_hash[ctx->reconfigCount+1][1] =  1; // set LTR
+        p_param->reconf_hash[ctx->reconfigCount+1][2] =  0;
+        p_param->reconf_hash[ctx->reconfigCount+2][0] =  16;  // long term ref frame poc
+        p_param->reconf_hash[ctx->reconfigCount+2][1] =  1;  // set LTR
+        p_param->reconf_hash[ctx->reconfigCount+2][2] =  0;
+        p_param->reconf_hash[ctx->reconfigCount+3][0] =  17;  // long term ref frame poc
+        p_param->reconf_hash[ctx->reconfigCount+3][1] =  1;  // set LTR
+        p_param->reconf_hash[ctx->reconfigCount+3][2] =  0;
+        p_param->reconf_hash[ctx->reconfigCount+4][0] =  24;  // long term ref frame poc
+        p_param->reconf_hash[ctx->reconfigCount+4][1] =  1; // set LTR
+        p_param->reconf_hash[ctx->reconfigCount+4][2] =  0;
+        p_param->reconf_hash[ctx->reconfigCount+5][0] =  25;  // long term ref frame poc
+        p_param->reconf_hash[ctx->reconfigCount+5][1] =  1; // set LTR
+        p_param->reconf_hash[ctx->reconfigCount+5][2] =  0;
+        p_param->reconf_hash[ctx->reconfigCount+6][0] =  -1; // stop
+    }
+    /*
+    else if (p_param->reconf_demo_mode == XCODER_TEST_RECONF_RC_MIN_MAX_QP)
+    {
+        p_param->reconf_hash[ctx->reconfigCount][0] =  25;
+        p_param->reconf_hash[ctx->reconfigCount][1] =  10; // minQpI
+        p_param->reconf_hash[ctx->reconfigCount][2] =  12; // maxQpI
+        p_param->reconf_hash[ctx->reconfigCount][3] =  30; // minQpPB
+        p_param->reconf_hash[ctx->reconfigCount][4] =  32; // maxQpPB
+        p_param->reconf_hash[ctx->reconfigCount+1][0] =  -1;
+    }
+    */
+    else if (p_param->reconf_demo_mode == XCODER_TEST_RECONF_LTR_INTERVAL)
+    {
+        p_param->reconf_hash[ctx->reconfigCount][0] =  25; // current frame
+        p_param->reconf_hash[ctx->reconfigCount][1] =  5; // reconfig LTR interval to 5
+        p_param->reconf_hash[ctx->reconfigCount+1][0] =  -1;
+    }
+    else if (p_param->reconf_demo_mode == XCODER_TEST_INVALID_REF_FRAME)
+    {
+        p_param->reconf_hash[ctx->reconfigCount][0] =  24; // current frame
+        p_param->reconf_hash[ctx->reconfigCount][1] =  21; // invalidate ref frame poc
+        p_param->reconf_hash[ctx->reconfigCount+1][0] =  -1;
+    }
+    /*
+    else if (p_param->reconf_demo_mode == XCODER_TEST_FORCE_IDR_FRAME)
+    {
+        p_param->reconf_hash[ctx->reconfigCount][0] =  10;
+        p_param->reconf_hash[ctx->reconfigCount+1][0] =  -1;
+    }
+    */
+#endif
+    }
+    else if (ctx->api_ctx.session_run_state == SESSION_RUN_STATE_SEQ_CHANGE_OPENING) {
+        ctx->api_fme.data.frame.start_of_stream = 1;
+    }
+    else {
+        ctx->api_fme.data.frame.start_of_stream = 0;
+    }
+    if (is_input_fifo_empty(ctx)) {
+        av_log(avctx, AV_LOG_DEBUG,
+               "no frame in fifo to send, just send/receive ..\n");
+        if (ctx->eos_fme_received) {
+            av_log(avctx, AV_LOG_DEBUG,
+                   "no frame in fifo to send, send eos ..\n");
+        }
+    } else {
+        av_log(avctx, AV_LOG_DEBUG, "av_fifo_generic_peek fme\n");
+        av_fifo_generic_peek(ctx->fme_fifo, &ctx->buffered_fme, sizeof(AVFrame),
+                             NULL);
+        ctx->buffered_fme.extended_data = ctx->buffered_fme.data;
+    }
+
+    if (!ctx->eos_fme_received) {
+        int8_t bit_depth = 1;
+        ishwframe        = ctx->buffered_fme.format == AV_PIX_FMT_NI_QUAD;
+        if (ishwframe) {
+            // Superframe early cleanup of unused outputs
+            niFrameSurface1_t *pOutExtra;
+            if (ctx->buffered_fme.buf[1]) {
+                // NOLINTNEXTLINE(clang-diagnostic-incompatible-pointer-types)
+                pOutExtra= (niFrameSurface1_t *)ctx->buffered_fme.buf[1]->data;
+                if (pOutExtra->ui16FrameIdx != 0) {
+                    av_log(avctx, AV_LOG_DEBUG, "Unref unused index %d\n",
+                           pOutExtra->ui16FrameIdx);
+                } else {
+                    av_log(avctx, AV_LOG_ERROR,
+                           "ERROR: Should not be getting superframe with dead "
+                           "outputs\n");
+                }
+                av_buffer_unref(&ctx->buffered_fme.buf[1]);
+                if (ctx->buffered_fme.buf[2]) {
+                    // NOLINTNEXTLINE(clang-diagnostic-incompatible-pointer-types)
+                    pOutExtra = (niFrameSurface1_t *)ctx->buffered_fme.buf[2]->data;
+                    if (pOutExtra->ui16FrameIdx != 0) {
+                        av_log(avctx, AV_LOG_DEBUG, "Unref unused index %d\n",
+                               pOutExtra->ui16FrameIdx);
+                    } else {
+                        av_log(
+                            avctx, AV_LOG_ERROR,
+                            "ERROR: Should not be getting superframe with dead "
+                            "outputs\n");
+                    }
+                    av_buffer_unref(&ctx->buffered_fme.buf[2]);
+                }
+            }
+
+            pOutExtra = (niFrameSurface1_t *)ctx->buffered_fme.data[3];
+            bit_depth = pOutExtra->bit_depth;
+
+            switch (bit_depth) {
+            case 1:
+            case 2:
+                break;
+            default:
+                av_log(avctx, AV_LOG_ERROR, "ERROR: Unknown bit depth %d!\n", bit_depth);
+                return AVERROR_INVALIDDATA;
+            }
+        } else {
+            if (AV_PIX_FMT_YUV420P10BE == ctx->buffered_fme.format ||
+                AV_PIX_FMT_YUV420P10LE == ctx->buffered_fme.format ||
+                AV_PIX_FMT_P010LE == ctx->buffered_fme.format) {
+                bit_depth = 2;
+            }
+        }
+
+#ifdef SEQUENCE_CHANGE_INJECT
+        if ((ctx->buffered_fme.height && ctx->buffered_fme.width) &&
+            (ctx->buffered_fme.height != avctx->height ||
+             ctx->buffered_fme.width != avctx->width ||
+             bit_depth != ctx->api_ctx.bit_depth_factor ||
+             (ctx->api_ctx.frame_num > 0 && ctx->api_ctx.frame_num % 10 == 0)))
+#else
+        if ((ctx->buffered_fme.height && ctx->buffered_fme.width) &&
+            (ctx->buffered_fme.height != avctx->height ||
+             ctx->buffered_fme.width != avctx->width ||
+             bit_depth != ctx->api_ctx.bit_depth_factor))
+#endif
+        {
+            av_log(avctx, AV_LOG_INFO,
+                   "xcoder_send_frame resolution change %dx%d "
+                   "-> %dx%d or bit depth change %d -> %d\n",
+                   avctx->width, avctx->height, ctx->buffered_fme.width,
+                   ctx->buffered_fme.height, ctx->api_ctx.bit_depth_factor,
+                   bit_depth);
+
+            ctx->api_ctx.session_run_state =
+                SESSION_RUN_STATE_SEQ_CHANGE_DRAINING;
+            ctx->eos_fme_received = 1;
+
+            // have to queue this frame if not done so: an empty queue
+            if (is_input_fifo_empty(ctx)) {
+                av_log(avctx, AV_LOG_TRACE,
+                       "resolution change when fifo empty, frame "
+                       "#%" PRIu64 " being queued ..\n",
+                       ctx->api_ctx.frame_num);
+                // unref buffered frame (this buffered frame is taken from input
+                // AVFrame) because we are going to send EOS (instead of sending
+                // buffered frame)
+                if (frame != &ctx->buffered_fme) {
+                    // For FFmpeg-n4.4+ receive_packet interface the buffered_fme is fetched from
+                    // ff_alloc_get_frame rather than passed as function argument. So we need to
+                    // judge whether they are the same object. If they are the same do NOT unreference
+                    // any of them because we need to enqueue it later.
+                    av_frame_unref(&ctx->buffered_fme);
+                }
+                enqueue_frame(avctx, frame);
+            }
+        }
+    }
+
+    ctx->api_fme.data.frame.preferred_characteristics_data_len = 0;
+    ctx->api_fme.data.frame.end_of_stream                      = 0;
+    ctx->api_fme.data.frame.force_key_frame =
+        ctx->api_fme.data.frame.use_cur_src_as_long_term_pic =
+            ctx->api_fme.data.frame.use_long_term_ref = 0;
+
+    ctx->api_fme.data.frame.sei_total_len =
+        ctx->api_fme.data.frame.sei_cc_offset = ctx->api_fme.data.frame
+                                                    .sei_cc_len =
+            ctx->api_fme.data.frame.sei_hdr_mastering_display_color_vol_offset =
+                ctx->api_fme.data.frame
+                    .sei_hdr_mastering_display_color_vol_len =
+                    ctx->api_fme.data.frame
+                        .sei_hdr_content_light_level_info_offset =
+                        ctx->api_fme.data.frame
+                            .sei_hdr_content_light_level_info_len =
+                            ctx->api_fme.data.frame.sei_hdr_plus_offset =
+                                ctx->api_fme.data.frame.sei_hdr_plus_len = 0;
+
+    ctx->api_fme.data.frame.roi_len      = 0;
+    ctx->api_fme.data.frame.reconf_len   = 0;
+    ctx->api_fme.data.frame.force_pic_qp = 0;
+
+    if (SESSION_RUN_STATE_SEQ_CHANGE_DRAINING ==
+            ctx->api_ctx.session_run_state ||
+        (ctx->eos_fme_received && is_input_fifo_empty(ctx))) {
+        av_log(avctx, AV_LOG_VERBOSE, "XCoder start flushing\n");
+        ctx->api_fme.data.frame.end_of_stream = 1;
+        ctx->encoder_flushing                 = 1;
+    } else {
+        format_in_use = ctx->buffered_fme.format;
+        // NETINT_INTERNAL - currently only for internal testing
+        // reset encoder change data buffer for reconf parameters
+        if (p_param->reconf_demo_mode > XCODER_TEST_RECONF_OFF &&
+            p_param->reconf_demo_mode < XCODER_TEST_RECONF_END) {
+            memset(ctx->api_ctx.enc_change_params, 0,
+                   sizeof(ni_encoder_change_params_t));
+        }
+
+        // extra data starts with metadata header, various aux data sizes
+        // have been reset above
+        ctx->api_fme.data.frame.extra_data_len =
+            NI_APP_ENC_FRAME_META_DATA_SIZE;
+
+        ctx->api_fme.data.frame.ni_pict_type    = 0;
+
+        switch (p_param->reconf_demo_mode) {
+        case XCODER_TEST_RECONF_BR:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                aux_data = ni_frame_new_aux_data(
+                    &dec_frame, NI_FRAME_AUX_DATA_BITRATE, sizeof(int32_t));
+                if (!aux_data) {
+                    return AVERROR(ENOMEM);
+                }
+                *((int32_t *)aux_data->data) =
+                    p_param->reconf_hash[ctx->reconfigCount][1];
+
+                ctx->reconfigCount++;
+                if (p_param->cfg_enc_params.hrdEnable)
+                {
+                    ctx->api_fme.data.frame.force_key_frame = 1;
+                    ctx->api_fme.data.frame.ni_pict_type = PIC_TYPE_IDR;
+                }
+            }
+            break;
+        /* // not required by customer
+        case XCODER_TEST_RECONF_INTRAPRD:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ctx->api_ctx.enc_change_params->enable_option |=
+                    NI_SET_CHANGE_PARAM_INTRA_PARAM;
+                ctx->api_ctx.enc_change_params->intraQP =
+                    p_param->reconf_hash[ctx->reconfigCount][1];
+                ctx->api_ctx.enc_change_params->intraPeriod =
+                    p_param->reconf_hash[ctx->reconfigCount][2];
+                ctx->api_ctx.enc_change_params->repeatHeaders =
+                    p_param->reconf_hash[ctx->reconfigCount][3];
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu reconf "
+                       "intraQP %d intraPeriod %d repeatHeaders %d\n",
+                       ctx->api_ctx.frame_num,
+                       ctx->api_ctx.enc_change_params->intraQP,
+                       ctx->api_ctx.enc_change_params->intraPeriod,
+                       ctx->api_ctx.enc_change_params->repeatHeaders);
+
+                ctx->api_fme.data.frame.reconf_len =
+                    sizeof(ni_encoder_change_params_t);
+                ctx->reconfigCount++;
+            }
+            break;
+        */
+        // reconfig VUI parameters
+        case XCODER_TEST_RECONF_VUI_HRD:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                aux_data = ni_frame_new_aux_data(&dec_frame,
+                                                 NI_FRAME_AUX_DATA_VUI,
+                                                 sizeof(ni_vui_hrd_t));
+                if (!aux_data) {
+                    return AVERROR(ENOMEM);
+                }
+                ni_vui_hrd_t *vui = (ni_vui_hrd_t *)aux_data->data;
+                vui->colorDescPresent =
+                    p_param->reconf_hash[ctx->reconfigCount][1];
+                vui->colorPrimaries =
+                    p_param->reconf_hash[ctx->reconfigCount][2];
+                vui->colorTrc =
+                    p_param->reconf_hash[ctx->reconfigCount][3];
+                vui->colorSpace =
+                    p_param->reconf_hash[ctx->reconfigCount][4];
+                vui->aspectRatioWidth =
+                    p_param->reconf_hash[ctx->reconfigCount][5];
+                vui->aspectRatioHeight =
+                    p_param->reconf_hash[ctx->reconfigCount][6];
+                vui->videoFullRange =
+                    p_param->reconf_hash[ctx->reconfigCount][7];
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu reconf "
+                       "vui colorDescPresent %d colorPrimaries %d "
+                        "colorTrc %d colorSpace %d aspectRatioWidth %d "
+                       "aspectRatioHeight %d videoFullRange %d\n",
+                       ctx->api_ctx.frame_num, vui->colorDescPresent,
+                       vui->colorPrimaries, vui->colorTrc,
+                       vui->colorSpace, vui->aspectRatioWidth,
+                       vui->aspectRatioHeight, vui->videoFullRange);
+
+                ctx->reconfigCount++;
+            }
+            break;
+        // long term ref
+        case XCODER_TEST_RECONF_LONG_TERM_REF:
+            // the reconf file data line format for this is:
+            // <frame-number>:useCurSrcAsLongtermPic,useLongtermRef where
+            // values will stay the same on every frame until changed.
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                AVFrameSideData *ltr_sd;
+                AVNetintLongTermRef *p_ltr;
+                ltr_sd = av_frame_new_side_data(
+                    &ctx->buffered_fme, AV_FRAME_DATA_NETINT_LONG_TERM_REF,
+                    sizeof(AVNetintLongTermRef));
+                if (ltr_sd) {
+                    p_ltr = (AVNetintLongTermRef *)ltr_sd->data;
+                    p_ltr->use_cur_src_as_long_term_pic =
+                        (uint8_t)p_param->reconf_hash[ctx->reconfigCount][1];
+                    p_ltr->use_long_term_ref =
+                        (uint8_t)p_param->reconf_hash[ctx->reconfigCount][2];
+                    av_log(avctx, AV_LOG_TRACE,
+                           "xcoder_send_frame: frame #%lu metadata "
+                           "use_cur_src_as_long_term_pic %d use_long_term_ref "
+                           "%d\n",
+                           ctx->api_ctx.frame_num,
+                           p_ltr->use_cur_src_as_long_term_pic,
+                           p_ltr->use_long_term_ref);
+                }
+                ctx->reconfigCount++;
+            }
+            break;
+            /* // not required by customer
+            // reconfig min / max QP
+            case XCODER_TEST_RECONF_RC_MIN_MAX_QP:
+                if (ctx->api_ctx.frame_num ==
+                    p_param->reconf_hash[ctx->reconfigCount][0]) {
+                    ctx->api_ctx.enc_change_params->enable_option |=
+                        NI_SET_CHANGE_PARAM_RC_MIN_MAX_QP;
+                    ctx->api_ctx.enc_change_params->minQpI =
+                        p_param->reconf_hash[ctx->reconfigCount][1];
+                    ctx->api_ctx.enc_change_params->maxQpI =
+                        p_param->reconf_hash[ctx->reconfigCount][2];
+                    ctx->api_ctx.enc_change_params->minQpPB =
+                        p_param->reconf_hash[ctx->reconfigCount][3];
+                    ctx->api_ctx.enc_change_params->maxQpPB =
+                        p_param->reconf_hash[ctx->reconfigCount][4];
+
+                    ctx->api_fme.data.frame.reconf_len =
+                        sizeof(ni_encoder_change_params_t);
+                    ctx->reconfigCount++;
+                }
+                break;
+            */
+#ifdef QUADRA
+        // reconfig LTR interval
+        case XCODER_TEST_RECONF_LTR_INTERVAL:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                aux_data = ni_frame_new_aux_data(&dec_frame,
+                                                 NI_FRAME_AUX_DATA_LTR_INTERVAL,
+                                                 sizeof(int32_t));
+                if (!aux_data) {
+                    return AVERROR(ENOMEM);
+                }
+                *((int32_t *)aux_data->data) =
+                    p_param->reconf_hash[ctx->reconfigCount][1];
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu reconf "
+                       "ltrInterval %d\n",
+                       ctx->api_ctx.frame_num,
+                       p_param->reconf_hash[ctx->reconfigCount][1]);
+
+                ctx->reconfigCount++;
+            }
+            break;
+        // invalidate reference frames
+        case XCODER_TEST_INVALID_REF_FRAME:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                aux_data = ni_frame_new_aux_data(
+                    &dec_frame, NI_FRAME_AUX_DATA_INVALID_REF_FRAME,
+                    sizeof(int32_t));
+                if (!aux_data) {
+                    return AVERROR(ENOMEM);
+                }
+                *((int32_t *)aux_data->data) =
+                    p_param->reconf_hash[ctx->reconfigCount][1];
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu reconf "
+                       "invalidFrameNum %d\n",
+                       ctx->api_ctx.frame_num,
+                       p_param->reconf_hash[ctx->reconfigCount][1]);
+
+                ctx->reconfigCount++;
+            }
+            break;
+        // reconfig framerate
+        case XCODER_TEST_RECONF_FRAMERATE:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ni_framerate_t *framerate;
+
+                aux_data = ni_frame_new_aux_data(&dec_frame,
+                                                 NI_FRAME_AUX_DATA_FRAMERATE,
+                                                 sizeof(ni_framerate_t));
+                if (!aux_data) {
+                    return AVERROR(ENOMEM);
+                }
+
+                framerate = (ni_framerate_t *)aux_data->data;
+                framerate->framerate_num =
+                    (int32_t)p_param->reconf_hash[ctx->reconfigCount][1];
+                framerate->framerate_denom =
+                    (int32_t)p_param->reconf_hash[ctx->reconfigCount][2];
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu reconf "
+                       "framerate (%d/%d)\n",
+                       ctx->api_ctx.frame_num, framerate->framerate_num,
+                       framerate->framerate_denom);
+                ctx->reconfigCount++;
+            }
+            break;
+            // force IDR frame test code
+            /*
+            case XCODER_TEST_FORCE_IDR_FRAME:
+            if (ctx->api_ctx.frame_num ==
+            p_param->reconf_hash[ctx->reconfigCount][0])
+            {
+                ctx->api_fme.data.frame.force_key_frame = 1;
+                ctx->api_fme.data.frame.ni_pict_type = PIC_TYPE_IDR;
+                ctx->reconfigCount ++;
+            }
+            break;
+            */
+            // force IDR frame through API test code
+        case XCODER_TEST_FORCE_IDR_FRAME:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ni_force_idr_frame_type(&ctx->api_ctx);
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu force IDR frame\n",
+                       ctx->api_ctx.frame_num);
+
+                ctx->reconfigCount++;
+            }
+            break;
+            // reconfig bit rate through API test code
+        case XCODER_TEST_RECONF_BR_API:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ni_reconfig_bitrate(
+                    &ctx->api_ctx, p_param->reconf_hash[ctx->reconfigCount][1]);
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu API reconfig BR %d\n",
+                       ctx->api_ctx.frame_num,
+                       p_param->reconf_hash[ctx->reconfigCount][1]);
+
+                ctx->reconfigCount++;
+            }
+            break;
+        case XCODER_TEST_RECONF_VUI_HRD_API:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ni_vui_hrd_t vui;
+                vui.colorDescPresent =
+                    p_param->reconf_hash[ctx->reconfigCount][1];
+                vui.colorPrimaries =
+                    p_param->reconf_hash[ctx->reconfigCount][2];
+                vui.colorTrc =
+                    p_param->reconf_hash[ctx->reconfigCount][3];
+                vui.colorSpace =
+                    p_param->reconf_hash[ctx->reconfigCount][4];
+                vui.aspectRatioWidth =
+                    p_param->reconf_hash[ctx->reconfigCount][5];
+                vui.aspectRatioHeight =
+                    p_param->reconf_hash[ctx->reconfigCount][6];
+                vui.videoFullRange =
+                    p_param->reconf_hash[ctx->reconfigCount][7];
+                ni_reconfig_vui(&ctx->api_ctx, &vui);
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu reconf "
+                       "vui colorDescPresent %d colorPrimaries %d "
+                       "colorTrc %d colorSpace %d aspectRatioWidth %d "
+                       "aspectRatioHeight %d videoFullRange %d\n",
+                       ctx->api_ctx.frame_num, vui.colorDescPresent,
+                       vui.colorPrimaries, vui.colorTrc,
+                       vui.colorSpace, vui.aspectRatioWidth,
+                       vui.aspectRatioHeight, vui.videoFullRange);
+                ctx->reconfigCount++;
+            }
+            break;
+        case XCODER_TEST_RECONF_LTR_API:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ni_long_term_ref_t ltr;
+                ltr.use_cur_src_as_long_term_pic =
+                    (uint8_t)p_param->reconf_hash[ctx->reconfigCount][1];
+                ltr.use_long_term_ref =
+                    (uint8_t)p_param->reconf_hash[ctx->reconfigCount][2];
+
+                ni_set_ltr(&ctx->api_ctx, &ltr);
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame(): frame #%lu API set LTR\n",
+                       ctx->api_ctx.frame_num);
+                ctx->reconfigCount++;
+            }
+            break;
+        case XCODER_TEST_RECONF_LTR_INTERVAL_API:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ni_set_ltr_interval(
+                    &ctx->api_ctx, p_param->reconf_hash[ctx->reconfigCount][1]);
+                av_log(
+                    avctx, AV_LOG_TRACE,
+                    "xcoder_send_frame(): frame #%lu API set LTR interval %d\n",
+                    ctx->api_ctx.frame_num,
+                    p_param->reconf_hash[ctx->reconfigCount][1]);
+                ctx->reconfigCount++;
+            }
+            break;
+        case XCODER_TEST_INVALID_REF_FRAME_API:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ni_set_frame_ref_invalid(
+                    &ctx->api_ctx, p_param->reconf_hash[ctx->reconfigCount][1]);
+                av_log(
+                    avctx, AV_LOG_TRACE,
+                    "xcoder_send_frame(): frame #%lu API set frame ref invalid "
+                    "%d\n",
+                    ctx->api_ctx.frame_num,
+                    p_param->reconf_hash[ctx->reconfigCount][1]);
+                ctx->reconfigCount++;
+            }
+            break;
+        case XCODER_TEST_RECONF_FRAMERATE_API:
+            if (ctx->api_ctx.frame_num ==
+                p_param->reconf_hash[ctx->reconfigCount][0]) {
+                ni_framerate_t framerate;
+                framerate.framerate_num =
+                    (int32_t)p_param->reconf_hash[ctx->reconfigCount][1];
+                framerate.framerate_denom =
+                    (int32_t)p_param->reconf_hash[ctx->reconfigCount][2];
+                ni_reconfig_framerate(&ctx->api_ctx, &framerate);
+                av_log(avctx, AV_LOG_TRACE,
+                       "xcoder_send_frame: frame #%lu API reconfig framerate "
+                       "(%d/%d)\n",
+                       ctx->api_ctx.frame_num,
+                       p_param->reconf_hash[ctx->reconfigCount][1],
+                       p_param->reconf_hash[ctx->reconfigCount][2]);
+
+                ctx->reconfigCount++;
+            }
+            break;
+#endif
+      case XCODER_TEST_RECONF_OFF:
+      default:
+        ;
+      }
+
+    // long term reference frame support
+    side_data = av_frame_get_side_data(&ctx->buffered_fme,
+                                       AV_FRAME_DATA_NETINT_LONG_TERM_REF);
+    if (side_data && (side_data->size == sizeof(AVNetintLongTermRef))) {
+        aux_data =
+            ni_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_LONG_TERM_REF,
+                                  sizeof(ni_long_term_ref_t));
+        if (aux_data) {
+            memcpy(aux_data->data, side_data->data, side_data->size);
+        }
+    }
+
+    // support VFR
+    if (ctx->api_param.enable_vfr)
+    {
+        int cur_fps = 0, pre_fps = 0;
+
+        pre_fps = ctx->api_ctx.prev_fps;
+
+        if (ctx->buffered_fme.pts > ctx->api_ctx.prev_pts)
+        {
+          ctx->api_ctx.passed_time_in_timebase_unit += ctx->buffered_fme.pts - ctx->api_ctx.prev_pts;
+          ctx->api_ctx.count_frame_num_in_sec++;
+          //change the FrameRate for VFR
+          //1. Only when the fps change, setting the new bitrate
+          //2. The interval between two framerate chagne settings shall be greater than 1 seconds
+          //   or at the start the transcoding
+          if (ctx->api_ctx.passed_time_in_timebase_unit >= (avctx->time_base.den / avctx->time_base.num))
+          {
+            cur_fps = ctx->api_ctx.count_frame_num_in_sec;
+            if ((ctx->api_ctx.frame_num != 0) && (pre_fps != cur_fps) &&
+                ((ctx->api_ctx.frame_num < ctx->api_param.cfg_enc_params.frame_rate) ||
+                 (ctx->api_ctx.frame_num - ctx->api_ctx.last_change_framenum >= ctx->api_param.cfg_enc_params.frame_rate)))
+            {
+              aux_data = ni_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_FRAMERATE, sizeof(ni_framerate_t));
+              if (aux_data)
+              {
+                ni_framerate_t *framerate = (ni_framerate_t *)aux_data->data;
+                framerate->framerate_num = cur_fps;
+                framerate->framerate_denom = 1;
+              }
+
+              ctx->api_ctx.last_change_framenum = ctx->api_ctx.frame_num;
+              ctx->api_ctx.prev_fps = cur_fps;
+            }
+            ctx->api_ctx.count_frame_num_in_sec = 0;
+            ctx->api_ctx.passed_time_in_timebase_unit = 0;
+          }
+          ctx->api_ctx.prev_pts = ctx->buffered_fme.pts;
+        }
+        else if (ctx->buffered_fme.pts < ctx->api_ctx.prev_pts)
+        {
+          //error handle for the case that pts jump back
+          //this may cause a little error in the bitrate setting, This little error is acceptable.
+          //As long as the subsequent, PTS is normal, it will be repaired quickly.
+          ctx->api_ctx.prev_pts = ctx->buffered_fme.pts;
+        }
+        else
+        {
+          //do nothing, when the pts of two adjacent frames are the same
+          //this may cause a little error in the bitrate setting, This little error is acceptable.
+          //As long as the subsequent, PTS is normal, it will be repaired quickly.
+        }
+    }
+
+    // force pic qp demo mode: initial QP (200 frames) -> QP value specified by
+    // ForcePicQpDemoMode (100 frames) -> initial QP (remaining frames)
+    if (p_param->force_pic_qp_demo_mode)
+    {
+      if (ctx->api_ctx.frame_num >= 300)
+      {
+          ctx->api_fme.data.frame.force_pic_qp =
+              p_param->cfg_enc_params.rc.intra_qp;
+      }
+      else if (ctx->api_ctx.frame_num >= 200)
+      {
+        ctx->api_fme.data.frame.force_pic_qp = p_param->force_pic_qp_demo_mode;
+      }
+    }
+
+    // supply QP map if ROI enabled and if ROIs passed in
+    // Note: ROI demo mode takes higher priority over side data !
+    side_data = av_frame_get_side_data(&ctx->buffered_fme, AV_FRAME_DATA_REGIONS_OF_INTEREST);
+
+    if (!p_param->roi_demo_mode && p_param->cfg_enc_params.roi_enable &&
+        side_data) {
+        aux_data = ni_frame_new_aux_data(
+            &dec_frame, NI_FRAME_AUX_DATA_REGIONS_OF_INTEREST, side_data->size);
+        if (aux_data) {
+            memcpy(aux_data->data, side_data->data, side_data->size);
+        }
+    }
+
+    // Note: when ROI demo modes enabled, supply ROI map for the specified range
+    //       frames, and 0 map for others
+    if (QUADRA && p_param->roi_demo_mode &&
+        p_param->cfg_enc_params.roi_enable) {
+        if (ctx->api_ctx.frame_num > 90 && ctx->api_ctx.frame_num < 300) {
+            ctx->api_fme.data.frame.roi_len = ctx->api_ctx.roi_len;
+        } else {
+            ctx->api_fme.data.frame.roi_len = 0;
+        }
+        // when ROI enabled, always have a data buffer for ROI
+        // Note: this is handled separately from ROI through side/aux data
+        ctx->api_fme.data.frame.extra_data_len += ctx->api_ctx.roi_len;
+    }
+
+    // SEI (HDR)
+    // content light level info
+    if (!(p_param->cfg_enc_params.HDR10CLLEnable)) // not user set
+    {
+        side_data = av_frame_get_side_data(&ctx->buffered_fme, AV_FRAME_DATA_CONTENT_LIGHT_LEVEL);
+
+        if (side_data && side_data->size == sizeof(AVContentLightMetadata)) {
+            aux_data = ni_frame_new_aux_data(
+                &dec_frame, NI_FRAME_AUX_DATA_CONTENT_LIGHT_LEVEL,
+                sizeof(ni_content_light_level_t));
+            if (aux_data) {
+                memcpy(aux_data->data, side_data->data, side_data->size);
+            }
+        }
+    } else if ((AV_CODEC_ID_H264 == avctx->codec_id ||
+                ctx->api_ctx.bit_depth_factor == 1) &&
+               ctx->api_ctx.light_level_data_len == 0)
+    // User input maxCLL so create SEIs for h264 and don't touch for (h265 &&
+    // hdr10) since that is conveyed in config step
+    ////Quadra autoset only for hdr10 format with hevc
+    {
+        aux_data = ni_frame_new_aux_data(&dec_frame,
+                                         NI_FRAME_AUX_DATA_CONTENT_LIGHT_LEVEL,
+                                         sizeof(ni_content_light_level_t));
+        if (aux_data) {
+            ni_content_light_level_t *cll =
+                (ni_content_light_level_t *)(aux_data->data);
+            cll->max_cll  = p_param->cfg_enc_params.HDR10MaxLight;
+            cll->max_fall = p_param->cfg_enc_params.HDR10AveLight;
+        }
+    }
+
+    // mastering display color volume
+    if (!(p_param->cfg_enc_params.HDR10Enable)) // not user set
+    {
+        side_data = av_frame_get_side_data(&ctx->buffered_fme, AV_FRAME_DATA_MASTERING_DISPLAY_METADATA);
+        if (side_data && side_data->size == sizeof(AVMasteringDisplayMetadata))
+        {
+            aux_data = ni_frame_new_aux_data(
+                &dec_frame, NI_FRAME_AUX_DATA_MASTERING_DISPLAY_METADATA,
+                sizeof(ni_mastering_display_metadata_t));
+            if (aux_data) {
+                memcpy(aux_data->data, side_data->data, side_data->size);
+            }
+        }
+    }
+    else if ((AV_CODEC_ID_H264 == avctx->codec_id ||
+        ctx->api_ctx.bit_depth_factor == 1) &&
+        ctx->api_ctx.sei_hdr_mastering_display_color_vol_len == 0)
+        // User input masterDisplay so create SEIs for h264 and don't touch for (h265 &&
+        // hdr10) since that is conveyed in config step
+        ////Quadra autoset only for hdr10 format with hevc
+    {
+        aux_data = ni_frame_new_aux_data(&dec_frame,
+            NI_FRAME_AUX_DATA_MASTERING_DISPLAY_METADATA,
+            sizeof(ni_mastering_display_metadata_t));
+        if (aux_data) {
+            ni_mastering_display_metadata_t *mst_dsp =
+                (ni_mastering_display_metadata_t *)(aux_data->data);
+
+            //X, Y display primaries for RGB channels and white point(WP) in units of 0.00002
+            //and max, min luminance(L) values in units of 0.0001 nits
+            //xy are denom = 50000 num = HDR10dx0/y
+            mst_dsp->display_primaries[0][0].den = MASTERING_DISP_CHROMA_DEN;
+            mst_dsp->display_primaries[0][1].den = MASTERING_DISP_CHROMA_DEN;
+            mst_dsp->display_primaries[1][0].den = MASTERING_DISP_CHROMA_DEN;
+            mst_dsp->display_primaries[1][1].den = MASTERING_DISP_CHROMA_DEN;
+            mst_dsp->display_primaries[2][0].den = MASTERING_DISP_CHROMA_DEN;
+            mst_dsp->display_primaries[2][1].den = MASTERING_DISP_CHROMA_DEN;
+            mst_dsp->white_point[0].den = MASTERING_DISP_CHROMA_DEN;
+            mst_dsp->white_point[1].den = MASTERING_DISP_CHROMA_DEN;
+            mst_dsp->min_luminance.den = MASTERING_DISP_LUMA_DEN;
+            mst_dsp->max_luminance.den = MASTERING_DISP_LUMA_DEN;
+            mst_dsp->display_primaries[0][0].num = p_param->cfg_enc_params.HDR10dx0;
+            mst_dsp->display_primaries[0][1].num = p_param->cfg_enc_params.HDR10dy0;
+            mst_dsp->display_primaries[1][0].num = p_param->cfg_enc_params.HDR10dx1;
+            mst_dsp->display_primaries[1][1].num = p_param->cfg_enc_params.HDR10dy1;
+            mst_dsp->display_primaries[2][0].num = p_param->cfg_enc_params.HDR10dx2;
+            mst_dsp->display_primaries[2][1].num = p_param->cfg_enc_params.HDR10dy2;
+            mst_dsp->white_point[0].num = p_param->cfg_enc_params.HDR10wx;
+            mst_dsp->white_point[1].num = p_param->cfg_enc_params.HDR10wy;
+            mst_dsp->min_luminance.num = p_param->cfg_enc_params.HDR10minluma;
+            mst_dsp->max_luminance.num = p_param->cfg_enc_params.HDR10maxluma;
+            mst_dsp->has_primaries = 1;
+            mst_dsp->has_luminance = 1;
+        }
+    }
+    // SEI (HDR10+)
+    side_data = av_frame_get_side_data(&ctx->buffered_fme, AV_FRAME_DATA_DYNAMIC_HDR_PLUS);
+    if (side_data && side_data->size == sizeof(AVDynamicHDRPlus))
+    {
+        aux_data = ni_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_HDR_PLUS,
+                                         sizeof(ni_dynamic_hdr_plus_t));
+        if (aux_data) {
+            memcpy(aux_data->data, side_data->data, side_data->size);
+        }
+    } // hdr10+
+
+    // SEI (close caption)
+    side_data = av_frame_get_side_data(&ctx->buffered_fme, AV_FRAME_DATA_A53_CC);
+
+    if (side_data && side_data->size > 0)
+    {
+        aux_data = ni_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_A53_CC,
+                                         side_data->size);
+        if (aux_data) {
+            memcpy(aux_data->data, side_data->data, side_data->size);
+        }
+    }
+
+    // User data unregistered SEI
+    side_data =
+        av_frame_get_side_data(&ctx->buffered_fme, AV_FRAME_DATA_NETINT_UDU_SEI);
+    if (side_data && side_data->size > 0) {
+        aux_data = ni_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_UDU_SEI,
+                                         side_data->size);
+        if (aux_data) {
+            memcpy(aux_data->data, (uint8_t *)side_data->data, side_data->size);
+        }
+    }
+
+    if (ctx->api_ctx.force_frame_type) {
+        switch (ctx->buffered_fme.pict_type) {
+        case AV_PICTURE_TYPE_I:
+            ctx->api_fme.data.frame.ni_pict_type = PIC_TYPE_IDR;
+            break;
+        case AV_PICTURE_TYPE_P:
+            ctx->api_fme.data.frame.ni_pict_type = PIC_TYPE_P;
+            break;
+        default:
+            ;
+        }
+    }
+    else if (ctx->buffered_fme.pict_type == AV_PICTURE_TYPE_I)
+    {
+      ctx->api_fme.data.frame.force_key_frame = 1;
+      ctx->api_fme.data.frame.ni_pict_type = PIC_TYPE_IDR;
+    }
+
+    av_log(avctx, AV_LOG_TRACE,
+           "xcoder_send_frame: #%" PRIu64 " ni_pict_type %d"
+           " forced_header_enable %d intraPeriod %d\n",
+           ctx->api_ctx.frame_num, ctx->api_fme.data.frame.ni_pict_type,
+           p_param->cfg_enc_params.forced_header_enable,
+           p_param->cfg_enc_params.intra_period);
+
+    // whether should send SEI with this frame
+    send_sei_with_idr = ni_should_send_sei_with_frame(
+        &ctx->api_ctx, ctx->api_fme.data.frame.ni_pict_type, p_param);
+
+    // prep for auxiliary data (various SEI, ROI) in encode frame, based on the
+    // data returned in decoded frame
+    ni_enc_prep_aux_data(&ctx->api_ctx, &ctx->api_fme.data.frame, &dec_frame,
+                         ctx->api_ctx.codec_format, send_sei_with_idr,
+                         mdcv_data, cll_data, cc_data, udu_data, hdrp_data);
+
+    side_data = av_frame_get_side_data(&ctx->buffered_fme,
+                                       AV_FRAME_DATA_NETINT_CUSTOM_SEI);
+    if (side_data && side_data->size > 0)
+    {
+      int64_t local_pts = ctx->buffered_fme.pts;
+      uint8_t *p_src_sei_data, *p_dst_sei_data;
+      int sei_size;
+      uint8_t sei_type;
+      int size;
+      ni_custom_sei_set_t *src_custom_sei_set, *dst_custom_sei_set;
+      ni_custom_sei_t *p_src_custom_sei, *p_dst_custom_sei;
+
+      // if one picture can be skipped, nienc will send that frame but will not
+      // receive packet, therefore it will skip the free in receive packet as
+      // well and cause memory leak. So check the last pkt_custom_sei_set has
+      // been released or not.
+      dst_custom_sei_set = ctx->api_ctx.pkt_custom_sei_set[local_pts % NI_FIFO_SZ];
+      if (dst_custom_sei_set)
+      {
+          free(dst_custom_sei_set);
+      }
+
+      /* copy the whole SEI data */
+      src_custom_sei_set = (ni_custom_sei_set_t *)side_data->data;
+      dst_custom_sei_set = malloc(sizeof(ni_custom_sei_set_t));
+      if (dst_custom_sei_set == NULL)
+      {
+          av_log(avctx, AV_LOG_ERROR, "failed to allocate memory for custom sei data\n");
+          ret = AVERROR(ENOMEM);
+          return ret;
+      }
+      memset(dst_custom_sei_set, 0, sizeof(ni_custom_sei_set_t));
+
+      /* fill sei data */
+      for (i = 0; i < src_custom_sei_set->count; i++)
+      {
+          int len;
+          p_src_custom_sei = &src_custom_sei_set->custom_sei[i];
+          sei_size = p_src_custom_sei->size;
+          sei_type = p_src_custom_sei->type;
+          p_src_sei_data = &p_src_custom_sei->data[0];
+
+          p_dst_custom_sei = &dst_custom_sei_set->custom_sei[i];
+          p_dst_sei_data = &p_dst_custom_sei->data[0];
+          size = 0;
+
+          // long start code
+          p_dst_sei_data[size++] = 0x00;
+          p_dst_sei_data[size++] = 0x00;
+          p_dst_sei_data[size++] = 0x00;
+          p_dst_sei_data[size++] = 0x01;
+
+          if (AV_CODEC_ID_H264 == avctx->codec_id)
+          {
+            p_dst_sei_data[size++] = 0x06;   //nal type: SEI
+          }
+          else
+          {
+            p_dst_sei_data[size++] = 0x4e;   //nal type: SEI
+            p_dst_sei_data[size++] = 0x01;
+          }
+
+          // SEI type
+          p_dst_sei_data[size++] = sei_type;
+
+          // original payload size
+          len = sei_size;
+          while (len >= 0)
+          {
+            p_dst_sei_data[size++] = len > 0xff ? 0xff : len;
+            len -= 0xff;
+          }
+
+          // payload data
+          for (j = 0; j < sei_size && size < NI_MAX_SEI_DATA - 1; j++)
+          {
+              if (j >= 2 && !p_dst_sei_data[size - 2] && !p_dst_sei_data[size - 1] && p_src_sei_data[j] <= 0x03)
+              {
+                  /* insert 0x3 as emulation_prevention_three_byte */
+                  p_dst_sei_data[size++] = 0x03;
+              }
+              p_dst_sei_data[size++] = p_src_sei_data[j];
+          }
+
+          if (j != sei_size)
+          {
+              av_log(avctx, AV_LOG_WARNING, "%s: sei RBSP size out of limit(%d), "
+                     "idx=%u, type=%u, size=%d, custom_sei_loc=%d.\n", __func__,
+                     NI_MAX_SEI_DATA, i, sei_type, sei_size, p_src_custom_sei->location);
+              free(dst_custom_sei_set);
+              break;
+          }
+
+          // trailing byte
+          p_dst_sei_data[size++] = 0x80;
+
+          p_dst_custom_sei->size = size;
+          p_dst_custom_sei->type = sei_type;
+          p_dst_custom_sei->location = p_src_custom_sei->location;
+          av_log(avctx, AV_LOG_TRACE, "%s: custom sei idx %d type %u len %d loc %d.\n",
+                 __func__, i, sei_type, size, p_dst_custom_sei->location);
+      }
+
+      dst_custom_sei_set->count = src_custom_sei_set->count;
+      ctx->api_ctx.pkt_custom_sei_set[local_pts % NI_FIFO_SZ] = dst_custom_sei_set;
+      av_log(avctx, AV_LOG_TRACE, "%s: sei number %d pts %" PRId64 ".\n",
+             __func__, dst_custom_sei_set->count, local_pts);
+    }
+
+    if (ctx->api_fme.data.frame.sei_total_len > NI_ENC_MAX_SEI_BUF_SIZE)
+    {
+      av_log(avctx, AV_LOG_ERROR, "xcoder_send_frame: sei total length %u exceeds maximum sei size %u.\n",
+             ctx->api_fme.data.frame.sei_total_len, NI_ENC_MAX_SEI_BUF_SIZE);
+      ret = AVERROR(EINVAL);
+      return ret;
+    }
+
+    ctx->api_fme.data.frame.extra_data_len += ctx->api_fme.data.frame.sei_total_len;
+
+    // data layout requirement: leave space for reconfig data if at least one of
+    // reconfig, SEI or ROI is present
+    // Note: ROI is present when enabled, so use encode config flag instead of
+    //       frame's roi_len as it can be 0 indicating a 0'd ROI map setting !
+    if (ctx->api_fme.data.frame.reconf_len ||
+        ctx->api_fme.data.frame.sei_total_len ||
+        p_param->cfg_enc_params.roi_enable) {
+        ctx->api_fme.data.frame.extra_data_len +=
+            sizeof(ni_encoder_change_params_t);
+    }
+
+    ctx->api_fme.data.frame.pts = ctx->buffered_fme.pts;
+    ctx->api_fme.data.frame.dts = ctx->buffered_fme.pkt_dts;
+
+    ctx->api_fme.data.frame.video_width = avctx->width;
+    ctx->api_fme.data.frame.video_height = avctx->height;
+
+    ishwframe = ctx->buffered_fme.format == AV_PIX_FMT_NI_QUAD;
+    if (ctx->api_ctx.auto_dl_handle != 0 || (avctx->height < NI_MIN_HEIGHT) ||
+        (avctx->width < NI_MIN_WIDTH)) {
+        format_in_use = avctx->sw_pix_fmt;
+        ctx->api_ctx.hw_action = 0;
+        ishwframe     = 0;
+    }
+    isnv12frame = (format_in_use == AV_PIX_FMT_NV12 || format_in_use == AV_PIX_FMT_P010LE);
+
+    if (ishwframe)
+    {
+      ret = sizeof(niFrameSurface1_t);
+    }
+    else
+    {
+      ret = av_image_get_buffer_size(format_in_use,
+                                     ctx->buffered_fme.width, ctx->buffered_fme.height, 1);
+    }
+
+#if FF_API_PKT_PTS
+    // NOLINTNEXTLINE(clang-diagnostic-deprecated-declarations)
+    av_log(avctx, AV_LOG_TRACE, "xcoder_send_frame: pts=%" PRId64 ", pkt_dts=%" PRId64 ", pkt_pts=%" PRId64 "\n", ctx->buffered_fme.pts, ctx->buffered_fme.pkt_dts, ctx->buffered_fme.pkt_pts);
+#endif
+    av_log(avctx, AV_LOG_TRACE, "xcoder_send_frame: frame->format=%d, frame->width=%d, frame->height=%d, frame->pict_type=%d, size=%d\n", format_in_use, ctx->buffered_fme.width, ctx->buffered_fme.height, ctx->buffered_fme.pict_type, ret);
+    if (ret < 0)
+    {
+      return ret;
+    }
+
+    int dst_stride[NI_MAX_NUM_DATA_POINTERS]     = {0};
+    int height_aligned[NI_MAX_NUM_DATA_POINTERS] = {0};
+    int src_height[NI_MAX_NUM_DATA_POINTERS]     = {0};
+
+    if (QUADRA)
+    {
+        src_height[0] = ctx->buffered_fme.height;
+        src_height[1] = ctx->buffered_fme.height / 2;
+        src_height[2] = (isnv12frame) ? 0 : (ctx->buffered_fme.height / 2);
+
+        ni_get_hw_yuv420p_dim(ctx->buffered_fme.width,
+                              ctx->buffered_fme.height,
+                              ctx->api_ctx.bit_depth_factor, isnv12frame,
+                              dst_stride, height_aligned);
+
+        av_log(avctx, AV_LOG_TRACE,
+               "xcoder_send_frame frame->width %d "
+               "ctx->api_ctx.bit_depth_factor %d dst_stride[0/1/2] %d/%d/%d\n",
+               ctx->buffered_fme.width, ctx->api_ctx.bit_depth_factor,
+               dst_stride[0], dst_stride[1], dst_stride[2]);
+
+        if (alignment_2pass_wa && !ishwframe) {
+            if (isnv12frame) {
+                // for 2-pass encode output mismatch WA, need to extend (and
+                // pad) CbCr plane height, because 1st pass assume input 32
+                // align
+                height_aligned[1] = FFALIGN(height_aligned[0], 32) / 2;
+            } else {
+                // for 2-pass encode output mismatch WA, need to extend (and
+                // pad) Cr plane height, because 1st pass assume input 32 align
+                height_aligned[2] = FFALIGN(height_aligned[0], 32) / 2;
+            }
+        }
+    } else {
+        dst_stride[0] = ((frame->width + 31) / 32) * 32;
+        if (dst_stride[0] < NI_MIN_WIDTH) {
+            dst_stride[0] = NI_MIN_WIDTH;
+        }
+        dst_stride[0] *= ctx->api_ctx.bit_depth_factor;
+        dst_stride[1] = dst_stride[2] = dst_stride[0] / 2;
+
+        height_aligned[0] = ((frame->height + 7) / 8) * 8;
+        if (avctx->codec_id == AV_CODEC_ID_H264) {
+            height_aligned[0] = ((frame->height + 15) / 16) * 16;
+        }
+        if (height_aligned[0] < NI_MIN_HEIGHT) {
+            height_aligned[0] = NI_MIN_HEIGHT;
+        }
+        height_aligned[1] = height_aligned[2] = height_aligned[0] / 2;
+    }
+
+    // alignment(16) extra padding for H.264 encoding
+    if (ishwframe) {
+        uint8_t *dsthw;
+        const uint8_t *srchw;
+
+        ni_frame_buffer_alloc_hwenc(
+            &(ctx->api_fme.data.frame), ctx->buffered_fme.width,
+            ctx->buffered_fme.height,
+            (int)ctx->api_fme.data.frame.extra_data_len);
+        if (!ctx->api_fme.data.frame.p_data[3]) {
+            return AVERROR(ENOMEM);
+        }
+        dsthw       = ctx->api_fme.data.frame.p_data[3];
+        srchw = (const uint8_t *)ctx->buffered_fme.data[3];
+        av_log(avctx, AV_LOG_TRACE, "dst=%p src=%p len=%d\n", dsthw, srchw,
+               ctx->api_fme.data.frame.data_len[3]);
+        memcpy(dsthw, srchw, ctx->api_fme.data.frame.data_len[3]);
+        av_log(avctx, AV_LOG_TRACE,
+               "ctx->buffered_fme.data[3] %p memcpy to %p\n",
+               ctx->buffered_fme.data[3], dsthw);
+    } else // traditional yuv transfer
+    {
+       /* if (isnv12frame) {
+            ni_frame_buffer_alloc_nv(
+                &(ctx->api_fme.data.frame), ctx->buffered_fme.width,
+                height_aligned[0], dst_stride,
+                (int)ctx->api_fme.data.frame.extra_data_len,
+                alignment_2pass_wa);
+        } else {
+            ni_encoder_frame_buffer_alloc(
+                &(ctx->api_fme.data.frame), ctx->buffered_fme.width,
+                height_aligned[0], dst_stride,
+                (avctx->codec_id == AV_CODEC_ID_H264),
+                (int)ctx->api_fme.data.frame.extra_data_len,
+                alignment_2pass_wa);
+        }*/
+
+        av_log(avctx, AV_LOG_TRACE, "[0] %p stride[0] %u height %u data[1] %p data[3] %p\n",
+            ctx->buffered_fme.data[0], dst_stride[0], ctx->buffered_fme.height, ctx->buffered_fme.data[1],
+            ctx->buffered_fme.data[3]);
+        if (ctx->buffered_fme.data[3] > 0 &&
+            ((uintptr_t)ctx->buffered_fme.data[0] % NI_MEM_PAGE_ALIGNMENT) == 0 &&
+            dst_stride[0] == ctx->buffered_fme.linesize[0] && //stride aligned
+            dst_stride[1] == ctx->buffered_fme.linesize[1] && //stride aligned
+            ctx->buffered_fme.height % 2 == 0 && //even height
+            ctx->buffered_fme.data[0] + dst_stride[0] * ctx->buffered_fme.height == ctx->buffered_fme.data[1] &&
+            (isnv12frame || //contiguous?
+            ctx->buffered_fme.data[1] + dst_stride[1] * ctx->buffered_fme.height / 2 == ctx->buffered_fme.data[2]))
+        {
+            need_to_copy = 0;
+            ni_encoder_sw_frame_buffer_alloc(
+                !isnv12frame, &(ctx->api_fme.data.frame), ctx->buffered_fme.width,
+                height_aligned[0], dst_stride, (avctx->codec_id == AV_CODEC_ID_H264),
+                -1, alignment_2pass_wa); //no allocation required
+            ctx->api_fme.data.frame.p_buffer = ctx->buffered_fme.data[0];
+            ctx->api_fme.data.frame.p_data[0] = ctx->buffered_fme.data[0];
+            ctx->api_fme.data.frame.p_data[1] = ctx->buffered_fme.data[1];
+            if (!isnv12frame)
+                ctx->api_fme.data.frame.p_data[2] = ctx->buffered_fme.data[2];
+        }
+        else
+        {
+            ni_encoder_sw_frame_buffer_alloc(
+                !isnv12frame, &(ctx->api_fme.data.frame), ctx->buffered_fme.width,
+                height_aligned[0], dst_stride, (avctx->codec_id == AV_CODEC_ID_H264),
+                (int)ctx->api_fme.data.frame.extra_data_len, alignment_2pass_wa);
+        }
+        av_log(avctx, AV_LOG_TRACE, "%p need_to_copy %d! pts = %ld\n", ctx->api_fme.data.frame.p_buffer, need_to_copy, ctx->buffered_fme.pts);
+       if (!ctx->api_fme.data.frame.p_data[0]) {
+           return AVERROR(ENOMEM);
+      }
+
+      // if this is indeed sw frame, do the YUV data layout, otherwise may need
+      // to do frame download
+      if (ctx->buffered_fme.format != AV_PIX_FMT_NI_QUAD) {
+          av_log(
+              avctx, AV_LOG_TRACE,
+              "xcoder_send_frame: fme.data_len[0]=%d, "
+              "buf_fme->linesize=%d/%d/%d, dst alloc linesize = %d/%d/%d, "
+              "src height = %d/%d/%d, dst height aligned = %d/%d/%d, "
+              "force_key_frame=%d, extra_data_len=%d sei_size=%d "
+              "(hdr_content_light_level %u hdr_mastering_display_color_vol %u "
+              "hdr10+ %u cc %u udu %u prefC %u) roi_size=%u reconf_size=%u "
+              "force_pic_qp=%u "
+              "use_cur_src_as_long_term_pic %u use_long_term_ref %u\n",
+              ctx->api_fme.data.frame.data_len[0],
+              ctx->buffered_fme.linesize[0], ctx->buffered_fme.linesize[1],
+              ctx->buffered_fme.linesize[2], dst_stride[0], dst_stride[1],
+              dst_stride[2], src_height[0], src_height[1], src_height[2],
+              height_aligned[0], height_aligned[1], height_aligned[2],
+              ctx->api_fme.data.frame.force_key_frame,
+              ctx->api_fme.data.frame.extra_data_len,
+              ctx->api_fme.data.frame.sei_total_len,
+              ctx->api_fme.data.frame.sei_hdr_content_light_level_info_len,
+              ctx->api_fme.data.frame.sei_hdr_mastering_display_color_vol_len,
+              ctx->api_fme.data.frame.sei_hdr_plus_len,
+              ctx->api_fme.data.frame.sei_cc_len,
+              ctx->api_fme.data.frame.sei_user_data_unreg_len,
+              ctx->api_fme.data.frame.preferred_characteristics_data_len,
+              (p_param->cfg_enc_params.roi_enable ? ctx->api_ctx.roi_len : 0),
+              ctx->api_fme.data.frame.reconf_len,
+              ctx->api_fme.data.frame.force_pic_qp,
+              ctx->api_fme.data.frame.use_cur_src_as_long_term_pic,
+              ctx->api_fme.data.frame.use_long_term_ref);
+
+          // YUV part of the encoder input data layout
+          if (need_to_copy)
+          {
+              ni_copy_hw_yuv420p(
+                  (uint8_t **)(ctx->api_fme.data.frame.p_data),
+                  ctx->buffered_fme.data, ctx->buffered_fme.width,
+                  ctx->buffered_fme.height, ctx->api_ctx.bit_depth_factor,
+                  isnv12frame, p_param->cfg_enc_params.conf_win_right, dst_stride,
+                  height_aligned, ctx->buffered_fme.linesize, src_height);
+          }
+      } else {
+          ni_session_data_io_t *p_session_data;
+          ni_session_data_io_t niframe;
+          niFrameSurface1_t *src_surf;
+
+          av_log(avctx, AV_LOG_DEBUG,
+                 "xcoder_send_frame:Autodownload to be run: hdl: %d w: %d h: %d\n",
+                 ctx->api_ctx.auto_dl_handle, avctx->width, avctx->height);
+          avhwf_ctx =
+              (AVHWFramesContext *)ctx->buffered_fme.hw_frames_ctx->data;
+          nif_src_ctx = avhwf_ctx->internal->priv;
+
+          src_surf = (niFrameSurface1_t *)ctx->buffered_fme.data[3];
+
+          if (avctx->height < NI_MIN_HEIGHT || avctx->width < NI_MIN_WIDTH) {
+              int bit_depth;
+              int is_planar;
+
+              p_session_data = &niframe;
+              memset(&niframe, 0, sizeof(niframe));
+              bit_depth = ((avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P10LE) ||
+                           (avctx->sw_pix_fmt == AV_PIX_FMT_P010LE))
+                              ? 2
+                              : 1;
+              is_planar = (avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P) ||
+                          (avctx->sw_pix_fmt == AV_PIX_FMT_YUV420P10LE);
+
+              /* Allocate a minimal frame */
+              ni_frame_buffer_alloc(&niframe.data.frame, avctx->width,
+                                    avctx->height, 0, /* alignment */
+                                    1,                /* metadata */
+                                    bit_depth, 0,     /* hw_frame_count */
+                                    is_planar);
+          } else {
+              p_session_data = &(ctx->api_fme);
+          }
+
+          nif_src_ctx->api_ctx.is_auto_dl = true;
+          ret = ni_device_session_hwdl(&nif_src_ctx->api_ctx, p_session_data,
+                                       src_surf);
+          ishwframe = false;
+          if (ret <= 0) {
+              av_log(avctx, AV_LOG_ERROR,
+                     "nienc.c:ni_hwdl_frame() failed to retrieve frame\n");
+              return AVERROR_EXTERNAL;
+          }
+
+          if ((avctx->height < NI_MIN_HEIGHT) ||
+              (avctx->width < NI_MIN_WIDTH)) {
+              expand_ni_frame(avctx, &ctx->api_fme.data.frame,
+                              &p_session_data->data.frame, dst_stride,
+                              avctx->width, avctx->height, avctx->sw_pix_fmt);
+
+              ni_frame_buffer_free(&niframe.data.frame);
+          }
+      }
+    } // end if hwframe else
+
+    // auxiliary data part of the encoder input data layout
+    ni_enc_copy_aux_data(&ctx->api_ctx, &ctx->api_fme.data.frame, &dec_frame,
+                         ctx->api_ctx.codec_format, mdcv_data, cll_data,
+                         cc_data, udu_data, hdrp_data, ishwframe, isnv12frame);
+
+    ni_frame_buffer_free(&dec_frame);
+
+    // end of encode input frame data layout
+
+    } // end non seq change
+#ifdef MULTI_THREAD
+  if (ctx->encoder_flushing)
+  {
+    sent = ni_device_session_write(&ctx->api_ctx, &ctx->api_fme, NI_DEVICE_TYPE_ENCODER);
+
+    av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame encoder_flushing: size %d sent to xcoder\n", sent);
+
+    if (NI_RETCODE_ERROR_RESOURCE_UNAVAILABLE == sent)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame(): Sequence Change in progress, returning EAGAIN\n");
+      ret = AVERROR(EAGAIN);
+      return ret;
+    }
+
+    if (sent == -1)
+    {
+      ret = AVERROR(EAGAIN);
+    }
+    else
+    {
+      if (frame && ishwframe)
+      {
+          av_log(avctx, AV_LOG_TRACE, "AVframe_index = %d at head %d\n",
+                 ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+          av_frame_ref(ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]], frame);
+          av_log(avctx, AV_LOG_TRACE,
+                 "AVframe_index = %d popped from head %d\n",
+                 ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+          if (deq_free_frames(ctx) != 0) {
+              ret = AVERROR_EXTERNAL;
+              return ret;
+          }
+        //av_frame_ref(ctx->sframe_pool[((niFrameSurface1_t*)((uint8_t*)frame->data[3]))->ui16FrameIdx], frame);
+      }
+      // pushing input pts in circular FIFO
+      ctx->api_ctx.enc_pts_list[ctx->api_ctx.enc_pts_w_idx % NI_FIFO_SZ] = ctx->api_fme.data.frame.pts;
+      ctx->api_ctx.enc_pts_w_idx ++;
+
+      ret = 0;
+    }
+  }
+  //else if (ctx->buffered_fme && ishwframe)
+  else if (!ctx->eos_fme_received && ishwframe)
+  {
+    sent = ni_device_session_write(&ctx->api_ctx, &ctx->api_fme, NI_DEVICE_TYPE_ENCODER);
+    //ctx->sframe_pool[((niFrameSurface1_t*)((uint8_t*)frame->data[3]))->ui16FrameIdx] = av_buffer_ref(frame);
+    av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame: size %d sent to xcoder\n", sent);
+
+    if (NI_RETCODE_ERROR_RESOURCE_UNAVAILABLE == sent)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame(): Sequence Change in progress, returning EAGAIN\n");
+      ret = AVERROR(EAGAIN);
+      return ret;
+    }
+
+    if (sent == -1)
+    {
+      ret = AVERROR(EAGAIN);
+    }
+    else
+    {
+        av_log(avctx, AV_LOG_TRACE, "AVframe_index = %d at head %d\n",
+               ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+        av_frame_ref(ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]],
+                     ctx->buffered_fme);
+        av_log(avctx, AV_LOG_DEBUG, "AVframe_index = %d popped from head %d\n",
+               ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+        if (deq_free_frames(ctx) != 0) {
+            ret = AVERROR_EXTERNAL;
+            return ret;
+        }
+      //av_frame_ref(ctx->sframe_pool[((niFrameSurface1_t*)((uint8_t*)frame->data[3]))->ui16FrameIdx], frame);
+      ret = 0;
+    }
+  }
+  else
+  {
+    uint8_t id = find_session_idx(NI_INVALID_SESSION_ID);
+    if (id < NI_MAX_NUM_SESSIONS)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame current: find_session_idx %d\n", id);
+      av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame current: ctx->api_ctx.session_id %d\n", ctx->api_ctx.session_id);
+      write_thread_args[id].session_id = ctx->api_ctx.session_id;
+      write_thread_args[id].ctx = ctx;
+      av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame: session_id %d\n", write_thread_args[id].session_id);
+      av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame: ctx %p\n", write_thread_args[id].ctx);
+      //saved the current session id
+      if ((ret = pthread_create(&write_thread_args[id].thread, NULL, write_frame_thread, (void *)&write_thread_args[id])))
+      {
+        av_log(avctx, AV_LOG_ERROR, "pthread_create failed: %s. Try to increase `ulimit -v` or decrease `ulimit -s`.\n", strerror(ret));
+        return AVERROR(ret);
+      }
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_ERROR,"no more session map for this id %d\n",ctx->api_ctx.session_id);
+      return AVERROR_EXTERNAL;
+    }
+  }
+
+#else
+  sent = ni_device_session_write(&ctx->api_ctx, &ctx->api_fme, NI_DEVICE_TYPE_ENCODER);
+  av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame: size %d sent to xcoder\n", sent);
+
+   // device session write does not return resource unavail, may add later when support strict timeout
+  /*
+  if (NI_RETCODE_ERROR_RESOURCE_UNAVAILABLE == sent)
+  {
+    //av_log(avctx, AV_LOG_ERROR, "xcoder_send_frame(): failure sent (%d) , "
+    //       "returning EIO\n", sent);
+    //ret = AVERROR(EIO);
+
+    // if rejected due to sequence change in progress, revert resolution
+    // setting and will do it again next time.
+    if (ctx->api_fme.data.frame.start_of_stream &&
+        (avctx->width != orig_avctx_width ||
+         avctx->height != orig_avctx_height))
+    {
+      avctx->width = orig_avctx_width;
+      avctx->height = orig_avctx_height;
+    }
+    av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame(): Sequence Change in progress, returning EAGAIN\n");
+    ret = AVERROR(EAGAIN);
+    return ret;
+  }
+  */
+
+  // return EIO at error
+  if (NI_RETCODE_ERROR_VPU_RECOVERY == sent)
+  {
+    sent = xcoder_encode_reset(avctx);
+    if (sent < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "xcoder_send_frame(): VPU recovery failed:%d, returning EIO\n", sent);
+      ret = AVERROR(EIO);
+    }
+  }
+  else if (sent < 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "xcoder_send_frame(): failure sent (%d) , "
+           "returning EIO\n", sent);
+    ret = AVERROR(EIO);
+
+    // if rejected due to sequence change in progress, revert resolution
+    // setting and will do it again next time.
+    // TODO: don't see how AvCodec width / height could have changed
+    if (ctx->api_fme.data.frame.start_of_stream &&
+        (avctx->width != orig_avctx_width ||
+         avctx->height != orig_avctx_height))
+    {
+      avctx->width = orig_avctx_width;
+      avctx->height = orig_avctx_height;
+    }
+    return ret;
+  }
+  else
+  {
+    /*
+    if (sent < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "xcoder_send_frame(): failure sent (%d) , "
+             "returning EIO\n", sent);
+      ret = AVERROR(EIO);
+
+      // if rejected due to sequence change in progress, revert resolution
+      // setting and will do it again next time.
+      if (ctx->api_fme.data.frame.start_of_stream &&
+          (avctx->width != orig_avctx_width ||
+           avctx->height != orig_avctx_height))
+      {
+        avctx->width = orig_avctx_width;
+        avctx->height = orig_avctx_height;
+      }
+      return ret;
+    }
+    else
+    */
+    av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame(): sent (%d)\n", sent);
+    if (sent == 0)
+    {
+      // TODO: don't see how AvCodec width / height could have changed
+      // case of sequence change in progress
+      if (ctx->api_fme.data.frame.start_of_stream &&
+          (avctx->width != orig_avctx_width ||
+           avctx->height != orig_avctx_height))
+      {
+        avctx->width = orig_avctx_width;
+        avctx->height = orig_avctx_height;
+      }
+
+      // when buffer_full, drop the frame and return EAGAIN if in strict timeout
+      // mode, otherwise buffer the frame and it is to be sent out using encode2
+      // API: queue the frame only if not done so yet, i.e. queue is empty
+      // *and* it's a valid frame. ToWatch: what are other rc cases ?
+      if (ctx->api_ctx.status == NI_RETCODE_NVME_SC_WRITE_BUFFER_FULL)
+      {
+        ishwframe = ctx->buffered_fme.format == AV_PIX_FMT_NI_QUAD;
+        if (ishwframe)
+        {
+          // Do not queue frames to avoid FFmpeg stuck when multiple HW frames are queued up in nienc, causing decoder unable to acquire buffer, which led to FFmpeg stuck
+          av_log(avctx, AV_LOG_ERROR, "xcoder_send_frame(): device WRITE_BUFFER_FULL cause frame drop! (approx. Frame num #%" PRIu64 "\n", ctx->api_ctx.frame_num);
+          av_frame_unref(&ctx->buffered_fme);
+          ret = 0;
+        }
+        else
+        {
+          // TODO: enable when strict timeout mode is added
+          /*
+          if (ctx->api_param.strict_timeout_mode)
+          {
+            av_log(avctx, AV_LOG_ERROR, "xcoder_send_frame(): Error Strict timeout period exceeded, returning EAGAIN\n");
+            ret = AVERROR(EAGAIN);
+          }
+          else
+          */
+          {
+            av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame(): Write buffer full, returning 1\n");
+            ret = 1;
+
+            if (frame && is_input_fifo_empty(ctx))
+            {
+              enqueue_frame(avctx, frame);
+            }
+          }
+        }
+      }
+    }
+    else
+    {
+      //if (ctx->buffered_fme && ishwframe)//may or may not break hwframe mode, to be tested
+      ishwframe = (ctx->buffered_fme.format == AV_PIX_FMT_NI_QUAD) &&
+                  (ctx->api_ctx.auto_dl_handle == 0) &&
+                  (avctx->height >= NI_MIN_HEIGHT) &&
+                  (avctx->width >= NI_MIN_WIDTH);
+
+      if (!ctx->eos_fme_received && ishwframe)
+      {
+          av_log(avctx, AV_LOG_TRACE, "AVframe_index = %d at head %d\n",
+                 ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+          av_frame_ref(
+              ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]],
+              &ctx->buffered_fme);
+          av_log(avctx, AV_LOG_TRACE,
+                 "AVframe_index = %d popped from free head %d\n",
+                 ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+          av_log(avctx, AV_LOG_TRACE,
+                 "ctx->buffered_fme.data[3] %p sframe_pool[%d]->data[3] %p\n",
+                 ctx->buffered_fme.data[3],
+                 ctx->aFree_Avframes_list[ctx->freeHead],
+                 ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]
+                     ->data[3]);
+          if (ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]
+                  ->data[3]) {
+              av_log(avctx, AV_LOG_DEBUG,
+                     "nienc.c sframe_pool[%d] trace ui16FrameIdx = [%u] sent\n",
+                     ctx->aFree_Avframes_list[ctx->freeHead],
+                     ((niFrameSurface1_t
+                           *)((uint8_t *)ctx
+                                  ->sframe_pool
+                                      [ctx->aFree_Avframes_list[ctx->freeHead]]
+                                  ->data[3]))
+                         ->ui16FrameIdx);
+              av_log(
+                  avctx, AV_LOG_TRACE,
+                  "xcoder_send_frame: after ref sframe_pool, hw frame "
+                  "av_buffer_get_ref_count=%d, data[3]=%p\n",
+                  av_buffer_get_ref_count(
+                      ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]
+                          ->buf[0]),
+                  ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]
+                      ->data[3]);
+          }
+        if (deq_free_frames(ctx) != 0)
+        {
+          ret = AVERROR_EXTERNAL;
+          return ret;
+        }
+      //av_frame_ref(ctx->sframe_pool[((niFrameSurface1_t*)((uint8_t*)frame->data[3]))->ui16FrameIdx], frame);
+      }
+
+      // only if it's NOT sequence change flushing (in which case only the eos
+      // was sent and not the first sc pkt) AND
+      // only after successful sending will it be removed from fifo
+      // TODO: if ni_device_session_write EOS, returned sent would have been 0, and following condition is always true
+      if (SESSION_RUN_STATE_SEQ_CHANGE_DRAINING !=
+          ctx->api_ctx.session_run_state)
+      {
+        if (! is_input_fifo_empty(ctx))
+        {
+          av_fifo_drain(ctx->fme_fifo, sizeof(AVFrame));
+          av_log(avctx, AV_LOG_DEBUG, "fme popped, fifo size: %lu\n",
+                 av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame));
+        }
+        av_frame_unref(&ctx->buffered_fme);
+        ishwframe = (ctx->buffered_fme.format == AV_PIX_FMT_NI_QUAD) &&
+                    (ctx->api_ctx.auto_dl_handle == 0);
+        if (ishwframe)
+        {
+            if (ctx->buffered_fme.buf[0])
+                av_log(avctx, AV_LOG_TRACE, "xcoder_send_frame: after unref buffered_fme, hw frame av_buffer_get_ref_count=%d\n", av_buffer_get_ref_count(ctx->buffered_fme.buf[0]));
+            else
+                av_log(avctx, AV_LOG_TRACE, "xcoder_send_frame: after unref buffered_fme, hw frame av_buffer_get_ref_count=0 (buf[0] is NULL)\n");
+        }
+      }
+      else
+      {
+        av_log(avctx, AV_LOG_TRACE, "XCoder frame(eos) sent, sequence changing!"
+               " NO fifo pop !\n");
+      }
+
+      // pushing input pts in circular FIFO
+      ctx->api_ctx.enc_pts_list[ctx->api_ctx.enc_pts_w_idx % NI_FIFO_SZ] = ctx->api_fme.data.frame.pts;
+      ctx->api_ctx.enc_pts_w_idx ++;
+
+      // have another check before return: if no more frames in fifo to send and
+      // we've got eos (NULL) frame from upper stream, flag for flushing
+      if (ctx->eos_fme_received && is_input_fifo_empty(ctx))
+      {
+        av_log(avctx, AV_LOG_DEBUG, "Upper stream EOS frame received, fifo "
+               "empty, start flushing ..\n");
+        ctx->encoder_flushing = 1;
+      }
+
+      ret = 0;
+    }
+    // pushing input pts in circular FIFO
+    // ctx->api_ctx.enc_pts_list[ctx->api_ctx.enc_pts_w_idx % NI_FIFO_SZ] =
+    // ctx->api_fme.data.frame.pts; ctx->api_ctx.enc_pts_w_idx ++;
+    // ret = 0;
+  }
+#endif
+  if (ctx->encoder_flushing)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "xcoder_send_frame flushing ..\n");
+    ret = ni_device_session_flush(&ctx->api_ctx, NI_DEVICE_TYPE_ENCODER);
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder send frame return %d \n", ret);
+  return ret;
+}
+
+static int xcoder_encode_reinit(AVCodecContext *avctx)
+{
+  int ret = 0;
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  bool ishwframe;
+  ni_device_handle_t device_handle = ctx->api_ctx.device_handle;
+  ni_device_handle_t blk_io_handle = ctx->api_ctx.blk_io_handle;
+  int hw_id = ctx->api_ctx.hw_id;
+  int bit_depth = 1;
+  int pix_fmt = AV_PIX_FMT_YUV420P;
+  int stride, ori_stride;
+  bool bIsSmallPicture = false;
+  AVFrame temp_frame;
+
+  // re-init avctx's resolution to the changed one that is
+  // stored in the first frame of the fifo
+  av_fifo_generic_peek(ctx->fme_fifo, &temp_frame , sizeof(AVFrame), NULL);
+  temp_frame.extended_data = temp_frame.data;
+  av_log(avctx, AV_LOG_INFO, "xcoder_receive_packet resolution "
+         "changing %dx%d -> %dx%d "
+         "format %d -> %d\n",
+         avctx->width, avctx->height,
+         temp_frame.width, temp_frame.height,
+         avctx->pix_fmt, temp_frame.format);
+
+  avctx->width = temp_frame.width;
+  avctx->height = temp_frame.height;
+  avctx->pix_fmt = temp_frame.format;
+  ishwframe = temp_frame.format == AV_PIX_FMT_NI_QUAD;
+
+  if (ishwframe)
+  {
+    bit_depth = (uint8_t)((niFrameSurface1_t*)((uint8_t*)temp_frame.data[3]))->bit_depth;
+  //int8_t bit_depth; //1 ==8bit per pixel, 2 ==10
+  //int8_t encoding_type; //planar
+    av_log(avctx, AV_LOG_INFO, "xcoder_receive_packet hw frame bit depth "
+           "changing %d -> %d\n",
+           ctx->api_ctx.bit_depth_factor, bit_depth);
+
+    if (bit_depth != ctx->api_ctx.bit_depth_factor)
+    {
+      switch (avctx->sw_pix_fmt)
+      {
+        case AV_PIX_FMT_YUV420P:
+          if (bit_depth == 2)
+          {
+            avctx->sw_pix_fmt = AV_PIX_FMT_YUV420P10LE;
+          }
+          break;
+        case AV_PIX_FMT_YUV420P10LE:
+        case AV_PIX_FMT_YUV420P10BE:
+          if (bit_depth == 1)
+          {
+            avctx->sw_pix_fmt = AV_PIX_FMT_YUV420P;
+          }
+          break;
+        case AV_PIX_FMT_NV12:
+          if (bit_depth == 2)
+          {
+            avctx->sw_pix_fmt = AV_PIX_FMT_P010LE;
+          }
+          break;
+        case AV_PIX_FMT_P010LE:
+          if (bit_depth == 1)
+          {
+            avctx->sw_pix_fmt = AV_PIX_FMT_NV12;
+          }
+          break;
+        default:
+          break;
+      }
+    }
+    pix_fmt = avctx->sw_pix_fmt;
+  }
+  else
+  {
+      switch (temp_frame.format)
+      {
+        case AV_PIX_FMT_YUV420P:
+        case AV_PIX_FMT_NV12:
+          bit_depth = 1;
+          break;
+        case AV_PIX_FMT_YUV420P10LE:
+        case AV_PIX_FMT_YUV420P10BE:
+        case AV_PIX_FMT_P010LE:
+          bit_depth = 2;
+          break;
+        default:
+          break;
+      }
+      pix_fmt = temp_frame.format;
+  }
+
+  ctx->eos_fme_received = 0;
+  ctx->encoder_eof = 0;
+  ctx->encoder_flushing = 0;
+  //ctx->started = 0;
+  ctx->firstPktArrived = 0;
+  ctx->spsPpsArrived = 0;
+  ctx->spsPpsHdrLen = 0;
+  if (ctx->p_spsPpsHdr) {
+      free(ctx->p_spsPpsHdr);
+      ctx->p_spsPpsHdr = NULL;
+  }
+
+  stride = FFALIGN(temp_frame.width, 128);
+  ori_stride = FFALIGN(ctx->api_ctx.ori_width, 128);
+  if (ctx->api_param.cfg_enc_params.lookAheadDepth) {
+    av_log(avctx, AV_LOG_DEBUG, "xcoder_encode_reinit 2-pass "
+           "lookaheadDepth %d\n",
+           ctx->api_param.cfg_enc_params.lookAheadDepth);
+      if ((temp_frame.width < 272) ||
+         (temp_frame.height < 256)) {
+        bIsSmallPicture = true;
+      }
+  }
+  else {
+      if ((temp_frame.width < NI_MIN_WIDTH) ||
+         (temp_frame.height < NI_MIN_HEIGHT)) {
+        bIsSmallPicture = true;
+      }
+  }
+
+  if (ctx->api_param.cfg_enc_params.multicoreJointMode) {
+    av_log(avctx, AV_LOG_DEBUG, "xcoder_encode_reinit multicore "
+           "joint mode\n");
+      if ((temp_frame.width < 256) ||
+         (temp_frame.height < 256)) {
+        bIsSmallPicture = true;
+      }
+  }
+
+  // fast sequence change without close / open only if new resolution < original resolution
+  if ((ori_stride*ctx->api_ctx.ori_height < stride*temp_frame.height) ||
+      //(ctx->api_ctx.ori_bit_depth_factor < bit_depth) ||
+      (ctx->api_ctx.ori_pix_fmt != pix_fmt) ||
+      bIsSmallPicture ||
+      (avctx->codec_id == AV_CODEC_ID_MJPEG)) {
+    xcoder_encode_close(avctx);
+    ret = xcoder_encode_init(avctx);
+  }
+  else {
+    if (avctx->codec_id == AV_CODEC_ID_AV1) {
+
+        // AV1 8x8 alignment HW limitation is now worked around by FW cropping input resolution
+        if (temp_frame.width % NI_PARAM_AV1_ALIGN_WIDTH_HEIGHT)
+            av_log(avctx, AV_LOG_ERROR,
+                   "resolution change: AV1 Picture Width not aligned to %d - picture will be cropped\n",
+                   NI_PARAM_AV1_ALIGN_WIDTH_HEIGHT);
+
+        if (temp_frame.height % NI_PARAM_AV1_ALIGN_WIDTH_HEIGHT)
+            av_log(avctx, AV_LOG_ERROR,
+                   "resolution change: AV1 Picture Height not aligned to %d - picture will be cropped\n",
+                   NI_PARAM_AV1_ALIGN_WIDTH_HEIGHT);
+    }
+    ret = xcoder_encode_sequence_change(avctx, temp_frame.width, temp_frame.height, bit_depth);
+  }
+
+  // keep device handle(s) open during sequence change to fix mem bin buffer not recycled
+  ctx->api_ctx.device_handle  = device_handle;
+  ctx->api_ctx.blk_io_handle = blk_io_handle;
+  ctx->api_ctx.hw_id = hw_id;
+  ctx->api_ctx.session_run_state = SESSION_RUN_STATE_SEQ_CHANGE_OPENING; // this state is referenced when sending first frame after sequence change
+
+  return ret;
+}
+
+int xcoder_receive_packet(AVCodecContext *avctx, AVPacket *pkt)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  int i, ret = 0;
+  int recv;
+  AVFrame *frame = NULL;
+  ni_packet_t *xpkt = &ctx->api_pkt.data.packet;
+  bool av1_output_frame = 0;
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder receive packet\n");
+
+  if (ctx->encoder_eof)
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "xcoder_receive_packet: EOS\n");
+    return AVERROR_EOF;
+  }
+
+  if (ni_packet_buffer_alloc(xpkt, NI_MAX_TX_SZ)) {
+      av_log(avctx, AV_LOG_ERROR,
+             "xcoder_receive_packet: packet buffer size %d allocation failed\n",
+             NI_MAX_TX_SZ);
+      return AVERROR(ENOMEM);
+  }
+
+  if (avctx->codec_id == AV_CODEC_ID_MJPEG && (!ctx->spsPpsArrived)) {
+      ctx->spsPpsArrived = 1;
+      // for Jpeg, start pkt_num counter from 1, because unlike video codecs
+      // (1st packet is header), there is no header for Jpeg
+      ctx->api_ctx.pkt_num = 1;
+  }
+
+  while (1)
+  {
+    xpkt->recycle_index = -1;
+    recv = ni_device_session_read(&ctx->api_ctx, &(ctx->api_pkt), NI_DEVICE_TYPE_ENCODER);
+
+    av_log(avctx, AV_LOG_TRACE,
+           "XCoder receive packet: xpkt.end_of_stream=%d, xpkt.data_len=%d, "
+           "xpkt.frame_type=%d, recv=%d, encoder_flushing=%d, encoder_eof=%d\n",
+           xpkt->end_of_stream, xpkt->data_len, xpkt->frame_type, recv,
+           ctx->encoder_flushing, ctx->encoder_eof);
+
+    if (recv <= 0)
+    {
+      ctx->encoder_eof = xpkt->end_of_stream;
+      /* not ready ?? */
+      if (ctx->encoder_eof || xpkt->end_of_stream)
+      {
+        if (SESSION_RUN_STATE_SEQ_CHANGE_DRAINING ==
+            ctx->api_ctx.session_run_state)
+        {
+          // after sequence change completes, reset codec state
+          av_log(avctx, AV_LOG_INFO, "xcoder_receive_packet 1: sequence "
+                 "change completed, return AVERROR(EAGAIN) and will reopen "
+                 "codec!\n");
+
+          ret = xcoder_encode_reinit(avctx);
+          av_log(avctx, AV_LOG_DEBUG, "xcoder_receive_packet: xcoder_encode_reinit ret %d\n", ret);
+          if (ret >= 0)
+          {
+            ret = AVERROR(EAGAIN);
+
+            xcoder_send_frame(avctx, NULL);
+
+            ctx->api_ctx.session_run_state = SESSION_RUN_STATE_NORMAL;
+          }
+          break;
+        }
+
+        ret = AVERROR_EOF;
+        av_log(avctx, AV_LOG_VERBOSE, "xcoder_receive_packet: got encoder_eof, return AVERROR_EOF\n");
+        break;
+      }
+      else
+      {
+          bool bIsReset = false;
+          if (NI_RETCODE_ERROR_VPU_RECOVERY == recv) {
+              xcoder_encode_reset(avctx);
+              bIsReset = true;
+          }
+        ret = AVERROR(EAGAIN);
+        if ((!ctx->encoder_flushing && !ctx->eos_fme_received) || bIsReset) // if encode session was reset, can't read again with invalid session, must break out first
+        {
+          av_log(avctx, AV_LOG_TRACE, "xcoder_receive_packet: NOT encoder_"
+                 "flushing, NOT eos_fme_received, return AVERROR(EAGAIN)\n");
+          break;
+        }
+      }
+    }
+    else
+    {
+      /* got encoded data back */
+      int meta_size = ctx->api_ctx.meta_size;
+      if (avctx->pix_fmt == AV_PIX_FMT_NI_QUAD && xpkt->recycle_index >= 0 &&
+          avctx->height >= NI_MIN_HEIGHT && avctx->width >= NI_MIN_WIDTH &&
+          xpkt->recycle_index < NI_GET_MAX_HWDESC_FRAME_INDEX(ctx->api_ctx.ddr_config)) {
+          int avframe_index =
+              recycle_index_2_avframe_index(ctx, xpkt->recycle_index);
+          av_log(avctx, AV_LOG_VERBOSE, "UNREF trace ui16FrameIdx = [%d].\n",
+                 xpkt->recycle_index);
+          if (avframe_index >= 0 && ctx->sframe_pool[avframe_index]) {
+              frame = ctx->sframe_pool[avframe_index];
+              ((niFrameSurface1_t *)((uint8_t *)frame->data[3]))
+                  ->device_handle =
+                  (int32_t)((int64_t)(ctx->api_ctx.blk_io_handle) &
+                            0xFFFFFFFF); // update handle to most recent alive
+
+              av_frame_unref(ctx->sframe_pool[avframe_index]);
+              av_log(avctx, AV_LOG_DEBUG,
+                     "AVframe_index = %d pushed to free tail %d\n",
+                     avframe_index, ctx->freeTail);
+              enq_free_frames(ctx, avframe_index);
+              // enqueue the index back to free
+              xpkt->recycle_index = -1;
+          } else {
+              av_log(avctx, AV_LOG_DEBUG,
+                     "can't push to tail - avframe_index %d sframe_pool %p\n",
+                     avframe_index, ctx->sframe_pool[avframe_index]);
+          }
+      }
+
+      if (! ctx->spsPpsArrived)
+      {
+        ret = AVERROR(EAGAIN);
+        ctx->spsPpsArrived = 1;
+        ctx->spsPpsHdrLen = recv - meta_size;
+        ctx->p_spsPpsHdr   = malloc(ctx->spsPpsHdrLen);
+        if (!ctx->p_spsPpsHdr) {
+            ret = AVERROR(ENOMEM);
+            break;
+        }
+
+        memcpy(ctx->p_spsPpsHdr, (uint8_t *)xpkt->p_data + meta_size,
+               xpkt->data_len - meta_size);
+        //printf("encoder: very first data chunk saved: %d !\n",
+        //       ctx->spsPpsHdrLen);
+
+        // start pkt_num counter from 1 to get the real first frame
+        ctx->api_ctx.pkt_num = 1;
+        // for low-latency mode, keep reading until the first frame is back
+        if (ctx->api_param.low_delay_mode)
+        {
+          av_log(avctx, AV_LOG_TRACE, "XCoder receive packet: low delay mode,"
+                 " keep reading until 1st pkt arrives\n");
+          continue;
+        }
+        break;
+      }
+
+      // handle pic skip
+      if (xpkt->frame_type == 3) // 0=I, 1=P, 2=B, 3=not coded / skip
+      {
+          ret = AVERROR(EAGAIN);
+          if (ctx->first_frame_pts == INT_MIN)
+              ctx->first_frame_pts = xpkt->pts;
+          if (AV_CODEC_ID_AV1 == avctx->codec_id) {
+              ctx->latest_dts = xpkt->pts;
+          } else if (ctx->total_frames_received < ctx->dtsOffset) {
+              // guess dts
+              ctx->latest_dts = ctx->first_frame_pts +
+                                ((ctx->gop_offset_count - ctx->dtsOffset) *
+                                 avctx->ticks_per_frame);
+              ctx->gop_offset_count++;
+          } else {
+              // get dts from pts FIFO
+              ctx->latest_dts =
+                  ctx->api_ctx
+                      .enc_pts_list[ctx->api_ctx.enc_pts_r_idx % NI_FIFO_SZ];
+              ctx->api_ctx.enc_pts_r_idx++;
+          }
+          if (ctx->latest_dts > xpkt->pts) {
+              ctx->latest_dts = xpkt->pts;
+          }
+          ctx->total_frames_received++;
+          
+          if (!ctx->encoder_flushing && ! ctx->eos_fme_received)
+          {
+            av_log(avctx, AV_LOG_TRACE, "xcoder_receive_packet: skip"
+                   " picture output, return AVERROR(EAGAIN)\n");
+            break;
+          }
+          else
+            continue;
+      }
+
+      // handle av1
+      if (avctx->codec_id == AV_CODEC_ID_AV1) {
+          av_log(
+              avctx, AV_LOG_TRACE,
+              "xcoder_receive_packet: AV1 xpkt buf %p size %d show_frame %d\n",
+              xpkt->p_data, xpkt->data_len, xpkt->av1_show_frame);
+          if (!xpkt->av1_show_frame && (ctx->total_frames_received >= 1)) {
+              xpkt->av1_p_buffer[xpkt->av1_buffer_index]    = xpkt->p_buffer;
+              xpkt->av1_p_data[xpkt->av1_buffer_index]      = xpkt->p_data;
+              xpkt->av1_buffer_size[xpkt->av1_buffer_index] = xpkt->buffer_size;
+              xpkt->av1_data_len[xpkt->av1_buffer_index]    = xpkt->data_len;
+              xpkt->av1_buffer_index++;
+              xpkt->p_buffer    = NULL;
+              xpkt->p_data      = NULL;
+              xpkt->buffer_size = 0;
+              xpkt->data_len    = 0;
+              if (xpkt->av1_buffer_index >= MAX_AV1_ENCODER_GOP_NUM) {
+                  av_log(avctx, AV_LOG_ERROR,
+                         "xcoder_receive_packet: recv AV1 not shown frame "
+                         "number %d >= %d, return AVERROR_EXTERNAL\n",
+                         xpkt->av1_buffer_index, MAX_AV1_ENCODER_GOP_NUM);
+                  ret = AVERROR_EXTERNAL;
+                  break;
+              } else if (!ctx->encoder_flushing && !ctx->eos_fme_received) {
+                  av_log(avctx, AV_LOG_TRACE,
+                         "xcoder_receive_packet: recv AV1 not shown frame, "
+                         "return AVERROR(EAGAIN)\n");
+                  ret = AVERROR(EAGAIN);
+                  break;
+              } else {
+                  if (ni_packet_buffer_alloc(xpkt, NI_MAX_TX_SZ)) {
+                      av_log(avctx, AV_LOG_ERROR,
+                             "xcoder_receive_packet: AV1 packet buffer size %d "
+                             "allocation failed during flush\n",
+                             NI_MAX_TX_SZ);
+                      ret = AVERROR(ENOMEM);
+                      break;
+                  }
+                  av_log(avctx, AV_LOG_TRACE,
+                         "xcoder_receive_packet: recv AV1 not shown frame "
+                         "during flush, continue..\n");
+                  continue;
+              }
+          }
+      }
+
+      uint32_t nalu_type = 0;
+      const uint8_t *p_start_code;
+      uint32_t stc = -1;
+      uint32_t copy_len = 0;
+      uint8_t *p_src = (uint8_t*)xpkt->p_data + meta_size;
+      uint8_t *p_end = p_src + (xpkt->data_len - meta_size);
+      int64_t local_pts = xpkt->pts;
+      int total_custom_sei_size = 0;
+      int custom_sei_count = 0;
+      ni_custom_sei_set_t *p_custom_sei_set;
+
+      p_custom_sei_set = ctx->api_ctx.pkt_custom_sei_set[local_pts % NI_FIFO_SZ];
+      if (p_custom_sei_set != NULL)
+      {
+        custom_sei_count = p_custom_sei_set->count;
+        for (i = 0; i < p_custom_sei_set->count; i++)
+        {
+          total_custom_sei_size += p_custom_sei_set->custom_sei[i].size;
+        }
+      }
+
+      if (custom_sei_count)
+      {
+        // if HRD or custom sei enabled, search for pic_timing or custom SEI insertion point by
+        // skipping non-VCL until video data is found.
+        p_start_code = p_src;
+        if(AV_CODEC_ID_HEVC == avctx->codec_id)
+        {
+          do
+          {
+            stc = -1;
+            p_start_code = avpriv_find_start_code(p_start_code, p_end, &stc);
+            nalu_type = (stc >> 1) & 0x3F;
+          } while (nalu_type > HEVC_NAL_RSV_VCL31);
+
+          // calc. length to copy
+          copy_len = p_start_code - 5 - p_src;
+        }
+        else if(AV_CODEC_ID_H264 == avctx->codec_id)
+        {
+          do
+          {
+            stc = -1;
+            p_start_code = avpriv_find_start_code(p_start_code, p_end, &stc);
+            nalu_type = stc & 0x1F;
+          } while (nalu_type > H264_NAL_IDR_SLICE);
+
+          // calc. length to copy
+          copy_len = p_start_code - 5 - p_src;
+        }
+        else
+        {
+          av_log(avctx, AV_LOG_ERROR, "xcoder_receive packet: codec %d not "
+               "supported for SEI !\n", avctx->codec_id);
+        }
+      }
+
+      if (avctx->codec_id == AV_CODEC_ID_MJPEG && !ctx->firstPktArrived) {
+          // there is no header for Jpeg, so skip header copy
+          ctx->firstPktArrived = 1;
+          if (ctx->first_frame_pts == INT_MIN)
+              ctx->first_frame_pts = xpkt->pts;
+      }
+
+      if (! ctx->firstPktArrived)
+      {
+        int sizeof_spspps_attached_to_idr = ctx->spsPpsHdrLen;
+        if ((avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) &&
+            (avctx->codec_id != AV_CODEC_ID_AV1)) {
+            sizeof_spspps_attached_to_idr = 0;
+        }
+        ctx->firstPktArrived = 1;
+        if (ctx->first_frame_pts == INT_MIN)
+            ctx->first_frame_pts = xpkt->pts;
+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59)
+        ret = ff_get_encode_buffer(avctx, pkt, xpkt->data_len - meta_size + sizeof_spspps_attached_to_idr + total_custom_sei_size, 0);
+#else
+        ret = ff_alloc_packet2(avctx, pkt, xpkt->data_len - meta_size + sizeof_spspps_attached_to_idr + total_custom_sei_size,
+                               xpkt->data_len - meta_size + sizeof_spspps_attached_to_idr + total_custom_sei_size);
+#endif
+        if (! ret)
+        {
+          uint8_t *p_dst, *p_side_data;
+
+          // fill in AVC/HEVC sidedata
+          if ((avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) &&
+              (avctx->extradata_size != ctx->spsPpsHdrLen ||
+               (memcmp(avctx->extradata, ctx->p_spsPpsHdr, ctx->spsPpsHdrLen) !=
+                0))) {
+              avctx->extradata_size = ctx->spsPpsHdrLen;
+              av_freep(&avctx->extradata);
+              avctx->extradata = av_mallocz(avctx->extradata_size +
+                                            AV_INPUT_BUFFER_PADDING_SIZE);
+              if (!avctx->extradata) {
+                  av_log(avctx, AV_LOG_ERROR,
+                         "Cannot allocate AVC/HEVC header of size %d.\n",
+                         avctx->extradata_size);
+                  return AVERROR(ENOMEM);
+              }
+              memcpy(avctx->extradata, ctx->p_spsPpsHdr, avctx->extradata_size);
+          }
+
+          p_side_data = av_packet_new_side_data(
+              pkt, AV_PKT_DATA_NEW_EXTRADATA, ctx->spsPpsHdrLen);
+          if (p_side_data)
+          {
+              memcpy(p_side_data, ctx->p_spsPpsHdr, ctx->spsPpsHdrLen);
+          }
+
+          p_dst = pkt->data;
+          if (sizeof_spspps_attached_to_idr)
+          {
+              memcpy(p_dst, ctx->p_spsPpsHdr, ctx->spsPpsHdrLen);
+              p_dst += ctx->spsPpsHdrLen;
+          }
+
+          if (custom_sei_count)
+          {
+              // copy buf_period
+              memcpy(p_dst, p_src, copy_len);
+              p_dst += copy_len;
+
+              for (i = 0; i < custom_sei_count; i++)
+              {
+                  // copy custom sei
+                  ni_custom_sei_t *p_custom_sei = &p_custom_sei_set->custom_sei[i];
+                  if (p_custom_sei->location == NI_CUSTOM_SEI_LOC_AFTER_VCL)
+                  {
+                      break;
+                  }
+                  memcpy(p_dst, &p_custom_sei->data[0], p_custom_sei->size);
+                  p_dst += p_custom_sei->size;
+              }
+
+              // copy the IDR data
+              memcpy(p_dst, p_src + copy_len,
+                     xpkt->data_len - meta_size - copy_len);
+              p_dst += xpkt->data_len - meta_size - copy_len;
+
+              // copy custom sei after slice
+              for (; i < custom_sei_count; i++)
+              {
+                  ni_custom_sei_t *p_custom_sei = &p_custom_sei_set->custom_sei[i];
+                  memcpy(p_dst, &p_custom_sei->data[0], p_custom_sei->size);
+                  p_dst += p_custom_sei->size;
+              }
+          }
+          else
+          {
+              memcpy(p_dst, (uint8_t*)xpkt->p_data + meta_size,
+                     xpkt->data_len - meta_size);
+          }
+        }
+      }
+      else
+      {
+          int temp_index;
+          uint32_t data_len = xpkt->data_len - meta_size + total_custom_sei_size;
+          if (avctx->codec_id == AV_CODEC_ID_AV1) {
+              av1_output_frame = 1;
+              for (temp_index = 0; temp_index < xpkt->av1_buffer_index;
+                   temp_index++) {
+                  data_len += xpkt->av1_data_len[temp_index] - meta_size;
+              }
+          }
+          // av_log(avctx, AV_LOG_TRACE, "xcoder_receive_packet: AV1 total pkt
+          // size %d\n", data_len);
+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59)
+          ret = ff_get_encode_buffer(avctx, pkt, data_len, 0);
+#else
+          ret = ff_alloc_packet2(avctx, pkt, data_len, data_len);
+#endif
+          if (!ret) {
+              uint8_t *p_dst = pkt->data;
+              if (avctx->codec_id == AV_CODEC_ID_AV1) {
+                  for (temp_index = 0; temp_index < xpkt->av1_buffer_index;
+                       temp_index++) {
+                      memcpy(p_dst,
+                             (uint8_t *)xpkt->av1_p_data[temp_index] +
+                                 meta_size,
+                             xpkt->av1_data_len[temp_index] - meta_size);
+                      // av_log(avctx, AV_LOG_TRACE, "xcoder_receive_packet: AV1
+                      // copy xpkt buf %p size %d\n",
+                      // (uint8_t*)xpkt->av1_p_data[temp_index] + meta_size,
+                      // xpkt->av1_data_len[temp_index] - meta_size);
+                      p_dst += (xpkt->av1_data_len[temp_index] - meta_size);
+                  }
+              }
+
+              if (custom_sei_count)
+              {
+                  // copy buf_period
+                  memcpy(p_dst, p_src, copy_len);
+                  p_dst += copy_len;
+    
+                  for (i = 0; i < custom_sei_count; i++)
+                  {
+                      // copy custom sei
+                      ni_custom_sei_t *p_custom_sei = &p_custom_sei_set->custom_sei[i];
+                      if (p_custom_sei->location == NI_CUSTOM_SEI_LOC_AFTER_VCL)
+                      {
+                          break;
+                      }
+                      memcpy(p_dst, &p_custom_sei->data[0], p_custom_sei->size);
+                      p_dst += p_custom_sei->size;
+                  }
+    
+                  // copy the IDR data
+                  memcpy(p_dst, p_src + copy_len,
+                         xpkt->data_len - meta_size - copy_len);
+                  p_dst += xpkt->data_len - meta_size - copy_len;
+    
+                  // copy custom sei after slice
+                  for (; i < custom_sei_count; i++)
+                  {
+                      ni_custom_sei_t *p_custom_sei = &p_custom_sei_set->custom_sei[i];
+                      memcpy(p_dst, &p_custom_sei->data[0], p_custom_sei->size);
+                      p_dst += p_custom_sei->size;
+                  }
+              } else {
+                  memcpy(p_dst, (uint8_t *)xpkt->p_data + meta_size,
+                         xpkt->data_len - meta_size);
+              }
+          }
+      }
+
+      // free buffer
+      if (custom_sei_count)
+      {
+        free(p_custom_sei_set);
+        ctx->api_ctx.pkt_custom_sei_set[local_pts % NI_FIFO_SZ] = NULL;
+      }
+
+      if (!ret)
+      {
+        if (xpkt->frame_type == 0)
+        {
+          pkt->flags |= AV_PKT_FLAG_KEY;
+        }
+
+        pkt->pts = xpkt->pts;
+        /* to ensure pts>dts for all frames, we assign a guess pts for the first 'dtsOffset' frames and then the pts from input stream
+         * is extracted from input pts FIFO.
+         * if GOP = IBBBP and PTSs = 0 1 2 3 4 5 .. then out DTSs = -3 -2 -1 0 1 ... and -3 -2 -1 are the guessed values
+         * if GOP = IBPBP and PTSs = 0 1 2 3 4 5 .. then out DTSs = -1 0 1 2 3 ... and -1 is the guessed value
+         * the number of guessed values is equal to dtsOffset
+         */
+        if (AV_CODEC_ID_AV1 == avctx->codec_id) {
+            pkt->dts = pkt->pts;
+            av_log(avctx, AV_LOG_TRACE, "Packet dts (av1): %ld\n", pkt->dts);
+        } else if (ctx->total_frames_received < ctx->dtsOffset) {
+            // guess dts
+            pkt->dts = ctx->first_frame_pts +
+                       ((ctx->gop_offset_count - ctx->dtsOffset) *
+                        avctx->ticks_per_frame);
+            ctx->gop_offset_count++;
+            av_log(avctx, AV_LOG_TRACE, "Packet dts (guessed): %ld\n",
+                   pkt->dts);
+        } else {
+            // get dts from pts FIFO
+            pkt->dts =
+                ctx->api_ctx
+                    .enc_pts_list[ctx->api_ctx.enc_pts_r_idx % NI_FIFO_SZ];
+            ctx->api_ctx.enc_pts_r_idx++;
+            av_log(avctx, AV_LOG_TRACE, "Packet dts: %ld\n", pkt->dts);
+        }
+        if (ctx->total_frames_received >= 1)
+        {
+          if (pkt->dts < ctx->latest_dts)
+          {
+            av_log(NULL, AV_LOG_WARNING, "dts: %ld < latest_dts: %ld.\n",
+                    pkt->dts, ctx->latest_dts);
+          }
+        }
+        if(pkt->dts > pkt->pts)
+        {
+          av_log(NULL, AV_LOG_WARNING, "dts: %ld, pts: %ld. Forcing dts = pts \n",
+                  pkt->dts, pkt->pts);
+          pkt->dts = pkt->pts;
+          av_log(avctx, AV_LOG_TRACE, "Force dts to: %ld\n", pkt->dts);
+        }
+        ctx->total_frames_received++;
+        ctx->latest_dts = pkt->dts;
+        // TODO: avg_frame_qp will be moved to libxcoder log after updating libxcoder to refer to ffmpeg log level
+        av_log(avctx, AV_LOG_DEBUG, "XCoder recv pkt #%" PRId64 ""
+               " pts %" PRId64 "  dts %" PRId64 "  size %d  st_index %d frame_type %u avg qp %u\n",
+               ctx->api_ctx.pkt_num - 1, pkt->pts, pkt->dts, pkt->size,
+               pkt->stream_index, xpkt->frame_type, xpkt->avg_frame_qp);
+
+        // printf("\n         Mux: pts  %lld  dts %lld   size %d  st_index %d \n\n", pkt->pts, pkt->dts, pkt->size, pkt->stream_index);
+      }
+      ctx->encoder_eof = xpkt->end_of_stream;
+      if (ctx->encoder_eof &&
+        SESSION_RUN_STATE_SEQ_CHANGE_DRAINING ==
+        ctx->api_ctx.session_run_state)
+      {
+        // after sequence change completes, reset codec state
+        av_log(avctx, AV_LOG_DEBUG, "xcoder_receive_packet 2: sequence change "
+          "completed, return 0 and will reopen codec !\n");
+        ret = xcoder_encode_reinit(avctx);
+        av_log(avctx, AV_LOG_DEBUG, "xcoder_receive_packet: xcoder_encode_reinit ret %d\n", ret);
+        if (ret >= 0)
+        {
+          xcoder_send_frame(avctx, NULL);
+          ctx->api_ctx.session_run_state = SESSION_RUN_STATE_NORMAL;
+        }
+      }
+      break;
+    }
+  }
+
+  if ((AV_CODEC_ID_AV1 == avctx->codec_id) && xpkt->av1_buffer_index &&
+      av1_output_frame) {
+      av_log(avctx, AV_LOG_TRACE,
+             "xcoder_receive_packet: ni_packet_buffer_free_av1 %d packtes\n",
+             xpkt->av1_buffer_index);
+      ni_packet_buffer_free_av1(xpkt);
+  }
+
+  av_log(avctx, AV_LOG_VERBOSE, "xcoder_receive_packet: return %d\n", ret);
+  return ret;
+}
+
+// for FFmpeg 4.4+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+int ff_xcoder_receive_packet(AVCodecContext *avctx, AVPacket *pkt)
+{
+    XCoderH265EncContext *ctx = avctx->priv_data;
+    AVFrame *frame = &ctx->buffered_fme;
+    int ret;
+
+    ret = ff_encode_get_frame(avctx, frame);
+    if (!ctx->encoder_flushing && ret >= 0 || ret == AVERROR_EOF)
+    {
+        ret = xcoder_send_frame(avctx, (ret == AVERROR_EOF ? NULL : frame));
+        if (ret < 0 && ret != AVERROR_EOF)
+        {
+            return ret;
+        }
+    }
+    // Once send_frame returns EOF go on receiving packets until EOS is met.
+    return xcoder_receive_packet(avctx, pkt);
+}
+#endif
+
+int xcoder_encode_frame(AVCodecContext *avctx, AVPacket *pkt,
+                        const AVFrame *frame, int *got_packet)
+{
+  XCoderH265EncContext *ctx = avctx->priv_data;
+  int ret;
+
+  av_log(avctx, AV_LOG_VERBOSE, "XCoder encode frame\n");
+
+  if (!ctx->encoder_flushing)
+  {
+    ret = xcoder_send_frame(avctx, frame);
+    if (ret < 0)
+    {
+      return ret;
+    }
+  }
+
+  ret = xcoder_receive_packet(avctx, pkt);
+  if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF)
+  {
+    *got_packet = 0;
+  }
+  else if (ret < 0)
+  {
+    return ret;
+  }
+  else
+  {
+    *got_packet = 1;
+  }
+
+  return 0;
+}
+
+bool free_frames_isempty(XCoderH265EncContext *ctx)
+{
+  return  (ctx->freeHead == ctx->freeTail);
+}
+
+bool free_frames_isfull(XCoderH265EncContext *ctx)
+{
+  return  (ctx->freeHead == ((ctx->freeTail == MAX_NUM_FRAMEPOOL_HWAVFRAME) ? 0 : ctx->freeTail + 1));
+}
+
+int deq_free_frames(XCoderH265EncContext *ctx)
+{
+  if (free_frames_isempty(ctx))
+  {
+    return -1;
+  }
+  ctx->aFree_Avframes_list[ctx->freeHead] = -1;
+  ctx->freeHead = (ctx->freeHead == MAX_NUM_FRAMEPOOL_HWAVFRAME) ? 0 : ctx->freeHead + 1;
+  return 0;
+}
+
+int enq_free_frames(XCoderH265EncContext *ctx, int idx)
+{
+  if (free_frames_isfull(ctx))
+  {
+    return -1;
+  }
+  ctx->aFree_Avframes_list[ctx->freeTail] = idx;
+  ctx->freeTail = (ctx->freeTail == MAX_NUM_FRAMEPOOL_HWAVFRAME) ? 0 : ctx->freeTail + 1;
+  return 0;
+}
+
+int recycle_index_2_avframe_index(XCoderH265EncContext *ctx, uint32_t recycleIndex)
+{
+  int i;
+  for (i = 0; i < MAX_NUM_FRAMEPOOL_HWAVFRAME; i++)
+  {
+    if (ctx->sframe_pool[i]->data[3])
+    {
+      if (((niFrameSurface1_t*)((uint8_t*)ctx->sframe_pool[i]->data[3]))->ui16FrameIdx == recycleIndex)
+      {
+        return i;
+      }
+      else
+      {
+        //av_log(NULL, AV_LOG_TRACE, "sframe_pool[%d] ui16FrameIdx %u != recycleIndex %u\n", i, ((niFrameSurface1_t*)((uint8_t*)ctx->sframe_pool[i]->data[3]))->ui16FrameIdx, recycleIndex);
+      }
+    }
+    else
+    {
+      //av_log(NULL, AV_LOG_TRACE, "sframe_pool[%d] data[3] NULL\n", i);
+    }
+  }
+  return -1;
+}
+
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+const AVCodecHWConfigInternal *ff_ni_enc_hw_configs[] = {
+  HW_CONFIG_ENCODER_FRAMES(NI_QUAD,  NI_QUADRA),
+  HW_CONFIG_ENCODER_DEVICE(NV12, NI_QUADRA),
+  HW_CONFIG_ENCODER_DEVICE(P010, NI_QUADRA),
+  HW_CONFIG_ENCODER_DEVICE(YUV420P, NI_QUADRA),
+  HW_CONFIG_ENCODER_DEVICE(YUV420P10, NI_QUADRA),
+  NULL,
+};
+#endif
diff --git a/libavcodec/nienc.h b/libavcodec/nienc.h
new file mode 100644
index 0000000000..b8eac98d4b
--- /dev/null
+++ b/libavcodec/nienc.h
@@ -0,0 +1,77 @@
+/*
+ * NetInt XCoder H.264/HEVC Encoder common code header
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_NIENC_H
+#define AVCODEC_NIENC_H
+
+#include <ni_rsrc_api.h>
+#include <ni_device_api.h>
+#include <ni_util.h>
+
+#include "libavutil/internal.h"
+
+#include "avcodec.h"
+#include "internal.h"
+#include "libavutil/opt.h"
+#include "libavutil/imgutils.h"
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+#include "hwconfig.h"
+#endif
+#include "nicodec.h"
+
+#define OFFSETENC(x) offsetof(XCoderH265EncContext, x)
+#define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM
+
+int xcoder_encode_init(AVCodecContext *avctx);
+
+int xcoder_encode_close(AVCodecContext *avctx);
+
+int xcoder_encode_sequence_change(AVCodecContext *avctx, int width, int height, int bit_depth_factor);
+
+int xcoder_send_frame(AVCodecContext *avctx, const AVFrame *frame);
+
+int xcoder_receive_packet(AVCodecContext *avctx, AVPacket *pkt);
+
+// for FFmpeg 4.4+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+int ff_xcoder_receive_packet(AVCodecContext *avctx, AVPacket *pkt);
+#endif
+
+int xcoder_encode_frame(AVCodecContext *avctx, AVPacket *pkt,
+                        const AVFrame *frame, int *got_packet);
+
+bool free_frames_isempty(XCoderH265EncContext *ctx);
+
+bool free_frames_isfull(XCoderH265EncContext *ctx);
+
+int deq_free_frames(XCoderH265EncContext *ctx);
+
+int enq_free_frames(XCoderH265EncContext *ctx, int idx);
+
+int recycle_index_2_avframe_index(XCoderH265EncContext *ctx, uint32_t recycleIndex);
+
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+extern const AVCodecHWConfigInternal *ff_ni_enc_hw_configs[];
+#endif
+
+#endif /* AVCODEC_NIENC_H */
diff --git a/libavcodec/nienc_av1.c b/libavcodec/nienc_av1.c
new file mode 100644
index 0000000000..702a459264
--- /dev/null
+++ b/libavcodec/nienc_av1.c
@@ -0,0 +1,115 @@
+/*
+ * NetInt XCoder HEVC Encoder
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "nienc.h"
+
+#include "codec_internal.h"
+
+#define OFFSETENC(x) offsetof(XCoderH265EncContext, x)
+#define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM
+static const AVOption enc_options[] = {
+    {"enc",
+     "Select which encoder to use by index. First is 0, second is 1, and so "
+     "on.",
+     OFFSETENC(dev_enc_idx),
+     AV_OPT_TYPE_INT,
+     {.i64 = BEST_DEVICE_LOAD},
+     -1,
+     INT_MAX,
+     VE,
+     "enc"},
+
+    {"iosize",
+     "Specify a custom NVMe IO transfer size (multiples of 4096 only).",
+     OFFSETENC(nvme_io_size),
+     AV_OPT_TYPE_INT,
+     {.i64 = BEST_DEVICE_LOAD},
+     -1,
+     INT_MAX,
+     VE,
+     "iosize"},
+
+    {"xcoder-params",
+     "Set the XCoder configuration using a :-separated list of key=value "
+     "parameters",
+     OFFSETENC(xcoder_opts),
+     AV_OPT_TYPE_STRING,
+     {0},
+     0,
+     0,
+     VE},
+
+    {"xcoder-gop",
+     "Set the XCoder custom gop using a :-separated list of key=value "
+     "parameters",
+     OFFSETENC(xcoder_gop),
+     AV_OPT_TYPE_STRING,
+     {0},
+     0,
+     0,
+     VE},
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSETENC(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     VE,
+     "keep_alive_timeout"},
+
+    {NULL}};
+
+static const AVClass av1_xcoderenc_class = {
+    .class_name = "av1_ni_quadra_enc",
+    .item_name  = av_default_item_name,
+    .option     = enc_options,
+    .version    = LIBAVUTIL_VERSION_INT,
+};
+
+const FFCodec ff_av1_ni_quadra_encoder = {
+    .p.name = "av1_ni_quadra_enc",
+    .p.long_name =
+        NULL_IF_CONFIG_SMALL("AV1 NetInt Quadra encoder v" NI_XCODER_REVISION),
+    .p.type = AVMEDIA_TYPE_VIDEO,
+    .p.id   = AV_CODEC_ID_AV1,
+    .init = xcoder_encode_init,
+// FFmpeg-n4.4+ has no more .send_frame;
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    FF_CODEC_RECEIVE_PACKET_CB(ff_xcoder_receive_packet),
+#else
+    .send_frame     = xcoder_send_frame,
+    FF_CODEC_RECEIVE_PACKET_CB(xcoder_receive_packet),
+#endif
+    FF_CODEC_ENCODE_CB(xcoder_encode_frame),
+    .close          = xcoder_encode_close,
+    .priv_data_size = sizeof(XCoderH265EncContext),
+    .p.priv_class     = &av1_xcoderenc_class,
+    .p.capabilities   = AV_CODEC_CAP_DELAY,
+    .p.pix_fmts =
+        (const enum AVPixelFormat[]){AV_PIX_FMT_YUV420P, AV_PIX_FMT_YUVJ420P,
+                                     AV_PIX_FMT_YUV420P10LE, AV_PIX_FMT_NV12,
+                                     AV_PIX_FMT_P010LE, AV_PIX_FMT_NI_QUAD,
+                                     AV_PIX_FMT_NONE},
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+    .hw_configs = ff_ni_enc_hw_configs,
+#endif
+};
diff --git a/libavcodec/nienc_h264.c b/libavcodec/nienc_h264.c
new file mode 100644
index 0000000000..5dddf8a95f
--- /dev/null
+++ b/libavcodec/nienc_h264.c
@@ -0,0 +1,80 @@
+/*
+ * NetInt XCoder H.264 Encoder
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "nienc.h"
+
+#include "codec_internal.h"
+
+static const AVOption enc_options[] = {
+  { "enc",       "Select which encoder to use by index. First is 0, second is 1, and so on.", OFFSETENC(dev_enc_idx),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VE, "enc" },
+    
+  { "iosize",       "Specify a custom NVMe IO transfer size (multiples of 4096 only).", OFFSETENC(nvme_io_size),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VE, "iosize" },
+    
+  { "xcoder-params", "Set the XCoder configuration using a :-separated list of key=value parameters", OFFSETENC(xcoder_opts), 
+    AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+
+  { "xcoder-gop", "Set the XCoder custom gop using a :-separated list of key=value parameters", OFFSETENC(xcoder_gop), 
+  AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+    
+  { "keep_alive_timeout",       "Specify a custom session keep alive timeout in seconds.", OFFSETENC(keep_alive_timeout),
+    AV_OPT_TYPE_INT, { .i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT }, NI_MIN_KEEP_ALIVE_TIMEOUT, NI_MAX_KEEP_ALIVE_TIMEOUT, VE, "keep_alive_timeout" },
+
+
+
+  { NULL }
+};
+
+static const AVClass h264_xcoderenc_class = {
+  .class_name = "h264_ni_quadra_enc",
+  .item_name = av_default_item_name,
+  .option = enc_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+const FFCodec ff_h264_ni_quadra_encoder = {
+    .p.name = "h264_ni_quadra_enc",
+    .p.long_name =
+        NULL_IF_CONFIG_SMALL("H.264 NetInt Quadra encoder v" NI_XCODER_REVISION),
+    .p.type = AVMEDIA_TYPE_VIDEO,
+    .p.id   = AV_CODEC_ID_H264,
+    .init = xcoder_encode_init,
+// FFmpeg-n4.4+ has no more .send_frame.
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    FF_CODEC_RECEIVE_PACKET_CB(ff_xcoder_receive_packet),
+#else
+    .send_frame     = xcoder_send_frame,
+    FF_CODEC_RECEIVE_PACKET_CB((xcoder_receive_packet),
+#endif
+    FF_CODEC_ENCODE_CB(xcoder_encode_frame),
+    .close          = xcoder_encode_close,
+    .priv_data_size = sizeof(XCoderH265EncContext),
+    .p.priv_class     = &h264_xcoderenc_class,
+    .p.capabilities   = AV_CODEC_CAP_DELAY,
+    .p.pix_fmts =
+        (const enum AVPixelFormat[]){AV_PIX_FMT_YUV420P, AV_PIX_FMT_YUVJ420P,
+                                     AV_PIX_FMT_YUV420P10LE, AV_PIX_FMT_NV12,
+                                     AV_PIX_FMT_P010LE, AV_PIX_FMT_NI_QUAD,
+                                     AV_PIX_FMT_NONE},
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+    .hw_configs = ff_ni_enc_hw_configs,
+#endif
+};
diff --git a/libavcodec/nienc_h264_logan.c b/libavcodec/nienc_h264_logan.c
new file mode 100644
index 0000000000..de8fd223e4
--- /dev/null
+++ b/libavcodec/nienc_h264_logan.c
@@ -0,0 +1,86 @@
+/*
+ * NetInt XCoder H.264 Encoder
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "nienc_logan.h"
+
+
+#define OFFSETENC(x) offsetof(XCoderLoganEncContext, x)
+#define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM
+static const AVOption enc_options[] = {
+  { "xcoder",    "Select which XCoder card to use.",  OFFSETENC(dev_xcoder),
+    AV_OPT_TYPE_STRING, { .str = "bestload" }, CHAR_MIN, CHAR_MAX, VE, "xcoder" },
+  { "bestload",      "Pick the least loaded XCoder/encoder available.", 0, AV_OPT_TYPE_CONST,
+    { .str = "bestload" }, 0, 0, VE, "xcoder" },
+
+  { "bestinst",      "Pick the XCoder/encoder with the least number of running encoding instances.", 0, AV_OPT_TYPE_CONST,
+    { .str = "bestinst" }, 0, 0, VE, "xcoder" },
+
+  { "list",      "List the available XCoder cards.", 0, AV_OPT_TYPE_CONST,
+    { .str = "list" }, 0, 0, VE, "xcoder" },
+
+  { "enc",       "Select which encoder to use by index. First is 0, second is 1, and so on.", OFFSETENC(dev_enc_idx),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VE, "enc" },
+    
+  { "keep_alive_timeout",       "Specify a custom session keep alive timeout in seconds.", OFFSETENC(keep_alive_timeout),
+    AV_OPT_TYPE_INT, { .i64 = NI_LOGAN_DEFAULT_KEEP_ALIVE_TIMEOUT }, NI_LOGAN_MIN_KEEP_ALIVE_TIMEOUT, NI_LOGAN_MAX_KEEP_ALIVE_TIMEOUT, VE, "keep_alive_timeout" },
+
+  { "xcoder-params", "Set the XCoder configuration using a :-separated list of key=value parameters", OFFSETENC(xcoder_opts), 
+    AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+
+  { "xcoder-gop", "Set the XCoder custom gop using a :-separated list of key=value parameters", OFFSETENC(xcoder_gop), 
+  AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+
+  { "set_high_priority",       "Specify a custom session set high priority in 0 or 1", OFFSETENC(set_high_priority),
+    AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 1, VE, "set_high_priority" },
+
+
+  { NULL }
+};
+
+static const AVClass h264_xcoderenc_class = {
+  .class_name = "h264_ni_logan_enc",
+  .item_name = av_default_item_name,
+  .option = enc_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+AVCodec ff_h264_ni_logan_encoder = {
+  .name           = "h264_ni_logan_enc",
+  .long_name      = NULL_IF_CONFIG_SMALL("H.264 NetInt Logan encoder v" NI_LOGAN_XCODER_REVISION),
+  .type           = AVMEDIA_TYPE_VIDEO,
+  .id             = AV_CODEC_ID_H264,
+  .init           = ff_xcoder_logan_encode_init,
+  .receive_packet = ff_xcoder_logan_receive_packet,
+  .encode2        = ff_xcoder_logan_encode_frame,
+  .close          = ff_xcoder_logan_encode_close,
+  .priv_data_size = sizeof(XCoderLoganEncContext),
+  .priv_class     = &h264_xcoderenc_class,
+  .capabilities   = AV_CODEC_CAP_DELAY,
+  .pix_fmts = (const enum AVPixelFormat[]) {
+                                            AV_PIX_FMT_YUV420P,
+                                            AV_PIX_FMT_YUV420P10BE,
+                                            AV_PIX_FMT_YUV420P10LE,
+                                            AV_PIX_FMT_YUVJ420P,
+                                            AV_PIX_FMT_NI_LOGAN,
+                                            AV_PIX_FMT_NONE},
+// Needed for yuvbypass on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+  .hw_configs     = ff_ni_logan_enc_hw_configs,
+#endif
+};
diff --git a/libavcodec/nienc_hevc.c b/libavcodec/nienc_hevc.c
new file mode 100644
index 0000000000..02dd5d90cc
--- /dev/null
+++ b/libavcodec/nienc_hevc.c
@@ -0,0 +1,80 @@
+/*
+ * NetInt XCoder HEVC Encoder
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "nienc.h"
+
+#include "codec_internal.h"
+
+static const AVOption enc_options[] = {
+  { "enc",       "Select which encoder to use by index. First is 0, second is 1, and so on.", OFFSETENC(dev_enc_idx),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VE, "enc" },
+  
+  { "iosize",       "Specify a custom NVMe IO transfer size (multiples of 4096 only).", OFFSETENC(nvme_io_size),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VE, "iosize" },
+    
+  { "xcoder-params", "Set the XCoder configuration using a :-separated list of key=value parameters", OFFSETENC(xcoder_opts), 
+    AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+
+  { "xcoder-gop", "Set the XCoder custom gop using a :-separated list of key=value parameters", OFFSETENC(xcoder_gop), 
+  AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+    
+  { "keep_alive_timeout",       "Specify a custom session keep alive timeout in seconds.", OFFSETENC(keep_alive_timeout),
+    AV_OPT_TYPE_INT, { .i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT }, NI_MIN_KEEP_ALIVE_TIMEOUT, NI_MAX_KEEP_ALIVE_TIMEOUT, VE, "keep_alive_timeout" },
+
+
+
+  { NULL }
+};
+
+static const AVClass h265_xcoderenc_class = {
+  .class_name = "h265_ni_quadra_enc",
+  .item_name = av_default_item_name,
+  .option = enc_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+const FFCodec ff_h265_ni_quadra_encoder = {
+    .p.name = "h265_ni_quadra_enc",
+    .p.long_name =
+        NULL_IF_CONFIG_SMALL("H.265 NetInt Quadra encoder v" NI_XCODER_REVISION),
+    .p.type = AVMEDIA_TYPE_VIDEO,
+    .p.id   = AV_CODEC_ID_H265,
+    .init = xcoder_encode_init,
+// FFmpeg-n4.4+ has no more .send_frame.
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    FF_CODEC_RECEIVE_PACKET_CB(ff_xcoder_receive_packet),
+#else
+    .send_frame     = xcoder_send_frame,
+    FF_CODEC_RECEIVE_PACKET_CB((xcoder_receive_packet),
+#endif
+    FF_CODEC_ENCODE_CB(xcoder_encode_frame),
+    .close          = xcoder_encode_close,
+    .priv_data_size = sizeof(XCoderH265EncContext),
+    .p.priv_class     = &h265_xcoderenc_class,
+    .p.capabilities   = AV_CODEC_CAP_DELAY,
+    .p.pix_fmts =
+        (const enum AVPixelFormat[]){AV_PIX_FMT_YUV420P, AV_PIX_FMT_YUVJ420P,
+                                     AV_PIX_FMT_YUV420P10, AV_PIX_FMT_NV12,
+                                     AV_PIX_FMT_P010LE, AV_PIX_FMT_NI_QUAD,
+                                     AV_PIX_FMT_NONE},
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+    .hw_configs = ff_ni_enc_hw_configs,
+#endif
+};
diff --git a/libavcodec/nienc_hevc_logan.c b/libavcodec/nienc_hevc_logan.c
new file mode 100644
index 0000000000..211d3961c3
--- /dev/null
+++ b/libavcodec/nienc_hevc_logan.c
@@ -0,0 +1,86 @@
+/*
+ * NetInt XCoder HEVC Encoder
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "nienc_logan.h"
+
+
+#define OFFSETENC(x) offsetof(XCoderLoganEncContext, x)
+#define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM
+static const AVOption enc_options[] = {
+  { "xcoder",    "Select which XCoder card to use.",  OFFSETENC(dev_xcoder),
+    AV_OPT_TYPE_STRING, { .str = "bestload" }, CHAR_MIN, CHAR_MAX, VE, "xcoder" },
+  { "bestload",      "Pick the least loaded XCoder/encoder available.", 0, AV_OPT_TYPE_CONST,
+    { .str = "bestload" }, 0, 0, VE, "xcoder" },
+
+  { "bestinst",      "Pick the XCoder/encoder with the least number of running encoding instances.", 0, AV_OPT_TYPE_CONST,
+    { .str = "bestinst" }, 0, 0, VE, "xcoder" },
+
+  { "list",      "List the available XCoder cards.", 0, AV_OPT_TYPE_CONST,
+    { .str = "list" }, 0, 0, VE, "xcoder" },
+
+  { "enc",       "Select which encoder to use by index. First is 0, second is 1, and so on.", OFFSETENC(dev_enc_idx),
+    AV_OPT_TYPE_INT, { .i64 = BEST_DEVICE_LOAD }, -1, INT_MAX, VE, "enc" },
+  
+  { "keep_alive_timeout",       "Specify a custom session keep alive timeout in seconds.", OFFSETENC(keep_alive_timeout),
+    AV_OPT_TYPE_INT, { .i64 = NI_LOGAN_DEFAULT_KEEP_ALIVE_TIMEOUT }, NI_LOGAN_MIN_KEEP_ALIVE_TIMEOUT, NI_LOGAN_MAX_KEEP_ALIVE_TIMEOUT, VE, "keep_alive_timeout" },
+
+  { "xcoder-params", "Set the XCoder configuration using a :-separated list of key=value parameters", OFFSETENC(xcoder_opts), 
+    AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+
+  { "xcoder-gop", "Set the XCoder custom gop using a :-separated list of key=value parameters", OFFSETENC(xcoder_gop), 
+  AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+
+  { "set_high_priority",       "Specify a custom session set high priority in 0 or 1", OFFSETENC(set_high_priority),
+    AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 1, VE, "set_high_priority" },
+
+
+  { NULL }
+};
+
+static const AVClass h265_xcoderenc_class = {
+  .class_name = "h265_ni_logan_enc",
+  .item_name = av_default_item_name,
+  .option = enc_options,
+  .version = LIBAVUTIL_VERSION_INT,
+};
+
+AVCodec ff_h265_ni_logan_encoder = {
+  .name           = "h265_ni_logan_enc",
+  .long_name      = NULL_IF_CONFIG_SMALL("H.265 NetInt Logan encoder v" NI_LOGAN_XCODER_REVISION),
+  .type           = AVMEDIA_TYPE_VIDEO,
+  .id             = AV_CODEC_ID_H265,
+  .init           = ff_xcoder_logan_encode_init,
+  .receive_packet = ff_xcoder_logan_receive_packet,
+  .encode2        = ff_xcoder_logan_encode_frame,
+  .close          = ff_xcoder_logan_encode_close,
+  .priv_data_size = sizeof(XCoderLoganEncContext),
+  .priv_class     = &h265_xcoderenc_class,
+  .capabilities   = AV_CODEC_CAP_DELAY,
+  .pix_fmts = (const enum AVPixelFormat[]) {
+                                            AV_PIX_FMT_YUV420P,
+                                            AV_PIX_FMT_YUV420P10BE,
+                                            AV_PIX_FMT_YUV420P10LE,
+                                            AV_PIX_FMT_YUVJ420P,
+                                            AV_PIX_FMT_NI_LOGAN,
+                                            AV_PIX_FMT_NONE},
+// Needed for yuvbypass on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+  .hw_configs     = ff_ni_logan_enc_hw_configs,
+#endif
+};
diff --git a/libavcodec/nienc_jpeg.c b/libavcodec/nienc_jpeg.c
new file mode 100644
index 0000000000..c4b69b9b8d
--- /dev/null
+++ b/libavcodec/nienc_jpeg.c
@@ -0,0 +1,108 @@
+/*
+ * NetInt XCoder H.264 Encoder
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "nienc.h"
+
+#include "codec_internal.h"
+
+static const AVOption enc_options[] = {
+    {"enc",
+     "Select which encoder to use by index. First is 0, second is 1, and so "
+     "on.",
+     OFFSETENC(dev_enc_idx),
+     AV_OPT_TYPE_INT,
+     {.i64 = BEST_DEVICE_LOAD},
+     -1,
+     INT_MAX,
+     VE,
+     "enc"},
+    {"iosize",
+     "Specify a custom NVMe IO transfer size (multiples of 4096 only).",
+     OFFSETENC(nvme_io_size),
+     AV_OPT_TYPE_INT,
+     {.i64 = BEST_DEVICE_LOAD},
+     -1,
+     INT_MAX,
+     VE,
+     "iosize"},
+    {"xcoder-params",
+     "Set the XCoder configuration using a :-separated list of key=value "
+     "parameters",
+     OFFSETENC(xcoder_opts),
+     AV_OPT_TYPE_STRING,
+     {0},
+     0,
+     0,
+     VE},
+    {"xcoder-gop",
+     "Set the XCoder custom gop using a :-separated list of key=value "
+     "parameters",
+     OFFSETENC(xcoder_gop),
+     AV_OPT_TYPE_STRING,
+     {0},
+     0,
+     0,
+     VE},
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSETENC(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     VE,
+     "keep_alive_timeout"},
+    {NULL}};
+
+static const AVClass jpeg_xcoderenc_class = {
+    .class_name = "jpeg_ni_quadra_enc",
+    .item_name  = av_default_item_name,
+    .option     = enc_options,
+    .version    = LIBAVUTIL_VERSION_INT,
+};
+
+const FFCodec ff_jpeg_ni_quadra_encoder = {
+    .p.name = "jpeg_ni_quadra_enc",
+    .p.long_name =
+        NULL_IF_CONFIG_SMALL("JPEG NetInt Quadra encoder v" NI_XCODER_REVISION),
+    .p.type = AVMEDIA_TYPE_VIDEO,
+    .p.id   = AV_CODEC_ID_MJPEG,
+    .init = xcoder_encode_init,
+// FFmpeg-n4.4+ has no more .send_frame;
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 134)
+    FF_CODEC_RECEIVE_PACKET_CB(ff_xcoder_receive_packet),
+#else
+    .send_frame     = xcoder_send_frame,
+    FF_CODEC_RECEIVE_PACKET_CB(xcoder_receive_packet),
+#endif
+    FF_CODEC_ENCODE_CB(xcoder_encode_frame),
+    .close          = xcoder_encode_close,
+    .priv_data_size = sizeof(XCoderH265EncContext),
+    .p.priv_class     = &jpeg_xcoderenc_class,
+    .p.capabilities   = AV_CODEC_CAP_DELAY,
+    .p.pix_fmts = // Quadra encoder preprocessor can convert 10-bit input to 8-bit
+                // before encoding to Jpeg
+    (const enum AVPixelFormat[]){AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_NI_QUAD,
+                                 AV_PIX_FMT_NONE},
+// Needed for hwframe on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82)
+    .hw_configs = ff_ni_enc_hw_configs,
+#endif
+};
diff --git a/libavcodec/nienc_logan.c b/libavcodec/nienc_logan.c
new file mode 100644
index 0000000000..1c07d00f10
--- /dev/null
+++ b/libavcodec/nienc_logan.c
@@ -0,0 +1,3795 @@
+/*
+ * NetInt XCoder H.264/HEVC Encoder common code
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/mastering_display_metadata.h"
+#include "libavcodec/put_bits.h"
+#include "libavcodec/golomb.h"
+#include "libavcodec/hevc.h"
+#include "libavcodec/hevc_sei.h"
+#include "libavcodec/h264.h"
+#include "libavcodec/h264_sei.h"
+#include "libavutil/hdr_dynamic_metadata.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/hwcontext_ni_logan.h"
+#include "bytestream.h"
+#include "nienc_logan.h"
+#include "ni_av_codec_logan.h"
+
+#define ODD2EVEN(X) ((X&1)&&(X>31))?(X+1):(X)
+#define BR_SHIFT  6
+#define CPB_SHIFT 4
+
+#ifdef NIENC_MULTI_THREAD
+threadpool_t pool;
+int sessionCounter = 0;
+
+typedef struct _write_thread_arg_struct_t
+{
+  pthread_mutex_t mutex; //mutex
+  pthread_cond_t cond;   //cond
+  int running;
+  XCoderLoganEncContext *ctx;
+  ni_logan_retcode_t ret;
+}write_thread_arg_struct_t;
+#endif
+
+typedef enum
+{
+  SLICE_TYPE_B = 0,
+  SLICE_TYPE_P = 1,
+  SLICE_TYPE_I = 2,
+  SLICE_TYPE_MP = 3
+} slice_type_t;
+
+typedef enum
+{
+  GOP_PRESET_CUSTOM        = 0,
+  GOP_PRESET_I_1           = 1,
+  GOP_PRESET_P_1           = 2,
+  GOP_PRESET_B_1           = 3,
+  GOP_PRESET_BP_2          = 4,
+  GOP_PRESET_BBBP_3        = 5,
+  GOP_PRESET_LP_4          = 6,
+  GOP_PRESET_LD_4          = 7,
+  GOP_PRESET_RA_8          = 8,
+  // single_ref
+  GOP_PRESET_SP_1          = 9,
+  GOP_PRESET_BSP_2         = 10,
+  GOP_PRESET_BBBSP_3       = 11,
+  GOP_PRESET_LSP_4         = 12,
+
+  // newly added
+  GOP_PRESET_BBP_3         = 13,
+  GOP_PRESET_BBSP_3        = 14,
+  GOP_PRESET_BBBBBBBP_8    = 15,
+  GOP_PRESET_BBBBBBBSP_8   = 16,
+  NUM_GOP_PRESET_NUM       = 17,
+} gop_preset_t;
+
+static const int32_t GOP_SIZE[NUM_GOP_PRESET_NUM] = {0, 1, 1, 1, 2, 4, 4, 4, 8, 1, 2, 4, 4};
+static const int32_t LT_GOP_PRESET_I_1[6] = {SLICE_TYPE_I,  1, 0, 0, 0, 0};
+static const int32_t LT_GOP_PRESET_P_1[6] = {SLICE_TYPE_MP, 1, 1, 0, 0, -1};
+static const int32_t LT_GOP_PRESET_B_1[6] = {SLICE_TYPE_B,  1, 1, 0, 0, -1};
+// gop_size = 2
+static const int32_t LT_GOP_PRESET_BP_2[12] =
+{
+  SLICE_TYPE_MP, 2, 1, 0, 0, -2,
+  SLICE_TYPE_B,  1, 3, 0, 0, 2,
+};
+// gop_size = 4
+static const int32_t LT_GOP_PRESET_BBBP_4[24] =
+{
+  SLICE_TYPE_MP, 4, 1, 0, 0, -4,
+  SLICE_TYPE_B,  2, 3, 0, 0, 4,
+  SLICE_TYPE_B,  1, 5, 0, 0, 2,
+  SLICE_TYPE_B,  3, 5, 0, 2, 4,
+};
+
+static const int32_t LT_GOP_PRESET_LP_4[24] =
+{
+  SLICE_TYPE_MP, 1, 5, 0, 0, -4,
+  SLICE_TYPE_MP, 2, 3, 0, 1, 0,
+  SLICE_TYPE_MP, 3, 5, 0, 2, 0,
+  SLICE_TYPE_MP, 4, 1, 0, 3, 0,
+};
+static const int32_t LT_GOP_PRESET_LD_4[24] =
+{
+  SLICE_TYPE_B, 1, 5, 0, 0, -4,
+  SLICE_TYPE_B, 2, 3, 0, 1, 0,
+  SLICE_TYPE_B, 3, 5, 0, 2, 0,
+  SLICE_TYPE_B, 4, 1, 0, 3, 0,
+};
+// gop_size = 8
+static const int32_t LT_GOP_PRESET_RA_8[48] =
+{
+  SLICE_TYPE_B, 8, 1, 0, 0, -8,
+  SLICE_TYPE_B, 4, 3, 0, 0, 8,
+  SLICE_TYPE_B, 2, 5, 0, 0, 4,
+  SLICE_TYPE_B, 1, 8, 0, 0, 2,
+  SLICE_TYPE_B, 3, 8, 0, 2, 4,
+  SLICE_TYPE_B, 6, 5, 0, 4, 8,
+  SLICE_TYPE_B, 5, 8, 0, 4, 6,
+  SLICE_TYPE_B, 7, 8, 0, 6, 8,
+};
+// single-ref-P
+static const int32_t LT_GOP_PRESET_SP_1[6] = {SLICE_TYPE_P, 1, 1, 0, 0, -1};
+
+static const int32_t LT_GOP_PRESET_BSP_2[12] =
+{
+  SLICE_TYPE_P, 2, 1, 0, 0, -2,
+  SLICE_TYPE_B, 1, 3, 0, 0, 2,
+};
+static const int32_t LT_GOP_PRESET_BBBSP_4[24] =
+{
+  SLICE_TYPE_P, 4, 1, 0, 0, -4,
+  SLICE_TYPE_B, 2, 3, 0, 0, 4,
+  SLICE_TYPE_B, 1, 5, 0, 0, 2,
+  SLICE_TYPE_B, 3, 5, 0, 2, 4,
+};
+static const int32_t LT_GOP_PRESET_LSP_4[24] =
+{
+  SLICE_TYPE_P, 1, 5, 0, 0, -4,
+  SLICE_TYPE_P, 2, 3, 0, 1, 0,
+  SLICE_TYPE_P, 3, 5, 0, 2, 0,
+  SLICE_TYPE_P, 4, 1, 0, 3, 0,
+};
+
+static const int32_t LT_GOP_PRESET_BBP_3[18] =
+{
+  SLICE_TYPE_MP, 3, 1, 0, 0, -3,
+  SLICE_TYPE_B, 1, 3, 0, 0, 3,
+  SLICE_TYPE_B, 2, 6, 0, 1, 3,
+};
+
+static const int32_t LT_GOP_PRESET_BBSP_3[18] =
+{
+  SLICE_TYPE_P, 3, 1, 0, 0, 0,
+  SLICE_TYPE_B, 1, 3, 0, 0, 3,
+  SLICE_TYPE_B, 2, 6, 0, 1, 3,
+};
+
+static const int32_t LT_GOP_PRESET_BBBBBBBP_8[48] =
+{
+  SLICE_TYPE_MP, 8, 1, 0, 0, -8,
+  SLICE_TYPE_B, 4, 3, 0, 0, 8,
+  SLICE_TYPE_B, 2, 5, 0, 0, 4,
+  SLICE_TYPE_B, 1, 8, 0, 0, 2,
+  SLICE_TYPE_B, 3, 8, 0, 2, 4,
+  SLICE_TYPE_B, 6, 5, 0, 4, 8,
+  SLICE_TYPE_B, 5, 8, 0, 4, 6,
+  SLICE_TYPE_B, 7, 8, 0, 6, 8,
+};
+static const int32_t LT_GOP_PRESET_BBBBBBBSP_8[48] =
+{
+  SLICE_TYPE_P, 8, 1, 0, 0, 0,
+  SLICE_TYPE_B, 4, 3, 0, 0, 8,
+  SLICE_TYPE_B, 2, 5, 0, 0, 4,
+  SLICE_TYPE_B, 1, 8, 0, 0, 2,
+  SLICE_TYPE_B, 3, 8, 0, 2, 4,
+  SLICE_TYPE_B, 6, 5, 0, 4, 8,
+  SLICE_TYPE_B, 5, 8, 0, 4, 6,
+  SLICE_TYPE_B, 7, 8, 0, 6, 8,
+};
+static const int32_t* GOP_PRESET[NUM_GOP_PRESET_NUM] =
+{
+  NULL,
+  LT_GOP_PRESET_I_1,
+  LT_GOP_PRESET_P_1,
+  LT_GOP_PRESET_B_1,
+  LT_GOP_PRESET_BP_2,
+  LT_GOP_PRESET_BBBP_4,
+  LT_GOP_PRESET_LP_4,
+  LT_GOP_PRESET_LD_4,
+  LT_GOP_PRESET_RA_8,
+
+  LT_GOP_PRESET_SP_1,
+  LT_GOP_PRESET_BSP_2,
+  LT_GOP_PRESET_BBBSP_4,
+  LT_GOP_PRESET_LSP_4,
+
+  LT_GOP_PRESET_BBP_3    ,
+  LT_GOP_PRESET_BBSP_3   ,
+  LT_GOP_PRESET_BBBBBBBP_8 ,
+  LT_GOP_PRESET_BBBBBBBSP_8,
+};
+
+static void init_gop_param(ni_logan_custom_gop_params_t *gopParam,
+                           ni_logan_encoder_params_t *param)
+{
+  int i;
+  int j;
+  int gopSize;
+  int gopPreset = param->hevc_enc_params.gop_preset_index;
+
+  // GOP_PRESET_IDX_CUSTOM
+  if (gopPreset == 0)
+  {
+    memcpy(gopParam, &param->hevc_enc_params.custom_gop_params,
+           sizeof(ni_logan_custom_gop_params_t));
+  }
+  else
+  {
+    const int32_t*  src_gop = GOP_PRESET[gopPreset];
+    gopSize = GOP_SIZE[gopPreset];
+    gopParam->custom_gop_size = gopSize;
+    for(i = 0, j = 0; i < gopSize; i++)
+    {
+      gopParam->pic_param[i].pic_type      = src_gop[j++];
+      gopParam->pic_param[i].poc_offset    = src_gop[j++];
+      gopParam->pic_param[i].pic_qp        = src_gop[j++] + param->hevc_enc_params.rc.intra_qp;
+      gopParam->pic_param[i].temporal_id   = src_gop[j++];
+      gopParam->pic_param[i].ref_poc_L0    = src_gop[j++];
+      gopParam->pic_param[i].ref_poc_L1    = src_gop[j++];
+    }
+  }
+}
+
+static int check_low_delay_flag(ni_logan_encoder_params_t *param,
+                                ni_logan_custom_gop_params_t *gopParam)
+{
+  int i;
+  int minVal = 0;
+  int low_delay = 0;
+  int gopPreset = param->hevc_enc_params.gop_preset_index;
+
+  // GOP_PRESET_IDX_CUSTOM
+  if (gopPreset == 0)
+  {
+    if (gopParam->custom_gop_size > 1)
+    {
+      minVal = gopParam->pic_param[0].poc_offset;
+      low_delay = 1;
+      for (i = 1; i < gopParam->custom_gop_size; i++)
+      {
+        if (minVal > gopParam->pic_param[i].poc_offset)
+        {
+          low_delay = 0;
+          break;
+        }
+        else
+        {
+          minVal = gopParam->pic_param[i].poc_offset;
+        }
+      }
+    }
+  }
+  else if (gopPreset == 1 || gopPreset == 2 || gopPreset == 3 ||
+           gopPreset == 6 || gopPreset == 7 || gopPreset == 9)
+  {
+    low_delay = 1;
+  }
+
+  return low_delay;
+}
+
+static int get_num_reorder_of_gop_structure(ni_logan_encoder_params_t *param)
+{
+  int i;
+  int j;
+  int ret_num_reorder = 0;
+  ni_logan_custom_gop_params_t gopParam;
+
+  init_gop_param(&gopParam, param);
+  for(i = 0; i < gopParam.custom_gop_size; i++)
+  {
+    int check_reordering_num = 0;
+    int num_reorder = 0;
+
+    ni_logan_gop_params_t *gopPicParam = &gopParam.pic_param[i];
+
+    for(j = 0; j < gopParam.custom_gop_size; j++)
+    {
+      ni_logan_gop_params_t *gopPicParamCand = &gopParam.pic_param[j];
+      if (gopPicParamCand->poc_offset <= gopPicParam->poc_offset)
+        check_reordering_num = j;
+    }
+
+    for(j = 0; j < check_reordering_num; j++)
+    {
+      ni_logan_gop_params_t *gopPicParamCand = &gopParam.pic_param[j];
+
+      if (gopPicParamCand->temporal_id <= gopPicParam->temporal_id &&
+          gopPicParamCand->poc_offset > gopPicParam->poc_offset)
+        num_reorder++;
+    }
+    ret_num_reorder = num_reorder;
+  }
+  return ret_num_reorder;
+}
+
+static int get_max_dec_pic_buffering_of_gop_structure(ni_logan_encoder_params_t *param)
+{
+  int max_dec_pic_buffering;
+  max_dec_pic_buffering = FFMIN(16/*MAX_NUM_REF*/, FFMAX(get_num_reorder_of_gop_structure(param) + 2, 6 /*maxnumreference in spec*/) + 1);
+  return max_dec_pic_buffering;
+}
+
+static int get_poc_of_gop_structure(ni_logan_encoder_params_t *param,
+                                    uint32_t frame_idx)
+{
+  int low_delay;
+  int gopSize;
+  int poc;
+  int gopIdx;
+  int gopNum;
+  ni_logan_custom_gop_params_t gopParam;
+
+  init_gop_param(&gopParam, param);
+  gopSize = gopParam.custom_gop_size;
+  low_delay = check_low_delay_flag(param, &gopParam);
+
+  if (low_delay)
+  {
+    poc = frame_idx;
+  }
+  else
+  {
+    gopIdx = frame_idx % gopSize;
+    gopNum = frame_idx / gopSize;
+    poc = gopParam.pic_param[gopIdx].poc_offset + (gopSize * gopNum);
+  }
+  //printf("get_poc_of_gop_structure frameIdx=%d, poc=%d, low_delay=%d, gopIdx=%d, gopNum=%d, gopSize=%d \n", frame_idx, poc, low_delay, gopIdx, gopNum, gopSize);
+
+  poc += gopSize - 1; // use gop_size - 1 as offset
+  return poc;
+}
+
+static inline int calc_scale(uint32_t x)
+{
+  static uint8_t lut[16] = {4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0};
+  int y, z = (((x & 0xffff) - 1) >> 27) & 16;
+  x >>= z;
+  z += y = (((x & 0xff) - 1) >> 28) & 8;
+  x >>= y;
+  z += y = (((x & 0xf) - 1) >> 29) & 4;
+  x >>= y;
+  return z + lut[x&0xf];
+}
+
+static inline int clip3(int min, int max, int a)
+{
+  return FFMIN(FFMAX(min, a), max);
+}
+
+static inline int calc_length(uint32_t x)
+{
+  static uint8_t lut[16] = {4, 3, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0};
+  int y, z = (((x >> 16) - 1) >> 27) & 16;
+  x >>= z ^ 16;
+  z += y = ((x - 0x100) >> 28) & 8;
+  x >>= y ^ 8;
+  z += y = ((x - 0x10) >> 29) & 4;
+  x >>= y ^ 4;
+  return z + lut[x];
+}
+
+static uint32_t encode_buffering_period_sei(ni_logan_encoder_params_t *p_param,
+                                            XCoderLoganEncContext *ctx,
+                                            uint32_t frame_idx,
+                                            uint8_t *p_buf)
+{
+  PutBitContext pbc;
+  int32_t payload_bit_size = 0, payload_byte_size = 0, put_bit_byte_size = 0;
+  uint32_t nal_initial_cpb_removal_delay, nal_initial_cpb_removal_offset;
+  int i;
+  uint32_t concatenation_flag = get_poc_of_gop_structure(p_param, frame_idx) == 0 ? 1 : 0;
+  if (ctx->api_ctx.frame_num == 0)
+  {
+    concatenation_flag = 1;
+  }
+  init_put_bits(&pbc, p_buf, NI_LOGAN_MAX_SEI_DATA);
+
+  payload_bit_size += 1; // bp_seq_parameter_set_id=0, 1 bit
+  payload_bit_size += 1; // irap_cpb_params_present_flag=0, 1 bit
+  payload_bit_size += 1; // concatenation_flag, 1 bit
+  // au_cpb_removal_delay_delta_minus1
+  payload_bit_size += (ctx->au_cpb_removal_delay_length_minus1 + 1);
+
+  // nal_hrd_parameters_present_flag=1
+  // CpbCnt = cpb_cnt_minus1[0] + 1 = 0 + 1
+  for (i = 0; i < 1; i++)
+  {
+    // nal_initial_cpb_removal_delay
+    payload_bit_size += ctx->initial_cpb_removal_delay_length_minus1 + 1;
+    // nal_initial_cpb_removal_offset
+    payload_bit_size += ctx->initial_cpb_removal_delay_length_minus1 + 1;
+  }
+
+  // vcl_hrd_parameters_present_flag=0
+
+  put_bits32(&pbc, 1); // NAL start code
+  // NAL unit header nal_unit_type=39, layer_id=0, temporal_id_plus1=1
+  put_bits(&pbc, 16, (39 << 9) | (0 << 3) | 1);
+  put_bits(&pbc, 8, 0); // payload_type=0 (buffering_period)
+  payload_byte_size = (payload_bit_size + 7) / 8;
+  put_bits(&pbc, 8, payload_byte_size);// payload size (byte)
+
+  // buffering period
+  set_ue_golomb_long(&pbc, 0); // bp_seq_parameter_set_id=0
+  put_bits(&pbc, 1, 0); // irap_cpb_params_present_flag=0
+  put_bits(&pbc, 1, concatenation_flag); // concatenation_flag
+  // au_cpb_removal_delay_delta_minus1=0
+  put_bits(&pbc, ctx->au_cpb_removal_delay_length_minus1 + 1, 0);
+
+  nal_initial_cpb_removal_delay =
+  (uint32_t)(90000 * ctx->cpb_size_unscale / ctx->bit_rate_unscale);
+  nal_initial_cpb_removal_offset =
+  (uint32_t)((90000 * ctx->cpb_size_unscale / ctx->bit_rate_unscale) -
+             nal_initial_cpb_removal_delay);
+
+  // nal_hrd_parameters_present_flag=1
+  // CpbCnt = cpb_cnt_minus1[0] + 1 = 0 + 1
+  for (i = 0; i < 1; i++)
+  {
+    // nal_initial_cpb_removal_delay
+    put_bits(&pbc, ctx->initial_cpb_removal_delay_length_minus1 + 1,
+             nal_initial_cpb_removal_delay);
+    // nal_initial_cpb_removal_offset
+    put_bits(&pbc, ctx->initial_cpb_removal_delay_length_minus1 + 1,
+             nal_initial_cpb_removal_offset);
+  }
+
+  // vcl_hrd_parameters_present_flag=0
+
+  if (payload_bit_size % 8)
+  {
+    // fill in bit 1 and padding 0s for byte alignment
+    put_bits(&pbc, 1, 1/*payload_bit_equal_to_one*/);
+    for (i = 0; i < 8 - (payload_bit_size % 8) - 1; i++)
+    {
+      put_bits(&pbc, 1, 0/*payload_bit_equal_to_zero*/);
+    }
+  }
+
+  // rbsp trailing stop bit and alignment padding 0s
+  put_bits(&pbc, 8, 0x80);
+
+  flush_put_bits(&pbc);
+  put_bit_byte_size = (put_bits_count(&pbc) + 7) / 8;
+
+  // emulation prevention checking of payload, skipping start code (4B) +
+  // NAL header (2B) + payload type (1B) + payload size (1B) = 8B
+  put_bit_byte_size += ni_logan_insert_emulation_prevent_bytes(
+    p_buf + 8, put_bit_byte_size - 8);
+
+  return put_bit_byte_size;
+}
+
+static uint32_t encode_pic_timing_sei2(ni_logan_encoder_params_t *p_param,
+                                       XCoderLoganEncContext *ctx,
+                                       uint8_t *p_buf, int is_i_or_idr,
+                                       int is_idr, uint32_t frame_idx)
+{
+  PutBitContext pbc;
+  int32_t payload_bit_size = 0, payload_byte_size = 0, put_bit_byte_size = 0;
+  uint32_t pic_dpb_output_delay = 0;
+  int num_reorder_pic;
+  uint64_t poc_pic;
+
+  init_put_bits(&pbc, p_buf, NI_LOGAN_MAX_SEI_DATA);
+
+  // frame_field_info_present_flag=0 TBD
+  //payload_bit_size += 4/*pic_struct*/ + 2/*source_scan_type*/ + 1/*duplicate_flag*/;
+
+  // CpbDpbDelaysPresentFlag=1
+  // au_cpb_removal_delay_length_minus1
+  payload_bit_size += (ctx->au_cpb_removal_delay_length_minus1 + 1);
+  // pic_dpb_output_delay
+  payload_bit_size += (ctx->dpb_output_delay_length_minus1+ 1);
+
+  // sub_pic_hrd_params_present_flag=0
+
+  put_bits32(&pbc, 1); // NAL start code
+  // NAL unit header nal_unit_type=39, layer_id=0, temporal_id_plus1=1
+  put_bits(&pbc, 16, (39 << 9) | (0 << 3) | 1);
+  put_bits(&pbc, 8, 1); // payload_type=1 (picture_timing)
+  payload_byte_size = (payload_bit_size + 7) / 8;
+  put_bits(&pbc, 8, payload_byte_size);// payload size (byte)
+
+  // pic timing
+
+  num_reorder_pic = get_num_reorder_of_gop_structure(p_param);
+  poc_pic = get_poc_of_gop_structure(p_param, frame_idx);
+  pic_dpb_output_delay = num_reorder_pic + poc_pic - frame_idx;
+
+  //printf(" ----> num_reorder_pic %d + poc_pic %llu - frame_idx %u\n", num_reorder_pic,
+  //poc_pic, frame_idx);
+  //printf(" ----> #%u %s au_cpb_removal_delay_minus1 %u  pic_dpb_output_delay %u\n", frame_idx, is_idr ? "is_idr" : " ", ctx->au_cpb_removal_delay_minus1, pic_dpb_output_delay);
+
+  // CpbDpbDelaysPresentFlag=1
+  // au_cpb_removal_delay_length_minus1
+  put_bits(&pbc, ctx->au_cpb_removal_delay_length_minus1 + 1,
+           ctx->au_cpb_removal_delay_minus1);
+  ctx->au_cpb_removal_delay_minus1++;
+
+  if (1 == p_param->hevc_enc_params.gop_preset_index &&
+      p_param->hevc_enc_params.intra_period)
+  {
+    if (0 == frame_idx || is_idr ||
+        0 == (ctx->au_cpb_removal_delay_minus1 % p_param->hevc_enc_params.intra_period))
+    {
+      ctx->au_cpb_removal_delay_minus1 = 0;
+    }
+  }
+  else if (is_i_or_idr)
+  {
+    ctx->au_cpb_removal_delay_minus1 = 0;
+  }
+
+  // pic_dpb_output_delay
+  put_bits(&pbc, ctx->dpb_output_delay_length_minus1 + 1, pic_dpb_output_delay);
+
+  if (payload_bit_size & 7)
+  {
+    put_bits(&pbc, 1, 1/*payload_bit_equal_to_one*/);
+    put_bits(&pbc, (8 - (payload_bit_size & 7)-1), 0/*payload_bit_equal_to_zero*/);
+  }
+
+  // rbsp trailing stop bit and alignment padding 0s
+  put_bits(&pbc, 8, 0x80);
+
+  flush_put_bits(&pbc);
+  put_bit_byte_size = (put_bits_count(&pbc) + 7) / 8;
+
+  // emulation prevention checking of payload, skipping start code (4B) +
+  // NAL header (2B) + payload type (1B) + payload size (1B) = 8B
+  put_bit_byte_size += ni_logan_insert_emulation_prevent_bytes(
+    p_buf + 8, put_bit_byte_size - 8);
+
+  return put_bit_byte_size;
+}
+
+#define SAMPLE_SPS_MAX_SUB_LAYERS_MINUS1 0
+#define MAX_VPS_MAX_SUB_LAYERS 16
+#define MAX_CPB_COUNT 16
+#define MAX_DURATION 0.5
+
+static void set_vui(AVCodecContext *avctx, ni_logan_encoder_params_t *p_param,
+                    XCoderLoganEncContext *ctx,
+                    enum AVColorPrimaries color_primaries,
+                    enum AVColorTransferCharacteristic color_trc,
+                    enum AVColorSpace color_space,
+                    int video_full_range_flag)
+{
+  int isHEVC = (AV_CODEC_ID_HEVC == avctx->codec_id ? 1 : 0);
+  PutBitContext pbcPutBitContext;
+  unsigned int aspect_ratio_idc = 255; // default: extended_sar
+  int nal_hrd_parameters_present_flag=1, vcl_hrd_parameters_present_flag=0;
+  int layer, cpb;
+  int maxcpboutputdelay;
+  int maxdpboutputdelay;
+  int maxdelay;
+  uint32_t vbvbuffersize = (p_param->bitrate / 1000) * p_param->hevc_enc_params.rc.rc_init_delay;
+  uint32_t vbvmaxbitrate = p_param->bitrate;
+  uint32_t vps_max_sub_layers_minus1 = SAMPLE_SPS_MAX_SUB_LAYERS_MINUS1;
+  uint32_t bit_rate_value_minus1[MAX_CPB_COUNT][MAX_VPS_MAX_SUB_LAYERS];
+  uint32_t cpb_size_value_minus1[MAX_CPB_COUNT][MAX_VPS_MAX_SUB_LAYERS];
+  uint32_t cpb_cnt_minus1[MAX_VPS_MAX_SUB_LAYERS];
+
+  uint32_t fixed_pic_rate_general_flag[MAX_VPS_MAX_SUB_LAYERS];
+  uint32_t fixed_pic_rate_within_cvs_flag[MAX_VPS_MAX_SUB_LAYERS];
+  uint32_t elemental_duration_in_tc_minus1[MAX_VPS_MAX_SUB_LAYERS];
+
+  uint32_t bit_rate_scale = 2;
+  uint32_t cpb_size_scale = 5;
+  uint32_t numUnitsInTick = 1000;
+  uint32_t timeScale;
+  int32_t i32frameRateInfo = p_param->hevc_enc_params.frame_rate;
+
+  init_put_bits(&pbcPutBitContext, p_param->ui8VuiRbsp, NI_LOGAN_MAX_VUI_SIZE);
+
+  if (avctx->sample_aspect_ratio.num==0)
+  {
+    // sample aspect ratio is 0, don't include aspect_ratio_idc in vui
+    put_bits(&pbcPutBitContext, 1, 0);  //  aspect_ratio_info_present_flag=0
+  }
+  else
+  {
+    // sample aspect ratio is non-zero, include aspect_ratio_idc in vui
+    put_bits(&pbcPutBitContext, 1, 1);  //  aspect_ratio_info_present_flag=1
+
+    if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(1, 1)))
+    {
+      aspect_ratio_idc = 1;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(12, 11)))
+    {
+      aspect_ratio_idc = 2;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(10, 11)))
+    {
+      aspect_ratio_idc = 3;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(16, 11)))
+    {
+      aspect_ratio_idc = 4;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(40, 33)))
+    {
+      aspect_ratio_idc = 5;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(24, 11)))
+    {
+      aspect_ratio_idc = 6;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(20, 11)))
+    {
+      aspect_ratio_idc = 7;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(32, 11)))
+    {
+      aspect_ratio_idc = 8;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(80, 33)))
+    {
+      aspect_ratio_idc = 9;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(18, 11)))
+    {
+      aspect_ratio_idc = 10;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(15, 11)))
+    {
+      aspect_ratio_idc = 11;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(64, 33)))
+    {
+      aspect_ratio_idc = 12;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(160, 99)))
+    {
+      aspect_ratio_idc = 13;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(4, 3)))
+    {
+      aspect_ratio_idc = 14;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(3, 2)))
+    {
+      aspect_ratio_idc = 15;
+    }
+    else if (! av_cmp_q(avctx->sample_aspect_ratio, av_make_q(2, 1)))
+    {
+      aspect_ratio_idc = 16;
+    }
+
+    put_bits(&pbcPutBitContext, 8, aspect_ratio_idc);  // aspect_ratio_idc
+    if (255 == aspect_ratio_idc)
+    {
+      put_bits(&pbcPutBitContext, 16, avctx->sample_aspect_ratio.num);//sar_width
+      put_bits(&pbcPutBitContext, 16, avctx->sample_aspect_ratio.den);//sar_height
+    }
+  }
+
+  put_bits(&pbcPutBitContext, 1, 0);  //  overscan_info_present_flag=0
+
+  // VUI Parameters
+  put_bits(&pbcPutBitContext, 1, 1);  //  video_signal_type_present_flag=1
+  put_bits(&pbcPutBitContext, 3, 5);  //  video_format=5 (unspecified)
+  put_bits(&pbcPutBitContext, 1, video_full_range_flag);
+  put_bits(&pbcPutBitContext, 1, 1);  //  colour_description_presenty_flag=1
+  put_bits(&pbcPutBitContext, 8, color_primaries);
+  put_bits(&pbcPutBitContext, 8, color_trc);
+  put_bits(&pbcPutBitContext, 8, color_space);
+
+  put_bits(&pbcPutBitContext, 1, 0);      //  chroma_loc_info_present_flag=0
+
+  if (isHEVC)
+  {   // H.265 Only VUI parameters
+    put_bits(&pbcPutBitContext, 1, 0);  //  neutral_chroma_indication_flag=0
+    put_bits(&pbcPutBitContext, 1, 0);  //  field_seq_flag=0
+    put_bits(&pbcPutBitContext, 1, 0);  //  frame_field_info_present_flag=0
+    put_bits(&pbcPutBitContext, 1, 0);  //  default_display_window_flag=0
+  }
+
+  put_bits(&pbcPutBitContext, 1, 1);      //  vui_timing_info_present_flag=1
+  p_param->pos_num_units_in_tick = put_bits_count(&pbcPutBitContext);
+  put_bits32(&pbcPutBitContext, 0);    //  vui_num_units_in_tick
+  p_param->pos_time_scale = put_bits_count(&pbcPutBitContext);
+  put_bits32(&pbcPutBitContext, 0);         //  vui_time_scale
+
+  if (isHEVC)
+  {
+    // H.265 Only VUI parameters
+    put_bits(&pbcPutBitContext, 1, 0);  //  vui_poc_proportional_to_timing_flag=0
+    if (! p_param->hrd_enable)
+    {
+      put_bits(&pbcPutBitContext, 1, 0);  //  vui_hrd_parameters_present_flag=0
+    }
+    else
+    {
+      put_bits(&pbcPutBitContext, 1, 1);  //  vui_hrd_parameters_present_flag=1
+
+      put_bits(&pbcPutBitContext, 1, 1); // nal_hrd_parameters_present_flag=1
+      put_bits(&pbcPutBitContext, 1, 0); // vcl_hrd_parameters_present_flag=0
+
+      put_bits(&pbcPutBitContext, 1, 0); // sub_pic_hrd_params_present_flag=0
+
+      ctx->initial_cpb_removal_delay_length_minus1 = 23;
+      ctx->au_cpb_removal_delay_length_minus1 = 23;
+
+      bit_rate_value_minus1[0][0] = 59374;
+      cpb_size_value_minus1[0][0] = 59374;
+      cpb_cnt_minus1[0] = 0;
+      fixed_pic_rate_general_flag[0] = 1;
+      fixed_pic_rate_within_cvs_flag[0] = 1;
+      elemental_duration_in_tc_minus1[0] = 0;
+
+      // normalize hrd size and rate to the value / scale notation
+      bit_rate_scale = clip3(0, 15, calc_scale(vbvmaxbitrate) - BR_SHIFT);
+      bit_rate_value_minus1[0][0] = (vbvmaxbitrate >> (bit_rate_scale + BR_SHIFT)) - 1;
+
+      cpb_size_scale = clip3(0, 15, calc_scale(vbvbuffersize) - CPB_SHIFT);
+      cpb_size_value_minus1[0][0] = (vbvbuffersize >> (cpb_size_scale + CPB_SHIFT)) - 1;
+
+      ctx->bit_rate_unscale = (bit_rate_value_minus1[0][0]+1) << (bit_rate_scale + BR_SHIFT);
+      ctx->cpb_size_unscale = (cpb_size_value_minus1[0][0]+1) << (cpb_size_scale + CPB_SHIFT);
+
+      if (p_param->fps_denominator != 0 &&
+          (p_param->fps_number % p_param->fps_denominator) != 0)
+      {
+        numUnitsInTick += 1;
+        i32frameRateInfo += 1;
+      }
+      timeScale = i32frameRateInfo * 1000;
+
+      maxcpboutputdelay = (int)(FFMIN(p_param->hevc_enc_params.intra_period * MAX_DURATION * timeScale / numUnitsInTick, INT_MAX));
+      maxdpboutputdelay = (int)(get_max_dec_pic_buffering_of_gop_structure(p_param) * MAX_DURATION * timeScale / numUnitsInTick);
+      maxdelay = (int)(90000.0 * ctx->cpb_size_unscale / ctx->bit_rate_unscale + 0.5);
+
+      ctx->initial_cpb_removal_delay_length_minus1 =
+      2 + clip3(4, 22, 32 - calc_length(maxdelay)) - 1;
+      ctx->au_cpb_removal_delay_length_minus1 =
+      clip3(4, 31, 32 - calc_length(maxcpboutputdelay)) - 1;
+      ctx->dpb_output_delay_length_minus1 =
+      clip3(4, 31, 32 - calc_length(maxdpboutputdelay)) - 1;
+
+      put_bits(&pbcPutBitContext, 4, bit_rate_scale); // bit_rate_scale
+      put_bits(&pbcPutBitContext, 4, cpb_size_scale); // cpb_size_scale
+
+      put_bits(&pbcPutBitContext, 5, ctx->initial_cpb_removal_delay_length_minus1);
+      put_bits(&pbcPutBitContext, 5, ctx->au_cpb_removal_delay_length_minus1);
+      put_bits(&pbcPutBitContext, 5, ctx->dpb_output_delay_length_minus1);
+
+      for (layer = 0; layer <= (int32_t)vps_max_sub_layers_minus1; layer++)
+      {
+        put_bits(&pbcPutBitContext, 1, fixed_pic_rate_general_flag[layer]);
+
+        if (! fixed_pic_rate_general_flag[layer])
+        {
+          put_bits(&pbcPutBitContext, 1, fixed_pic_rate_within_cvs_flag[layer]);
+        }
+
+        if (fixed_pic_rate_within_cvs_flag[layer])
+        {
+          set_ue_golomb_long(&pbcPutBitContext,
+                             elemental_duration_in_tc_minus1[layer]);
+        }
+
+        // low_delay_hrd_flag[layer] is not present and inferred to be 0
+
+        set_ue_golomb_long(&pbcPutBitContext, cpb_cnt_minus1[layer]);
+
+        if ((layer == 0 && nal_hrd_parameters_present_flag) ||
+            (layer == 1 && vcl_hrd_parameters_present_flag))
+        {
+          for(cpb = 0; cpb <= (int32_t)cpb_cnt_minus1[layer]; cpb++)
+          {
+            set_ue_golomb_long(&pbcPutBitContext,
+                               bit_rate_value_minus1[cpb][layer]);
+
+            set_ue_golomb_long(&pbcPutBitContext,
+                               cpb_size_value_minus1[cpb][layer]);
+
+            // cbr_flag is inferred to be 0 as well ?
+            put_bits(&pbcPutBitContext, 1, 0/*cbr_flag[cpb][layer]*/);
+          }
+        }
+      }
+    }
+    put_bits(&pbcPutBitContext, 1, 0);      //  bitstream_restriction_flag=0
+  }
+  else
+  {
+    int max_num_reorder_frames;
+    int num_ref_frames;
+    int max_dec_frame_buffering;
+    // H.264 Only VUI parameters
+    if (p_param->enable_vfr)
+    {
+      put_bits(&pbcPutBitContext, 1, 0);  //  fixed_frame_rate_flag=0
+    }
+    else
+    {
+      put_bits(&pbcPutBitContext, 1, 1);  //  fixed_frame_rate_flag=1
+    }
+    put_bits(&pbcPutBitContext, 1, 0);  //  nal_hrd_parameters_present_flag=0
+    put_bits(&pbcPutBitContext, 1, 0);  //  vui_hrd_parameters_present_flag=0
+    put_bits(&pbcPutBitContext, 1, 0);  //  pic_struct_present_flag=0
+
+    // this flag is set to 1 for H.264 to reduce decode delay, and fill in
+    // the rest of the section accordingly
+    put_bits(&pbcPutBitContext, 1, 1);  //  bitstream_restriction_flag=1
+    put_bits(&pbcPutBitContext, 1, 1);  //  motion_vectors_over_pic_boundaries_flag=1
+    set_ue_golomb_long(&pbcPutBitContext, 2); // max_bytes_per_pic_denom=2 (default)
+    set_ue_golomb_long(&pbcPutBitContext, 1); // max_bits_per_mb_denom=1 (default)
+    set_ue_golomb_long(&pbcPutBitContext, 15); // log2_max_mv_length_horizontal=15 (default)
+    set_ue_golomb_long(&pbcPutBitContext, 15); // log2_max_mv_length_vertical=15 (default)
+
+    // max_num_reorder_frames (0 for low delay gops)
+    max_num_reorder_frames = ni_logan_get_num_reorder_of_gop_structure(p_param);
+    set_ue_golomb_long(&pbcPutBitContext, max_num_reorder_frames);
+    // max_dec_frame_buffering
+    num_ref_frames = ni_logan_get_num_ref_frame_of_gop_structure(p_param);
+    max_dec_frame_buffering = (num_ref_frames > max_num_reorder_frames ?
+                               num_ref_frames : max_num_reorder_frames);
+    set_ue_golomb_long(&pbcPutBitContext, max_dec_frame_buffering);
+  }
+
+  p_param->ui32VuiDataSizeBits = put_bits_count(&pbcPutBitContext);
+  p_param->ui32VuiDataSizeBytes = (p_param->ui32VuiDataSizeBits + 7) / 8;
+  flush_put_bits(&pbcPutBitContext);      // flush bits
+}
+
+static int do_open_encoder_device(AVCodecContext *avctx,
+                                  XCoderLoganEncContext *ctx,
+                                  ni_logan_encoder_params_t *p_param)
+{
+  int ret;
+  int frame_width;
+  int frame_height;
+  int linesize_aligned;
+  int height_aligned;
+  int video_full_range_flag = 0;
+  AVFrame *in_frame = &ctx->buffered_fme;
+  NILOGANFramesContext *nif_src_ctx;
+  AVHWFramesContext *avhwf_ctx;
+  enum AVColorPrimaries color_primaries;
+  enum AVColorTransferCharacteristic color_trc;
+  enum AVColorSpace color_space;
+
+  if (in_frame->width > 0 && in_frame->height > 0)
+  {
+    frame_width = ODD2EVEN(in_frame->width);
+    frame_height = ODD2EVEN(in_frame->height);
+    color_primaries = in_frame->color_primaries;
+    color_trc = in_frame->color_trc;
+    color_space = in_frame->colorspace;
+    // Force frame color metrics if specified in command line
+    if (in_frame->color_primaries != avctx->color_primaries &&
+        avctx->color_primaries != AVCOL_PRI_UNSPECIFIED)
+    {
+      color_primaries = avctx->color_primaries;
+    }
+    if (in_frame->color_trc != avctx->color_trc &&
+        avctx->color_trc != AVCOL_TRC_UNSPECIFIED)
+    {
+      color_trc = avctx->color_trc;
+    }
+    if (in_frame->colorspace != avctx->colorspace &&
+        avctx->colorspace != AVCOL_SPC_UNSPECIFIED)
+    {
+      color_space = avctx->colorspace;
+    }
+  }
+  else
+  {
+    frame_width = ODD2EVEN(avctx->width);
+    frame_height = ODD2EVEN(avctx->height);
+    color_primaries = avctx->color_primaries;
+    color_trc = avctx->color_trc;
+    color_space = avctx->colorspace;
+  }
+
+  // if frame stride size is not as we expect it,
+  // adjust using xcoder-params conf_win_right
+  linesize_aligned = ((frame_width + 7) / 8) * 8;
+  if (avctx->codec_id == AV_CODEC_ID_H264)
+  {
+    linesize_aligned = ((frame_width + 15) / 16) * 16;
+  }
+
+  if (linesize_aligned < NI_LOGAN_MIN_WIDTH)
+  {
+    p_param->hevc_enc_params.conf_win_right += NI_LOGAN_MIN_WIDTH - frame_width;
+    linesize_aligned = NI_LOGAN_MIN_WIDTH;
+  }
+  else if (linesize_aligned > frame_width)
+  {
+    p_param->hevc_enc_params.conf_win_right += linesize_aligned - frame_width;
+  }
+  p_param->source_width = linesize_aligned;
+
+  height_aligned = ((frame_height + 7) / 8) * 8;
+  if (avctx->codec_id == AV_CODEC_ID_H264)
+  {
+    height_aligned = ((frame_height + 15) / 16) * 16;
+  }
+
+  if (height_aligned < NI_LOGAN_MIN_HEIGHT)
+  {
+    p_param->hevc_enc_params.conf_win_bottom += NI_LOGAN_MIN_HEIGHT - frame_height;
+    p_param->source_height = NI_LOGAN_MIN_HEIGHT;
+    height_aligned = NI_LOGAN_MIN_HEIGHT;
+  }
+  else if (height_aligned > frame_height)
+  {
+    p_param->hevc_enc_params.conf_win_bottom += height_aligned - frame_height;
+    p_param->source_height = height_aligned;
+  }
+
+  // DolbyVision support
+  if (5 == p_param->dolby_vision_profile &&
+      AV_CODEC_ID_HEVC == avctx->codec_id)
+  {
+    color_primaries = color_trc = color_space = 2;
+    video_full_range_flag = 1;
+  }
+
+  // According to the pixel format or color range from the incoming video
+  if (avctx->color_range == AVCOL_RANGE_JPEG ||
+      avctx->pix_fmt == AV_PIX_FMT_YUVJ420P)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "%s set video_full_range_flag\n", __FUNCTION__);
+    video_full_range_flag = 1;
+  }
+
+  // HDR HLG support
+  if ((5 == p_param->dolby_vision_profile &&
+       AV_CODEC_ID_HEVC == avctx->codec_id) ||
+      color_primaries == AVCOL_PRI_BT2020 ||
+      color_trc == AVCOL_TRC_SMPTE2084 ||
+      color_trc == AVCOL_TRC_ARIB_STD_B67 ||
+      color_space == AVCOL_SPC_BT2020_NCL ||
+      color_space == AVCOL_SPC_BT2020_CL)
+  {
+    p_param->hdrEnableVUI = 1;
+    set_vui(avctx, p_param, ctx,
+           color_primaries, color_trc, color_space, video_full_range_flag);
+    av_log(avctx, AV_LOG_VERBOSE, "XCoder HDR color info color_primaries: %d "
+           "color_trc: %d color_space %d video_full_range_flag %d sar %d/%d\n",
+           color_primaries, color_trc, color_space, video_full_range_flag,
+           avctx->sample_aspect_ratio.num, avctx->sample_aspect_ratio.den);
+  }
+  else
+  {
+    p_param->hdrEnableVUI = 0;
+    set_vui(avctx, p_param, ctx,
+            color_primaries, color_trc, color_space, video_full_range_flag);
+  }
+
+  ctx->api_ctx.hw_id = ctx->dev_enc_idx;
+  strcpy(ctx->api_ctx.dev_xcoder, ctx->dev_xcoder);
+
+  if (in_frame->width > 0 && in_frame->height > 0)
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "XCoder buffered_fme.linesize: %d/%d/%d "
+           "width/height %dx%d conf_win_right %d  conf_win_bottom %d , "
+           "color primaries %u trc %u space %u\n",
+           in_frame->linesize[0], in_frame->linesize[1], in_frame->linesize[2],
+           in_frame->width, in_frame->height,
+           p_param->hevc_enc_params.conf_win_right,
+           p_param->hevc_enc_params.conf_win_bottom,
+           color_primaries, color_trc, color_space);
+
+    if (in_frame->format == AV_PIX_FMT_NI_LOGAN)
+    {
+      ni_logan_hwframe_surface_t *surface = (ni_logan_hwframe_surface_t *)in_frame->data[3];
+#ifdef _WIN32
+      int64_t handle = (((int64_t) surface->device_handle_ext) << 32) | surface->device_handle;
+      ctx->api_ctx.sender_handle = (ni_device_handle_t) handle;
+#else
+      ctx->api_ctx.sender_handle = (ni_device_handle_t) surface->device_handle;
+#endif
+      ctx->api_ctx.hw_action = NI_LOGAN_CODEC_HW_ENABLE;
+      av_log(avctx, AV_LOG_VERBOSE, "XCoder frame sender_handle:%p, hw_id:%d\n",
+             (void *) ctx->api_ctx.sender_handle, ctx->api_ctx.hw_id);
+    }
+
+    if (in_frame->hw_frames_ctx && ctx->api_ctx.hw_id == -1)
+    {
+      avhwf_ctx = (AVHWFramesContext*) in_frame->hw_frames_ctx->data;
+      nif_src_ctx = avhwf_ctx->internal->priv;
+      ctx->api_ctx.hw_id = nif_src_ctx->api_ctx.hw_id;
+      av_log(avctx, AV_LOG_VERBOSE, "%s: hw_id -1 collocated to %d \n",
+             __FUNCTION__, ctx->api_ctx.hw_id);
+    }
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "XCoder frame width/height %dx%d conf_win_right"
+           " %d  conf_win_bottom %d color primaries %u trc %u space %u\n",
+           avctx->width, avctx->height, p_param->hevc_enc_params.conf_win_right,
+           p_param->hevc_enc_params.conf_win_bottom, avctx->color_primaries,
+           avctx->color_trc, avctx->colorspace);
+  }
+
+  ret = ni_logan_device_session_open(&ctx->api_ctx, NI_LOGAN_DEVICE_TYPE_ENCODER);
+  // As the file handle may change we need to assign back
+  ctx->dev_xcoder_name = ctx->api_ctx.dev_xcoder_name;
+  ctx->blk_xcoder_name = ctx->api_ctx.blk_xcoder_name;
+  ctx->dev_enc_idx = ctx->api_ctx.hw_id;
+
+  if (ret == NI_LOGAN_RETCODE_INVALID_PARAM)
+  {
+    av_log(avctx, AV_LOG_ERROR, "%s\n", ctx->api_ctx.param_err_msg);
+  }
+  if (ret != 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Failed to open encoder (status = %d), "
+           "critical error or resource unavailable\n", ret);
+    ret = AVERROR_EXTERNAL;
+    // ff_xcoder_logan_encode_close(avctx); will be called at codec close
+    return ret;
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "XCoder %s Index %d (inst: %d) opened "
+           "successfully\n", ctx->dev_xcoder_name, ctx->dev_enc_idx,
+           ctx->api_ctx.session_id);
+  }
+
+  return ret;
+}
+
+static void do_close_encoder_device(XCoderLoganEncContext *ctx)
+{
+  ni_logan_device_session_close(&ctx->api_ctx, ctx->encoder_eof,
+                          NI_LOGAN_DEVICE_TYPE_ENCODER);
+#ifdef _WIN32
+  ni_logan_device_close(ctx->api_ctx.device_handle);
+#elif __linux__
+  ni_logan_device_close(ctx->api_ctx.device_handle);
+  ni_logan_device_close(ctx->api_ctx.blk_io_handle);
+#endif
+  ctx->api_ctx.device_handle = NI_INVALID_DEVICE_HANDLE;
+  ctx->api_ctx.blk_io_handle = NI_INVALID_DEVICE_HANDLE;
+  ctx->api_ctx.auto_dl_handle = NI_INVALID_DEVICE_HANDLE;
+  ctx->api_ctx.sender_handle = NI_INVALID_DEVICE_HANDLE;
+}
+
+static int xcoder_logan_encoder_headers(AVCodecContext *avctx)
+{
+  // use a copy of encoder context, take care to restore original config
+  // cropping setting
+  int ret, recv, orig_conf_win_right, orig_conf_win_bottom;
+  ni_logan_packet_t *xpkt;
+  XCoderLoganEncContext ctx;
+  XCoderLoganEncContext *s = avctx->priv_data;
+  ni_logan_encoder_params_t *p_param = &s->api_param;
+
+  memcpy(&ctx, avctx->priv_data, sizeof(XCoderLoganEncContext));
+
+  orig_conf_win_right = p_param->hevc_enc_params.conf_win_right;
+  orig_conf_win_bottom = p_param->hevc_enc_params.conf_win_bottom;
+
+  ret = do_open_encoder_device(avctx, &ctx, p_param);
+  if (ret < 0)
+  {
+    return ret;
+  }
+
+  xpkt = &ctx.api_pkt.data.packet;
+  ni_logan_packet_buffer_alloc(xpkt, NI_LOGAN_MAX_TX_SZ);
+
+  for (; ;)
+  {
+    recv = ni_logan_device_session_read(&ctx.api_ctx, &(ctx.api_pkt),
+                                  NI_LOGAN_DEVICE_TYPE_ENCODER);
+
+    if (recv > 0)
+    {
+      free(avctx->extradata);
+      avctx->extradata_size = recv - NI_LOGAN_FW_ENC_BITSTREAM_META_DATA_SIZE;
+      avctx->extradata = av_mallocz(avctx->extradata_size +
+                                    AV_INPUT_BUFFER_PADDING_SIZE);
+      memcpy(avctx->extradata,
+             (uint8_t*)xpkt->p_data + NI_LOGAN_FW_ENC_BITSTREAM_META_DATA_SIZE,
+             avctx->extradata_size);
+
+      av_log(avctx, AV_LOG_VERBOSE, "%s len: %d\n",
+             __FUNCTION__, avctx->extradata_size);
+      break;
+    }
+    else if (recv == NI_LOGAN_RETCODE_SUCCESS)
+    {
+      continue;
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_ERROR, "%s error: %d", __FUNCTION__, recv);
+      break;
+    }
+  }
+
+  do_close_encoder_device(&ctx);
+
+  ni_logan_packet_buffer_free(&ctx.api_pkt.data.packet);
+  ni_logan_rsrc_free_device_context(ctx.rsrc_ctx);
+  ctx.rsrc_ctx = NULL;
+
+  p_param->hevc_enc_params.conf_win_right = orig_conf_win_right;
+  p_param->hevc_enc_params.conf_win_bottom = orig_conf_win_bottom;
+
+  return (recv < 0 ? recv : ret);
+}
+
+static int xcoder_logan_setup_encoder(AVCodecContext *avctx)
+{
+  XCoderLoganEncContext *s = avctx->priv_data;
+  int i, ret = 0;
+  ni_logan_encoder_params_t *p_param = &s->api_param;
+  ni_logan_encoder_params_t *pparams = NULL;
+  ni_logan_session_run_state_t prev_state = s->api_ctx.session_run_state;
+
+  av_log(avctx, AV_LOG_DEBUG, "%s\n", __FUNCTION__);
+  //s->api_ctx.session_id = NI_LOGAN_INVALID_SESSION_ID;
+  ni_logan_device_session_context_init(&(s->api_ctx));
+  s->api_ctx.session_run_state = prev_state;
+
+  s->api_ctx.codec_format = NI_LOGAN_CODEC_FORMAT_H264;
+  if (avctx->codec_id == AV_CODEC_ID_HEVC)
+  {
+    s->api_ctx.codec_format = NI_LOGAN_CODEC_FORMAT_H265;
+  }
+
+  s->firstPktArrived = 0;
+  s->spsPpsArrived = 0;
+  s->spsPpsHdrLen = 0;
+  s->p_spsPpsHdr = NULL;
+  s->xcode_load_pixel = 0;
+  s->reconfigCount = 0;
+  s->gotPacket = 0;
+  s->sentFrame = 0;
+  s->latest_dts = 0;
+
+  if (! s->vpu_reset &&
+      LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING != s->api_ctx.session_run_state)
+  {
+    av_log(avctx, AV_LOG_INFO, "Session state: %d allocate frame fifo.\n",
+           s->api_ctx.session_run_state);
+    // FIFO 4 * FPS length of frames
+    s->fme_fifo_capacity = 4 * avctx->time_base.den / (avctx->time_base.num * avctx->ticks_per_frame);
+    s->fme_fifo = av_fifo_alloc(s->fme_fifo_capacity * sizeof(AVFrame));
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_INFO, "Session seq change, fifo size: %" PRIu64 "\n",
+           av_fifo_size(s->fme_fifo) / sizeof(AVFrame));
+  }
+
+  if (! s->fme_fifo)
+  {
+    return AVERROR(ENOMEM);
+  }
+  s->eos_fme_received = 0;
+
+  //Xcoder User Configuration
+  ret = ni_logan_encoder_init_default_params(p_param, avctx->framerate.num, avctx->framerate.den,
+                                       avctx->bit_rate, ODD2EVEN(avctx->width),
+                                       ODD2EVEN(avctx->height));
+  if (ret == NI_LOGAN_RETCODE_PARAM_ERROR_WIDTH_TOO_BIG)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Width: too big\n");
+    return AVERROR_EXTERNAL;
+  }
+  if (ret == NI_LOGAN_RETCODE_PARAM_ERROR_WIDTH_TOO_SMALL)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Width: too small\n");
+    return AVERROR_EXTERNAL;
+  }
+  if (ret == NI_LOGAN_RETCODE_PARAM_ERROR_HEIGHT_TOO_BIG)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Height: too big\n");
+    return AVERROR_EXTERNAL;
+  }
+  if (ret == NI_LOGAN_RETCODE_PARAM_ERROR_HEIGHT_TOO_SMALL)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Height: too small\n");
+    return AVERROR_EXTERNAL;
+  }
+  if (ret == NI_LOGAN_RETCODE_PARAM_ERROR_AREA_TOO_BIG)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Invalid Picture Width x Height: exceeds %d\n",
+           NI_LOGAN_MAX_RESOLUTION_AREA);
+    return AVERROR_EXTERNAL;
+  }
+  if (ret < 0)
+  {
+    int i;
+    av_log(avctx, AV_LOG_ERROR, "Error setting preset or log.\n");
+    av_log(avctx, AV_LOG_INFO, "Possible presets:");
+    for (i = 0; g_logan_xcoder_preset_names[i]; i++)
+      av_log(avctx, AV_LOG_INFO, " %s", g_logan_xcoder_preset_names[i]);
+    av_log(avctx, AV_LOG_INFO, "\n");
+
+    av_log(avctx, AV_LOG_INFO, "Possible log:");
+    for (i = 0; g_logan_xcoder_log_names[i]; i++)
+      av_log(avctx, AV_LOG_INFO, " %s", g_logan_xcoder_log_names[i]);
+    av_log(avctx, AV_LOG_INFO, "\n");
+
+    return AVERROR(EINVAL);
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "pix_fmt is %d, sw_pix_fmt is %d\n",
+         avctx->pix_fmt, avctx->sw_pix_fmt);
+  if (avctx->pix_fmt != AV_PIX_FMT_NI_LOGAN)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "sw_pix_fmt assigned to pix_fmt was %d, "
+           "is now %d\n", avctx->pix_fmt, avctx->sw_pix_fmt);
+    avctx->sw_pix_fmt = avctx->pix_fmt;
+  }
+  else
+  {
+    p_param->hwframes = 1;
+    av_log(avctx, AV_LOG_DEBUG, "p_param->hwframes = %d\n", p_param->hwframes);
+  }
+
+  switch (avctx->sw_pix_fmt)
+  {
+    case AV_PIX_FMT_YUV420P:
+    case AV_PIX_FMT_YUV420P10BE:
+    case AV_PIX_FMT_YUV420P10LE:
+    case AV_PIX_FMT_YUVJ420P:
+      break;
+    default:
+      av_log(avctx, AV_LOG_ERROR, "Error: pixel format %d not supported.\n",
+             avctx->sw_pix_fmt);
+      return AVERROR_INVALIDDATA;
+  }
+
+  if (s->xcoder_opts)
+  {
+    AVDictionary *dict = NULL;
+    AVDictionaryEntry *en = NULL;
+
+    if (!av_dict_parse_string(&dict, s->xcoder_opts, "=", ":", 0))
+    {
+      while ((en = av_dict_get(dict, "", en, AV_DICT_IGNORE_SUFFIX)))
+      {
+        int parse_ret = ni_logan_encoder_params_set_value(p_param, en->key, en->value, &s->api_ctx);
+        switch (parse_ret)
+        {
+          case NI_LOGAN_RETCODE_PARAM_INVALID_NAME:
+            av_log(avctx, AV_LOG_ERROR, "Unknown option: %s.\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_ERROR_TOO_BIG:
+            av_log(avctx, AV_LOG_ERROR, "Invalid %s: too big\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_ERROR_TOO_SMALL:
+            av_log(avctx, AV_LOG_ERROR, "Invalid %s: too small\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_ERROR_OOR:
+            av_log(avctx, AV_LOG_ERROR, "Invalid %s: out of range\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_ERROR_ZERO:
+            av_log(avctx, AV_LOG_ERROR, "Error setting option %s to value 0\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_INVALID_VALUE:
+            av_log(avctx, AV_LOG_ERROR, "Invalid value for %s: %s.\n", en->key, en->value);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_GOP_INTRA_INCOMPATIBLE:
+            av_log(avctx, AV_LOG_ERROR, "Invalid value for %s: %s incompatible with GOP structure.\n", en->key, en->value);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_FAILURE:
+            av_log(avctx, AV_LOG_ERROR, "Generic failure during xcoder-params setting for %s\n", en->key);
+            return AVERROR_EXTERNAL;
+          default:
+            break;
+        }
+      }
+      av_dict_free(&dict);
+
+      if (ni_logan_encoder_params_check(p_param, s->api_ctx.codec_format) !=
+          NI_LOGAN_RETCODE_SUCCESS)
+      {
+        av_log(avctx, AV_LOG_ERROR, "Validate encode parameters failed\n");
+        return AVERROR_EXTERNAL;
+      }
+    }
+  }
+
+  if (p_param->enable_vfr)
+  {
+    //in the vfr mode, Customer WangSu may reset time base to a very large value, such as 1000.
+    //At this time, the calculated framerate depends on timebase and ticket_per_frame is incorrect.
+    //So we choose to set the default framerate 30.
+    //If the calucluated framerate is correct, we will keep the original calculated framerate value
+    //Assume the frame between 5-120 is correct.
+    //using the time_base to initial timing info
+    if (p_param->hevc_enc_params.frame_rate < 5 || p_param->hevc_enc_params.frame_rate > 120)
+    {
+      p_param->hevc_enc_params.frame_rate = 30;
+    }
+    s->api_ctx.ui32timing_scale = avctx->time_base.den;
+    s->api_ctx.ui32num_unit_in_tick = avctx->time_base.num;
+    s->api_ctx.prev_bitrate = p_param->bitrate;
+    s->api_ctx.init_bitrate = p_param->bitrate;
+    s->api_ctx.last_change_framenum = 0;
+    s->api_ctx.fps_change_detect_count = 0;
+  }
+
+  if (s->xcoder_gop)
+  {
+    AVDictionary *dict = NULL;
+    AVDictionaryEntry *en = NULL;
+
+    if (!av_dict_parse_string(&dict, s->xcoder_gop, "=", ":", 0))
+    {
+      while ((en = av_dict_get(dict, "", en, AV_DICT_IGNORE_SUFFIX)))
+      {
+        int parse_ret = ni_logan_encoder_gop_params_set_value(p_param, en->key, en->value);
+        switch (parse_ret)
+        {
+          case NI_LOGAN_RETCODE_PARAM_INVALID_NAME:
+            av_log(avctx, AV_LOG_ERROR, "Unknown option: %s.\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_ERROR_TOO_BIG:
+            av_log(avctx, AV_LOG_ERROR, "Invalid custom GOP parameters: %s too big\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_ERROR_TOO_SMALL:
+            av_log(avctx, AV_LOG_ERROR, "Invalid custom GOP parameters: %s too small\n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_ERROR_OOR:
+            av_log(avctx, AV_LOG_ERROR, "Invalid custom GOP parameters: %s out of range \n", en->key);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_ERROR_ZERO:
+             av_log(avctx, AV_LOG_ERROR, "Invalid custom GOP paramaters: Error setting option %s to value 0\n", en->key);
+             return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_PARAM_INVALID_VALUE:
+            av_log(avctx, AV_LOG_ERROR, "Invalid value for GOP param %s: %s.\n", en->key, en->value);
+            return AVERROR_EXTERNAL;
+          case NI_LOGAN_RETCODE_FAILURE:
+            av_log(avctx, AV_LOG_ERROR, "Generic failure during xcoder-params setting for %s\n", en->key);
+            return AVERROR_EXTERNAL;
+          default:
+            break;
+        }
+      }
+      av_dict_free(&dict);
+    }
+  }
+
+  s->api_ctx.p_session_config = &s->api_param;
+  pparams = &s->api_param;
+  switch (pparams->hevc_enc_params.gop_preset_index)
+  {
+    /* dts_offset is the max number of non-reference frames in a GOP
+     * (derived from x264/5 algo) In case of IBBBP the first dts of the I frame should be input_pts-(3*ticks_per_frame)
+     * In case of IBP the first dts of the I frame should be input_pts-(1*ticks_per_frame)
+     * thus we ensure pts>dts in all cases
+     * */
+    case 1 /*PRESET_IDX_ALL_I*/:
+    case 2 /*PRESET_IDX_IPP*/:
+    case 6 /*PRESET_IDX_IPPPP*/:
+    case 9 /*PRESET_IDX_SP*/:
+      s->dts_offset = 0;
+      break;
+    /* ts requires dts/pts of I frame not same when there are B frames in streams */
+    case 3 /*PRESET_IDX_IBBB*/:
+    case 4 /*PRESET_IDX_IBPBP*/:
+    case 7 /*PRESET_IDX_IBBBB*/:
+      s->dts_offset = 1;
+      break;
+    case 5 /*PRESET_IDX_IBBBP*/:
+      s->dts_offset = 2;
+      break;
+    case 8 /*PRESET_IDX_RA_IB*/:
+      s->dts_offset = 3;
+      break;
+    default:
+      // TBD need user to specify offset
+      s->dts_offset = 7;
+      av_log(avctx, AV_LOG_VERBOSE, "dts offset default to 7, TBD\n");
+      break;
+  }
+  if (1 == pparams->force_frame_type)
+  {
+    s->dts_offset = 7;
+  }
+
+  av_log(avctx, AV_LOG_INFO, "dts offset: %d\n", s->dts_offset);
+
+  if (0 == strcmp(s->dev_xcoder, LIST_DEVICES_STR))
+  {
+    av_log(avctx, AV_LOG_DEBUG, "XCoder: printing out all xcoder devices and "
+           "their load, and exit ...\n");
+    ni_logan_rsrc_print_all_devices_capability();
+    return AVERROR_EXIT;
+  }
+
+  //overwrite keep alive timeout value here with a custom value if it was provided
+  // provided
+  // if xcoder option is set then overwrite the (legacy) decoder option
+  uint32_t xcoder_timeout = s->api_param.hevc_enc_params.keep_alive_timeout;
+  if (xcoder_timeout != NI_LOGAN_DEFAULT_KEEP_ALIVE_TIMEOUT)
+  {
+    s->api_ctx.keep_alive_timeout = xcoder_timeout;
+  }
+  else
+  {
+    s->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+  }
+  av_log(avctx, AV_LOG_VERBOSE, "Custom NVMe Keep Alive Timeout set to = %d\n",
+         s->api_ctx.keep_alive_timeout);
+  //overwrite set_high_priority value here with a custom value if it was provided
+  uint32_t xcoder_high_priority = s->api_param.hevc_enc_params.set_high_priority;
+  if(xcoder_high_priority != 0)
+  {
+    s->api_ctx.set_high_priority = xcoder_high_priority;
+  }
+  else
+  {
+    s->api_ctx.set_high_priority = s->set_high_priority;
+  }
+  av_log(avctx, AV_LOG_VERBOSE, "Custom NVMe set_high_priority set to = %d\n",
+         s->api_ctx.set_high_priority);
+  avctx->bit_rate = pparams->bitrate;
+  s->total_frames_received = 0;
+  s->encoder_eof = 0;
+  s->roi_side_data_size = s->nb_rois = 0;
+  s->av_rois = NULL;
+  s->avc_roi_map = NULL;
+  s->hevc_sub_ctu_roi_buf = NULL;
+  s->hevc_roi_map = NULL;
+  s->api_ctx.src_bit_depth = 8;
+  s->api_ctx.src_endian = NI_LOGAN_FRAME_LITTLE_ENDIAN;
+  s->api_ctx.roi_len = 0;
+  s->api_ctx.roi_avg_qp = 0;
+  s->api_ctx.bit_depth_factor = 1;
+  if (AV_PIX_FMT_YUV420P10BE == avctx->sw_pix_fmt ||
+      AV_PIX_FMT_YUV420P10LE == avctx->sw_pix_fmt)
+  {
+    s->api_ctx.bit_depth_factor = 2;
+    s->api_ctx.src_bit_depth = 10;
+    if (AV_PIX_FMT_YUV420P10BE == avctx->sw_pix_fmt)
+    {
+      s->api_ctx.src_endian = NI_LOGAN_FRAME_BIG_ENDIAN;
+    }
+  }
+
+  // DolbyVision, HRD and AUD settings
+  if (AV_CODEC_ID_HEVC == avctx->codec_id)
+  {
+    if (5 == pparams->dolby_vision_profile)
+    {
+      pparams->hrd_enable = pparams->enable_aud = 1;
+      pparams->hevc_enc_params.forced_header_enable = NI_LOGAN_ENC_REPEAT_HEADERS_ALL_KEY_FRAMES;
+      pparams->hevc_enc_params.decoding_refresh_type = 2;
+    }
+    if (pparams->hrd_enable)
+    {
+      pparams->hevc_enc_params.rc.enable_rate_control = 1;
+    }
+  }
+
+  // init HW AVFRAME pool
+  s->freeHead = 0;
+  s->freeTail = 0;
+  for (i = 0; i < LOGAN_MAX_NUM_FRAMEPOOL_HWAVFRAME; i++)
+  {
+    s->sframe_pool[i] = av_frame_alloc();
+    if (!s->sframe_pool[i])
+    {
+      return AVERROR(ENOMEM);
+    }
+    s->aFree_Avframes_list[i] = i;
+    s->freeTail++;
+  }
+  s->aFree_Avframes_list[i] = -1;
+
+  // init HDR SEI stuff
+  s->api_ctx.sei_hdr_content_light_level_info_len =
+  s->api_ctx.light_level_data_len =
+  s->api_ctx.sei_hdr_mastering_display_color_vol_len =
+  s->api_ctx.mdcv_max_min_lum_data_len = 0;
+  s->api_ctx.p_master_display_meta_data = NULL;
+
+  // init HRD SEI stuff (TBD: value after recovery ?)
+  s->au_cpb_removal_delay_minus1 = 0;
+
+  memset( &(s->api_fme), 0, sizeof(ni_logan_session_data_io_t) );
+  memset( &(s->api_pkt), 0, sizeof(ni_logan_session_data_io_t) );
+
+  if (avctx->width > 0 && avctx->height > 0)
+  {
+    ni_logan_frame_buffer_alloc(&(s->api_fme.data.frame),
+                          ODD2EVEN(avctx->width),
+                          ODD2EVEN(avctx->height),
+                          0,
+                          0,
+                          s->api_ctx.bit_depth_factor,
+                          (s->buffered_fme.format == AV_PIX_FMT_NI_LOGAN));
+  }
+
+  // generate encoded bitstream headers in advance if configured to do so
+  if (pparams->generate_enc_hdrs)
+  {
+    ret = xcoder_logan_encoder_headers(avctx);
+  }
+
+  return ret;
+}
+
+av_cold int ff_xcoder_logan_encode_init(AVCodecContext *avctx)
+{
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+  int ret;
+
+  ni_log_set_level(ff_to_ni_log_level(av_log_get_level()));
+
+  av_log(avctx, AV_LOG_DEBUG, "%s\n", __FUNCTION__);
+
+  if (ctx->dev_xcoder == NULL)
+  {
+    av_log(avctx, AV_LOG_ERROR, "Error: XCoder option dev_xcoder is null\n");
+    return AVERROR_INVALIDDATA;
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_VERBOSE, "XCoder options: dev_xcoder: %s dev_enc_idx "
+           "%d\n", ctx->dev_xcoder, ctx->dev_enc_idx);
+  }
+
+  if (ctx->api_ctx.session_run_state == LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING)
+  {
+    ctx->dev_enc_idx = ctx->orig_dev_enc_idx;
+  }
+  else
+  {
+    ctx->orig_dev_enc_idx = ctx->dev_enc_idx;
+  }
+
+#ifdef NIENC_MULTI_THREAD
+  if (sessionCounter == 0)
+  {
+    threadpool_init(&pool);
+  }
+  sessionCounter++;
+#endif
+
+  if ((ret = xcoder_logan_setup_encoder(avctx)) < 0)
+  {
+    ff_xcoder_logan_encode_close(avctx);
+    return ret;
+  }
+
+#ifdef _WIN32
+  // For windows opening the encoder when init will take less time.
+  // If HW frame detected then open in xcoder_send_frame function.
+  if (avctx->pix_fmt != AV_PIX_FMT_NI_LOGAN)
+  {
+    // NETINT_INTERNAL - currently only for internal testing
+    ni_logan_encoder_params_t *p_param = &ctx->api_param;
+    ret = do_open_encoder_device(avctx, ctx, p_param);
+    if (ret < 0)
+    {
+      ff_xcoder_logan_encode_close(avctx);
+      return ret;
+    }
+  }
+#endif
+  ctx->vpu_reset = 0;
+
+  return 0;
+}
+
+static int is_logan_input_fifo_empty(XCoderLoganEncContext *ctx)
+{
+  return av_fifo_size(ctx->fme_fifo) < sizeof(AVFrame);
+}
+
+static int is_logan_input_fifo_full(XCoderLoganEncContext *ctx)
+{
+  return av_fifo_space(ctx->fme_fifo) < sizeof(AVFrame);
+}
+
+static int xcoder_logan_encode_reset(AVCodecContext *avctx)
+{
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+  av_log(avctx, AV_LOG_WARNING, "%s\n", __FUNCTION__);
+  ctx->vpu_reset = 1;
+  ff_xcoder_logan_encode_close(avctx);
+  return ff_xcoder_logan_encode_init(avctx);
+}
+
+static int enqueue_logan_frame(AVCodecContext *avctx, const AVFrame *inframe)
+{
+  int ret;
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+
+  // expand frame buffer fifo if not enough space
+  if (is_logan_input_fifo_full(ctx))
+  {
+    ret = av_fifo_realloc2(ctx->fme_fifo, 2 * av_fifo_size(ctx->fme_fifo));
+    if (ret < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "Enc av_fifo_realloc2 NO MEMORY !!!\n");
+      return ret;
+    }
+    if ((av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame) % 100) == 0)
+    {
+      av_log(avctx, AV_LOG_INFO, "Enc fifo being extended to: %" PRIu64 "\n",
+             av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame));
+    }
+    av_assert0(0 == av_fifo_size(ctx->fme_fifo) % sizeof(AVFrame));
+  }
+
+  if (inframe == &ctx->buffered_fme)
+  {
+    // For FFmpeg-n4.4+ receive_packet interface the buffered_fme is fetched from
+    // ff_alloc_get_frame rather than passed as function argument. So we need to
+    // judge whether they are the same object. If they are the same NO need to do
+    // any reference before queue operation.
+    av_fifo_generic_write(ctx->fme_fifo, (void *)inframe, sizeof(*inframe), NULL);
+  }
+  else
+  {
+    AVFrame temp_frame;
+    memset(&temp_frame, 0, sizeof(AVFrame));
+    // In case double free for external input frame and our buffered frame.
+    av_frame_ref(&temp_frame, inframe);
+    av_fifo_generic_write(ctx->fme_fifo, &temp_frame, sizeof(*inframe), NULL);
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "fme queued pts:%" PRId64 ", fifo size: %" PRIu64 "\n",
+         inframe->pts, av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame));
+
+  return ret;
+}
+
+#ifdef NIENC_MULTI_THREAD
+static void* write_frame_thread(void* arg)
+{
+  write_thread_arg_struct_t *args = (write_thread_arg_struct_t *) arg;
+  XCoderLoganEncContext *ctx = args->ctx;
+  int ret;
+  int sent;
+
+  pthread_mutex_lock(&args->mutex);
+  args->running = 1;
+  av_log(ctx, AV_LOG_DEBUG, "%s: session_id %d, device_handle %d\n",
+         __FUNCTION__, ctx->api_ctx.session_id, ctx->api_ctx.device_handle);
+
+  av_log(ctx, AV_LOG_DEBUG, "%s: ctx %p\n", __FUNCTION__, ctx);
+
+  sent = ni_logan_device_session_write(&ctx->api_ctx, &ctx->api_fme, NI_LOGAN_DEVICE_TYPE_ENCODER);
+
+  av_log(ctx, AV_LOG_DEBUG, "%s: size %d sent to xcoder\n", __FUNCTION__, sent);
+
+  if (NI_LOGAN_RETCODE_ERROR_RESOURCE_UNAVAILABLE == sent)
+  {
+    av_log(ctx, AV_LOG_DEBUG, "%s(): Sequence Change in progress, returning "
+           "EAGAIN\n", __FUNCTION__);
+    ret = AVERROR(EAGAIN);
+  }
+  else if (NI_LOGAN_RETCODE_ERROR_VPU_RECOVERY == sent)
+  {
+    sent = xcoder_logan_encode_reset(ctx);
+  }
+
+  if (sent < 0)
+  {
+    ret = AVERROR(EIO);
+  }
+  else
+  {
+    //pushing input pts in circular FIFO
+    ctx->api_ctx.enc_pts_list[ctx->api_ctx.enc_pts_w_idx % NI_LOGAN_FIFO_SZ] = ctx->api_fme.data.frame.pts;
+    ctx->api_ctx.enc_pts_w_idx++;
+    ret = 0;
+  }
+
+  args->ret = ret;
+  av_log(ctx, AV_LOG_DEBUG, "%s: ret %d\n", __FUNCTION__, ret);
+  pthread_cond_signal(&args->cond);
+  args->running = 0;
+  pthread_mutex_unlock(&args->mutex);
+  return NULL;
+}
+#endif
+
+int ff_xcoder_logan_encode_close(AVCodecContext *avctx)
+{
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+  ni_logan_retcode_t ret = NI_LOGAN_RETCODE_FAILURE;
+  int i;
+
+#ifdef NIENC_MULTI_THREAD
+  sessionCounter--;
+  if (sessionCounter == 0)
+  {
+    threadpool_destroy(&pool);
+  }
+#endif
+
+  for (i = 0; i < LOGAN_MAX_NUM_FRAMEPOOL_HWAVFRAME; i++)
+  {
+    //any remaining stored AVframes that have not been unref will die here
+    av_frame_free(&(ctx->sframe_pool[i]));
+    ctx->sframe_pool[i] = NULL;
+  }
+
+  do_close_encoder_device(ctx);
+
+  if (ctx->api_ctx.p_master_display_meta_data)
+  {
+    free(ctx->api_ctx.p_master_display_meta_data);
+    ctx->api_ctx.p_master_display_meta_data = NULL;
+  }
+
+  av_log(avctx, AV_LOG_DEBUG, "%s (status = %d)\n", __FUNCTION__, ret);
+  ni_logan_frame_buffer_free( &(ctx->api_fme.data.frame) );
+  ni_logan_packet_buffer_free( &(ctx->api_pkt.data.packet) );
+
+  av_log(avctx, AV_LOG_DEBUG, "fifo size: %" PRIu64 "\n",
+         av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame));
+  if (! ctx->vpu_reset &&
+      ctx->api_ctx.session_run_state != LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING)
+  {
+    av_fifo_free(ctx->fme_fifo);
+    av_log(avctx, AV_LOG_DEBUG, " , freed.\n");
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_DEBUG, " , kept.\n");
+  }
+
+  ni_logan_device_session_context_clear(&ctx->api_ctx);
+  ni_logan_rsrc_free_device_context(ctx->rsrc_ctx);
+  ctx->rsrc_ctx = NULL;
+
+  free(ctx->p_spsPpsHdr);
+  ctx->p_spsPpsHdr = NULL;
+
+  free(ctx->av_rois);
+  free(ctx->avc_roi_map);
+  free(ctx->hevc_sub_ctu_roi_buf);
+  free(ctx->hevc_roi_map);
+  ctx->av_rois = NULL;
+  ctx->avc_roi_map = NULL;
+  ctx->hevc_sub_ctu_roi_buf = NULL;
+  ctx->hevc_roi_map = NULL;
+  ctx->roi_side_data_size = ctx->nb_rois = 0;
+  ctx->started = 0;
+
+  return 0;
+}
+
+static int xcoder_send_frame(AVCodecContext *avctx, const AVFrame *frame)
+{
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+  int ret = 0;
+  int sent;
+  int orig_avctx_width = avctx->width, orig_avctx_height = avctx->height;
+  AVFrameSideData *side_data;
+  AVHWFramesContext *avhwf_ctx;
+  NILOGANFramesContext *nif_src_ctx;
+  int is_hwframe;
+  int format_in_use;
+  int frame_width, frame_height;
+
+#ifdef NIENC_MULTI_THREAD
+  av_log(avctx, AV_LOG_DEBUG, "%s start %p, session_id %d, device_handle %d\n",
+         __FUNCTION__, ctx->api_ctx.session_info, ctx->api_ctx.session_id,
+         ctx->api_ctx.device_handle);
+  if (ctx->api_ctx.session_id != NI_LOGAN_INVALID_SESSION_ID &&
+      ctx->api_ctx.device_handle != NI_INVALID_DEVICE_HANDLE)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "%s start %p\n", __FUNCTION__,
+           ctx->api_ctx.session_info);
+    if (ctx->api_ctx.session_info != NULL)
+    {
+      write_thread_arg_struct_t *write_thread_args = (write_thread_arg_struct_t *)ctx->api_ctx.session_info;
+      pthread_mutex_lock(&write_thread_args->mutex);
+      av_log(avctx, AV_LOG_DEBUG, "thread start waiting session_id %d\n",
+             ctx->api_ctx.session_id);
+      if (write_thread_args->running == 1)
+      {
+        pthread_cond_wait(&write_thread_args->cond, &write_thread_args->mutex);
+        av_log(avctx, AV_LOG_DEBUG, "thread get waiting session_id %d\n",
+               ctx->api_ctx.session_id);
+      }
+      if (write_thread_args->ret == AVERROR(EAGAIN))
+      {
+        av_log(avctx, AV_LOG_ERROR, "%s: ret %d\n", __FUNCTION__,
+               write_thread_args->ret);
+        pthread_mutex_unlock(&write_thread_args->mutex);
+        free(write_thread_args);
+        ctx->api_ctx.session_info = NULL;
+        return AVERROR(EAGAIN);
+      }
+      pthread_mutex_unlock(&write_thread_args->mutex);
+      free(write_thread_args);
+      ctx->api_ctx.session_info = NULL;
+      av_log(avctx, AV_LOG_DEBUG, "thread free session_id %d\n",
+             ctx->api_ctx.session_id);
+    }
+  }
+#endif
+  ni_logan_encoder_params_t *p_param = &ctx->api_param; // NETINT_INTERNAL - currently only for internal testing
+
+  av_log(avctx, AV_LOG_DEBUG, "%s pkt_size %d %dx%d  avctx: %dx%d\n", __FUNCTION__,
+         frame ? frame->pkt_size : -1, frame ? frame->width : -1,
+         frame ? frame->height : -1, avctx->width, avctx->height);
+
+  if (ctx->encoder_flushing)
+  {
+    if (! frame && is_logan_input_fifo_empty(ctx))
+    {
+      av_log(avctx, AV_LOG_DEBUG, "XCoder EOF: null frame && fifo empty\n");
+      return AVERROR_EOF;
+    }
+  }
+
+  if (! frame)
+  {
+    if (LOGAN_SESSION_RUN_STATE_QUEUED_FRAME_DRAINING == ctx->api_ctx.session_run_state)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "null frame, send queued frame\n");
+    }
+    else
+    {
+      ctx->eos_fme_received = 1;
+      av_log(avctx, AV_LOG_DEBUG, "null frame, ctx->eos_fme_received = 1\n");
+    }
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_DEBUG, "%s #%"PRIu64"\n", __FUNCTION__,
+           ctx->api_ctx.frame_num);
+
+    // queue up the frame if fifo is NOT empty, or sequence change ongoing !
+    if (!is_logan_input_fifo_empty(ctx) ||
+        LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING == ctx->api_ctx.session_run_state)
+    {
+      enqueue_logan_frame(avctx, frame);
+
+      if (LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING ==
+          ctx->api_ctx.session_run_state)
+      {
+        av_log(avctx, AV_LOG_TRACE, "XCoder doing sequence change, frame #"
+               "%"PRIu64" queued and return 0 !\n", ctx->api_ctx.frame_num);
+        return 0;
+      }
+    }
+    else if (frame != &ctx->buffered_fme)
+    {
+      // For FFmpeg-n4.4+ receive_packet interface the buffered_fme is fetched from
+      // ff_alloc_get_frame rather than passed as function argument. So we need to
+      // judge whether they are the same object. If they are the same NO need to do
+      // any reference.
+      ret = av_frame_ref(&ctx->buffered_fme, frame);
+    }
+  }
+
+  if (is_logan_input_fifo_empty(ctx))
+  {
+    av_log(avctx, AV_LOG_DEBUG, "no frame in fifo to send, just send/receive ..\n");
+    if (ctx->eos_fme_received)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "no frame in fifo to send, send eos ..\n");
+      // if received eos but not sent any frame, there is no need to continue the following process
+      if (ctx->started == 0)
+      {
+        av_log(avctx, AV_LOG_DEBUG, "session is not open, send eos, return EOF\n");
+        return AVERROR_EOF;
+      }
+    }
+  }
+  else
+  {
+    av_fifo_generic_peek(ctx->fme_fifo, &ctx->buffered_fme,
+                         sizeof(AVFrame), NULL);
+    ctx->buffered_fme.extended_data = ctx->buffered_fme.data;
+  }
+
+  frame_width = ODD2EVEN(ctx->buffered_fme.width);
+  frame_height = ODD2EVEN(ctx->buffered_fme.height);
+  is_hwframe = (ctx->buffered_fme.format == AV_PIX_FMT_NI_LOGAN);
+
+  // leave encoder instance open to when the first frame buffer arrives so that
+  // its stride size is known and handled accordingly.
+  if (ctx->started == 0)
+  {
+#ifdef _WIN32
+    if (ctx->buffered_fme.width != avctx->width ||
+       ctx->buffered_fme.height != avctx->height ||
+       ctx->buffered_fme.color_primaries != avctx->color_primaries ||
+       ctx->buffered_fme.color_trc != avctx->color_trc ||
+       ctx->buffered_fme.colorspace != avctx->colorspace)
+    {
+      av_log(avctx, AV_LOG_INFO, "WARNING reopen device Width: %d-%d, "
+             "Height: %d-%d, color_primaries: %d-%d, color_trc: %d-%d, "
+             "color_space: %d-%d\n",
+             ctx->buffered_fme.width, avctx->width,
+             ctx->buffered_fme.height, avctx->height,
+             ctx->buffered_fme.color_primaries, avctx->color_primaries,
+             ctx->buffered_fme.color_trc, avctx->color_trc,
+             ctx->buffered_fme.colorspace, avctx->colorspace);
+      do_close_encoder_device(ctx);
+      // Errror when set this parameters in ni_logan_encoder_params_set_value !!!!!!
+      p_param->hevc_enc_params.conf_win_right = 0;
+      p_param->hevc_enc_params.conf_win_bottom = 0;
+
+      if ((ret = do_open_encoder_device(avctx, ctx, p_param)) < 0)
+      {
+        return ret;
+      }
+    }
+    else if (is_hwframe) // if hw-frame detected for windows then open here.
+#endif
+    {
+      if ((ret = do_open_encoder_device(avctx, ctx, p_param)) < 0)
+      {
+        return ret;
+      }
+    }
+    ctx->api_fme.data.frame.start_of_stream = 1;
+    ctx->started = 1;
+  }
+  else
+  {
+    ctx->api_fme.data.frame.start_of_stream = 0;
+  }
+
+  if ((ctx->buffered_fme.height && ctx->buffered_fme.width) &&
+      (ctx->buffered_fme.height != avctx->height ||
+       ctx->buffered_fme.width != avctx->width))
+  {
+    av_log(avctx, AV_LOG_INFO, "%s resolution change %dx%d -> %dx%d\n",
+           __FUNCTION__, avctx->width, avctx->height, ctx->buffered_fme.width,
+           ctx->buffered_fme.height);
+    ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING;
+    ctx->eos_fme_received = 1;
+
+    // have to queue this frame if not done so: an empty queue
+    if (is_logan_input_fifo_empty(ctx))
+    {
+      av_log(avctx, AV_LOG_TRACE, "%s resolution change when fifo empty, frame "
+             "#%"PRIu64" being queued\n", __FUNCTION__, ctx->api_ctx.frame_num);
+      if (frame != &ctx->buffered_fme)
+      {
+        // For FFmpeg-n4.4+ receive_packet interface the buffered_fme is fetched from
+        // ff_alloc_get_frame rather than passed as function argument. So we need to
+        // judge whether they are the same object. If they are the same do NOT
+        // unreference any of them because we need to enqueue it later.
+        av_frame_unref(&ctx->buffered_fme);
+      }
+      enqueue_logan_frame(avctx, frame);
+    }
+  }
+
+  ctx->api_fme.data.frame.preferred_characteristics_data_len = 0;
+  ctx->api_fme.data.frame.end_of_stream = 0;
+  ctx->api_fme.data.frame.force_key_frame
+  = ctx->api_fme.data.frame.use_cur_src_as_long_term_pic
+  = ctx->api_fme.data.frame.use_long_term_ref = 0;
+
+  ctx->api_fme.data.frame.sei_total_len
+  = ctx->api_fme.data.frame.sei_cc_offset = ctx->api_fme.data.frame.sei_cc_len
+  = ctx->api_fme.data.frame.sei_hdr_mastering_display_color_vol_offset
+  = ctx->api_fme.data.frame.sei_hdr_mastering_display_color_vol_len
+  = ctx->api_fme.data.frame.sei_hdr_content_light_level_info_offset
+  = ctx->api_fme.data.frame.sei_hdr_content_light_level_info_len
+  = ctx->api_fme.data.frame.sei_hdr_plus_offset
+  = ctx->api_fme.data.frame.sei_hdr_plus_len = 0;
+
+  ctx->api_fme.data.frame.roi_len = 0;
+  ctx->api_fme.data.frame.reconf_len = 0;
+  ctx->api_fme.data.frame.force_pic_qp = 0;
+
+  // employ a ni_logan_frame_t to represent decode frame when using new libxcoder
+  // API to prepare side data
+  ni_logan_frame_t dec_frame = {0};
+  ni_aux_data_t *aux_data = NULL;
+
+  if (LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING == ctx->api_ctx.session_run_state ||
+      (ctx->eos_fme_received && is_logan_input_fifo_empty(ctx)))
+  {
+    av_log(avctx, AV_LOG_DEBUG, "XCoder start flushing\n");
+    ctx->api_fme.data.frame.end_of_stream = 1;
+    ctx->encoder_flushing = 1;
+  }
+  else
+  {
+    // NETINT_INTERNAL - currently only for internal testing
+    // allocate memory for reconf parameters only once and reuse it
+    if (! ctx->api_ctx.enc_change_params &&
+        p_param->reconf_demo_mode > LOGAN_XCODER_TEST_RECONF_OFF &&
+        p_param->reconf_demo_mode <= LOGAN_XCODER_TEST_RECONF_RC_MIN_MAX_QP)
+    {
+      ctx->api_ctx.enc_change_params =
+      calloc(1, sizeof(ni_logan_encoder_change_params_t));
+      if (! ctx->api_ctx.enc_change_params)
+      {
+        return AVERROR(ENOMEM);
+      }
+    }
+    if (ctx->api_ctx.enc_change_params)
+    {
+      memset(ctx->api_ctx.enc_change_params,
+             0, sizeof(ni_logan_encoder_change_params_t));
+    }
+
+    ctx->api_fme.data.frame.extra_data_len = NI_LOGAN_APP_ENC_FRAME_META_DATA_SIZE;
+
+    switch (p_param->reconf_demo_mode)
+    {
+      case LOGAN_XCODER_TEST_RECONF_BR:
+        if (ctx->api_ctx.frame_num == p_param->reconf_hash[ctx->reconfigCount][0])
+        {
+          ctx->api_ctx.enc_change_params->enable_option |=
+          NI_LOGAN_SET_CHANGE_PARAM_RC_TARGET_RATE;
+          ctx->api_ctx.enc_change_params->bitRate =
+          p_param->reconf_hash[ctx->reconfigCount][1];
+
+          ctx->api_fme.data.frame.extra_data_len +=
+          sizeof(ni_logan_encoder_change_params_t);
+          ctx->api_fme.data.frame.reconf_len =
+          sizeof(ni_logan_encoder_change_params_t);
+
+          ctx->reconfigCount++;
+        }
+        break;
+      case LOGAN_XCODER_TEST_RECONF_INTRAPRD:
+        if (ctx->api_ctx.frame_num == p_param->reconf_hash[ctx->reconfigCount][0])
+        {
+          ctx->api_ctx.enc_change_params->enable_option |=
+          NI_LOGAN_SET_CHANGE_PARAM_INTRA_PARAM;
+          ctx->api_ctx.enc_change_params->intraQP =
+          p_param->reconf_hash[ctx->reconfigCount][1];
+          ctx->api_ctx.enc_change_params->intraPeriod =
+          p_param->reconf_hash[ctx->reconfigCount][2];
+          ctx->api_ctx.enc_change_params->repeatHeaders =
+          p_param->reconf_hash[ctx->reconfigCount][3];
+          av_log(avctx, AV_LOG_TRACE, "xcoder_send_frame: frame #%lu reconf "
+                 "intraQP %d intraPeriod %d repeatHeaders %d\n",
+                 ctx->api_ctx.frame_num,
+                 ctx->api_ctx.enc_change_params->intraQP,
+                 ctx->api_ctx.enc_change_params->intraPeriod,
+                 ctx->api_ctx.enc_change_params->repeatHeaders);
+
+          ctx->api_fme.data.frame.extra_data_len +=
+          sizeof(ni_logan_encoder_change_params_t);
+          ctx->api_fme.data.frame.reconf_len =
+          sizeof(ni_logan_encoder_change_params_t);
+
+          ctx->reconfigCount++;
+        }
+        break;
+    case LOGAN_XCODER_TEST_RECONF_LONG_TERM_REF:
+      // the reconf file data line format for this is:
+      // <frame-number>:useCurSrcAsLongtermPic,useLongtermRef where
+      // values will stay the same on every frame until changed.
+      if (ctx->api_ctx.frame_num >= p_param->reconf_hash[ctx->reconfigCount][0])
+      {
+        AVFrameSideData *ltr_sd;
+        AVNetintLongTermRef *p_ltr;
+        ltr_sd = av_frame_new_side_data(&ctx->buffered_fme,
+                                        AV_FRAME_DATA_NETINT_LONG_TERM_REF,
+                                        sizeof(AVNetintLongTermRef));
+        if (ltr_sd)
+        {
+          p_ltr = (AVNetintLongTermRef *)ltr_sd->data;
+          p_ltr->use_cur_src_as_long_term_pic
+          = p_param->reconf_hash[ctx->reconfigCount][1];
+          p_ltr->use_long_term_ref
+          = p_param->reconf_hash[ctx->reconfigCount][2];
+        }
+      }
+      if (ctx->api_ctx.frame_num + 1 ==
+          p_param->reconf_hash[ctx->reconfigCount + 1][0])
+      {
+        ctx->reconfigCount++;
+      }
+      break;
+    case LOGAN_XCODER_TEST_RECONF_VUI_HRD:
+      // the reconf file format for this is:
+      // <frame-number>:<vui-file-name-in-digits>,<number-of-bits-of-vui-rbsp>
+      if (ctx->api_ctx.frame_num == p_param->reconf_hash[ctx->reconfigCount][0])
+      {
+        char file_name[64];
+        FILE *vui_file;
+        snprintf(file_name, 64, "%d",
+                 p_param->reconf_hash[ctx->reconfigCount][1]);
+        vui_file = fopen(file_name, "rb");
+        if (! vui_file)
+        {
+          av_log(avctx, AV_LOG_ERROR, "Error VUI reconf file: %s\n", file_name);
+        }
+        else
+        {
+          int nb_bytes_by_bits =
+          (p_param->reconf_hash[ctx->reconfigCount][2] + 7) / 8;
+          size_t nb_bytes = fread(ctx->api_ctx.enc_change_params->vuiRbsp,
+                                  1, NI_LOGAN_MAX_VUI_SIZE, vui_file);
+          if (nb_bytes != nb_bytes_by_bits)
+          {
+            av_log(avctx, AV_LOG_ERROR, "Error VUI file size %d bytes != "
+                   "specified %d bits (%d bytes) !\n", (int)nb_bytes,
+                   p_param->reconf_hash[ctx->reconfigCount][2], nb_bytes_by_bits);
+          }
+          else
+          {
+            ctx->api_ctx.enc_change_params->enable_option |=
+            NI_LOGAN_SET_CHANGE_PARAM_VUI_HRD_PARAM;
+            ctx->api_ctx.enc_change_params->encodeVuiRbsp = 1;
+            ctx->api_ctx.enc_change_params->vuiDataSizeBits =
+            p_param->reconf_hash[ctx->reconfigCount][2];
+            ctx->api_ctx.enc_change_params->vuiDataSizeBytes = nb_bytes;
+            av_log(avctx, AV_LOG_DEBUG, "Reconf VUI %d bytes (%d bits)\n",
+                   (int)nb_bytes, p_param->reconf_hash[ctx->reconfigCount][2]);
+            ctx->api_fme.data.frame.extra_data_len +=
+            sizeof(ni_logan_encoder_change_params_t);
+            ctx->api_fme.data.frame.reconf_len =
+            sizeof(ni_logan_encoder_change_params_t);
+
+            ctx->reconfigCount++;
+          }
+
+          fclose(vui_file);
+        }
+      }
+      break;
+    case LOGAN_XCODER_TEST_RECONF_RC:
+      if (ctx->api_ctx.frame_num == p_param->reconf_hash[ctx->reconfigCount][0])
+      {
+        ctx->api_ctx.enc_change_params->enable_option |= NI_LOGAN_SET_CHANGE_PARAM_RC;
+        ctx->api_ctx.enc_change_params->hvsQPEnable =
+        p_param->reconf_hash[ctx->reconfigCount][1];
+        ctx->api_ctx.enc_change_params->hvsQpScale =
+        p_param->reconf_hash[ctx->reconfigCount][2];
+        ctx->api_ctx.enc_change_params->vbvBufferSize =
+        p_param->reconf_hash[ctx->reconfigCount][3];
+        ctx->api_ctx.enc_change_params->mbLevelRcEnable =
+        p_param->reconf_hash[ctx->reconfigCount][4];
+        ctx->api_ctx.enc_change_params->fillerEnable =
+        p_param->reconf_hash[ctx->reconfigCount][5];
+        av_log(avctx, AV_LOG_TRACE, "xcoder_send_frame: frame #%lu reconf "
+               "hvsQPEnable %d hvsQpScale %d vbvBufferSize %d mbLevelRcEnable "
+               "%d fillerEnable %d\n",
+               ctx->api_ctx.frame_num,
+               ctx->api_ctx.enc_change_params->hvsQPEnable,
+               ctx->api_ctx.enc_change_params->hvsQpScale,
+               ctx->api_ctx.enc_change_params->vbvBufferSize,
+               ctx->api_ctx.enc_change_params->mbLevelRcEnable,
+               ctx->api_ctx.enc_change_params->fillerEnable);
+
+        ctx->api_fme.data.frame.extra_data_len +=
+        sizeof(ni_logan_encoder_change_params_t);
+        ctx->api_fme.data.frame.reconf_len = sizeof(ni_logan_encoder_change_params_t);
+        ctx->reconfigCount++;
+      }
+      break;
+    case LOGAN_XCODER_TEST_RECONF_RC_MIN_MAX_QP:
+      if (ctx->api_ctx.frame_num == p_param->reconf_hash[ctx->reconfigCount][0])
+      {
+        ctx->api_ctx.enc_change_params->enable_option |=
+        NI_LOGAN_SET_CHANGE_PARAM_RC_MIN_MAX_QP;
+        ctx->api_ctx.enc_change_params->minQpI =
+        p_param->reconf_hash[ctx->reconfigCount][1];
+        ctx->api_ctx.enc_change_params->maxQpI =
+        p_param->reconf_hash[ctx->reconfigCount][2];
+        ctx->api_ctx.enc_change_params->maxDeltaQp =
+        p_param->reconf_hash[ctx->reconfigCount][3];
+        ctx->api_ctx.enc_change_params->minQpP =
+        p_param->reconf_hash[ctx->reconfigCount][4];
+        ctx->api_ctx.enc_change_params->minQpB =
+        p_param->reconf_hash[ctx->reconfigCount][5];
+        ctx->api_ctx.enc_change_params->maxQpP =
+        p_param->reconf_hash[ctx->reconfigCount][6];
+        ctx->api_ctx.enc_change_params->maxQpB =
+        p_param->reconf_hash[ctx->reconfigCount][7];
+        av_log(avctx, AV_LOG_TRACE, "xcoder_send_frame: frame #%lu reconf "
+               "minQpI %d maxQpI %d maxDeltaQp %d minQpP "
+               "%d minQpB %d maxQpP %d maxQpB %d\n",
+               ctx->api_ctx.frame_num, ctx->api_ctx.enc_change_params->minQpI,
+               ctx->api_ctx.enc_change_params->maxQpI,
+               ctx->api_ctx.enc_change_params->maxDeltaQp,
+               ctx->api_ctx.enc_change_params->minQpP,
+               ctx->api_ctx.enc_change_params->minQpB,
+               ctx->api_ctx.enc_change_params->maxQpP,
+               ctx->api_ctx.enc_change_params->maxQpB);
+
+        ctx->api_fme.data.frame.extra_data_len +=
+        sizeof(ni_logan_encoder_change_params_t);
+        ctx->api_fme.data.frame.reconf_len = sizeof(ni_logan_encoder_change_params_t);
+        ctx->reconfigCount++;
+      }
+      break;
+      case LOGAN_XCODER_TEST_RECONF_OFF:
+      default:
+        ;
+    }
+
+    // long term reference frame support
+    side_data = av_frame_get_side_data(&ctx->buffered_fme,
+                                       AV_FRAME_DATA_NETINT_LONG_TERM_REF);
+    if (side_data && (side_data->size == sizeof(AVNetintLongTermRef)))
+    {
+      aux_data = ni_logan_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_LONG_TERM_REF,
+                                       sizeof(ni_long_term_ref_t));
+      if (aux_data)
+      {
+        memcpy(aux_data->data, side_data->data, side_data->size);
+      }
+
+      if (ctx->api_fme.data.frame.reconf_len == 0)
+      {
+        ctx->reconfigCount++;
+      }
+    }
+
+    // NetInt target bitrate reconfiguration support
+    side_data = av_frame_get_side_data(&ctx->buffered_fme,
+                                       AV_FRAME_DATA_NETINT_BITRATE);
+    if (side_data && (side_data->size == sizeof(int32_t)))
+    {
+      if (ctx->api_param.enable_vfr)
+      {
+        //ctx->api_params.hevc_enc_params.frame_rate is the default framerate when vfr enabled
+        int32_t bitrate = *((int32_t *)side_data->data);
+        ctx->api_ctx.init_bitrate = bitrate;
+        bitrate = bitrate * ctx->api_param.hevc_enc_params.frame_rate / ctx->api_ctx.prev_fps;
+        *(int32_t *)side_data->data = bitrate;
+        ctx->api_ctx.prev_bitrate = bitrate;
+      }
+
+      aux_data = ni_logan_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_BITRATE,
+                                       sizeof(int32_t));
+      if (aux_data)
+      {
+        memcpy(aux_data->data, side_data->data, side_data->size);
+      }
+
+      if (ctx->api_fme.data.frame.reconf_len == 0)
+      {
+        ctx->reconfigCount++;
+      }
+    }
+
+    // NetInt support VFR by reconfig bitrate and vui
+    if (ctx->api_param.enable_vfr)
+    {
+      int32_t cur_fps = 0, bit_rate = 0;
+
+      if (ctx->buffered_fme.pts > ctx->api_ctx.prev_pts)
+      {
+        ctx->api_ctx.passed_time_in_timebase_unit += ctx->buffered_fme.pts - ctx->api_ctx.prev_pts;
+        ctx->api_ctx.count_frame_num_in_sec++;
+        //change the bitrate for VFR
+        //1. Only when the fps change, setting the new bitrate
+        //2. The interval between two bitrate change settings shall be greater than 1 seconds(hardware limiation)
+        //   or at the start the transcoding, we should detect the init frame rate(30) and the actual framerate
+        if (ctx->api_ctx.passed_time_in_timebase_unit >= (avctx->time_base.den / avctx->time_base.num))
+        {
+          cur_fps = ctx->api_ctx.count_frame_num_in_sec;
+          bit_rate = (int)(ctx->api_param.hevc_enc_params.frame_rate * (ctx->api_ctx.init_bitrate / cur_fps));
+          if ((ctx->api_ctx.frame_num != 0) && (bit_rate != ctx->api_ctx.prev_bitrate) &&
+              ((ctx->api_ctx.frame_num < ctx->api_param.hevc_enc_params.frame_rate) || 
+               ((uint32_t)(ctx->api_ctx.frame_num - ctx->api_ctx.last_change_framenum) >= ctx->api_param.hevc_enc_params.frame_rate)))
+          {
+            //adjust the upper and lower limits of bitrate each time
+            bit_rate = av_clip(bit_rate, ctx->api_ctx.prev_bitrate / 2, ctx->api_ctx.prev_bitrate * 3 / 2);
+
+            aux_data = ni_logan_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_BITRATE,
+                                         sizeof(int32_t));
+            if (aux_data)
+            {
+              memcpy(aux_data->data, &bit_rate, sizeof(int32_t));
+            }
+
+            ctx->api_ctx.prev_bitrate = bit_rate;
+            ctx->api_ctx.last_change_framenum = ctx->api_ctx.frame_num;
+            ctx->api_ctx.prev_fps     = cur_fps;
+          }
+          ctx->api_ctx.count_frame_num_in_sec = 0;
+          ctx->api_ctx.passed_time_in_timebase_unit = 0;
+        }
+        ctx->api_ctx.prev_pts = ctx->buffered_fme.pts;
+      }
+      else if (ctx->buffered_fme.pts < ctx->api_ctx.prev_pts)
+      {
+        //error handle for the case that pts jump back
+        //this may cause a little error in the bitrate setting, This little error is acceptable.
+        //As long as the subsequent, PTS is normal, it will be repaired quickly.
+        ctx->api_ctx.prev_pts = ctx->buffered_fme.pts;
+      }
+      else
+      {
+        //do nothing, when the pts of two adjacent frames are the same
+        //this may cause a little error in the bitrate setting, This little error is acceptable.
+        //As long as the subsequent, PTS is normal, it will be repaired quickly.
+      }
+    }
+
+    // force pic qp demo mode: initial QP (200 frames) -> QP value specified by
+    // ForcePicQpDemoMode (100 frames) -> initial QP (remaining frames)
+    if (p_param->force_pic_qp_demo_mode)
+    {
+      if (ctx->api_ctx.frame_num >= 300)
+      {
+        ctx->api_fme.data.frame.force_pic_qp =
+        p_param->hevc_enc_params.rc.intra_qp;
+      }
+      else if (ctx->api_ctx.frame_num >= 200)
+      {
+        ctx->api_fme.data.frame.force_pic_qp = p_param->force_pic_qp_demo_mode;
+      }
+    }
+    // END NETINT_INTERNAL - currently only for internal testing
+
+    // SEI (HDR)
+    // content light level info
+    AVFrameSideData *hdr_side_data;
+
+    hdr_side_data = av_frame_get_side_data(
+      &ctx->buffered_fme, AV_FRAME_DATA_CONTENT_LIGHT_LEVEL);
+    if (hdr_side_data && hdr_side_data->size == sizeof(AVContentLightMetadata))
+    {
+      aux_data = ni_logan_frame_new_aux_data(&dec_frame,
+                                       NI_FRAME_AUX_DATA_CONTENT_LIGHT_LEVEL,
+                                       sizeof(ni_content_light_level_t));
+      if (aux_data)
+      {
+        memcpy(aux_data->data, hdr_side_data->data, hdr_side_data->size);
+      }
+    }
+
+    // mastering display color volume
+    hdr_side_data = av_frame_get_side_data(
+      &ctx->buffered_fme, AV_FRAME_DATA_MASTERING_DISPLAY_METADATA);
+    if (hdr_side_data &&
+        hdr_side_data->size == sizeof(AVMasteringDisplayMetadata))
+    {
+      aux_data = ni_logan_frame_new_aux_data(
+        &dec_frame, NI_FRAME_AUX_DATA_MASTERING_DISPLAY_METADATA,
+        sizeof(ni_mastering_display_metadata_t));
+      if (aux_data)
+      {
+        memcpy(aux_data->data, hdr_side_data->data, hdr_side_data->size);
+      }
+    }
+
+    // SEI (HDR10+)
+    AVFrameSideData *s_data = av_frame_get_side_data(
+      &ctx->buffered_fme, AV_FRAME_DATA_DYNAMIC_HDR_PLUS);
+    if (s_data && s_data->size == sizeof(AVDynamicHDRPlus))
+    {
+      aux_data = ni_logan_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_HDR_PLUS,
+                                       sizeof(ni_dynamic_hdr_plus_t));
+      if (aux_data)
+      {
+        memcpy(aux_data->data, s_data->data, s_data->size);
+      }
+    } // hdr10+
+
+    // SEI (close caption)
+    side_data = av_frame_get_side_data(&ctx->buffered_fme,AV_FRAME_DATA_A53_CC);
+    if (side_data && side_data->size > 0)
+    {
+      aux_data = ni_logan_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_A53_CC,
+                                       side_data->size);
+      if (aux_data)
+      {
+        memcpy(aux_data->data, side_data->data, side_data->size);
+      }
+    }
+
+    // supply QP map if ROI enabled and if ROIs passed in
+    const AVFrameSideData *p_sd = av_frame_get_side_data(
+      &ctx->buffered_fme, AV_FRAME_DATA_REGIONS_OF_INTEREST);
+    if (p_param->hevc_enc_params.roi_enable && p_sd)
+    {
+      aux_data = ni_logan_frame_new_aux_data(&dec_frame,
+                                       NI_FRAME_AUX_DATA_REGIONS_OF_INTEREST,
+                                       p_sd->size);
+      if (aux_data)
+      {
+        memcpy(aux_data->data, p_sd->data, p_sd->size);
+      }
+    }
+
+    // User data unregistered SEI
+    side_data = av_frame_get_side_data(&ctx->buffered_fme,
+                                       AV_FRAME_DATA_NETINT_UDU_SEI);
+    if (side_data && side_data->size > 0)
+    {
+      uint8_t *sei_data = (uint8_t *)side_data->data;
+      aux_data = ni_logan_frame_new_aux_data(&dec_frame, NI_FRAME_AUX_DATA_UDU_SEI,
+                                       side_data->size);
+      if (aux_data)
+      {
+        memcpy(aux_data->data, (uint8_t *)side_data->data, side_data->size);
+      }
+    }
+
+    ctx->api_fme.data.frame.pts = ctx->buffered_fme.pts;
+    ctx->api_fme.data.frame.dts = ctx->buffered_fme.pkt_dts;
+    ctx->api_fme.data.frame.video_width = ODD2EVEN(avctx->width);
+    ctx->api_fme.data.frame.video_height = ODD2EVEN(avctx->height);
+
+    ctx->api_fme.data.frame.ni_logan_pict_type = 0;
+
+    if (ctx->api_ctx.force_frame_type)
+    {
+      switch (ctx->buffered_fme.pict_type)
+      {
+        case AV_PICTURE_TYPE_I:
+          ctx->api_fme.data.frame.ni_logan_pict_type = LOGAN_PIC_TYPE_FORCE_IDR;
+          break;
+        case AV_PICTURE_TYPE_P:
+          ctx->api_fme.data.frame.ni_logan_pict_type = LOGAN_PIC_TYPE_P;
+          break;
+        default:
+          ;
+      }
+    }
+    else if (AV_PICTURE_TYPE_I == ctx->buffered_fme.pict_type)
+    {
+      ctx->api_fme.data.frame.force_key_frame = 1;
+      ctx->api_fme.data.frame.ni_logan_pict_type = LOGAN_PIC_TYPE_FORCE_IDR;
+    }
+
+    // whether should send SEI with this frame
+    int send_sei_with_idr = ni_logan_should_send_sei_with_frame(
+      &ctx->api_ctx, ctx->api_fme.data.frame.ni_logan_pict_type, p_param);
+
+    av_log(avctx, AV_LOG_TRACE, "%s: #%"PRIu64" ni_logan_pict_type %d "
+           "forced_header_enable %d intraPeriod %d send_sei_with_idr: %s\n",
+           __FUNCTION__,
+           ctx->api_ctx.frame_num, ctx->api_fme.data.frame.ni_logan_pict_type,
+           p_param->hevc_enc_params.forced_header_enable,
+           p_param->hevc_enc_params.intra_period,
+           send_sei_with_idr ? "Yes" : "No");
+
+    // data buffer for various SEI: HDR mastering display color volume, HDR
+    // content light level, close caption, User data unregistered, HDR10+ etc.
+    uint8_t mdcv_data[NI_LOGAN_MAX_SEI_DATA];
+    uint8_t cll_data[NI_LOGAN_MAX_SEI_DATA];
+    uint8_t cc_data[NI_LOGAN_MAX_SEI_DATA];
+    uint8_t udu_data[NI_LOGAN_MAX_SEI_DATA];
+    uint8_t hdrp_data[NI_LOGAN_MAX_SEI_DATA];
+
+    // prep for auxiliary data (various SEI, ROI) in encode frame, based on the
+    // data returned in decoded frame
+    ni_logan_enc_prep_aux_data(&ctx->api_ctx, &ctx->api_fme.data.frame, &dec_frame,
+                         ctx->api_ctx.codec_format, send_sei_with_idr,
+                         mdcv_data, cll_data, cc_data, udu_data, hdrp_data);
+
+    // DolbyVision (HRD SEI), HEVC only for now
+    uint8_t hrd_buf[NI_LOGAN_MAX_SEI_DATA];
+    uint32_t hrd_sei_len = 0; // HRD SEI size in bytes
+    if (AV_CODEC_ID_HEVC == avctx->codec_id && p_param->hrd_enable)
+    {
+      if (send_sei_with_idr)
+      {
+        hrd_sei_len += encode_buffering_period_sei(p_param, ctx,
+                                                   ctx->api_ctx.frame_num + 1,
+                                                   hrd_buf);
+      }
+      //printf(" ^^^^ frame_num %u  idr %d\n", ctx->api_ctx.frame_num, send_sei_with_idr);
+      // pic_timing SEI will inserted after encoding
+      ctx->api_fme.data.frame.sei_total_len += hrd_sei_len;
+    }
+
+    side_data = av_frame_get_side_data(&ctx->buffered_fme,
+                                       AV_FRAME_DATA_NETINT_CUSTOM_SEI);
+    if (side_data && side_data->size > 0)
+    {
+      int i = 0;
+      int64_t local_pts = ctx->buffered_fme.pts;
+      ni_logan_all_custom_sei_t *src_all_custom_sei = (ni_logan_all_custom_sei_t *)side_data->data;
+      ni_logan_custom_sei_t *src_custom_sei = NULL;
+      uint8_t *src_sei_data = NULL;
+      int custom_sei_size = 0;
+      int custom_sei_size_trans = 0;
+      uint8_t custom_sei_type;
+      uint8_t sei_idx;
+      int sei_len;
+      ni_logan_all_custom_sei_t *dst_all_custom_sei = NULL;
+      ni_logan_custom_sei_t *dst_custom_sei = NULL;
+      uint8_t *dst_sei_data = NULL;
+
+      dst_all_custom_sei = malloc(sizeof(ni_logan_all_custom_sei_t));
+      if (dst_all_custom_sei == NULL)
+      {
+        av_log(avctx, AV_LOG_ERROR, "failed to allocate memory for custom sei "
+               "data, len:%" PRIu64 ".\n", sizeof(ni_logan_all_custom_sei_t));
+        ret = AVERROR(ENOMEM);
+        return ret;
+      }
+      memset(dst_all_custom_sei, 0 ,sizeof(ni_logan_all_custom_sei_t));
+
+      for (sei_idx = 0; sei_idx < src_all_custom_sei->custom_sei_cnt; sei_idx++)
+      {
+        src_custom_sei = &src_all_custom_sei->ni_custom_sei[sei_idx];
+
+        custom_sei_type = src_custom_sei->custom_sei_type;
+        custom_sei_size = src_custom_sei->custom_sei_size;
+        src_sei_data = src_custom_sei->custom_sei_data;
+
+        dst_custom_sei = &dst_all_custom_sei->ni_custom_sei[sei_idx];
+        dst_sei_data = dst_custom_sei->custom_sei_data;
+        sei_len = 0;
+
+        /* fill sei buffer */
+        // long start code
+        dst_sei_data[sei_len++] = 0x00;
+        dst_sei_data[sei_len++] = 0x00;
+        dst_sei_data[sei_len++] = 0x00;
+        dst_sei_data[sei_len++] = 0x01;
+        if (AV_CODEC_ID_H264 == avctx->codec_id)
+        {
+          dst_sei_data[sei_len++] = 0x06;   //nal type: SEI
+        }
+        else
+        {
+          dst_sei_data[sei_len++] = 0x4e;   //nal type: SEI
+          dst_sei_data[sei_len++] = 0x01;
+        }
+
+        // SEI type
+        dst_sei_data[sei_len++] = custom_sei_type;
+
+        // original payload size
+        custom_sei_size_trans = custom_sei_size;
+        while (custom_sei_size_trans >= 0)
+        {
+          dst_sei_data[sei_len++] = (custom_sei_size_trans > 0xFF ? 0xFF : (uint8_t)custom_sei_size_trans);
+          custom_sei_size_trans -= 0xFF;
+        }
+
+        // payload data
+        for (i = 0; (i < custom_sei_size) && (sei_len < (NI_LOGAN_MAX_CUSTOM_SEI_SZ - 2)); i++)
+        {
+          if ((2 <= i) && !dst_sei_data[sei_len - 2] && !dst_sei_data[sei_len - 1] && (src_sei_data[i] <= 0x03))
+          {
+            /* insert 0x3 as emulation_prevention_three_byte */
+            dst_sei_data[sei_len++] = 0x03;
+          }
+          dst_sei_data[sei_len++] = src_sei_data[i];
+        }
+
+        if (i != custom_sei_size)
+        {
+          av_log(avctx, AV_LOG_WARNING, "%s: sei RBSP size out of limit(%d), "
+                 "idx=%u, type=%u, size=%d, custom_sei_loc=%d\n", __FUNCTION__,
+                 NI_LOGAN_MAX_CUSTOM_SEI_SZ, sei_idx, custom_sei_type,
+                 custom_sei_size, src_custom_sei->custom_sei_loc);
+          free(dst_all_custom_sei);
+          dst_all_custom_sei = NULL;
+          break;
+        }
+
+        // trailing byte
+        dst_sei_data[sei_len++] = 0x80;
+
+        dst_custom_sei->custom_sei_size = sei_len;
+        dst_custom_sei->custom_sei_type = custom_sei_type;
+        dst_custom_sei->custom_sei_loc = src_custom_sei->custom_sei_loc;
+        av_log(avctx, AV_LOG_TRACE, "%s: sei idx=%u,type=%u, len=%d, "
+               "custom_sei_loc=%d\n", __FUNCTION__, sei_idx, custom_sei_type,
+               sei_len, dst_custom_sei->custom_sei_loc);
+      }
+
+      if (dst_all_custom_sei)
+      {
+        dst_all_custom_sei->custom_sei_cnt = src_all_custom_sei->custom_sei_cnt;
+        ctx->api_ctx.pkt_custom_sei[local_pts % NI_LOGAN_FIFO_SZ] = dst_all_custom_sei;
+        av_log(avctx, AV_LOG_TRACE, "%s: sei number %d pts %" PRId64 ".\n",
+               __FUNCTION__, dst_all_custom_sei->custom_sei_cnt, local_pts);
+      }
+    }
+
+    if (ctx->api_fme.data.frame.sei_total_len > NI_LOGAN_ENC_MAX_SEI_BUF_SIZE)
+    {
+      av_log(avctx, AV_LOG_ERROR, "%s: sei total length %u exceeds maximum sei "
+             "size %u.\n", __FUNCTION__, ctx->api_fme.data.frame.sei_total_len,
+             NI_LOGAN_ENC_MAX_SEI_BUF_SIZE);
+      ret = AVERROR(EINVAL);
+      return ret;
+    }
+
+    ctx->api_fme.data.frame.extra_data_len += ctx->api_fme.data.frame.sei_total_len;
+    // FW layout requirement: leave space for reconfig data if SEI and/or ROI
+    // is present
+    if ((ctx->api_fme.data.frame.sei_total_len ||
+         ctx->api_fme.data.frame.roi_len)
+        && !ctx->api_fme.data.frame.reconf_len)
+    {
+      ctx->api_fme.data.frame.extra_data_len += sizeof(ni_logan_encoder_change_params_t);
+    }
+
+    if (ctx->api_ctx.auto_dl_handle != NI_INVALID_DEVICE_HANDLE)
+    {
+      is_hwframe = 0;
+      format_in_use = avctx->sw_pix_fmt;
+      av_log(avctx, AV_LOG_TRACE, "%s: Autodownload mode, disable hw frame\n",
+             __FUNCTION__);
+    }
+    else
+    {
+      format_in_use = ctx->buffered_fme.format;
+    }
+
+    if (is_hwframe)
+    {
+      ret = sizeof(ni_logan_hwframe_surface_t);
+    }
+    else
+    {
+      ret = av_image_get_buffer_size(format_in_use,
+                                     ctx->buffered_fme.width,
+                                     ctx->buffered_fme.height, 1);
+    }
+#if FF_API_PKT_PTS
+    av_log(avctx, AV_LOG_TRACE, "%s: pts=%" PRId64 ", pkt_dts=%" PRId64 ", "
+           "pkt_pts=%" PRId64 "\n", __FUNCTION__, ctx->buffered_fme.pts,
+           ctx->buffered_fme.pkt_dts, ctx->buffered_fme.pkt_pts);
+#endif
+    av_log(avctx, AV_LOG_TRACE, "%s: buffered_fme.format=%d, width=%d, "
+           "height=%d, pict_type=%d, key_frame=%d, size=%d\n", __FUNCTION__,
+           format_in_use, ctx->buffered_fme.width, ctx->buffered_fme.height,
+           ctx->buffered_fme.pict_type, ctx->buffered_fme.key_frame, ret);
+
+    if (ret < 0)
+    {
+      return ret;
+    }
+
+    if (is_hwframe)
+    {
+      uint8_t *dsthw;
+      const uint8_t *srchw;
+      ni_logan_frame_buffer_alloc_hwenc(&(ctx->api_fme.data.frame),
+                                  ODD2EVEN(ctx->buffered_fme.width),
+                                  ODD2EVEN(ctx->buffered_fme.height),
+                                  ctx->api_fme.data.frame.extra_data_len);
+      if (!ctx->api_fme.data.frame.p_data[3])
+      {
+        return AVERROR(ENOMEM);
+      }
+
+      dsthw = ctx->api_fme.data.frame.p_data[3];
+      srchw = (const uint8_t *) ctx->buffered_fme.data[3];
+      av_log(avctx, AV_LOG_TRACE, "dst=%p src=%p, len =%d\n", dsthw, srchw, ctx->api_fme.data.frame.data_len[3]);
+      memcpy(dsthw, srchw, ctx->api_fme.data.frame.data_len[3]);
+      av_log(avctx, AV_LOG_TRACE, "session_id:%u, FrameIdx:%d, %d, W-%u, H-%u, bit_depth:%d, encoding_type:%d\n",
+             ((ni_logan_hwframe_surface_t *)dsthw)->ui16SessionID,
+             ((ni_logan_hwframe_surface_t *)dsthw)->i8FrameIdx,
+             ((ni_logan_hwframe_surface_t *)dsthw)->i8InstID,
+             ((ni_logan_hwframe_surface_t *)dsthw)->ui16width,
+             ((ni_logan_hwframe_surface_t *)dsthw)->ui16height,
+             ((ni_logan_hwframe_surface_t *)dsthw)->bit_depth,
+             ((ni_logan_hwframe_surface_t *)dsthw)->encoding_type);
+    }
+    else
+    {
+      int dst_stride[NI_LOGAN_MAX_NUM_DATA_POINTERS] = {0};
+      int dst_height_aligned[NI_LOGAN_MAX_NUM_DATA_POINTERS] = {0};
+      int src_height[NI_LOGAN_MAX_NUM_DATA_POINTERS] = {0};
+
+      src_height[0] = ctx->buffered_fme.height;
+      src_height[1] = src_height[2] = ctx->buffered_fme.height / 2;
+
+      ni_logan_get_hw_yuv420p_dim(frame_width, frame_height,
+                            ctx->api_ctx.bit_depth_factor,
+                            avctx->codec_id == AV_CODEC_ID_H264,
+                            dst_stride, dst_height_aligned);
+
+      // alignment(16) extra padding for H.264 encoding
+      ni_logan_encoder_frame_buffer_alloc(&(ctx->api_fme.data.frame),
+                                    ODD2EVEN(ctx->buffered_fme.width),
+                                    ODD2EVEN(ctx->buffered_fme.height),
+                                    dst_stride,
+                                    (avctx->codec_id == AV_CODEC_ID_H264),
+                                    ctx->api_fme.data.frame.extra_data_len,
+                                    ctx->api_ctx.bit_depth_factor);
+      if (!ctx->api_fme.data.frame.p_data[0])
+      {
+        return AVERROR(ENOMEM);
+      }
+
+      if (ctx->api_ctx.auto_dl_handle == NI_INVALID_DEVICE_HANDLE)
+      {
+        av_log(avctx, AV_LOG_TRACE, "%s: api_fme.data_len[0]=%d,"
+               "buffered_fme.linesize=%d/%d/%d, dst alloc linesize = %d/%d/%d, "
+               "src height = %d/%d%d, dst height aligned = %d/%d/%d, "
+               "ctx->api_fme.force_key_frame=%d, extra_data_len=%d sei_size=%u "
+               "(hdr_content_light_level %u hdr_mastering_display_color_vol %u "
+               "hdr10+ %u cc %u udu %u prefC %u hrd %u) "
+               "reconf_size=%u roi_size=%u force_pic_qp=%u "
+               "use_cur_src_as_long_term_pic %u use_long_term_ref %u\n",
+               __FUNCTION__, ctx->api_fme.data.frame.data_len[0],
+               ctx->buffered_fme.linesize[0],
+               ctx->buffered_fme.linesize[1],
+               ctx->buffered_fme.linesize[2],
+               dst_stride[0], dst_stride[1], dst_stride[2],
+               src_height[0], src_height[1], src_height[2],
+               dst_height_aligned[0], dst_height_aligned[1], dst_height_aligned[2],
+               ctx->api_fme.data.frame.force_key_frame,
+               ctx->api_fme.data.frame.extra_data_len,
+               ctx->api_fme.data.frame.sei_total_len,
+               ctx->api_fme.data.frame.sei_hdr_content_light_level_info_len,
+               ctx->api_fme.data.frame.sei_hdr_mastering_display_color_vol_len,
+               ctx->api_fme.data.frame.sei_hdr_plus_len,
+               ctx->api_fme.data.frame.sei_cc_len,
+               ctx->api_fme.data.frame.sei_user_data_unreg_len,
+               ctx->api_fme.data.frame.preferred_characteristics_data_len,
+               hrd_sei_len,
+               ctx->api_fme.data.frame.reconf_len,
+               ctx->api_fme.data.frame.roi_len,
+               ctx->api_fme.data.frame.force_pic_qp,
+               ctx->api_fme.data.frame.use_cur_src_as_long_term_pic,
+               ctx->api_fme.data.frame.use_long_term_ref);
+
+        // YUV part of the encoder input data layout
+        ni_logan_copy_hw_yuv420p((uint8_t **) ctx->api_fme.data.frame.p_data,
+                           ctx->buffered_fme.data, ctx->buffered_fme.width,
+                           ctx->buffered_fme.height,
+                           ctx->api_ctx.bit_depth_factor,
+                           dst_stride, dst_height_aligned,
+                           ctx->buffered_fme.linesize, src_height);
+
+        av_log(avctx, AV_LOG_TRACE, "After memcpy p_data 0:0x%p, 1:0x%p, 2:0x%p"
+               " len:0:%d 1:%d 2:%d\n",
+               ctx->api_fme.data.frame.p_data[0],
+               ctx->api_fme.data.frame.p_data[1],
+               ctx->api_fme.data.frame.p_data[2],
+               ctx->api_fme.data.frame.data_len[0],
+               ctx->api_fme.data.frame.data_len[1],
+               ctx->api_fme.data.frame.data_len[2]);
+      }
+      else
+      {
+        ni_logan_hwframe_surface_t *src_surf;
+        ni_logan_session_data_io_t *p_session_data;
+        av_log(avctx, AV_LOG_TRACE, "%s: Autodownload to be run\n", __FUNCTION__);
+        avhwf_ctx = (AVHWFramesContext*)ctx->buffered_fme.hw_frames_ctx->data;
+        nif_src_ctx = avhwf_ctx->internal->priv;
+        src_surf = (ni_logan_hwframe_surface_t*)ctx->buffered_fme.data[3];
+        p_session_data = &ctx->api_fme;
+
+        ret = ni_logan_device_session_hwdl(&nif_src_ctx->api_ctx, p_session_data, src_surf);
+        if (ret <= 0)
+        {
+          av_log(avctx, AV_LOG_ERROR, "nienc.c:ni_logan_hwdl_frame() failed to retrieve frame\n");
+          return AVERROR_EXTERNAL;
+        }
+      }
+    }
+
+    // auxiliary data part of the encoder input data layout
+    ni_logan_enc_copy_aux_data(&ctx->api_ctx, &ctx->api_fme.data.frame, &dec_frame,
+                         ctx->api_ctx.codec_format, mdcv_data, cll_data,
+                         cc_data, udu_data, hdrp_data);
+    ni_logan_frame_buffer_free(&dec_frame);
+
+    // fill in HRD SEI if available
+    if (hrd_sei_len)
+    {
+      uint8_t *dst = (uint8_t *)ctx->api_fme.data.frame.p_data[3] +
+      ctx->api_fme.data.frame.data_len[3] + NI_LOGAN_APP_ENC_FRAME_META_DATA_SIZE;
+
+      // skip data portions already filled in until the HRD SEI part;
+      // reserve reconfig size if any of sei, roi or reconfig is present
+      if (ctx->api_fme.data.frame.reconf_len ||
+          ctx->api_fme.data.frame.roi_len ||
+          ctx->api_fme.data.frame.sei_total_len)
+      {
+        dst += sizeof(ni_logan_encoder_change_params_t);
+      }
+
+      // skip any of the following data types enabled, to get to HRD location:
+      // - ROI map
+      // - HDR mastering display color volume
+      // - HDR content light level info
+      // - HLG preferred characteristics SEI
+      // - close caption
+      // - HDR10+
+      // - User data unregistered SEI
+      dst += ctx->api_fme.data.frame.roi_len +
+      ctx->api_fme.data.frame.sei_hdr_mastering_display_color_vol_len +
+      ctx->api_fme.data.frame.sei_hdr_content_light_level_info_len +
+      ctx->api_fme.data.frame.preferred_characteristics_data_len +
+      ctx->api_fme.data.frame.sei_cc_len +
+      ctx->api_fme.data.frame.sei_hdr_plus_len +
+      ctx->api_fme.data.frame.sei_user_data_unreg_len;
+
+      memcpy(dst, hrd_buf, hrd_sei_len);
+    }
+
+    ctx->sentFrame = 1;
+  }
+#ifdef NIENC_MULTI_THREAD
+  if (ctx->encoder_flushing)
+  {
+    sent = ni_logan_device_session_write(&ctx->api_ctx, &ctx->api_fme, NI_LOGAN_DEVICE_TYPE_ENCODER);
+
+    av_log(avctx, AV_LOG_DEBUG, "%s encoder_flushing: size %d sent to xcoder\n",
+           __FUNCTION__, sent);
+
+    if (NI_LOGAN_RETCODE_ERROR_RESOURCE_UNAVAILABLE == sent)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "%s(): Sequence Change in progress, return "
+             " EAGAIN\n", __FUNCTION__);
+      ret = AVERROR(EAGAIN);
+      return ret;
+    }
+    else if (NI_LOGAN_RETCODE_ERROR_VPU_RECOVERY == sent)
+    {
+      sent = xcoder_logan_encode_reset(avctx);
+    }
+
+    if (sent < 0)
+    {
+      ret = AVERROR(EIO);
+    }
+    else
+    {
+      if (frame && is_hwframe)
+      {
+        av_frame_ref(ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]], frame);
+        av_log(avctx, AV_LOG_DEBUG, "AVframe_index = %d popped from head %d\n",
+               ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+        if (deq_logan_free_frames(ctx)!= 0)
+        {
+          ret = AVERROR_EXTERNAL;
+          return ret;
+        }
+      }
+      //pushing input pts in circular FIFO
+      ctx->api_ctx.enc_pts_list[ctx->api_ctx.enc_pts_w_idx % NI_LOGAN_FIFO_SZ] = ctx->api_fme.data.frame.pts;
+      ctx->api_ctx.enc_pts_w_idx++;
+      ret = 0;
+    }
+  }
+  else if (is_hwframe)
+  {
+    sent = ni_logan_device_session_write(&ctx->api_ctx, &ctx->api_fme, NI_LOGAN_DEVICE_TYPE_ENCODER);
+    //ctx->sframe_pool[((niFrameSurface1_t*)((uint8_t*)frame->data[3]))->i8FrameIdx] = av_buffer_ref(frame);
+    av_log(avctx, AV_LOG_DEBUG, "%s: size %d sent to xcoder\n",
+           __FUNCTION__, sent);
+
+    if (NI_LOGAN_RETCODE_ERROR_RESOURCE_UNAVAILABLE == sent)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "%s(): Sequence Change in progress, return "
+             "EAGAIN\n", __FUNCTION__);
+      ret = AVERROR(EAGAIN);
+      return ret;
+    }
+
+    if (sent == -1)
+    {
+      ret = AVERROR(EAGAIN);
+    }
+    else
+    {
+      av_frame_ref(ctx->sframe_pool[((ni_logan_hwframe_surface_t*)((uint8_t*)frame->data[3]))->i8FrameIdx], frame);
+      av_log(avctx, AV_LOG_DEBUG, "AVframe_index = %d popped from head %d\n",
+             ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+      if (deq_logan_free_frames(ctx) != 0)
+      {
+        ret = AVERROR_EXTERNAL;
+        return ret;
+      }
+      //av_frame_ref(ctx->sframe_pool[((ni_logan_hwframe_surface_t*)((uint8_t*)frame->data[3]))->ui16FrameIdx], frame);
+      ret = 0;
+    }
+  }
+  else
+  {
+    av_log(avctx, AV_LOG_DEBUG, "%s start 111 %p, session_info %d, "
+           "device_handle %d\n", __FUNCTION__, ctx->api_ctx.session_info,
+           ctx->api_ctx.session_id, ctx->api_ctx.device_handle);
+    if ((ctx->api_ctx.session_id != NI_LOGAN_INVALID_SESSION_ID) && (ctx->api_ctx.device_handle != NI_INVALID_DEVICE_HANDLE))
+    {
+      av_log(avctx, AV_LOG_DEBUG, "%s start 111 %p\n",
+             __FUNCTION__, ctx->api_ctx.session_info);
+      write_thread_arg_struct_t *write_thread_args = (write_thread_arg_struct_t *)malloc(sizeof(write_thread_arg_struct_t));
+      pthread_mutex_init(&write_thread_args->mutex, NULL);
+      pthread_cond_init(&write_thread_args->cond, NULL);
+      write_thread_args->running = 0;
+      write_thread_args->ctx = ctx;
+      av_log(avctx, AV_LOG_DEBUG, "%s: session_id %d, device_handle %d\n",
+             __FUNCTION__, ctx->api_ctx.session_id, ctx->api_ctx.device_handle);
+      av_log(avctx, AV_LOG_DEBUG, "%s: ctx %p\n",
+             __FUNCTION__, write_thread_args->ctx);
+      ctx->api_ctx.session_info = (void *)write_thread_args;
+      write_thread_args->running = 1;
+      ret = threadpool_auto_add_task_thread(&pool, write_frame_thread, write_thread_args, 1);
+      if (ret < 0)
+      {
+        av_log(avctx, AV_LOG_ERROR, "failed to add_task_thread to threadpool\n");
+        return ret;
+      }
+    }
+  }
+#else
+  sent = ni_logan_device_session_write(&ctx->api_ctx, &ctx->api_fme, NI_LOGAN_DEVICE_TYPE_ENCODER);
+  av_log(avctx, AV_LOG_DEBUG, "%s: pts %lld dts %lld size %d sent to xcoder\n",
+         __FUNCTION__, ctx->api_fme.data.frame.pts, ctx->api_fme.data.frame.dts, sent);
+
+  // return EIO at error
+  if (NI_LOGAN_RETCODE_ERROR_VPU_RECOVERY == sent)
+  {
+    ret = xcoder_logan_encode_reset(avctx);
+    if (ret < 0)
+    {
+      av_log(avctx, AV_LOG_ERROR, "%s(): VPU recovery failed:%d, return EIO\n",
+             __FUNCTION__, sent);
+      ret = AVERROR(EIO);
+    }
+    return ret;
+  }
+  else if (sent < 0)
+  {
+    av_log(avctx, AV_LOG_ERROR, "%s(): failure sent (%d) , return EIO\n",
+           __FUNCTION__, sent);
+    ret = AVERROR(EIO);
+
+    // if rejected due to sequence change in progress, revert resolution
+    // setting and will do it again next time.
+    if (ctx->api_fme.data.frame.start_of_stream &&
+        (avctx->width != orig_avctx_width ||
+         avctx->height != orig_avctx_height))
+    {
+      avctx->width = orig_avctx_width;
+      avctx->height = orig_avctx_height;
+    }
+    return ret;
+  }
+  else if (sent == 0)
+  {
+    // case of sequence change in progress
+    if (ctx->api_fme.data.frame.start_of_stream &&
+        (avctx->width != orig_avctx_width ||
+         avctx->height != orig_avctx_height))
+    {
+      avctx->width = orig_avctx_width;
+      avctx->height = orig_avctx_height;
+    }
+
+    // when buffer_full, drop the frame and return EAGAIN if in strict timeout
+    // mode, otherwise buffer the frame and it is to be sent out using encode2
+    // API: queue the frame only if not done so yet, i.e. queue is empty
+    // *and* it's a valid frame. ToWatch: what are other rc cases ?
+    if (ctx->api_ctx.status == NI_LOGAN_RETCODE_NVME_SC_WRITE_BUFFER_FULL)
+    {
+      if (ctx->api_param.strict_timeout_mode)
+      {
+        av_log(avctx, AV_LOG_ERROR, "%s: Error Strict timeout period exceeded, "
+               "return EAGAIN\n", __FUNCTION__);
+        ret = AVERROR(EAGAIN);
+      }
+      else
+      {
+        av_log(avctx, AV_LOG_DEBUG, "%s: Write buffer full, returning 1\n",
+               __FUNCTION__);
+        ret = 1;
+        if (frame && is_logan_input_fifo_empty(ctx))
+        {
+          enqueue_logan_frame(avctx, frame);
+        }
+      }
+    }
+  }
+  else
+  {
+    if (!ctx->eos_fme_received && is_hwframe)
+    {
+      av_frame_ref(ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]], &ctx->buffered_fme);
+      av_log(avctx, AV_LOG_DEBUG, "AVframe_index = %d popped from free head %d\n", ctx->aFree_Avframes_list[ctx->freeHead], ctx->freeHead);
+      av_log(avctx, AV_LOG_TRACE, "ctx->buffered_fme.data[3] %p sframe_pool[%d]->data[3] %p\n",
+             ctx->buffered_fme.data[3], ctx->aFree_Avframes_list[ctx->freeHead],
+             ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]->data[3]);
+      if (ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]->data[3])
+      {
+        av_log(avctx, AV_LOG_DEBUG, "sframe_pool[%d] ui16FrameIdx %u, device_handle %" PRId64 ".\n",
+               ctx->aFree_Avframes_list[ctx->freeHead],
+               ((ni_logan_hwframe_surface_t*)((uint8_t*)ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]->data[3]))->i8FrameIdx,
+               ((ni_logan_hwframe_surface_t*)((uint8_t*)ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]->data[3]))->device_handle);
+        av_log(avctx, AV_LOG_TRACE, "%s: after ref sframe_pool, hw frame av_buffer_get_ref_count=%d, data[3]=%p\n",
+               __FUNCTION__, av_buffer_get_ref_count(ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]->buf[0]),
+               ctx->sframe_pool[ctx->aFree_Avframes_list[ctx->freeHead]]->data[3]);
+      }
+      if (deq_logan_free_frames(ctx) != 0)
+      {
+        av_log(avctx, AV_LOG_ERROR, "free frames is empty\n");
+        ret = AVERROR_EXTERNAL;
+        return ret;
+      }
+    }
+
+    // only if it's NOT sequence change flushing (in which case only the eos
+    // was sent and not the first sc pkt) AND
+    // only after successful sending will it be removed from fifo
+    if (LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING != ctx->api_ctx.session_run_state)
+    {
+      if (! is_logan_input_fifo_empty(ctx))
+      {
+        av_fifo_drain(ctx->fme_fifo, sizeof(AVFrame));
+        av_log(avctx, AV_LOG_DEBUG, "fme popped pts:%" PRId64 ", "
+               "fifo size: %" PRIu64 "\n",  ctx->buffered_fme.pts,
+               av_fifo_size(ctx->fme_fifo) / sizeof(AVFrame));
+      }
+      av_frame_unref(&ctx->buffered_fme);
+    }
+    else
+    {
+      av_log(avctx, AV_LOG_TRACE, "XCoder frame(eos) sent, sequence changing! NO fifo pop !\n");
+    }
+
+    //pushing input pts in circular FIFO
+    ctx->api_ctx.enc_pts_list[ctx->api_ctx.enc_pts_w_idx % NI_LOGAN_FIFO_SZ] = ctx->api_fme.data.frame.pts;
+    ctx->api_ctx.enc_pts_w_idx++;
+    ret = 0;
+
+    // have another check before return: if no more frames in fifo to send and
+    // we've got eos (NULL) frame from upper stream, flag for flushing
+    if (ctx->eos_fme_received && is_logan_input_fifo_empty(ctx))
+    {
+      av_log(avctx, AV_LOG_DEBUG, "Upper stream EOS frame received, fifo empty, start flushing ..\n");
+      ctx->encoder_flushing = 1;
+    }
+  }
+#endif
+  if (ctx->encoder_flushing)
+  {
+    av_log(avctx, AV_LOG_DEBUG, "%s flushing ..\n", __FUNCTION__);
+    ret = ni_logan_device_session_flush(&ctx->api_ctx, NI_LOGAN_DEVICE_TYPE_ENCODER);
+  }
+
+  return ret;
+}
+
+static int xcoder_logan_encode_reinit(AVCodecContext *avctx)
+{
+  int ret = 0;
+  AVFrame tmp_fme;
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+  ni_logan_session_run_state_t prev_state = ctx->api_ctx.session_run_state;
+
+  ctx->eos_fme_received = 0;
+  ctx->encoder_eof = 0;
+  ctx->encoder_flushing = 0;
+
+  if (ctx->api_ctx.pts_table && ctx->api_ctx.dts_queue)
+  {
+    ff_xcoder_logan_encode_close(avctx);
+    ctx->api_ctx.session_run_state = prev_state;
+  }
+  ctx->started = 0;
+  ctx->firstPktArrived = 0;
+  ctx->spsPpsArrived = 0;
+  ctx->spsPpsHdrLen = 0;
+  ctx->p_spsPpsHdr = NULL;
+
+  // and re-init avctx's resolution to the changed one that is
+  // stored in the first frame of the fifo
+  av_fifo_generic_peek(ctx->fme_fifo, &tmp_fme, sizeof(AVFrame), NULL);
+  av_log(avctx, AV_LOG_INFO, "%s resolution changing %dx%d -> %dx%d\n",
+         __FUNCTION__, avctx->width, avctx->height, tmp_fme.width, tmp_fme.height);
+  avctx->width = tmp_fme.width;
+  avctx->height = tmp_fme.height;
+
+  ret = ff_xcoder_logan_encode_init(avctx);
+  ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_NORMAL;
+
+  while ((ret >= 0) && !is_logan_input_fifo_empty(ctx))
+  {
+    ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_QUEUED_FRAME_DRAINING;
+    ret = xcoder_send_frame(avctx, NULL);
+
+    // new resolution changes or buffer full should break flush.
+    // if needed, add new cases here
+    if (LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING == ctx->api_ctx.session_run_state)
+    {
+      av_log(avctx, AV_LOG_DEBUG, "%s(): break flush queued frames, "
+             "resolution changes again, session_run_state=%d, status=%d\n",
+             __FUNCTION__, ctx->api_ctx.session_run_state, ctx->api_ctx.status);
+      break;
+    }
+    else if (NI_LOGAN_RETCODE_NVME_SC_WRITE_BUFFER_FULL == ctx->api_ctx.status)
+    {
+      ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_NORMAL;
+      av_log(avctx, AV_LOG_DEBUG, "%s(): break flush queued frames, "
+            "because of buffer full, session_run_state=%d, status=%d\n",
+            __FUNCTION__, ctx->api_ctx.session_run_state, ctx->api_ctx.status);
+      break;
+    }
+    else
+    {
+      ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_NORMAL;
+      av_log(avctx, AV_LOG_DEBUG, "%s(): continue to flush queued frames, "
+             "ret=%d\n", __FUNCTION__, ret);
+    }
+  }
+
+  return ret;
+}
+
+static int xcoder_logan_receive_packet(AVCodecContext *avctx, AVPacket *pkt)
+{
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+  ni_logan_encoder_params_t *p_param = &ctx->api_param;
+  int ret = 0;
+  int recv;
+  ni_logan_packet_t *xpkt = &ctx->api_pkt.data.packet;
+
+  av_log(avctx, AV_LOG_DEBUG, "%s\n", __FUNCTION__);
+
+  if (ctx->encoder_eof)
+  {
+    av_log(avctx, AV_LOG_TRACE, "%s: EOS\n", __FUNCTION__);
+    return AVERROR_EOF;
+  }
+
+  ni_logan_packet_buffer_alloc(xpkt, NI_LOGAN_MAX_TX_SZ);
+  while (1)
+  {
+    xpkt->recycle_index = -1;
+    recv = ni_logan_device_session_read(&ctx->api_ctx, &ctx->api_pkt, NI_LOGAN_DEVICE_TYPE_ENCODER);
+
+    av_log(avctx, AV_LOG_TRACE, "%s: xpkt.end_of_stream=%d, xpkt.data_len=%d, "
+           "recv=%d, encoder_flushing=%d, encoder_eof=%d\n", __FUNCTION__,
+           xpkt->end_of_stream, xpkt->data_len, recv, ctx->encoder_flushing,
+           ctx->encoder_eof);
+
+    if (recv <= 0)
+    {
+      ctx->encoder_eof = xpkt->end_of_stream;
+      /* not ready ?? */
+      if (ctx->encoder_eof || xpkt->end_of_stream)
+      {
+        if (LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING ==
+            ctx->api_ctx.session_run_state)
+        {
+          // after sequence change completes, reset codec state
+          av_log(avctx, AV_LOG_INFO, "%s 1: sequence change completed, return "
+                 "EAGAIN and will reopen " "codec!\n", __FUNCTION__);
+
+          ret = xcoder_logan_encode_reinit(avctx);
+          if (ret >= 0)
+          {
+            ret = AVERROR(EAGAIN);
+          }
+          break;
+        }
+
+        ret = AVERROR_EOF;
+        av_log(avctx, AV_LOG_TRACE, "%s: got encoder_eof, "
+               "return AVERROR_EOF\n", __FUNCTION__);
+        break;
+      }
+      else
+      {
+        if (NI_LOGAN_RETCODE_ERROR_VPU_RECOVERY == recv)
+        {
+          ret = xcoder_logan_encode_reset(avctx);
+          if (ret < 0)
+          {
+            av_log(avctx, AV_LOG_ERROR, "%s(): VPU recovery failed:%d, "
+                   "returning EIO\n", __FUNCTION__, recv);
+            ret = AVERROR(EIO);
+          }
+          return ret;
+        }
+
+        if (recv < 0)
+        {
+          if ((NI_LOGAN_RETCODE_ERROR_INVALID_SESSION == recv) && !ctx->started)  // session may be in recovery state, return EAGAIN
+          {
+            av_log(avctx, AV_LOG_ERROR, "%s: VPU might be reset, "
+                   "invalid session id\n", __FUNCTION__);
+            ret = AVERROR(EAGAIN);
+          }
+          else
+          {
+            av_log(avctx, AV_LOG_ERROR, "%s: Persistent failure, "
+                   "returning EIO,ret=%d\n", __FUNCTION__, recv);
+            ret = AVERROR(EIO);
+          }
+          ctx->gotPacket = 0;
+          ctx->sentFrame = 0;
+          break;
+        }
+
+        if (ctx->api_param.low_delay_mode && ctx->sentFrame && !ctx->gotPacket)
+        {
+          av_log(avctx, AV_LOG_TRACE, "%s: low delay mode, keep reading until "
+                 "pkt arrives\n", __FUNCTION__);
+          continue;
+        }
+
+        ctx->gotPacket = 0;
+        ctx->sentFrame = 0;
+        if (!is_logan_input_fifo_empty(ctx) &&
+            (LOGAN_SESSION_RUN_STATE_NORMAL == ctx->api_ctx.session_run_state) &&
+            (NI_LOGAN_RETCODE_NVME_SC_WRITE_BUFFER_FULL != ctx->api_ctx.status))
+        {
+          ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_QUEUED_FRAME_DRAINING;
+          ret = xcoder_send_frame(avctx, NULL);
+
+          // if session_run_state is changed in xcoder_send_frame, keep it
+          if (LOGAN_SESSION_RUN_STATE_QUEUED_FRAME_DRAINING == ctx->api_ctx.session_run_state)
+          {
+            ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_NORMAL;
+          }
+          if (ret < 0)
+          {
+            av_log(avctx, AV_LOG_ERROR, "%s(): xcoder_send_frame 1 error, "
+                   "ret=%d\n", __FUNCTION__, ret);
+            return ret;
+          }
+          continue;
+        }
+        ret = AVERROR(EAGAIN);
+        if (! ctx->encoder_flushing && ! ctx->eos_fme_received)
+        {
+          av_log(avctx, AV_LOG_TRACE, "%s: NOT encoder_flushing, NOT "
+                 "eos_fme_received, return AVERROR(EAGAIN)\n", __FUNCTION__);
+          break;
+        }
+      }
+    }
+    else
+    {
+      /* got encoded data back */
+      int meta_size = NI_LOGAN_FW_ENC_BITSTREAM_META_DATA_SIZE;
+      if (avctx->pix_fmt == AV_PIX_FMT_NI_LOGAN && xpkt->recycle_index >= 0 && xpkt->recycle_index < 1056)
+      {
+        int avframe_index;
+        av_log(avctx, AV_LOG_VERBOSE, "UNREF index %d.\n", xpkt->recycle_index);
+        avframe_index = recycle_logan_index_2_avframe_index(ctx, xpkt->recycle_index);
+        if (avframe_index >=0 && ctx->sframe_pool[avframe_index])
+        {
+          AVFrame *frame = ctx->sframe_pool[avframe_index];
+          void *opaque = av_buffer_get_opaque(frame->buf[0]);
+          // This opaque would carry the valid event handle to help release the
+          // hwframe surface descriptor for windows target.
+          opaque = (void *) ctx->api_ctx.event_handle;
+          av_log(avctx, AV_LOG_TRACE, "%s: after ref sframe_pool, hw frame "
+                 "av_buffer_get_ref_count=%d, data[3]=%p event handle:%p\n",
+                 __FUNCTION__, av_buffer_get_ref_count(frame->buf[0]),
+                 frame->data[3], opaque);
+          av_frame_unref(frame);
+          av_log(avctx, AV_LOG_DEBUG, "AVframe_index = %d pushed to free tail "
+                 "%d\n", avframe_index, ctx->freeTail);
+          if (enq_logan_free_frames(ctx, avframe_index) != 0)
+          {
+            av_log(avctx, AV_LOG_ERROR, "free frames is full\n");
+          }
+          av_log(avctx, AV_LOG_DEBUG, "enq head %d, tail %d\n",ctx->freeHead, ctx->freeTail);
+          //enqueue the index back to free
+          xpkt->recycle_index = -1;
+        }
+        else
+        {
+          av_log(avctx, AV_LOG_DEBUG, "can't push to tail - avframe_index %d sframe_pool %p\n",
+                 avframe_index, ctx->sframe_pool[avframe_index]);
+        }
+      }
+
+      if (! ctx->spsPpsArrived)
+      {
+        ret = AVERROR(EAGAIN);
+        ctx->spsPpsArrived = 1;
+        ctx->spsPpsHdrLen = recv - meta_size;
+        ctx->p_spsPpsHdr = malloc(ctx->spsPpsHdrLen);
+        if (! ctx->p_spsPpsHdr)
+        {
+          ret = AVERROR(ENOMEM);
+          break;
+        }
+
+        memcpy(ctx->p_spsPpsHdr, (uint8_t*)xpkt->p_data + meta_size, xpkt->data_len - meta_size);
+
+        // start pkt_num counter from 1 to get the real first frame
+        ctx->api_ctx.pkt_num = 1;
+        // for low-latency mode, keep reading until the first frame is back
+        if (ctx->api_param.low_delay_mode)
+        {
+          av_log(avctx, AV_LOG_TRACE, "%s: low delay mode, keep reading until "
+                 "1st pkt arrives\n", __FUNCTION__);
+          continue;
+        }
+        break;
+      }
+      ctx->gotPacket = 1;
+      ctx->sentFrame = 0;
+
+      uint8_t pic_timing_buf[NI_LOGAN_MAX_SEI_DATA];
+      uint32_t pic_timing_sei_len = 0;
+      int nalu_type = 0;
+      const uint8_t *p_start_code;
+      uint32_t stc = -1;
+      uint32_t copy_len = 0;
+      uint8_t *p_src = (uint8_t*)xpkt->p_data + meta_size;
+      uint8_t *p_end = p_src + (xpkt->data_len - meta_size);
+      int is_idr = 0;
+      int64_t local_pts = xpkt->pts;
+      int custom_sei_cnt = 0;
+      int total_custom_sei_len = 0;
+      int sei_idx = 0;
+      ni_logan_all_custom_sei_t *ni_logan_all_custom_sei;
+      ni_logan_custom_sei_t *ni_custom_sei;
+      if (ctx->api_ctx.pkt_custom_sei[local_pts % NI_LOGAN_FIFO_SZ])
+      {
+        ni_logan_all_custom_sei = ctx->api_ctx.pkt_custom_sei[local_pts % NI_LOGAN_FIFO_SZ];
+        custom_sei_cnt = ni_logan_all_custom_sei->custom_sei_cnt;
+        for (sei_idx = 0; sei_idx < custom_sei_cnt; sei_idx++)
+        {
+          total_custom_sei_len += ni_logan_all_custom_sei->ni_custom_sei[sei_idx].custom_sei_size;
+        }
+      }
+
+      if (p_param->hrd_enable || custom_sei_cnt)
+      {
+        // if HRD or custom sei enabled, search for pic_timing or custom SEI insertion point by
+        // skipping non-VCL until video data is found.
+        p_start_code = p_src;
+        if(AV_CODEC_ID_HEVC == avctx->codec_id)
+        {
+          do
+          {
+            stc = -1;
+            p_start_code = avpriv_find_start_code(p_start_code, p_end, &stc);
+            nalu_type = (stc >> 1) & 0x3F;
+          } while (nalu_type > HEVC_NAL_RSV_VCL31);
+
+          // calc. length to copy
+          copy_len = p_start_code - 5 - p_src;
+        }
+        else if(AV_CODEC_ID_H264 == avctx->codec_id)
+        {
+          do
+          {
+            stc = -1;
+            p_start_code = avpriv_find_start_code(p_start_code, p_end, &stc);
+            nalu_type = stc & 0x1F;
+          } while (nalu_type > H264_NAL_IDR_SLICE);
+
+          // calc. length to copy
+          copy_len = p_start_code - 5 - p_src;
+        }
+        else
+        {
+          av_log(avctx, AV_LOG_ERROR, "%s: codec %d not supported for SEI !\n",
+                 __FUNCTION__, avctx->codec_id);
+        }
+
+        if (p_param->hrd_enable)
+        {
+          int is_i_or_idr;
+          if (HEVC_NAL_IDR_W_RADL == nalu_type || HEVC_NAL_IDR_N_LP == nalu_type)
+          {
+            is_idr = 1;
+          }
+          is_i_or_idr = (LOGAN_PIC_TYPE_I   == xpkt->frame_type ||
+                         LOGAN_PIC_TYPE_IDR == xpkt->frame_type ||
+                         LOGAN_PIC_TYPE_CRA == xpkt->frame_type);
+          pic_timing_sei_len = encode_pic_timing_sei2(p_param, ctx,
+                               pic_timing_buf, is_i_or_idr, is_idr, xpkt->pts);
+          // returned pts is display number
+        }
+      }
+
+      if (! ctx->firstPktArrived)
+      {
+        int sizeof_spspps_attached_to_idr = ctx->spsPpsHdrLen;
+
+        // if not enable forced repeat header, check AV_CODEC_FLAG_GLOBAL_HEADER flag
+        // to determine whether to add a SPS/PPS header in the first packat
+        if ((avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) &&
+            (p_param->hevc_enc_params.forced_header_enable != NI_LOGAN_ENC_REPEAT_HEADERS_ALL_KEY_FRAMES) &&
+             (p_param->hevc_enc_params.forced_header_enable != NI_LOGAN_ENC_REPEAT_HEADERS_ALL_I_FRAMES))
+        {
+          sizeof_spspps_attached_to_idr = 0;
+        }
+        ctx->firstPktArrived = 1;
+        ctx->first_frame_pts = xpkt->pts;
+
+        ret = ff_get_encode_buffer(avctx, pkt, xpkt->data_len - meta_size + sizeof_spspps_attached_to_idr + total_custom_sei_len + pic_timing_sei_len, 0);
+        if (! ret)
+        {
+          uint8_t *p_side_data, *p_dst;
+          // fill in AVC/HEVC sidedata
+          if ((avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) &&
+              (avctx->extradata_size != ctx->spsPpsHdrLen ||
+               memcmp(avctx->extradata, ctx->p_spsPpsHdr, ctx->spsPpsHdrLen)))
+          {
+            avctx->extradata_size = ctx->spsPpsHdrLen;
+            free(avctx->extradata);
+            avctx->extradata = av_mallocz(avctx->extradata_size +
+                                          AV_INPUT_BUFFER_PADDING_SIZE);
+            if (! avctx->extradata)
+            {
+              av_log(avctx, AV_LOG_ERROR, "Cannot allocate AVC/HEVC header of "
+                     "size %d.\n", avctx->extradata_size);
+              return AVERROR(ENOMEM);
+            }
+            memcpy(avctx->extradata, ctx->p_spsPpsHdr, avctx->extradata_size);
+          }
+
+          p_side_data = av_packet_new_side_data(pkt, AV_PKT_DATA_NEW_EXTRADATA,
+                                                ctx->spsPpsHdrLen);
+          if (p_side_data)
+          {
+            memcpy(p_side_data, ctx->p_spsPpsHdr, ctx->spsPpsHdrLen);
+          }
+
+          p_dst = pkt->data;
+          if (sizeof_spspps_attached_to_idr)
+          {
+            memcpy(p_dst, ctx->p_spsPpsHdr, ctx->spsPpsHdrLen);
+            p_dst += ctx->spsPpsHdrLen;
+          }
+
+          // 1st pkt, skip buffering_period SEI and insert pic_timing SEI
+          if (pic_timing_sei_len || custom_sei_cnt)
+          {
+            // copy buf_period
+            memcpy(p_dst, p_src, copy_len);
+            p_dst += copy_len;
+
+            // copy custom sei before slice
+            sei_idx = 0;
+            while (sei_idx < custom_sei_cnt)
+            {
+              ni_custom_sei = &ni_logan_all_custom_sei->ni_custom_sei[sei_idx];
+              if (ni_custom_sei->custom_sei_loc == NI_LOGAN_CUSTOM_SEI_LOC_AFTER_VCL)
+              {
+                break;
+              }
+              memcpy(p_dst, ni_custom_sei->custom_sei_data, ni_custom_sei->custom_sei_size);
+              p_dst += ni_custom_sei->custom_sei_size;
+              sei_idx++;
+            }
+
+            // copy pic_timing
+            if (pic_timing_sei_len)
+            {
+              memcpy(p_dst, pic_timing_buf, pic_timing_sei_len);
+              p_dst += pic_timing_sei_len;
+            }
+
+            // copy the IDR data
+            memcpy(p_dst, p_src + copy_len,
+                   xpkt->data_len - meta_size - copy_len);
+            p_dst += (xpkt->data_len - meta_size - copy_len);
+
+            // copy custom sei after slice
+            while (sei_idx < custom_sei_cnt)
+            {
+              ni_custom_sei = &ni_logan_all_custom_sei->ni_custom_sei[sei_idx];
+              memcpy(p_dst, ni_custom_sei->custom_sei_data, ni_custom_sei->custom_sei_size);
+              p_dst += ni_custom_sei->custom_sei_size;
+              sei_idx++;
+            }
+          }
+          else
+          {
+            memcpy(p_dst, (uint8_t*)xpkt->p_data + meta_size,
+                   xpkt->data_len - meta_size);
+          }
+        }
+
+        // free buffer
+        if (custom_sei_cnt)
+        {
+          free(ctx->api_ctx.pkt_custom_sei[local_pts % NI_LOGAN_FIFO_SZ]);
+          ctx->api_ctx.pkt_custom_sei[local_pts % NI_LOGAN_FIFO_SZ] = NULL;
+        }
+      }
+      else
+      {
+        // insert header when intraRefresh is enabled and forced header mode is 1 (all key frames)
+        // for every intraRefreshMinPeriod key frames, pkt counting starts from 1, e.g. for
+        // cycle of 100, the header is forced on frame 102, 202, ...;
+        // note that api_ctx.pkt_num returned is the actual index + 1
+        int intra_refresh_hdr_sz = 0;
+        if (ctx->p_spsPpsHdr && ctx->spsPpsHdrLen &&
+            (p_param->hevc_enc_params.forced_header_enable == NI_LOGAN_ENC_REPEAT_HEADERS_ALL_KEY_FRAMES) &&
+            (1 == p_param->hevc_enc_params.intra_mb_refresh_mode ||
+             2 == p_param->hevc_enc_params.intra_mb_refresh_mode ||
+             3 == p_param->hevc_enc_params.intra_mb_refresh_mode) &&
+            p_param->ui32minIntraRefreshCycle > 0 &&
+            ctx->api_ctx.pkt_num > 3 &&
+            0 == ((ctx->api_ctx.pkt_num - 3) % p_param->ui32minIntraRefreshCycle))
+        {
+          intra_refresh_hdr_sz = ctx->spsPpsHdrLen;
+          av_log(avctx, AV_LOG_TRACE, "%s pkt %" PRId64 " force header on "
+                 "intraRefreshMinPeriod %u\n", __FUNCTION__,
+                 ctx->api_ctx.pkt_num - 1, p_param->ui32minIntraRefreshCycle);
+        }
+
+        ret = ff_get_encode_buffer(avctx, pkt, xpkt->data_len - meta_size + total_custom_sei_len + pic_timing_sei_len + intra_refresh_hdr_sz, 0);
+        if (! ret)
+        {
+          uint8_t *p_dst = pkt->data;
+          if (intra_refresh_hdr_sz)
+          {
+            memcpy(p_dst, ctx->p_spsPpsHdr, intra_refresh_hdr_sz);
+            p_dst += intra_refresh_hdr_sz;
+          }
+          // insert pic_timing if required
+          if (pic_timing_sei_len || custom_sei_cnt)
+          {
+            // for non-IDR, skip AUD and insert
+            // for IDR, skip AUD VPS SPS PPS buf_period and insert
+            memcpy(p_dst, p_src, copy_len);
+            p_dst += copy_len;
+
+            // copy custom sei before slice
+            sei_idx = 0;
+            while (sei_idx < custom_sei_cnt)
+            {
+              ni_custom_sei = &ni_logan_all_custom_sei->ni_custom_sei[sei_idx];
+              if (ni_custom_sei->custom_sei_loc == NI_LOGAN_CUSTOM_SEI_LOC_AFTER_VCL)
+              {
+                break;
+              }
+              memcpy(p_dst, ni_custom_sei->custom_sei_data, ni_custom_sei->custom_sei_size);
+              p_dst += ni_custom_sei->custom_sei_size;
+              sei_idx++;
+            }
+
+            // copy pic_timing
+            if (pic_timing_sei_len)
+            {
+              memcpy(p_dst, pic_timing_buf, pic_timing_sei_len);
+              p_dst += pic_timing_sei_len;
+            }
+
+            // copy the video data
+            memcpy(p_dst, p_src + copy_len,
+                   xpkt->data_len - meta_size - copy_len);
+            p_dst += (xpkt->data_len - meta_size - copy_len);
+
+            // copy custom sei after slice
+            while (sei_idx < custom_sei_cnt)
+            {
+              ni_custom_sei = &ni_logan_all_custom_sei->ni_custom_sei[sei_idx];
+              memcpy(p_dst, ni_custom_sei->custom_sei_data, ni_custom_sei->custom_sei_size);
+              p_dst += ni_custom_sei->custom_sei_size;
+              sei_idx++;
+            }
+          }
+          else
+          {
+            memcpy(p_dst, (uint8_t*)xpkt->p_data + meta_size,
+                   xpkt->data_len - meta_size);
+          }
+        }
+
+        // free buffer
+        if (custom_sei_cnt)
+        {
+          free(ctx->api_ctx.pkt_custom_sei[local_pts % NI_LOGAN_FIFO_SZ]);
+          ctx->api_ctx.pkt_custom_sei[local_pts % NI_LOGAN_FIFO_SZ] = NULL;
+        }
+      }
+      if (!ret)
+      {
+        if (LOGAN_PIC_TYPE_IDR == xpkt->frame_type ||
+            LOGAN_PIC_TYPE_CRA == xpkt->frame_type)
+        {
+          pkt->flags |= AV_PKT_FLAG_KEY;
+        }
+        pkt->pts = xpkt->pts;
+        /* to ensure pts>dts for all frames, we assign a guess pts for the first 'dts_offset' frames and then the pts from input stream
+         * is extracted from input pts FIFO.
+         * if GOP = IBBBP and PTSs = 0 1 2 3 4 5 .. then out DTSs = -3 -2 -1 0 1 ... and -3 -2 -1 are the guessed values
+         * if GOP = IBPBP and PTSs = 0 1 2 3 4 5 .. then out DTSs = -1 0 1 2 3 ... and -1 is the guessed value
+         * the number of guessed values is equal to dts_offset
+         */
+        if (ctx->total_frames_received < ctx->dts_offset)
+        {
+          // guess dts
+          pkt->dts = ctx->first_frame_pts + (ctx->total_frames_received - ctx->dts_offset) * avctx->ticks_per_frame;
+        }
+        else
+        {
+          // get dts from pts FIFO
+          pkt->dts = ctx->api_ctx.enc_pts_list[ctx->api_ctx.enc_pts_r_idx % NI_LOGAN_FIFO_SZ];
+          ctx->api_ctx.enc_pts_r_idx++;
+        }
+
+        if (ctx->total_frames_received >= 1)
+        {
+          if (pkt->dts < ctx->latest_dts)
+          {
+            av_log(avctx, AV_LOG_WARNING, "dts: %" PRId64 ". < latest_dts: "
+                   "%" PRId64 ".\n", pkt->dts, ctx->latest_dts);
+          }
+        }
+
+        if (pkt->dts > pkt->pts)
+        {
+          av_log(avctx, AV_LOG_WARNING, "dts: %" PRId64 ", pts: %" PRId64 ". "
+                 "Forcing dts = pts\n", pkt->dts, pkt->pts);
+          pkt->dts = pkt->pts;
+        }
+        ctx->total_frames_received++;
+        ctx->latest_dts = pkt->dts;
+        av_log(avctx, AV_LOG_DEBUG, "%s pkt %" PRId64 " pts %" PRId64 " "
+               "dts %" PRId64 "  size %d  st_index %d \n", __FUNCTION__,
+               ctx->api_ctx.pkt_num - 1, pkt->pts, pkt->dts, pkt->size,
+               pkt->stream_index);
+      }
+      ctx->encoder_eof = xpkt->end_of_stream;
+
+      if (ctx->encoder_eof &&
+          LOGAN_SESSION_RUN_STATE_SEQ_CHANGE_DRAINING ==
+          ctx->api_ctx.session_run_state)
+      {
+        // after sequence change completes, reset codec state
+        av_log(avctx, AV_LOG_TRACE, "%s 2: sequence change completed, "
+               "return 0 and will reopen codec !\n", __FUNCTION__);
+        ret = xcoder_logan_encode_reinit(avctx);
+      }
+      else if(!is_logan_input_fifo_empty(ctx) &&
+              (LOGAN_SESSION_RUN_STATE_NORMAL == ctx->api_ctx.session_run_state) &&
+              (NI_LOGAN_RETCODE_NVME_SC_WRITE_BUFFER_FULL != ctx->api_ctx.status))
+      {
+        ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_QUEUED_FRAME_DRAINING;
+        ret = xcoder_send_frame(avctx, NULL);
+
+        // if session_run_state is changed in xcoder_send_frame, keep it
+        if (LOGAN_SESSION_RUN_STATE_QUEUED_FRAME_DRAINING == ctx->api_ctx.session_run_state)
+        {
+          ctx->api_ctx.session_run_state = LOGAN_SESSION_RUN_STATE_NORMAL;
+        }
+        if (ret < 0)
+        {
+          av_log(avctx, AV_LOG_ERROR, "%s: xcoder_send_frame 2 error, ret=%d\n",
+                 __FUNCTION__, ret);
+          return ret;
+        }
+      }
+      break;
+    }
+  }
+
+  return ret;
+}
+
+int ff_xcoder_logan_encode_frame(AVCodecContext *avctx, AVPacket *pkt,
+                           const AVFrame *frame, int *got_packet)
+{
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+  int ret;
+
+  av_log(avctx, AV_LOG_DEBUG, "%s\n", __FUNCTION__);
+
+  ret = xcoder_send_frame(avctx, frame);
+  // return immediately for critical errors
+  if (AVERROR(ENOMEM) == ret || AVERROR_EXTERNAL == ret ||
+      (ret < 0 && ctx->encoder_eof))
+  {
+    return ret;
+  }
+
+  ret = xcoder_logan_receive_packet(avctx, pkt);
+  if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF)
+  {
+    *got_packet = 0;
+  }
+  else if (ret < 0)
+  {
+    return ret;
+  }
+  else
+  {
+    *got_packet = 1;
+  }
+
+  return 0;
+}
+
+int ff_xcoder_logan_receive_packet(AVCodecContext *avctx, AVPacket *pkt)
+{
+  XCoderLoganEncContext *ctx = avctx->priv_data;
+  AVFrame *frame = &ctx->buffered_fme;
+  int ret;
+
+  ret = ff_encode_get_frame(avctx, frame);
+  if (!ctx->encoder_flushing && ret >= 0 || ret == AVERROR_EOF)
+  {
+    ret = xcoder_send_frame(avctx, (ret == AVERROR_EOF ? NULL : frame));
+    if (ret < 0 && ret != AVERROR_EOF)
+    {
+      return ret;
+    }
+  }
+  // Once send_frame returns EOF go on receiving packets until EOS is met.
+  return xcoder_logan_receive_packet(avctx, pkt);
+}
+
+bool free_logan_frames_isempty(XCoderLoganEncContext *ctx)
+{
+  return  (ctx->freeHead == ctx->freeTail);
+}
+
+bool free_logan_frames_isfull(XCoderLoganEncContext *ctx)
+{
+  return  (ctx->freeHead == ((ctx->freeTail == LOGAN_MAX_NUM_FRAMEPOOL_HWAVFRAME) ? 0 : ctx->freeTail + 1));
+}
+
+int deq_logan_free_frames(XCoderLoganEncContext *ctx)
+{
+  if (free_logan_frames_isempty(ctx))
+  {
+    return -1;
+  }
+  ctx->aFree_Avframes_list[ctx->freeHead] = -1;
+  ctx->freeHead = (ctx->freeHead == LOGAN_MAX_NUM_FRAMEPOOL_HWAVFRAME) ? 0 : ctx->freeHead + 1;
+  return 0;
+}
+
+int enq_logan_free_frames(XCoderLoganEncContext *ctx, int idx)
+{
+  if (free_logan_frames_isfull(ctx))
+  {
+    return -1;
+  }
+  ctx->aFree_Avframes_list[ctx->freeTail] = idx;
+  ctx->freeTail = (ctx->freeTail == LOGAN_MAX_NUM_FRAMEPOOL_HWAVFRAME) ? 0 : ctx->freeTail + 1;
+  return 0;
+}
+
+int recycle_logan_index_2_avframe_index(XCoderLoganEncContext *ctx, uint32_t recycleIndex)
+{
+  int i;
+  for (i = 0; i < LOGAN_MAX_NUM_FRAMEPOOL_HWAVFRAME; i++)
+  {
+    if (ctx->sframe_pool[i]->data[3])
+    {
+      if (((ni_logan_hwframe_surface_t*)((uint8_t*)ctx->sframe_pool[i]->data[3]))->i8FrameIdx == recycleIndex)
+      {
+        return i;
+      }
+      else
+      {
+        //av_log(NULL, AV_LOG_TRACE, "sframe_pool[%d] ui16FrameIdx %u != recycleIndex %u\n", i, ((niFrameSurface1_t*)((uint8_t*)ctx->sframe_pool[i]->data[3]))->ui16FrameIdx, recycleIndex);
+      }
+    }
+    else
+    {
+      //av_log(NULL, AV_LOG_TRACE, "sframe_pool[%d] data[3] NULL\n", i);
+    }
+  }
+  return -1;
+}
+
+// Needed for yuvbypass on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82))
+const AVCodecHWConfigInternal *ff_ni_logan_enc_hw_configs[] = {
+  HW_CONFIG_ENCODER_FRAMES(NI_LOGAN,  NI_LOGAN),
+  HW_CONFIG_ENCODER_DEVICE(YUV420P, NI_LOGAN),
+  HW_CONFIG_ENCODER_DEVICE(YUV420P10, NI_LOGAN),
+  NULL,
+};
+#endif
diff --git a/libavcodec/nienc_logan.h b/libavcodec/nienc_logan.h
new file mode 100644
index 0000000000..cbec89959c
--- /dev/null
+++ b/libavcodec/nienc_logan.h
@@ -0,0 +1,67 @@
+/*
+ * NetInt XCoder H.264/HEVC Encoder common code header
+ * Copyright (c) 2018-2019 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_NIENC_LOGAN_H
+#define AVCODEC_NIENC_LOGAN_H
+
+#include <ni_rsrc_api_logan.h>
+#include <ni_util_logan.h>
+#include <ni_device_api_logan.h>
+
+#include "libavutil/internal.h"
+
+#include "avcodec.h"
+#include "encode.h"
+#include "internal.h"
+#include "libavutil/opt.h"
+#include "libavutil/imgutils.h"
+// Needed for yuvbypass on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82))
+#include "hwconfig.h"
+#endif
+
+#include "nicodec_logan.h"
+
+int ff_xcoder_logan_encode_init(AVCodecContext *avctx);
+  
+int ff_xcoder_logan_encode_close(AVCodecContext *avctx);
+
+int ff_xcoder_logan_receive_packet(AVCodecContext *avctx, AVPacket *pkt);
+
+int ff_xcoder_logan_encode_frame(AVCodecContext *avctx, AVPacket *pkt,
+                           const AVFrame *frame, int *got_packet);
+
+bool free_logan_frames_isempty(XCoderLoganEncContext *ctx);
+
+bool free_logan_frames_isfull(XCoderLoganEncContext *ctx);
+
+int deq_logan_free_frames(XCoderLoganEncContext *ctx);
+
+int enq_logan_free_frames(XCoderLoganEncContext *ctx, int idx);
+
+int recycle_logan_index_2_avframe_index(XCoderLoganEncContext *ctx, uint32_t recycleIndex);
+
+// Needed for yuvbypass on FFmpeg-n4.3+
+#if (LIBAVCODEC_VERSION_MAJOR >= 59 || (LIBAVCODEC_VERSION_MAJOR >= 58 && LIBAVCODEC_VERSION_MINOR >= 82))
+extern const AVCodecHWConfigInternal *ff_ni_logan_enc_hw_configs[];
+#endif
+
+#endif /* AVCODEC_NIENC_LOGAN_H */
diff --git a/libavcodec/packet.h b/libavcodec/packet.h
index 404d520071..39ddf2ccaa 100644
--- a/libavcodec/packet.h
+++ b/libavcodec/packet.h
@@ -299,6 +299,11 @@ enum AVPacketSideDataType {
      */
     AV_PKT_DATA_DYNAMIC_HDR10_PLUS,
 
+    /**
+     * NETINT: HEVC tile/slice index in one frame.
+     */
+    AV_PKT_DATA_SLICE_ADDR,
+
     /**
      * The number of side data types.
      * This is not part of the public API/ABI in the sense that it may
diff --git a/libavfilter/Makefile b/libavfilter/Makefile
index 30cc329fb6..fed276b9d6 100644
--- a/libavfilter/Makefile
+++ b/libavfilter/Makefile
@@ -193,6 +193,7 @@ OBJS-$(CONFIG_AVGBLUR_OPENCL_FILTER)         += vf_avgblur_opencl.o opencl.o \
 OBJS-$(CONFIG_AVGBLUR_VULKAN_FILTER)         += vf_avgblur_vulkan.o vulkan.o vulkan_filter.o
 OBJS-$(CONFIG_BBOX_FILTER)                   += bbox.o vf_bbox.o
 OBJS-$(CONFIG_BENCH_FILTER)                  += f_bench.o
+OBJS-$(CONFIG_BG_NI_QUADRA_FILTER)           += vf_bg_ni.o
 OBJS-$(CONFIG_BILATERAL_FILTER)              += vf_bilateral.o
 OBJS-$(CONFIG_BITPLANENOISE_FILTER)          += vf_bitplanenoise.o
 OBJS-$(CONFIG_BLACKDETECT_FILTER)            += vf_blackdetect.o
@@ -238,6 +239,7 @@ OBJS-$(CONFIG_COPY_FILTER)                   += vf_copy.o
 OBJS-$(CONFIG_COREIMAGE_FILTER)              += vf_coreimage.o
 OBJS-$(CONFIG_COVER_RECT_FILTER)             += vf_cover_rect.o lavfutils.o
 OBJS-$(CONFIG_CROP_FILTER)                   += vf_crop.o
+OBJS-$(CONFIG_CROP_NI_QUADRA_FILTER)         += vf_crop_ni.o nifilter.o
 OBJS-$(CONFIG_CROPDETECT_FILTER)             += vf_cropdetect.o
 OBJS-$(CONFIG_CUE_FILTER)                    += f_cue.o
 OBJS-$(CONFIG_CURVES_FILTER)                 += vf_curves.o
@@ -328,6 +330,8 @@ OBJS-$(CONFIG_HUESATURATION_FILTER)          += vf_huesaturation.o
 OBJS-$(CONFIG_HWDOWNLOAD_FILTER)             += vf_hwdownload.o
 OBJS-$(CONFIG_HWMAP_FILTER)                  += vf_hwmap.o
 OBJS-$(CONFIG_HWUPLOAD_CUDA_FILTER)          += vf_hwupload_cuda.o
+OBJS-$(CONFIG_HWUPLOAD_NI_QUADRA_FILTER)     += vf_hwupload_ni_quadra.o
+OBJS-$(CONFIG_HWUPLOAD_NI_LOGAN_FILTER)      += vf_hwupload_ni_logan.o
 OBJS-$(CONFIG_HWUPLOAD_FILTER)               += vf_hwupload.o
 OBJS-$(CONFIG_HYSTERESIS_FILTER)             += vf_hysteresis.o framesync.o
 OBJS-$(CONFIG_ICCDETECT_FILTER)              += vf_iccdetect.o fflcms2.o
@@ -386,6 +390,7 @@ OBJS-$(CONFIG_OCR_FILTER)                    += vf_ocr.o
 OBJS-$(CONFIG_OCV_FILTER)                    += vf_libopencv.o
 OBJS-$(CONFIG_OSCILLOSCOPE_FILTER)           += vf_datascope.o
 OBJS-$(CONFIG_OVERLAY_FILTER)                += vf_overlay.o framesync.o
+OBJS-$(CONFIG_OVERLAY_NI_QUADRA_FILTER)      += vf_overlay_ni.o framesync.o
 OBJS-$(CONFIG_OVERLAY_CUDA_FILTER)           += vf_overlay_cuda.o framesync.o vf_overlay_cuda.ptx.o \
                                                 cuda/load_helper.o
 OBJS-$(CONFIG_OVERLAY_OPENCL_FILTER)         += vf_overlay_opencl.o opencl.o \
@@ -395,6 +400,7 @@ OBJS-$(CONFIG_OVERLAY_VAAPI_FILTER)          += vf_overlay_vaapi.o framesync.o v
 OBJS-$(CONFIG_OVERLAY_VULKAN_FILTER)         += vf_overlay_vulkan.o vulkan.o vulkan_filter.o
 OBJS-$(CONFIG_OWDENOISE_FILTER)              += vf_owdenoise.o
 OBJS-$(CONFIG_PAD_FILTER)                    += vf_pad.o
+OBJS-$(CONFIG_PAD_NI_QUADRA_FILTER)          += vf_pad_ni.o nifilter.o
 OBJS-$(CONFIG_PAD_OPENCL_FILTER)             += vf_pad_opencl.o opencl.o opencl/pad.o
 OBJS-$(CONFIG_PALETTEGEN_FILTER)             += vf_palettegen.o
 OBJS-$(CONFIG_PALETTEUSE_FILTER)             += vf_paletteuse.o framesync.o
@@ -432,11 +438,14 @@ OBJS-$(CONFIG_RGBASHIFT_FILTER)              += vf_chromashift.o
 OBJS-$(CONFIG_ROBERTS_FILTER)                += vf_convolution.o
 OBJS-$(CONFIG_ROBERTS_OPENCL_FILTER)         += vf_convolution_opencl.o opencl.o \
                                                 opencl/convolution.o
+OBJS-$(CONFIG_ROI_NI_QUADRA_FILTER)          += vf_roi_ni.o
 OBJS-$(CONFIG_ROTATE_FILTER)                 += vf_rotate.o
+OBJS-$(CONFIG_ROTATE_NI_QUADRA_FILTER)       += vf_rotate_ni.o nifilter.o
 OBJS-$(CONFIG_SAB_FILTER)                    += vf_sab.o
 OBJS-$(CONFIG_SCALE_FILTER)                  += vf_scale.o scale_eval.o
 OBJS-$(CONFIG_SCALE_CUDA_FILTER)             += vf_scale_cuda.o scale_eval.o \
                                                 vf_scale_cuda.ptx.o cuda/load_helper.o
+OBJS-$(CONFIG_SCALE_NI_QUADRA_FILTER)        += vf_scale_ni.o nifilter.o
 OBJS-$(CONFIG_SCALE_NPP_FILTER)              += vf_scale_npp.o scale_eval.o
 OBJS-$(CONFIG_SCALE_QSV_FILTER)              += vf_scale_qsv.o
 OBJS-$(CONFIG_SCALE_VAAPI_FILTER)            += vf_scale_vaapi.o scale_eval.o vaapi_vpp.o
@@ -446,6 +455,7 @@ OBJS-$(CONFIG_SCALE2REF_NPP_FILTER)          += vf_scale_npp.o scale_eval.o
 OBJS-$(CONFIG_SCDET_FILTER)                  += vf_scdet.o
 OBJS-$(CONFIG_SCHARR_FILTER)                 += vf_convolution.o
 OBJS-$(CONFIG_SCROLL_FILTER)                 += vf_scroll.o
+OBJS-$(CONFIG_SDL_NI_QUADRA_FILTER)          += vf_sdl_ni.o
 OBJS-$(CONFIG_SEGMENT_FILTER)                += f_segment.o
 OBJS-$(CONFIG_SELECT_FILTER)                 += f_select.o
 OBJS-$(CONFIG_SELECTIVECOLOR_FILTER)         += vf_selectivecolor.o
@@ -475,6 +485,7 @@ OBJS-$(CONFIG_SOBEL_OPENCL_FILTER)           += vf_convolution_opencl.o opencl.o
                                                 opencl/convolution.o
 OBJS-$(CONFIG_SITI_FILTER)                   += vf_siti.o
 OBJS-$(CONFIG_SPLIT_FILTER)                  += split.o
+OBJS-$(CONFIG_SPLIT_NI_QUADRA_FILTER)        += vf_split_ni.o
 OBJS-$(CONFIG_SPP_FILTER)                    += vf_spp.o qp_table.o
 OBJS-$(CONFIG_SR_FILTER)                     += vf_sr.o
 OBJS-$(CONFIG_SSIM_FILTER)                   += vf_ssim.o framesync.o
@@ -538,6 +549,7 @@ OBJS-$(CONFIG_XFADE_FILTER)                  += vf_xfade.o
 OBJS-$(CONFIG_XFADE_OPENCL_FILTER)           += vf_xfade_opencl.o opencl.o opencl/xfade.o
 OBJS-$(CONFIG_XMEDIAN_FILTER)                += vf_xmedian.o framesync.o
 OBJS-$(CONFIG_XSTACK_FILTER)                 += vf_stack.o framesync.o
+OBJS-$(CONFIG_XSTACK_NI_QUADRA_FILTER)       += vf_stack_ni.o framesync.o
 OBJS-$(CONFIG_YADIF_FILTER)                  += vf_yadif.o yadif_common.o
 OBJS-$(CONFIG_YADIF_CUDA_FILTER)             += vf_yadif_cuda.o vf_yadif_cuda.ptx.o \
                                                 yadif_common.o cuda/load_helper.o
@@ -546,6 +558,8 @@ OBJS-$(CONFIG_YADIF_VIDEOTOOLBOX_FILTER)     += vf_yadif_videotoolbox.o \
                                                 metal/utils.o \
                                                 yadif_common.o
 OBJS-$(CONFIG_YAEPBLUR_FILTER)               += vf_yaepblur.o
+OBJS-$(CONFIG_YUV444TO420_NI_QUADRA_FILTER)  += vf_yuv444to420_ni.o
+OBJS-$(CONFIG_YUV420TO444_NI_QUADRA_FILTER)  += vf_yuv420to444_ni.o
 OBJS-$(CONFIG_ZMQ_FILTER)                    += f_zmq.o
 OBJS-$(CONFIG_ZOOMPAN_FILTER)                += vf_zoompan.o
 OBJS-$(CONFIG_ZSCALE_FILTER)                 += vf_zscale.o
diff --git a/libavfilter/allfilters.c b/libavfilter/allfilters.c
index 5ebacfde27..6368eec30f 100644
--- a/libavfilter/allfilters.c
+++ b/libavfilter/allfilters.c
@@ -179,6 +179,7 @@ extern const AVFilter ff_vf_avgblur_opencl;
 extern const AVFilter ff_vf_avgblur_vulkan;
 extern const AVFilter ff_vf_bbox;
 extern const AVFilter ff_vf_bench;
+extern const AVFilter ff_vf_bg_ni_quadra;
 extern const AVFilter ff_vf_bilateral;
 extern const AVFilter ff_vf_bitplanenoise;
 extern const AVFilter ff_vf_blackdetect;
@@ -220,6 +221,7 @@ extern const AVFilter ff_vf_copy;
 extern const AVFilter ff_vf_coreimage;
 extern const AVFilter ff_vf_cover_rect;
 extern const AVFilter ff_vf_crop;
+extern const AVFilter ff_vf_crop_ni_quadra;
 extern const AVFilter ff_vf_cropdetect;
 extern const AVFilter ff_vf_cue;
 extern const AVFilter ff_vf_curves;
@@ -308,6 +310,8 @@ extern const AVFilter ff_vf_huesaturation;
 extern const AVFilter ff_vf_hwdownload;
 extern const AVFilter ff_vf_hwmap;
 extern const AVFilter ff_vf_hwupload;
+extern const AVFilter ff_vf_hwupload_ni_quadra;
+extern const AVFilter ff_vf_hwupload_ni_logan;
 extern const AVFilter ff_vf_hwupload_cuda;
 extern const AVFilter ff_vf_hysteresis;
 extern const AVFilter ff_vf_iccdetect;
@@ -367,6 +371,7 @@ extern const AVFilter ff_vf_ocr;
 extern const AVFilter ff_vf_ocv;
 extern const AVFilter ff_vf_oscilloscope;
 extern const AVFilter ff_vf_overlay;
+extern const AVFilter ff_vf_overlay_ni_quadra;
 extern const AVFilter ff_vf_overlay_opencl;
 extern const AVFilter ff_vf_overlay_qsv;
 extern const AVFilter ff_vf_overlay_vaapi;
@@ -374,6 +379,7 @@ extern const AVFilter ff_vf_overlay_vulkan;
 extern const AVFilter ff_vf_overlay_cuda;
 extern const AVFilter ff_vf_owdenoise;
 extern const AVFilter ff_vf_pad;
+extern const AVFilter ff_vf_pad_ni_quadra;
 extern const AVFilter ff_vf_pad_opencl;
 extern const AVFilter ff_vf_palettegen;
 extern const AVFilter ff_vf_paletteuse;
@@ -408,10 +414,13 @@ extern const AVFilter ff_vf_reverse;
 extern const AVFilter ff_vf_rgbashift;
 extern const AVFilter ff_vf_roberts;
 extern const AVFilter ff_vf_roberts_opencl;
+extern const AVFilter ff_vf_roi_ni_quadra;
 extern const AVFilter ff_vf_rotate;
+extern const AVFilter ff_vf_rotate_ni_quadra;
 extern const AVFilter ff_vf_sab;
 extern const AVFilter ff_vf_scale;
 extern const AVFilter ff_vf_scale_cuda;
+extern const AVFilter ff_vf_scale_ni_quadra;
 extern const AVFilter ff_vf_scale_npp;
 extern const AVFilter ff_vf_scale_qsv;
 extern const AVFilter ff_vf_scale_vaapi;
@@ -421,6 +430,7 @@ extern const AVFilter ff_vf_scale2ref_npp;
 extern const AVFilter ff_vf_scdet;
 extern const AVFilter ff_vf_scharr;
 extern const AVFilter ff_vf_scroll;
+extern const AVFilter ff_vf_sdl_ni_quadra;
 extern const AVFilter ff_vf_segment;
 extern const AVFilter ff_vf_select;
 extern const AVFilter ff_vf_selectivecolor;
@@ -449,6 +459,7 @@ extern const AVFilter ff_vf_smartblur;
 extern const AVFilter ff_vf_sobel;
 extern const AVFilter ff_vf_sobel_opencl;
 extern const AVFilter ff_vf_split;
+extern const AVFilter ff_vf_split_ni_quadra;
 extern const AVFilter ff_vf_spp;
 extern const AVFilter ff_vf_sr;
 extern const AVFilter ff_vf_ssim;
@@ -509,10 +520,13 @@ extern const AVFilter ff_vf_xfade;
 extern const AVFilter ff_vf_xfade_opencl;
 extern const AVFilter ff_vf_xmedian;
 extern const AVFilter ff_vf_xstack;
+extern const AVFilter ff_vf_xstack_ni_quadra;
 extern const AVFilter ff_vf_yadif;
 extern const AVFilter ff_vf_yadif_cuda;
 extern const AVFilter ff_vf_yadif_videotoolbox;
 extern const AVFilter ff_vf_yaepblur;
+extern const AVFilter ff_vf_yuv420to444_ni_quadra;
+extern const AVFilter ff_vf_yuv444to420_ni_quadra;
 extern const AVFilter ff_vf_zmq;
 extern const AVFilter ff_vf_zoompan;
 extern const AVFilter ff_vf_zscale;
diff --git a/libavfilter/nifilter.c b/libavfilter/nifilter.c
new file mode 100644
index 0000000000..460ca955de
--- /dev/null
+++ b/libavfilter/nifilter.c
@@ -0,0 +1,272 @@
+/*
+ * Copyright (c) 2020 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * video common filter routines
+ */
+
+#include <stdio.h>
+
+#include <ni_device_api.h>
+
+#include "avfilter.h"
+#include "nifilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+#include "libavutil/eval.h"
+#include "libavutil/avstring.h"
+#include "libavutil/internal.h"
+#include "libavutil/libm.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/mathematics.h"
+#include "libavutil/opt.h"
+#include "nifilter.h"
+
+typedef struct gc620_pixel_fmts {
+  enum AVPixelFormat pix_fmt_ffmpeg;
+  int                pix_fmt_gc620;
+} gc620_pixel_fmts_t;
+
+static struct gc620_pixel_fmts gc620_pixel_fmt_list[] = {
+    {AV_PIX_FMT_NV12, GC620_NV12},
+    {AV_PIX_FMT_NV21, GC620_NV21},
+    {AV_PIX_FMT_YUV420P, GC620_I420},
+    {AV_PIX_FMT_P010LE, GC620_P010_MSB},
+    {AV_PIX_FMT_YUV420P10LE, GC620_I010},
+    {AV_PIX_FMT_YUYV422, GC620_YUYV},
+    {AV_PIX_FMT_UYVY422, GC620_UYVY},
+    {AV_PIX_FMT_NV16, GC620_NV16},
+    {AV_PIX_FMT_RGBA, GC620_RGBA8888},
+    {AV_PIX_FMT_BGR0, GC620_BGRX8888},
+    {AV_PIX_FMT_BGRA, GC620_BGRA8888},
+    {AV_PIX_FMT_ABGR, GC620_ABGR8888},
+    {AV_PIX_FMT_ARGB, GC620_ARGB8888},
+    {AV_PIX_FMT_BGR565LE, GC620_RGB565},
+    {AV_PIX_FMT_RGB565LE, GC620_BGR565},
+    {AV_PIX_FMT_RGB555LE, GC620_B5G5R5X1},
+    {AV_PIX_FMT_BGRP, GC620_RGB888_PLANAR}};
+
+int ff_ni_ffmpeg_to_gc620_pix_fmt(enum AVPixelFormat pix_fmt)
+{
+  int i, tablesz;
+
+  tablesz = sizeof(gc620_pixel_fmt_list)/sizeof(struct gc620_pixel_fmts);
+
+  /* linear search through table to find if the pixel format is supported */
+  for (i = 0; i < tablesz; i++)
+  {
+    if (gc620_pixel_fmt_list[i].pix_fmt_ffmpeg == pix_fmt)
+    {
+      return gc620_pixel_fmt_list[i].pix_fmt_gc620;
+    }
+  }
+  return -1;
+}
+
+int ff_ni_copy_device_to_host_frame(AVFrame *dst, const ni_frame_t *src, int pix_fmt)
+{
+#if 0
+  printf("Copying device to host frame: pixel format %d\n",pix_fmt);
+  printf("dst->data[0] = %p;dst->data[1] = %p; dst->data[2] = %p\n",
+         dst->data[0],dst->data[1],dst->data[2]);
+  printf("dst->linesize[0] = %d;dst->linesize[1]=%d;dst->linesize[2]=%d\n",
+         dst->linesize[0],dst->linesize[1],dst->linesize[2]);
+  printf("src->p_data[0] = %p; src->p_data[1] = %p; src->p_data[2] = %p\n",
+         src->p_data[0], src->p_data[1], src->p_data[2]);
+  printf("src->data_len[0] = %d; src->data_len[1] = %d; src->data_len[2] = %d\n",
+         src->data_len[0],src->data_len[1],src->data_len[2]);
+#endif
+
+  switch (pix_fmt)
+  {
+    /* packed */
+    case GC620_RGBA8888:
+    case GC620_BGRA8888:
+    case GC620_ABGR8888:
+    case GC620_ARGB8888:
+    case GC620_RGB565:
+    case GC620_BGR565:
+    case GC620_B5G5R5X1:
+    case GC620_YUYV:
+      memcpy(dst->data[0],src->p_data[0],src->data_len[0]);
+      break;
+
+    /* semi-planar */
+    case GC620_NV12:
+    case GC620_NV21:
+    case GC620_P010_MSB:
+    case GC620_NV16:
+      memcpy(dst->data[0], src->p_data[0], src->data_len[0]);
+      memcpy(dst->data[1], src->p_data[1], src->data_len[1]);
+      break;
+
+    /* planar */
+    case GC620_I420:
+    case GC620_I010:
+      memcpy(dst->data[0], src->p_data[0], src->data_len[0]);
+      memcpy(dst->data[1], src->p_data[1], src->data_len[1]);
+      memcpy(dst->data[2], src->p_data[2], src->data_len[2]);
+      break;
+
+    default:
+      return -1;
+  }
+
+  return 0;
+}
+
+int ff_ni_copy_host_to_device_frame(ni_frame_t *dst, const AVFrame *src, int pix_fmt)
+{
+#if 0
+  printf("Copying host to device: pixel format %d\n",pix_fmt);
+  printf("dst->p_data[0] = %p; dst->p_data[1] = %p; dst->p_data[2] = %p\n",
+         dst->p_data[0],dst->p_data[1],dst->p_data[2]);
+  printf("dst->data_len[0] = %d; dst->data_len[1] = %d; dst->data_len[2] = %d\n",
+         dst->data_len[0], dst->data_len[1], dst->data_len[2]);
+  printf("src->data[0] = %p; src->data[1] = %p; src->data[2] = %p\n",
+         src->data[0], src->data[1], src->data[2]);
+  printf("src->linesize[0] = %d; src->linesize[1] = %d; src->linesize[2] = %d\n",
+          src->linesize[0], src->linesize[1], src->linesize[2]);
+#endif
+  switch (pix_fmt)
+  {
+    /* packed */
+    case GC620_RGBA8888:
+    case GC620_BGRA8888:
+    case GC620_ABGR8888:
+    case GC620_ARGB8888:
+    case GC620_RGB565:
+    case GC620_BGR565:
+    case GC620_B5G5R5X1:
+    case GC620_YUYV:
+      memcpy(dst->p_data[0], src->data[0], dst->data_len[0]);
+      dst->pixel_format = pix_fmt;
+      break;
+
+    /* planar */
+    case GC620_I420:
+    case GC620_I010:
+      memcpy(dst->p_data[0], src->data[0], dst->data_len[0]);
+      memcpy(dst->p_data[1], src->data[1], dst->data_len[1]);
+      memcpy(dst->p_data[2], src->data[2], dst->data_len[2]);
+      dst->pixel_format = pix_fmt;
+      break;
+
+    /* semi-planar */
+    case GC620_NV12:
+    case GC620_NV21:
+    case GC620_P010_MSB:
+    case GC620_NV16:
+      memcpy(dst->p_data[0], src->data[0], dst->data_len[0]);
+      memcpy(dst->p_data[0], src->data[0], dst->data_len[0]);
+      dst->pixel_format = pix_fmt;
+      break;
+
+    default:
+      dst->pixel_format = -1;
+      return -1;
+  }
+
+  return 0;
+}
+
+void ff_ni_frame_free(void *opaque, uint8_t *data)
+{
+  int ret;
+
+  if (data)
+  {
+    niFrameSurface1_t* p_data3 = (niFrameSurface1_t*)((uint8_t*)data);
+    if (p_data3->ui16FrameIdx != 0)
+    {
+      av_log(NULL, AV_LOG_DEBUG, "Recycle trace ui16FrameIdx = [%d] DevHandle %d\n", p_data3->ui16FrameIdx, p_data3->device_handle);
+      ret = ni_hwframe_buffer_recycle(p_data3, p_data3->device_handle);
+      if (ret != NI_RETCODE_SUCCESS)
+      {
+        av_log(NULL, AV_LOG_ERROR, "ERROR Failed to recycle trace ui16FrameIdx = [%d] DevHandle %d\n", p_data3->ui16FrameIdx, p_data3->device_handle);
+      }
+    }
+    // buffer is created by av_malloc, so use av_free to release.
+    av_free(data);
+  }
+};
+
+
+int ff_ni_build_frame_pool(ni_session_context_t *ctx,
+                           int width, int height,
+                           enum AVPixelFormat out_format,
+                           int pool_size)
+{
+  int rc;
+  int scaler_format;
+  int options;
+
+  scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(out_format);
+  options = NI_SCALER_FLAG_IO |  NI_SCALER_FLAG_PC;
+
+  /* Allocate a pool of frames by the scaler */
+  rc = ni_device_alloc_frame(ctx,
+                             FFALIGN(width,2),
+                             FFALIGN(height,2),
+                             scaler_format,
+                             options,
+                             0, // rec width
+                             0, // rec height
+                             0, // rec X pos
+                             0, // rec Y pos
+                             pool_size, // rgba color/pool size
+                             0, // frame index
+                             NI_DEVICE_TYPE_SCALER);
+
+    return rc;
+}
+
+void ff_ni_clone_hwframe_ctx(AVHWFramesContext *in_frames_ctx,
+                             AVHWFramesContext *out_frames_ctx)
+{
+  AVNIFramesContext *in_frames_hwctx;
+  AVNIFramesContext *out_frames_hwctx;
+  NIFramesContext *out_ni_frames_ctx;
+  NIFramesContext *in_ni_frames_ctx;
+
+  /*
+   * Warning: ugly hacks lie ahead...
+   *
+   * We clone the incoming hardware frame context to the output
+   * frame context including its internal data. This should really
+   * be set up by the ni_frames_init() hwcontext driver.
+   */
+
+  out_frames_hwctx = out_frames_ctx->hwctx;
+  in_frames_hwctx = in_frames_ctx->hwctx;
+
+  memcpy(out_frames_ctx->internal->priv,
+         in_frames_ctx->internal->priv, sizeof(NIFramesContext));
+
+  memcpy(out_frames_hwctx,in_frames_hwctx, sizeof(AVNIFramesContext));
+
+  in_ni_frames_ctx = in_frames_ctx->internal->priv;
+  out_ni_frames_ctx = out_frames_ctx->internal->priv;
+
+  ni_device_session_copy(&in_ni_frames_ctx->api_ctx, &out_ni_frames_ctx->api_ctx);
+
+}
diff --git a/libavfilter/nifilter.h b/libavfilter/nifilter.h
new file mode 100644
index 0000000000..b290d25464
--- /dev/null
+++ b/libavfilter/nifilter.h
@@ -0,0 +1,50 @@
+/*
+ * XCoder Filter Lib Wrapper
+ *
+ * Copyright (c) 2020 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * XCoder codec lib wrapper.
+ */
+
+#ifndef AVFILTER_NIFILTER_H
+#define AVFILTER_NIFILTER_H
+
+#include "libavutil/pixdesc.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_ni_quad.h"
+
+#include <ni_device_api.h>
+
+#define DEFAULT_NI_FILTER_POOL_SIZE     4
+
+int ff_ni_ffmpeg_to_gc620_pix_fmt(enum AVPixelFormat pix_fmt);
+int ff_ni_copy_device_to_host_frame(AVFrame *dst, const ni_frame_t *src, int pix_fmt);
+int ff_ni_copy_host_to_device_frame(ni_frame_t *dst, const AVFrame *src, int pix_fmt);
+int ff_ni_build_frame_pool(ni_session_context_t *ctx,int width,int height, enum AVPixelFormat out_format,int pool_size);
+
+void ff_ni_frame_free(void *opaque, uint8_t *data);
+void ff_ni_clone_hwframe_ctx(AVHWFramesContext *in_frames_ctx,
+                             AVHWFramesContext *out_frames_ctx);
+
+#endif
diff --git a/libavfilter/vf_bg_ni.c b/libavfilter/vf_bg_ni.c
new file mode 100644
index 0000000000..d2cea90678
--- /dev/null
+++ b/libavfilter/vf_bg_ni.c
@@ -0,0 +1,1523 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <float.h>
+#include <math.h>
+#include <stdio.h>
+#include <string.h>
+#include <unistd.h>
+
+#include "libavutil/buffer.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/hwcontext_ni_quad.h"
+#include "libavutil/log.h"
+#include "libavutil/opt.h"
+#include "libswscale/swscale.h"
+
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#if HAVE_IO_H
+#include <io.h>
+#endif
+#include "ni_device_api.h"
+#include "ni_util.h"
+#include "nifilter.h"
+#include "video.h"
+
+#include "libavutil/avassert.h"
+
+// used for OpenImage
+#include <libavcodec/avcodec.h>
+#include <libavfilter/buffersink.h>
+#include <libavfilter/buffersrc.h>
+#include <libavformat/avformat.h>
+#include <libavutil/bprint.h>
+#include <libavutil/pixfmt.h>
+#include <libavutil/time.h>
+#include <libavutil/timecode.h>
+#include <stdlib.h>
+
+typedef struct _ni_roi_network_layer {
+    int32_t width;
+    int32_t height;
+    int32_t channel;
+    int32_t classes;
+    int32_t component;
+    int32_t output_number;
+    float *output;
+} ni_roi_network_layer_t;
+
+typedef struct _ni_roi_network {
+    int32_t netw;
+    int32_t neth;
+    ni_network_data_t raw;
+    ni_roi_network_layer_t *layers;
+} ni_roi_network_t;
+
+typedef struct HwScaleContext {
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+} HwScaleContext;
+
+typedef struct AiContext {
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_src_frame;
+    ni_session_data_io_t api_dst_pkt;
+} AiContext;
+
+typedef struct OverlayContext {
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+} OverlayContext;
+
+typedef struct NiBgContext {
+    const AVClass *class;
+    int device_idx;
+
+    AVBufferRef *hwdevice;
+    AVBufferRef *hwframe;
+
+    AVBufferRef *hw_frames_ctx;
+
+    /* roi */
+    AVBufferRef *out_frames_ref;
+
+    /* ai */
+    int initialized;
+    const char *nb_file; /* path to network binary */
+    const char *bg_img;  /* path to background img */
+    int use_default_bg;  /* use_default_bg */
+
+    AiContext *ai_ctx;
+    ni_roi_network_t network;
+    HwScaleContext *hws_ctx;
+
+    /* overlay */
+    OverlayContext *overlay_ctx;
+
+    /* bg */
+    uint8_t *mask_data;
+    int bg_frame_size;
+    // AVFrame *bg_frame;
+    AVFrame *alpha_mask_frame;
+    AVFrame *alpha_large_frame;
+    AVFrame *alpha_mask_hwframe;
+    AVFrame *alpha_enlarge_frame;
+    int keep_alive_timeout; /* keep alive timeout setting */
+} NiBgContext;
+
+static void cleanup_ai_context(AVFilterContext *ctx, NiBgContext *s) {
+    ni_retcode_t retval;
+    AiContext *ai_ctx = s->ai_ctx;
+
+    if (ai_ctx) {
+        ni_frame_buffer_free(&ai_ctx->api_src_frame.data.frame);
+        ni_packet_buffer_free(&ai_ctx->api_dst_pkt.data.packet);
+
+        retval =
+            ni_device_session_close(&ai_ctx->api_ctx, 1, NI_DEVICE_TYPE_AI);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "%s: failed to close ai session. retval %d\n", __func__,
+                   retval);
+        }
+        av_free(ai_ctx);
+        s->ai_ctx = NULL;
+    }
+}
+
+static int init_ai_context(AVFilterContext *ctx, NiBgContext *s,
+                           AVFrame *frame) {
+    ni_retcode_t retval;
+    AiContext *ai_ctx;
+    ni_roi_network_t *network = &s->network;
+    int ret;
+    int hwframe = frame->format == AV_PIX_FMT_NI_QUAD ? 1 : 0;
+
+#if HAVE_IO_H
+    if ((s->nb_file == NULL) || (_access(s->nb_file, R_OK) != 0)) {
+#else
+    if ((s->nb_file == NULL) || (access(s->nb_file, R_OK) != 0)) {
+#endif
+        av_log(ctx, AV_LOG_ERROR, "invalid network binary path\n");
+        return AVERROR(EINVAL);
+    }
+
+    ai_ctx = av_mallocz(sizeof(AiContext));
+    if (!ai_ctx) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate ai context\n");
+        return AVERROR(ENOMEM);
+    }
+
+    ni_device_session_context_init(&ai_ctx->api_ctx);
+    if (hwframe) {
+        AVHWFramesContext *pAVHFWCtx;
+        AVNIDeviceContext *pAVNIDevCtx;
+        int cardno;
+
+        pAVHFWCtx   = (AVHWFramesContext *)frame->hw_frames_ctx->data;
+        pAVNIDevCtx = (AVNIDeviceContext *)pAVHFWCtx->device_ctx->hwctx;
+        cardno      = ni_get_cardno(frame);
+
+        ai_ctx->api_ctx.device_handle = pAVNIDevCtx->cards[cardno];
+        ai_ctx->api_ctx.blk_io_handle = pAVNIDevCtx->cards[cardno];
+        ai_ctx->api_ctx.hw_action     = NI_CODEC_HW_ENABLE;
+        ai_ctx->api_ctx.hw_id         = cardno;
+    }
+
+    ai_ctx->api_ctx.device_type = NI_DEVICE_TYPE_AI;
+    ai_ctx->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+
+    retval = ni_device_session_open(&ai_ctx->api_ctx, NI_DEVICE_TYPE_AI);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to open ai session. retval %d\n",
+               retval);
+        return AVERROR(EIO);
+    }
+
+    retval = ni_ai_config_network_binary(&ai_ctx->api_ctx, &network->raw,
+                                         s->nb_file);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to configure ai session. retval %d\n",
+               retval);
+        ret = AVERROR(EIO);
+        goto failed_out;
+    }
+
+    if (!hwframe) {
+        retval = ni_ai_frame_buffer_alloc(&ai_ctx->api_src_frame.data.frame,
+                                          &network->raw);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR, "failed to allocate ni frame\n");
+            ret = AVERROR(ENOMEM);
+            goto failed_out;
+        }
+    }
+
+    retval = ni_ai_packet_buffer_alloc(&ai_ctx->api_dst_pkt.data.packet,
+                                       &network->raw);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate ni packet\n");
+        ret = AVERROR(ENOMEM);
+        goto failed_out;
+    }
+
+    s->ai_ctx = ai_ctx;
+    return 0;
+
+failed_out:
+    cleanup_ai_context(ctx, s);
+    return ret;
+}
+
+static void ni_destroy_network(AVFilterContext *ctx,
+                               ni_roi_network_t *network) {
+    if (network) {
+        int i;
+
+        for (i = 0; i < network->raw.output_num; i++) {
+            if (network->layers[i].output) {
+                free(network->layers[i].output);
+                network->layers[i].output = NULL;
+            }
+        }
+
+        free(network->layers);
+        network->layers = NULL;
+    }
+}
+
+static int ni_create_network(AVFilterContext *ctx, ni_roi_network_t *network) {
+    int ret;
+    int i;
+    ni_network_data_t *ni_network = &network->raw;
+
+    av_log(ctx, AV_LOG_VERBOSE, "network input number %d, output number %d\n",
+           ni_network->input_num, ni_network->output_num);
+
+    if (ni_network->input_num == 0 || ni_network->output_num == 0) {
+        av_log(ctx, AV_LOG_ERROR, "invalid network layer\n");
+        return AVERROR(EINVAL);
+    }
+
+    /* only support one input for now */
+    if (ni_network->input_num != 1) {
+        av_log(ctx, AV_LOG_ERROR,
+               "network input layer number %d not supported\n",
+               ni_network->input_num);
+        return AVERROR(EINVAL);
+    }
+
+    /*
+     * create network and its layers. i don't know whether this is platform
+     * specific or not. maybe i shall add a create network api to do this.
+     */
+    network->layers =
+        malloc(sizeof(ni_roi_network_layer_t) * ni_network->output_num);
+    if (!network->layers) {
+        av_log(ctx, AV_LOG_ERROR, "cannot allocate network layer memory\n");
+        return AVERROR(ENOMEM);
+    }
+
+    for (i = 0; i < ni_network->output_num; i++) {
+        network->layers[i].width     = ni_network->linfo.out_param[i].sizes[0];
+        network->layers[i].height    = ni_network->linfo.out_param[i].sizes[1];
+        network->layers[i].channel   = ni_network->linfo.out_param[i].sizes[2];
+        network->layers[i].component = 3;
+        network->layers[i].classes =
+            (network->layers[i].channel / network->layers[i].component) -
+            (4 + 1);
+        network->layers[i].output_number =
+            ni_ai_network_layer_dims(&ni_network->linfo.out_param[i]);
+        av_assert0(network->layers[i].output_number ==
+                   network->layers[i].width * network->layers[i].height *
+                       network->layers[i].channel);
+
+        network->layers[i].output =
+            malloc(network->layers[i].output_number * sizeof(float));
+        if (!network->layers[i].output) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "failed to allocate network layer %d output buffer\n", i);
+            ret = AVERROR(ENOMEM);
+            goto out;
+        }
+
+        av_log(ctx, AV_LOG_DEBUG,
+               "network layer %d: w %d, h %d, ch %d, co %d, cl %d\n", i,
+               network->layers[i].width, network->layers[i].height,
+               network->layers[i].channel, network->layers[i].component,
+               network->layers[i].classes);
+    }
+
+    network->netw = ni_network->linfo.in_param[0].sizes[0];
+    network->neth = ni_network->linfo.in_param[0].sizes[1];
+
+    return 0;
+out:
+    ni_destroy_network(ctx, network);
+    return ret;
+}
+
+static av_cold int init_hwframe_scale(AVFilterContext *ctx, NiBgContext *s,
+                                      enum AVPixelFormat format,
+                                      AVFrame *frame) {
+    ni_retcode_t retval;
+    HwScaleContext *hws_ctx;
+    int ret;
+    AVHWFramesContext *pAVHFWCtx;
+    AVNIDeviceContext *pAVNIDevCtx;
+    int cardno;
+
+    hws_ctx = av_mallocz(sizeof(HwScaleContext));
+    if (!hws_ctx) {
+        av_log(ctx, AV_LOG_ERROR, "could not allocate hwframe ctx\n");
+        return AVERROR(ENOMEM);
+    }
+
+    ni_device_session_context_init(&hws_ctx->api_ctx);
+
+    pAVHFWCtx   = (AVHWFramesContext *)frame->hw_frames_ctx->data;
+    pAVNIDevCtx = (AVNIDeviceContext *)pAVHFWCtx->device_ctx->hwctx;
+    cardno      = ni_get_cardno(frame);
+
+    hws_ctx->api_ctx.device_handle     = pAVNIDevCtx->cards[cardno];
+    hws_ctx->api_ctx.blk_io_handle     = pAVNIDevCtx->cards[cardno];
+    hws_ctx->api_ctx.device_type       = NI_DEVICE_TYPE_SCALER;
+    hws_ctx->api_ctx.scaler_operation  = NI_SCALER_OPCODE_SCALE;
+    hws_ctx->api_ctx.hw_id             = cardno;
+    hws_ctx->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+
+    retval = ni_device_session_open(&hws_ctx->api_ctx, NI_DEVICE_TYPE_SCALER);
+    if (retval < 0) {
+        av_log(ctx, AV_LOG_ERROR, "could not open scaler session\n");
+        ret = AVERROR(EIO);
+        goto out;
+    }
+
+    /* Create scale frame pool on device */
+    retval = ff_ni_build_frame_pool(&hws_ctx->api_ctx, s->network.netw,
+                                    s->network.neth, format,
+                                    DEFAULT_NI_FILTER_POOL_SIZE);
+    if (retval < 0) {
+        av_log(ctx, AV_LOG_ERROR, "could not build frame pool\n");
+        ni_device_session_close(&hws_ctx->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ret = AVERROR(EIO);
+        goto out;
+    }
+
+    s->hws_ctx = hws_ctx;
+    return 0;
+out:
+    av_free(hws_ctx);
+    return ret;
+}
+
+static void cleanup_hwframe_scale(AVFilterContext *ctx, NiBgContext *s) {
+    HwScaleContext *hws_ctx = s->hws_ctx;
+
+    if (hws_ctx) {
+        ni_frame_buffer_free(&hws_ctx->api_dst_frame.data.frame);
+        ni_device_session_close(&hws_ctx->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+
+        av_free(hws_ctx);
+        s->hws_ctx = NULL;
+    }
+}
+
+static int init_hwframe_overlay(AVFilterContext *ctx, NiBgContext *s,
+                                AVFrame *main_frame) {
+    ni_retcode_t retcode;
+    OverlayContext *overlay_ctx;
+    AVHWFramesContext *pAVHFWCtx;
+    AVNIDeviceContext *pAVNIDevCtx;
+    int main_cardno;
+    int ret;
+
+    overlay_ctx = av_mallocz(sizeof(OverlayContext));
+    if (!overlay_ctx) {
+        av_log(ctx, AV_LOG_ERROR, "could not allocate overlay ctx\n");
+        return AVERROR(ENOMEM);
+    }
+
+    ni_device_session_context_init(&overlay_ctx->api_ctx);
+
+    pAVHFWCtx   = (AVHWFramesContext *)main_frame->hw_frames_ctx->data;
+    pAVNIDevCtx = (AVNIDeviceContext *)pAVHFWCtx->device_ctx->hwctx;
+    main_cardno = ni_get_cardno(main_frame);
+
+    overlay_ctx->api_ctx.device_handle = pAVNIDevCtx->cards[main_cardno];
+    overlay_ctx->api_ctx.blk_io_handle = pAVNIDevCtx->cards[main_cardno];
+
+    overlay_ctx->api_ctx.hw_id             = main_cardno;
+    overlay_ctx->api_ctx.device_type       = NI_DEVICE_TYPE_SCALER;
+    overlay_ctx->api_ctx.scaler_operation  = NI_SCALER_OPCODE_OVERLAY;
+    overlay_ctx->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+
+    retcode =
+        ni_device_session_open(&overlay_ctx->api_ctx, NI_DEVICE_TYPE_SCALER);
+    if (retcode < 0) {
+        av_log(ctx, AV_LOG_ERROR, "Can't open device session on card %d\n",
+               main_cardno);
+        ret = AVERROR(EIO);
+        goto fail_out;
+    }
+
+    /* Create frame pool on device */
+    ret = ff_ni_build_frame_pool(&overlay_ctx->api_ctx, main_frame->width,
+                                 main_frame->height, pAVHFWCtx->sw_format,
+                                 DEFAULT_NI_FILTER_POOL_SIZE);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "can not build overlay frame pool\n");
+        ni_device_session_close(&overlay_ctx->api_ctx, 1,
+                                NI_DEVICE_TYPE_SCALER);
+        goto fail_out;
+    }
+
+    s->overlay_ctx = overlay_ctx;
+    return 0;
+
+fail_out:
+    av_free(overlay_ctx);
+    return ret;
+}
+
+static void cleanup_hwframe_overlay(AVFilterContext *ctx, NiBgContext *s) {
+    OverlayContext *overlay_ctx = s->overlay_ctx;
+
+    if (overlay_ctx) {
+        ni_frame_buffer_free(&overlay_ctx->api_dst_frame.data.frame);
+        ni_device_session_close(&overlay_ctx->api_ctx, 1,
+                                NI_DEVICE_TYPE_SCALER);
+
+        av_free(overlay_ctx);
+        s->overlay_ctx = NULL;
+    }
+}
+
+static av_cold int nibg_init(AVFilterContext *ctx) {
+    NiBgContext *s = ctx->priv;
+    char buf[64]   = {0};
+
+    snprintf(buf, sizeof(buf), "%d", s->device_idx);
+
+    return av_hwdevice_ctx_create(&s->hwdevice, AV_HWDEVICE_TYPE_NI_QUADRA, buf, NULL,
+                                  0);
+}
+
+static av_cold void nibg_uninit(AVFilterContext *ctx) {
+    NiBgContext *s            = ctx->priv;
+    ni_roi_network_t *network = &s->network;
+
+    av_buffer_unref(&s->hwframe);
+    av_buffer_unref(&s->hwdevice);
+
+    av_buffer_unref(&s->hw_frames_ctx);
+
+    /* roi */
+    av_buffer_unref(&s->out_frames_ref);
+
+    /* ai */
+    cleanup_ai_context(ctx, s);
+    ni_destroy_network(ctx, network);
+
+    /* bg */
+    av_frame_free(&s->alpha_mask_frame);
+    av_frame_free(&s->alpha_large_frame);
+
+    cleanup_hwframe_scale(ctx, s);
+
+    cleanup_hwframe_overlay(ctx, s);
+}
+
+static int nibg_query_formats(AVFilterContext *ctx) {
+    NiBgContext *nictx                 = ctx->priv;
+    AVHWFramesConstraints *constraints = NULL;
+    const enum AVPixelFormat *input_pix_fmts, *output_pix_fmts;
+    AVFilterFormats *input_formats = NULL;
+    int err, i;
+
+    if (!nictx->hwdevice)
+        return AVERROR(ENOMEM);
+
+    constraints = av_hwdevice_get_hwframe_constraints(nictx->hwdevice, NULL);
+    if (!constraints) {
+        err = AVERROR(EINVAL);
+        goto fail;
+    }
+
+    input_pix_fmts  = constraints->valid_sw_formats;
+    output_pix_fmts = constraints->valid_hw_formats;
+
+    input_formats = ff_make_format_list(output_pix_fmts);
+    if (!input_formats) {
+        err = AVERROR(ENOMEM);
+        goto fail;
+    }
+    if (input_pix_fmts) {
+        for (i = 0; input_pix_fmts[i] != AV_PIX_FMT_NONE; i++) {
+            err = ff_add_format(&input_formats, input_pix_fmts[i]);
+            if (err < 0)
+                goto fail;
+        }
+    }
+
+    // Needed for FFmpeg-n4.4+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+    if ((err = ff_formats_ref(input_formats, &ctx->inputs[0]->outcfg.formats)) < 0 ||
+        (err = ff_formats_ref(ff_make_format_list(output_pix_fmts),
+            &ctx->outputs[0]->incfg.formats)) < 0)
+#else
+    if ((err = ff_formats_ref(input_formats, &ctx->inputs[0]->out_formats)) < 0 ||
+        (err = ff_formats_ref(ff_make_format_list(output_pix_fmts),
+            &ctx->outputs[0]->in_formats)) < 0)
+#endif
+        goto fail;
+
+    av_hwframe_constraints_free(&constraints);
+    return 0;
+
+fail:
+    av_buffer_unref(&nictx->hwdevice);
+    av_hwframe_constraints_free(&constraints);
+    return err;
+}
+
+static int nibg_config_output(AVFilterLink *outlink) {
+    AVFilterContext *ctx = outlink->src;
+    AVFilterLink *inlink = ctx->inputs[0];
+    NiBgContext *s       = ctx->priv;
+
+    AVHWFramesContext *hwframe_ctx;
+    int ret;
+
+    AVHWFramesContext *in_frames_ctx;
+    AVHWFramesContext *out_frames_ctx;
+
+    av_log(ctx, AV_LOG_DEBUG, "%s\n", __func__);
+
+    av_buffer_unref(&s->hwframe);
+#if 0
+    if (inlink->format == outlink->format) {
+        // The input is already a hardware format, so we just want to
+        // pass through the input frames in their own hardware context.
+        if (!inlink->hw_frames_ctx) {
+            av_log(ctx, AV_LOG_ERROR, "No input hwframe context.\n");
+            return AVERROR(EINVAL);
+        }
+
+        outlink->hw_frames_ctx = av_buffer_ref(inlink->hw_frames_ctx);
+        if (!outlink->hw_frames_ctx)
+            return AVERROR(ENOMEM);
+
+        return 0;
+    }
+#endif
+
+    if (inlink->hw_frames_ctx == NULL) {
+        av_log(ctx, AV_LOG_DEBUG, "swframe\n");
+        return 0;
+    }
+
+    /* uploader */
+    s->hwframe = av_hwframe_ctx_alloc(s->hwdevice);
+    if (!s->hwframe)
+        return AVERROR(ENOMEM);
+
+    av_log(ctx, AV_LOG_DEBUG, "inlink wxh %dx%d\n", inlink->w, inlink->h);
+
+    hwframe_ctx         = (AVHWFramesContext *)s->hwframe->data;
+    hwframe_ctx->format = AV_PIX_FMT_NI_QUAD;
+    //    hwframe_ctx->sw_format = inlink->format;
+    //    hwframe_ctx->sw_format = AV_PIX_FMT_YUV420P;
+    hwframe_ctx->sw_format = AV_PIX_FMT_RGBA;
+    hwframe_ctx->width     = inlink->w;
+    hwframe_ctx->height    = inlink->h;
+
+    ret = av_hwframe_ctx_init(s->hwframe);
+    if (ret < 0)
+        return ret;
+
+    s->hw_frames_ctx = av_buffer_ref(s->hwframe);
+    if (!s->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    /* roi */
+    outlink->w = inlink->w;
+    outlink->h = inlink->h;
+
+    av_log(ctx, AV_LOG_DEBUG, "outlink wxh %dx%d\n", outlink->w, outlink->h);
+
+    in_frames_ctx = (AVHWFramesContext *)ctx->inputs[0]->hw_frames_ctx->data;
+
+    s->out_frames_ref = av_hwframe_ctx_alloc(in_frames_ctx->device_ref);
+    if (!s->out_frames_ref)
+        return AVERROR(ENOMEM);
+
+    out_frames_ctx = (AVHWFramesContext *)s->out_frames_ref->data;
+
+    ff_ni_clone_hwframe_ctx(in_frames_ctx, out_frames_ctx);
+
+    out_frames_ctx->format            = AV_PIX_FMT_NI_QUAD;
+    out_frames_ctx->width             = outlink->w;
+    out_frames_ctx->height            = outlink->h;
+    out_frames_ctx->sw_format         = in_frames_ctx->sw_format;
+    out_frames_ctx->initial_pool_size = NI_BG_ID;
+
+    av_hwframe_ctx_init(s->out_frames_ref);
+
+    av_buffer_unref(&ctx->outputs[0]->hw_frames_ctx);
+    ctx->outputs[0]->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    if (!ctx->outputs[0]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return 0;
+}
+
+static AVFrame *import_bg_frame(AVFilterContext *ctx, NiBgContext *s, int src_w,
+                                int src_h, enum AVPixelFormat src_pixfmt,
+                                const char *imageFileName,
+                                AVFrame *input_frame) // ovleray network nb
+{
+    //FFmpeg3.4.2 requirement only
+#if ((LIBAVFILTER_VERSION_MAJOR <= 6) && (LIBAVFILTER_VERSION_MINOR <= 107))
+    av_register_all();
+#endif
+    AVFormatContext *pFormatCtx = NULL;
+
+    if (avformat_open_input(&(pFormatCtx), imageFileName, NULL, NULL) != 0) {
+        av_log(ctx, AV_LOG_ERROR, "Can't open image file '%s'\n",
+               imageFileName);
+        return NULL;
+    }
+
+    if (avformat_find_stream_info(pFormatCtx, NULL) < 0) {
+        av_log(ctx, AV_LOG_ERROR, "Can't find stream\n");
+        return NULL;
+    }
+
+    av_dump_format(pFormatCtx, 0, imageFileName,
+                   0); // when the fourth set to 1, it produce segment fault
+                       // error av_dump_format(pFormatCtx, 0, imageFileName, 1);
+                       // the last param is: is_output select whether the
+                       // specified context is an input(0) or output(1)
+
+    AVCodecContext *pCodecCtx;
+    int index =
+        av_find_best_stream(pFormatCtx, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, 0);
+
+#if LIBAVFILTER_VERSION_MAJOR >= 9 || LIBAVFILTER_VERSION_MAJOR == 8 && LIBAVFILTER_VERSION_MINOR >= 24
+    const AVCodec *dec =
+#else
+    AVCodec *dec =
+#endif
+        avcodec_find_decoder(pFormatCtx->streams[index]->codecpar->codec_id);
+    pCodecCtx = avcodec_alloc_context3(dec);
+    avcodec_parameters_to_context(pCodecCtx,
+                                  pFormatCtx->streams[index]->codecpar);
+
+    // Find the decoder for the video stream
+#if LIBAVFILTER_VERSION_MAJOR >= 9 || LIBAVFILTER_VERSION_MAJOR == 8 && LIBAVFILTER_VERSION_MINOR >= 24
+    const AVCodec *pCodec =
+#else
+    AVCodec *pCodec =
+#endif
+        avcodec_find_decoder(pCodecCtx->codec_id);
+    if (!pCodec) {
+        av_log(ctx, AV_LOG_ERROR, "Codec not found\n");
+        return NULL;
+    }
+
+    // Open codec
+    if (avcodec_open2(pCodecCtx, pCodec, NULL) < 0) {
+        av_log(ctx, AV_LOG_ERROR, "Could not open codec\n");
+        return NULL;
+    }
+
+    AVFrame *pFrame;
+
+    pFrame = av_frame_alloc();
+
+    if (!pFrame) {
+        av_log(ctx, AV_LOG_ERROR, "Can't allocate memory for AVFrame\n");
+        return NULL;
+    }
+
+    AVPacket packet;
+    packet.data = NULL;
+    packet.size = 0;
+
+    while (av_read_frame(pFormatCtx, &packet) >= 0) {
+        if (packet.stream_index != index) {
+            continue;
+        }
+        int ret = 0;
+        ret     = avcodec_send_packet(pCodecCtx, &packet);
+        if (ret < 0) {
+            av_log(ctx, AV_LOG_ERROR, "avcodec_send_packet failed");
+            return NULL;
+        }
+        ret = avcodec_receive_frame(pCodecCtx, pFrame);
+        if (ret < 0) {
+            av_log(ctx, AV_LOG_ERROR, "avcodec_receive_frame failed");
+            return NULL;
+        }
+    }
+
+    AVFrame *dst = av_frame_alloc();
+
+    enum AVPixelFormat dst_pixfmt = src_pixfmt;
+
+    dst->format = (int)dst_pixfmt;
+    dst->width  = src_w;
+    dst->height = src_h;
+
+    int numBytes =
+        av_image_get_buffer_size(dst->format, dst->width, dst->height, 1);
+    uint8_t *buffer = (uint8_t *)av_malloc(numBytes * sizeof(uint8_t));
+    av_image_fill_arrays(dst->data, dst->linesize, buffer, dst->format,
+                         dst->width, dst->height, 1);
+
+    struct SwsContext *convert_ctx = NULL;
+
+    convert_ctx = sws_getContext(pFrame->width, pFrame->height,
+                                 pCodecCtx->pix_fmt, dst->width, dst->height,
+                                 dst->format, SWS_POINT, NULL, NULL, NULL);
+
+    sws_scale(convert_ctx, (const uint8_t *const *)pFrame->data,
+              pFrame->linesize, 0, pFrame->height, dst->data, dst->linesize);
+    sws_freeContext(convert_ctx);
+
+    av_frame_free(&pFrame);
+    avformat_close_input(&(pFormatCtx));
+    avcodec_free_context(&pCodecCtx);
+
+    return dst;
+}
+
+static AVFrame *create_bg_frame(AVFilterContext *ctx, int src_w, int src_h,
+                                enum AVPixelFormat src_pixfmt) {
+
+    NiBgContext *s   = ctx->priv;
+    s->bg_frame_size = s->network.netw * s->network.neth;
+
+    /*create bg frame*/
+    AVFrame *dst = av_frame_alloc();
+
+    int dst_w = src_w;
+    int dst_h = src_h;
+    // enum AVPixelFormat dst_pixfmt = AV_PIX_FMT_YUV420P; //AV_PIX_FMT_RGBA;
+    enum AVPixelFormat dst_pixfmt = src_pixfmt;
+
+    dst->format = (int)dst_pixfmt;
+    dst->width  = dst_w;
+    dst->height = dst_h;
+
+    int numBytes =
+        av_image_get_buffer_size(dst->format, dst->width, dst->height,
+                                 1); // pFrame->width, pFrame->height
+
+    uint8_t *buffer = (uint8_t *)av_malloc(numBytes * sizeof(uint8_t));
+
+    av_image_fill_arrays(dst->data, dst->linesize, buffer, dst->format,
+                         dst->width, dst->height, 1);
+
+    const int dst_linesize = dst->linesize[0];
+
+    av_log(ctx, AV_LOG_DEBUG, "create_frame function: dst_linesize: %d \n",
+           dst_linesize);
+
+    /*copy value to dst(bg_frame) frame*/
+    int size_Y  = dst->width * dst->height;
+    int size_UV = dst->width * dst->height / 4;
+
+    uint8_t *Y_value = malloc(size_Y * sizeof(uint8_t));
+    uint8_t *U_value = malloc(size_UV * sizeof(uint8_t));
+    uint8_t *V_value = malloc(size_UV * sizeof(uint8_t));
+    uint8_t *A_value = malloc(s->bg_frame_size * sizeof(uint8_t));
+
+    av_log(ctx, AV_LOG_DEBUG,
+           "create_bg_frame dst->linesize[0] %d dst->linesize[1] %d "
+           "dst->linesize[2] %d dst->linesize[3] %d\n",
+           dst->linesize[0], dst->linesize[1], dst->linesize[2],
+           dst->linesize[3]);
+
+    for (int i = 0; i < size_Y; i++) {
+        Y_value[i] = 149;
+    }
+
+    for (int i = 0; i < size_UV; i++) {
+        U_value[i] = 43;
+    }
+
+    for (int i = 0; i < size_UV; i++) {
+        V_value[i] = 21;
+    }
+
+    for (int i = 0; i < s->bg_frame_size; i++) {
+        A_value[i] = 21;
+    }
+
+    // copy the mask_data to the alpha_mask_frame
+    av_image_copy_plane(dst->data[0], dst->linesize[0], Y_value,
+                        dst->linesize[0], dst->linesize[0], dst->height);
+
+    av_image_copy_plane(dst->data[1], dst->linesize[1], U_value,
+                        dst->linesize[1], dst->linesize[1], (dst->height) / 2);
+
+    av_image_copy_plane(dst->data[2], dst->linesize[2], V_value,
+                        dst->linesize[2], dst->linesize[2], (dst->height) / 2);
+
+    av_image_copy_plane(dst->data[3], dst->linesize[3], A_value,
+                        dst->linesize[3], dst->linesize[3], dst->height);
+
+    av_free(Y_value);
+    av_free(U_value);
+    av_free(V_value);
+    av_free(A_value);
+    return dst;
+}
+
+static AVFrame *create_frame(AVFilterContext *ctx, int src_w, int src_h,
+                             enum AVPixelFormat src_pixfmt) {
+
+    AVFrame *dst = av_frame_alloc();
+
+    int dst_w                     = src_w;
+    int dst_h                     = src_h;
+    enum AVPixelFormat dst_pixfmt = src_pixfmt;
+
+    dst->format = (int)dst_pixfmt;
+    dst->width  = dst_w;
+    dst->height = dst_h;
+
+    int numBytes =
+        av_image_get_buffer_size(dst->format, dst->width, dst->height,
+                                 1); // pFrame->width, pFrame->height
+
+    uint8_t *buffer = (uint8_t *)av_malloc(numBytes * sizeof(uint8_t));
+
+    av_image_fill_arrays(dst->data, dst->linesize, buffer, dst->format,
+                         dst->width, dst->height, 1);
+
+    const int dst_linesize = dst->linesize[0];
+
+    av_log(ctx, AV_LOG_DEBUG, "create_frame function: dst_linesize: %d \n",
+           dst_linesize);
+
+    return dst;
+}
+
+static int ni_get_mask(AVFilterContext *ctx, uint8_t *mask_data,
+                       ni_roi_network_t *network) {
+    uint8_t Y_MIN = 0;
+    uint8_t Y_MAX = 255;
+
+    ni_roi_network_layer_t *l = &network->layers[0];
+
+    av_log(ctx, AV_LOG_DEBUG,
+           "network->netw: %d network->neth: %d mask_size %d \n", network->netw,
+           network->neth, network->netw * network->neth);
+
+    int mask_size = network->netw * network->neth;
+
+    if (!mask_data) {
+        av_log(ctx, AV_LOG_ERROR, "cannot allocate s->mask_data memory\n");
+        return AVERROR(ENOMEM);
+    }
+
+    // nchw proprocessing
+    /* for (int i = 0; i<mask_size;i++){
+        if(l->output[i] > l->output[i+mask_size]){
+           mask_data[i] = Y_MAX;
+
+        }else{
+            mask_data[i] = Y_MIN;
+        }
+    } */
+
+    // nhwc proprocessing
+    for (int i = 0; i < mask_size; i++) {
+        if (l->output[2 * i] > l->output[2 * i + 1]) {
+            mask_data[i] = Y_MAX;
+
+        } else {
+            mask_data[i] = Y_MIN;
+        }
+        // av_log(ctx, AV_LOG_INFO,
+        //       "%d %d\n", i,  mask_data[i]);
+    }
+
+    av_log(ctx, AV_LOG_DEBUG, "lw=%d, lh=%d, ln=%d, lo=%d, nw=%d, nh=%d, \n",
+           l->width, l->height, l->component, l->output_number, network->netw,
+           network->neth);
+    return 0;
+}
+
+static int get_alpha_mask_frame(AVFilterContext *ctx, AVFrame *in,
+                                uint8_t *mask_data) {
+    NiBgContext *s = ctx->priv;
+
+    av_image_copy_plane(
+        s->alpha_mask_frame->data[3], s->alpha_mask_frame->linesize[3],
+        mask_data, s->alpha_mask_frame->linesize[3],
+        s->alpha_mask_frame->linesize[3], s->alpha_mask_frame->height);
+
+    av_log(ctx, AV_LOG_DEBUG,
+           "get_alpha_mask_frame function: alpha_mask_frame->width: %d "
+           "alpha_mask_frame->height:%d alpha_mask_frame->format:%d \n",
+           s->alpha_mask_frame->width, s->alpha_mask_frame->height,
+           s->alpha_mask_frame->format);
+    struct SwsContext *convert_ctx = NULL;
+
+    convert_ctx = sws_getContext(
+        s->alpha_mask_frame->width, s->alpha_mask_frame->height,
+        s->alpha_mask_frame->format, s->alpha_large_frame->width,
+        s->alpha_large_frame->height, s->alpha_large_frame->format,
+        SWS_FAST_BILINEAR, NULL, NULL, NULL);
+
+    // s->alpha_mask_frame is small frame (144x256) AV_PIX_FMT_YUVA420P,
+    // s->alpha_large_frame is large frame (1280x720) AV_PIX_FMT_RGBA
+    sws_scale(convert_ctx, (const uint8_t *const *)s->alpha_mask_frame->data,
+              s->alpha_mask_frame->linesize, 0, s->alpha_mask_frame->height,
+              s->alpha_large_frame->data, s->alpha_large_frame->linesize);
+
+    sws_freeContext(convert_ctx);
+
+    s->alpha_enlarge_frame = s->alpha_large_frame;
+    return 0;
+}
+
+static int ni_bg_config_input(AVFilterContext *ctx, AVFrame *frame) {
+    NiBgContext *s = ctx->priv;
+    int ret;
+
+    if (s->initialized)
+        return 0;
+
+    if (frame->color_range == AVCOL_RANGE_JPEG) {
+        av_log(ctx, AV_LOG_ERROR,
+               "WARNING: Full color range input, limited color output\n");
+    }
+
+    ret = init_ai_context(ctx, s, frame);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to initialize ai context\n");
+        return ret;
+    }
+
+    ret = ni_create_network(ctx, &s->network);
+    if (ret != 0) {
+        goto fail_out;
+    }
+
+    ret = init_hwframe_scale(ctx, s, AV_PIX_FMT_BGRP, frame); // AV_PIX_FMT_RGBA
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR,
+               "could not initialized hwframe scale context\n");
+        goto fail_out;
+    }
+
+    ret = init_hwframe_overlay(ctx, s, frame);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR,
+               "could not initialized hwframe overlay context\n");
+        goto fail_out;
+    }
+
+    s->mask_data = malloc(s->network.netw * s->network.neth * sizeof(uint8_t));
+
+    if (!s->mask_data) {
+        av_log(ctx, AV_LOG_ERROR, "cannot allocate sctx->mask_datamemory\n");
+        return AVERROR(ENOMEM);
+    }
+
+    enum AVPixelFormat alpha_mask_pixfmt = AV_PIX_FMT_YUVA420P;
+
+    // int(use_default_bg)==0 -> import_bg_frame don't use default_bg
+    // int(use_default_bg)>0 -> create_bg_frame use default_bg
+
+    if (s->use_default_bg == 0) {
+        s->alpha_mask_frame =
+            import_bg_frame(ctx, s, s->network.netw, s->network.neth,
+                            alpha_mask_pixfmt, s->bg_img, frame);
+    } else {
+        s->alpha_mask_frame = create_bg_frame(
+            ctx, s->network.netw, s->network.neth, alpha_mask_pixfmt);
+    }
+
+    // create the new s->alpha_large_frame, which is the enlarged version of
+    // s->alpha_mask_frame
+    enum AVPixelFormat alpha_large_frame_format = AV_PIX_FMT_RGBA;
+    s->alpha_large_frame = create_frame(ctx, frame->width, frame->height,
+                                        alpha_large_frame_format);
+
+    av_log(ctx, AV_LOG_DEBUG,
+           "ni_bg_config_input get_alpha_mask_frame function: "
+           "alpha_mask_frame->width: %d alpha_mask_frame->height: %d "
+           "s->alpha_mask_frame->format: %d alpha_large_frame->width :%d "
+           "alpha_large_frame->height: %d alpha_large_frame->format: %d "
+           "frame->width: %d frame->height: %d frame->format: %d "
+           "frame->linesize[0]: %d\n",
+           s->alpha_mask_frame->width, s->alpha_mask_frame->height,
+           s->alpha_mask_frame->format, s->alpha_large_frame->width,
+           s->alpha_large_frame->height, s->alpha_large_frame->format,
+           frame->width, frame->height, frame->format, frame->linesize[0]);
+
+    s->initialized = 1;
+    return 0;
+
+fail_out:
+    cleanup_ai_context(ctx, s);
+
+    ni_destroy_network(ctx, &s->network);
+
+    return ret;
+}
+
+static int ni_hwframe_scale(AVFilterContext *ctx, NiBgContext *s, AVFrame *in,
+                            int w, int h,
+                            niFrameSurface1_t **filt_frame_surface) {
+    HwScaleContext *scale_ctx = s->hws_ctx;
+    int scaler_format;
+    ni_retcode_t retcode;
+    niFrameSurface1_t *frame_surface, *new_frame_surface;
+    AVHWFramesContext *pAVHFWCtx;
+
+    frame_surface = (niFrameSurface1_t *)in->data[3];
+
+    av_log(ctx, AV_LOG_DEBUG, "in frame surface frameIdx %d\n",
+           frame_surface->ui16FrameIdx);
+
+    pAVHFWCtx = (AVHWFramesContext *)in->hw_frames_ctx->data;
+
+    scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(pAVHFWCtx->sw_format);
+
+    retcode = ni_frame_buffer_alloc_hwenc(&scale_ctx->api_dst_frame.data.frame,
+                                          w, h, 0);
+    if (retcode != NI_RETCODE_SUCCESS)
+        return AVERROR(ENOMEM);
+
+    /*
+     * Allocate device input frame. This call won't actually allocate a frame,
+     * but sends the incoming hardware frame index to the scaler manager
+     */
+    retcode = ni_device_alloc_frame(
+        &scale_ctx->api_ctx, FFALIGN(in->width, 2), FFALIGN(in->height, 2),
+        scaler_format, 0, 0, 0, 0, 0, frame_surface->ui32nodeAddress,
+        frame_surface->ui16FrameIdx, NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(NULL, AV_LOG_DEBUG, "Can't allocate device input frame %d\n",
+               retcode);
+        return AVERROR(ENOMEM);
+    }
+
+    /* Allocate hardware device destination frame. This acquires a frame from
+     * the pool */
+    retcode = ni_device_alloc_frame(
+        &scale_ctx->api_ctx, FFALIGN(w, 2), FFALIGN(h, 2),
+        ff_ni_ffmpeg_to_gc620_pix_fmt(AV_PIX_FMT_BGRP), NI_SCALER_FLAG_IO, 0, 0,
+        0, 0, 0, -1, NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(NULL, AV_LOG_DEBUG, "Can't allocate device output frame %d\n",
+               retcode);
+        return AVERROR(ENOMEM);
+    }
+
+    /* Set the new frame index */
+    ni_device_session_read_hwdesc(
+        &scale_ctx->api_ctx, &scale_ctx->api_dst_frame, NI_DEVICE_TYPE_SCALER);
+    new_frame_surface =
+        (niFrameSurface1_t *)scale_ctx->api_dst_frame.data.frame.p_data[3];
+
+    *filt_frame_surface = new_frame_surface;
+
+    return 0;
+}
+
+static int ni_hwframe_overlay(AVFilterContext *ctx, NiBgContext *s,
+                              AVFrame *frame, AVFrame *overlay,
+                              AVFrame **output) {
+    AVHWFramesContext *main_frame_ctx;
+    AVFilterLink *outlink;
+    AVFrame *out;
+    niFrameSurface1_t *frame_surface, *new_frame_surface;
+    int flags, main_cardno;
+    int main_scaler_format, ovly_scaler_format;
+    ni_retcode_t retcode;
+    uint16_t tempFIDOverlay     = 0;
+    uint16_t tempFIDFrame       = 0;
+    OverlayContext *overlay_ctx = s->overlay_ctx;
+
+    outlink = ctx->outputs[0];
+
+    main_frame_ctx = (AVHWFramesContext *)frame->hw_frames_ctx->data;
+    main_scaler_format =
+        ff_ni_ffmpeg_to_gc620_pix_fmt(main_frame_ctx->sw_format);
+    main_cardno = ni_get_cardno(frame);
+
+    if (overlay) {
+        int ovly_cardno;
+        AVHWFramesContext *ovly_frame_ctx;
+        ovly_frame_ctx = (AVHWFramesContext *)overlay->hw_frames_ctx->data;
+        ovly_scaler_format =
+            ff_ni_ffmpeg_to_gc620_pix_fmt(ovly_frame_ctx->sw_format);
+        ovly_cardno = ni_get_cardno(overlay);
+
+        if (main_cardno != ovly_cardno) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "Main/Overlay frames on different cards\n");
+            return AVERROR(EINVAL);
+        }
+    } else
+        ovly_scaler_format = 0;
+
+    /* Allocate a ni_frame for the overlay output */
+    retcode = ni_frame_buffer_alloc_hwenc(
+        &overlay_ctx->api_dst_frame.data.frame, outlink->w, outlink->h, 0);
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate overlay output\n");
+        return AVERROR(ENOMEM);
+    }
+
+    if (overlay) {
+        frame_surface  = (niFrameSurface1_t *)overlay->data[3];
+        tempFIDOverlay = frame_surface->ui16FrameIdx;
+        av_log(ctx, AV_LOG_INFO,
+               "ovly_scaler_format %d, frameidx %d, wxh %dx%d\n",
+               ovly_scaler_format, tempFIDOverlay, overlay->width,
+               overlay->height);
+    }
+
+    /*
+     * Allocate device input frame for overlay picture. This call won't actually
+     * allocate a frame, but sends the incoming hardware frame index to the
+     * scaler manager.
+     */
+    retcode = ni_device_alloc_frame(
+        &overlay_ctx->api_ctx,                                   //
+        overlay ? FFALIGN(overlay->width, 2) : 0,                //
+        overlay ? FFALIGN(overlay->height, 2) : 0,               //
+        ovly_scaler_format,                                      //
+        0,                                                       //
+        overlay ? FFALIGN(overlay->width, 2) : 0,                //
+        overlay ? FFALIGN(overlay->height, 2) : 0,               //
+        0,                                                       //
+        0,                                                       //
+        frame_surface ? (int)frame_surface->ui32nodeAddress : 0, //
+        frame_surface ? frame_surface->ui16FrameIdx : 0,         //
+        NI_DEVICE_TYPE_SCALER);
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_DEBUG, "Can't assign frame for overlay input %d\n",
+               retcode);
+        return AVERROR(ENOMEM);
+    }
+
+    frame_surface = (niFrameSurface1_t *)frame->data[3];
+    if (frame_surface == NULL)
+        return AVERROR(EINVAL);
+
+    tempFIDFrame = frame_surface->ui16FrameIdx;
+
+    av_log(ctx, AV_LOG_INFO, "main frame: format %d, frameidx %d, wxh %dx%d\n",
+           main_scaler_format, tempFIDFrame, frame->width, frame->height);
+    /*
+     * Allocate device output frame from the pool. We also send down the frame
+     * index of the background frame to the scaler manager.
+     */
+    flags   = NI_SCALER_FLAG_IO;
+    retcode = ni_device_alloc_frame(&overlay_ctx->api_ctx,          //
+                                    FFALIGN(frame->width, 2),       //
+                                    FFALIGN(frame->height, 2),      //
+                                    main_scaler_format,             //
+                                    flags,                          //
+                                    FFALIGN(frame->width, 2),       //
+                                    FFALIGN(frame->height, 2),      //
+                                    0,                              // x
+                                    0,                              // y
+                                    frame_surface->ui32nodeAddress, //
+                                    frame_surface->ui16FrameIdx,    //
+                                    NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_DEBUG, "Can't allocate frame for output %d\n",
+               retcode);
+        return AVERROR(ENOMEM);
+    }
+
+    out = av_frame_alloc();
+    if (!out)
+        return AVERROR(ENOMEM);
+
+    av_frame_copy_props(out, frame);
+
+    out->width  = outlink->w;
+    out->height = outlink->h;
+    out->format = AV_PIX_FMT_NI_QUAD;
+
+    /* Quadra 2D engine always outputs limited color range */
+    out->color_range = AVCOL_RANGE_MPEG;
+
+    av_log(ctx, AV_LOG_INFO, "outlink wxh %dx%d\n", outlink->w, outlink->h);
+
+    /* Reference the new hw frames context */
+    out->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+    out->data[3]       = av_malloc(sizeof(niFrameSurface1_t));
+
+    if (!out->data[3]) {
+        av_frame_free(&out);
+        return AVERROR(ENOMEM);
+    }
+
+    /* Copy the frame surface from the incoming frame */
+    memcpy(out->data[3], frame->data[3], sizeof(niFrameSurface1_t));
+
+    /* Set the new frame index */
+    ni_device_session_read_hwdesc(&overlay_ctx->api_ctx,
+                                  &overlay_ctx->api_dst_frame,
+                                  NI_DEVICE_TYPE_SCALER);
+
+    frame_surface = (niFrameSurface1_t *)out->data[3];
+    new_frame_surface =
+        (niFrameSurface1_t *)overlay_ctx->api_dst_frame.data.frame.p_data[3];
+    frame_surface->ui16FrameIdx   = new_frame_surface->ui16FrameIdx;
+    frame_surface->ui16session_ID = new_frame_surface->ui16session_ID;
+    frame_surface->device_handle  = new_frame_surface->device_handle;
+    frame_surface->output_idx     = new_frame_surface->output_idx;
+    frame_surface->src_cpu        = new_frame_surface->src_cpu;
+
+    av_log(ctx, AV_LOG_INFO,
+           "%s:IN trace ui16FrameIdx = [%d] and [%d] --> out [%d] \n", __func__,
+           tempFIDFrame, tempFIDOverlay, frame_surface->ui16FrameIdx);
+
+    out->buf[0] = av_buffer_create(out->data[3], sizeof(niFrameSurface1_t),
+                                   ff_ni_frame_free, NULL, 0);
+
+    *output = out;
+    return 0;
+}
+
+static int ni_bg_process(AVFilterContext *ctx, ni_session_data_io_t *p_dst_pkt,
+                         AVFrame *in) {
+    NiBgContext *s = ctx->priv;
+    ni_retcode_t retval;
+    ni_roi_network_t *network = &s->network;
+    int ret;
+    int i;
+
+    for (i = 0; i < network->raw.output_num; i++) {
+        retval = ni_network_layer_convert_output(
+            network->layers[i].output,
+            network->layers[i].output_number * sizeof(float),
+            &p_dst_pkt->data.packet, &network->raw, i);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "failed to read layer %d output. retval %d\n", i, retval);
+            return AVERROR(EIO);
+        }
+    }
+
+    ret = ni_get_mask(ctx, s->mask_data, network);
+
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to get mask data.\n");
+        return ret;
+    }
+
+    av_log(ctx, AV_LOG_DEBUG, "s->mask_data %d\n", s->mask_data[2000]);
+
+    ret = get_alpha_mask_frame(ctx, in, s->mask_data);
+
+    if (ret == 0) {
+        av_log(ctx, AV_LOG_DEBUG,
+               "the s->alpha_enlarge_frame->width: %d "
+               "s->alpha_enlarge_frame->height: %d "
+               "s->alpha_enlarge_frame->format: %d "
+               "s->alpha_enlarge_frame->linesize[0]: %d \n",
+               s->alpha_enlarge_frame->width, s->alpha_enlarge_frame->height,
+               s->alpha_enlarge_frame->format,
+               s->alpha_enlarge_frame->linesize[0]);
+    } else {
+        av_log(ctx, AV_LOG_ERROR, "failed to s->alpha_enlarge_frame\n");
+        return ret;
+    }
+
+    return 0;
+}
+
+static int nibg_filter_frame(AVFilterLink *link, AVFrame *in) {
+    AVFilterContext *ctx = link->dst;
+    //    AVFilterLink  *outlink = ctx->outputs[0];
+    NiBgContext *s = ctx->priv;
+
+    int ret;
+
+    /* ai roi */
+    ni_roi_network_t *network;
+    ni_retcode_t retval;
+    AiContext *ai_ctx;
+
+    /* overlay */
+    AVFrame *realout;
+
+    //    if (in->format == outlink->format)
+    //        return ff_filter_frame(outlink, in);
+
+    av_log(ctx, AV_LOG_INFO, "entering %s\n", __func__);
+
+    if (!s->initialized) {
+        ret = ni_bg_config_input(ctx, in);
+        if (ret) {
+            av_log(ctx, AV_LOG_ERROR, "failed to config input\n");
+            return ret;
+        }
+    }
+
+    ai_ctx  = s->ai_ctx;
+    network = &s->network;
+    retval  = ni_ai_packet_buffer_alloc(&ai_ctx->api_dst_pkt.data.packet,
+                                       &network->raw);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate packet\n");
+        return AVERROR(EAGAIN);
+    }
+
+    if (in->format == AV_PIX_FMT_NI_QUAD) {
+        niFrameSurface1_t *filt_frame_surface;
+
+        ret = ni_hwframe_scale(ctx, s, in, network->netw, network->neth,
+                               &filt_frame_surface);
+        if (ret < 0) {
+            av_log(ctx, AV_LOG_ERROR, "Error run hwframe scale\n");
+            return ret;
+        }
+
+        av_log(ctx, AV_LOG_DEBUG, "filt frame surface frameIdx %d\n",
+               filt_frame_surface->ui16FrameIdx);
+
+        /* allocate output buffer */
+        retval = ni_device_alloc_frame(&ai_ctx->api_ctx, 0, 0, 0, 0, 0, 0, 0, 0,
+                                       filt_frame_surface->ui32nodeAddress,
+                                       filt_frame_surface->ui16FrameIdx,
+                                       NI_DEVICE_TYPE_AI);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR, "failed to alloc hw input frame\n");
+            return AVERROR(ENOMEM);
+        }
+
+        do {
+            retval = ni_device_session_read(
+                &ai_ctx->api_ctx, &ai_ctx->api_dst_pkt, NI_DEVICE_TYPE_AI);
+            if (retval < 0) {
+                av_log(ctx, AV_LOG_ERROR, "read hwdesc retval %d\n", retval);
+                return AVERROR(EIO);
+            } else if (retval > 0) {
+                ret = ni_bg_process(ctx, &ai_ctx->api_dst_pkt, in);
+                if (ret != 0) {
+                    av_log(ctx, AV_LOG_ERROR,
+                           "failed to read roi from packet\n");
+                    return ret;
+                }
+            }
+        } while (retval == 0);
+
+        ni_hwframe_buffer_recycle(filt_frame_surface,
+                                  filt_frame_surface->device_handle);
+    }
+
+    /* {
+        char alpha_pic[512];
+        static int frame_number = 0;
+        int n;
+
+        n=snprintf(alpha_pic, sizeof(alpha_pic),
+    "./bg_test/results/alpha_enlarge_frame_raw_1280x720_%d.rgba",
+    frame_number++); alpha_pic[n] = '\0'; save_raw_rgba_data(ctx,
+    s->alpha_enlarge_frame, alpha_pic);
+
+    } */
+
+    s->alpha_mask_hwframe = av_frame_alloc();
+    if (!s->alpha_mask_hwframe)
+        return AVERROR(ENOMEM);
+
+    av_log(ctx, AV_LOG_INFO, "get hw_frames_ctx\n");
+    ret = av_hwframe_get_buffer(s->hw_frames_ctx, s->alpha_mask_hwframe, 0);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to get buffer\n");
+        av_frame_free(&s->alpha_mask_hwframe);
+        return ret;
+    }
+
+    s->alpha_mask_hwframe->width  = in->width;
+    s->alpha_mask_hwframe->height = in->height;
+
+    ret = av_hwframe_transfer_data(s->alpha_mask_hwframe,
+                                   s->alpha_enlarge_frame, 0);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "Error transferring data to the Quadra\n");
+        goto fail;
+    }
+
+    {
+        niFrameSurface1_t *frame_surface =
+            (niFrameSurface1_t *)s->alpha_mask_hwframe->data[3];
+        av_log(ctx, AV_LOG_DEBUG, "s->alpha_mask_hwframe frameindex %d\n",
+               frame_surface->ui16FrameIdx);
+    }
+
+    ret = av_frame_copy_props(s->alpha_mask_hwframe, s->alpha_enlarge_frame);
+    if (ret < 0)
+        goto fail;
+
+    ret = ni_hwframe_overlay(ctx, s, in, s->alpha_mask_hwframe, &realout);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to do overlay\n");
+        goto fail;
+    }
+
+    av_frame_free(&in);
+    av_frame_free(&s->alpha_mask_hwframe);
+
+    return ff_filter_frame(ctx->outputs[0], realout);
+fail:
+    av_frame_free(&in);
+    av_frame_free(&s->alpha_mask_hwframe);
+    return ret;
+}
+
+#define OFFSET(x) offsetof(NiBgContext, x)
+#define FLAGS (AV_OPT_FLAG_FILTERING_PARAM | AV_OPT_FLAG_VIDEO_PARAM)
+static const AVOption nibg_options[] = {
+    {"nb", "path to network binary file", OFFSET(nb_file), AV_OPT_TYPE_STRING,
+     .flags = FLAGS},
+    {"bg_img", "path to network binary file", OFFSET(bg_img),
+     AV_OPT_TYPE_STRING, .flags = FLAGS},
+    {"use_default_bg",
+     "define use_default_bg",
+     OFFSET(use_default_bg),
+     AV_OPT_TYPE_INT,
+     {.i64 = 0},
+     0,
+     INT_MAX,
+     .flags = FLAGS},
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSET(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     FLAGS,
+     "keep_alive_timeout"},
+     
+    {NULL},
+};
+
+AVFILTER_DEFINE_CLASS(nibg);
+
+static const AVFilterPad nibg_inputs[] = {
+    {
+        .name = "default",
+        .type = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = nibg_filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    {NULL}
+#endif
+};
+
+static const AVFilterPad nibg_outputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = nibg_config_output,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    {NULL}
+#endif
+};
+
+AVFilter ff_vf_bg_ni_quadra = {
+    .name        = "ni_quadra_bg",
+    .description = NULL_IF_CONFIG_SMALL(
+        "NetInt Quadra upload a system memory frame to a device v" NI_XCODER_REVISION),
+
+    .init   = nibg_init,
+    .uninit = nibg_uninit,
+
+    .priv_size  = sizeof(NiBgContext),
+    .priv_class = &nibg_class,
+
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(nibg_inputs),
+    FILTER_OUTPUTS(nibg_outputs),
+    FILTER_QUERY_FUNC(nibg_query_formats),
+#else
+    .inputs  = nibg_inputs,
+    .outputs = nibg_outputs,
+    .query_formats = nibg_query_formats,
+#endif
+};
diff --git a/libavfilter/vf_crop.c b/libavfilter/vf_crop.c
index c7cbfa51ef..00e0137559 100644
--- a/libavfilter/vf_crop.c
+++ b/libavfilter/vf_crop.c
@@ -93,7 +93,8 @@ typedef struct CropContext {
 
 static int query_formats(AVFilterContext *ctx)
 {
-    int reject_flags = AV_PIX_FMT_FLAG_BITSTREAM | FF_PIX_FMT_FLAG_SW_FLAT_SUB;
+    // NETINT: reject hardware frames as input to software crop filter
+    int reject_flags = AV_PIX_FMT_FLAG_BITSTREAM | FF_PIX_FMT_FLAG_SW_FLAT_SUB | AV_PIX_FMT_FLAG_HWACCEL;
 
     return ff_set_common_formats(ctx, ff_formats_pixdesc_filter(0, reject_flags));
 }
diff --git a/libavfilter/vf_crop_ni.c b/libavfilter/vf_crop_ni.c
new file mode 100644
index 0000000000..f65d1446fd
--- /dev/null
+++ b/libavfilter/vf_crop_ni.c
@@ -0,0 +1,621 @@
+/*
+ * Copyright (c) 2007 Bobby Bingham
+ * Copyright (c) 2020 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * video crop filter
+ */
+
+#include <stdio.h>
+
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+#include "libavutil/eval.h"
+#include "libavutil/avstring.h"
+#include "libavutil/internal.h"
+#include "libavutil/libm.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/mathematics.h"
+#include "libavutil/opt.h"
+#include "nifilter.h"
+#include <ni_device_api.h>
+
+static const char *const var_names[] = {
+    "in_w", "iw",   ///< width  of the input video
+    "in_h", "ih",   ///< height of the input video
+    "out_w", "ow",  ///< width  of the cropped video
+    "out_h", "oh",  ///< height of the cropped video
+    "a",
+    "sar",
+    "dar",
+    "hsub",
+    "vsub",
+    "x",
+    "y",
+    "n",            ///< number of frame
+    "pos",          ///< position in the file
+    "t",            ///< timestamp expressed in seconds
+    NULL
+};
+
+enum var_name {
+    VAR_IN_W,  VAR_IW,
+    VAR_IN_H,  VAR_IH,
+    VAR_OUT_W, VAR_OW,
+    VAR_OUT_H, VAR_OH,
+    VAR_A,
+    VAR_SAR,
+    VAR_DAR,
+    VAR_HSUB,
+    VAR_VSUB,
+    VAR_X,
+    VAR_Y,
+    VAR_N,
+    VAR_POS,
+    VAR_T,
+    VAR_VARS_NB
+};
+
+typedef struct NetIntCropContext {
+    const AVClass *class;
+    int  x;             ///< x offset of the non-cropped area with respect to the input area
+    int  y;             ///< y offset of the non-cropped area with respect to the input area
+    int  w;             ///< width of the cropped area
+    int  h;             ///< height of the cropped area
+
+    AVRational out_sar; ///< output sample aspect ratio
+    int keep_aspect;    ///< keep display aspect ratio when cropping
+
+    int max_step[4];    ///< max pixel step for each plane, expressed as a number of bytes
+    int hsub, vsub;     ///< chroma subsampling
+    char *x_expr, *y_expr, *w_expr, *h_expr;
+    AVExpr *x_pexpr, *y_pexpr;  /* parsed expressions for x and y */
+    double var_values[VAR_VARS_NB];
+
+    AVBufferRef *out_frames_ref;
+
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+
+    int initialized;
+    int session_opened;
+    int keep_alive_timeout; /* keep alive timeout setting */
+
+    ni_frame_config_t frame_out;
+} NetIntCropContext;
+
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] =
+        {AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE};
+    AVFilterFormats *formats;
+
+    formats = ff_make_format_list(pix_fmts);
+
+    if (!formats)
+        return AVERROR(ENOMEM);
+
+    return ff_set_common_formats(ctx, formats);
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    NetIntCropContext *s = ctx->priv;
+
+    av_expr_free(s->x_pexpr);
+    s->x_pexpr = NULL;
+    av_expr_free(s->y_pexpr);
+    s->y_pexpr = NULL;
+
+    if (s->api_dst_frame.data.frame.p_buffer)
+        ni_frame_buffer_free(&s->api_dst_frame.data.frame);
+
+    if (s->session_opened) {
+        /* Close operation will free the device frames */
+        ni_device_session_close(&s->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ni_device_session_context_clear(&s->api_ctx);
+    }
+
+    av_buffer_unref(&s->out_frames_ref);
+}
+
+static inline int normalize_double(int *n, double d)
+{
+    int ret = 0;
+
+    if (isnan(d)) {
+        ret = AVERROR(EINVAL);
+    } else if (d > INT_MAX || d < INT_MIN) {
+        *n = d > INT_MAX ? INT_MAX : INT_MIN;
+        ret = AVERROR(EINVAL);
+    } else
+        *n = (int)lrint(d);
+
+    return ret;
+}
+
+static int config_input(AVFilterLink *link)
+{
+    AVFilterContext *ctx = link->dst;
+    AVHWFramesContext *hwctx = (AVHWFramesContext *)link->hw_frames_ctx->data;
+    NetIntCropContext *s = ctx->priv;
+    const AVPixFmtDescriptor *pix_desc = av_pix_fmt_desc_get(hwctx->sw_format);
+    int ret;
+    const char *expr;
+    double res;
+
+    s->var_values[VAR_IN_W]  = s->var_values[VAR_IW] = ctx->inputs[0]->w;
+    s->var_values[VAR_IN_H]  = s->var_values[VAR_IH] = ctx->inputs[0]->h;
+    s->var_values[VAR_A] = (double)link->w / (double)link->h;
+    s->var_values[VAR_SAR]   = link->sample_aspect_ratio.num ? av_q2d(link->sample_aspect_ratio) : 1;
+    s->var_values[VAR_DAR]   = s->var_values[VAR_A] * s->var_values[VAR_SAR];
+    s->var_values[VAR_HSUB]  = 1<<pix_desc->log2_chroma_w;
+    s->var_values[VAR_VSUB]  = 1<<pix_desc->log2_chroma_h;
+    s->var_values[VAR_X]     = NAN;
+    s->var_values[VAR_Y]     = NAN;
+    s->var_values[VAR_OUT_W] = s->var_values[VAR_OW] = NAN;
+    s->var_values[VAR_OUT_H] = s->var_values[VAR_OH] = NAN;
+    s->var_values[VAR_N]     = 0;
+    s->var_values[VAR_T]     = NAN;
+    s->var_values[VAR_POS]   = NAN;
+
+    av_image_fill_max_pixsteps(s->max_step, NULL, pix_desc);
+    s->hsub = pix_desc->log2_chroma_w;
+    s->vsub = pix_desc->log2_chroma_h;
+
+    if ((ret = av_expr_parse_and_eval(&res, (expr = s->w_expr),
+                                      var_names, s->var_values,
+                                      NULL, NULL, NULL, NULL, NULL, 0, ctx)) < 0)
+        goto fail_expr;
+    s->var_values[VAR_OUT_W] = s->var_values[VAR_OW] = res;
+    if ((ret = av_expr_parse_and_eval(&res, (expr = s->h_expr),
+                                      var_names, s->var_values,
+                                      NULL, NULL, NULL, NULL, NULL, 0, ctx)) < 0)
+        goto fail_expr;
+    s->var_values[VAR_OUT_H] = s->var_values[VAR_OH] = res;
+    /* evaluate again ow as it may depend on oh */
+    if ((ret = av_expr_parse_and_eval(&res, (expr = s->w_expr),
+                                      var_names, s->var_values,
+                                      NULL, NULL, NULL, NULL, NULL, 0, ctx)) < 0)
+        goto fail_expr;
+
+    s->var_values[VAR_OUT_W] = s->var_values[VAR_OW] = res;
+    if (normalize_double(&s->w, s->var_values[VAR_OUT_W]) < 0 ||
+        normalize_double(&s->h, s->var_values[VAR_OUT_H]) < 0) {
+        av_log(ctx, AV_LOG_ERROR,
+               "Too big value or invalid expression for out_w/ow or out_h/oh. "
+               "Maybe the expression for out_w:'%s' or for out_h:'%s' is self-referencing.\n",
+               s->w_expr, s->h_expr);
+        return AVERROR(EINVAL);
+    }
+
+    s->w &= ~((1 << s->hsub) - 1);
+    s->h &= ~((1 << s->vsub) - 1);
+
+    av_expr_free(s->x_pexpr);
+    av_expr_free(s->y_pexpr);
+    s->x_pexpr = s->y_pexpr = NULL;
+    if ((av_expr_parse(&s->x_pexpr, s->x_expr, var_names, NULL, NULL, NULL,
+                       NULL, 0, ctx) < 0) ||
+        (av_expr_parse(&s->y_pexpr, s->y_expr, var_names, NULL, NULL, NULL,
+                       NULL, 0, ctx) < 0))
+        return AVERROR(EINVAL);
+
+    if (s->keep_aspect) {
+        AVRational dar = av_mul_q(link->sample_aspect_ratio,
+                                  (AVRational){ link->w, link->h });
+        av_reduce(&s->out_sar.num, &s->out_sar.den,
+                  dar.num * s->h, dar.den * s->w, INT_MAX);
+    } else
+        s->out_sar = link->sample_aspect_ratio;
+
+    av_log(ctx, AV_LOG_VERBOSE, "w:%d h:%d sar:%d/%d -> w:%d h:%d sar:%d/%d\n",
+           link->w, link->h, link->sample_aspect_ratio.num, link->sample_aspect_ratio.den,
+           s->w, s->h, s->out_sar.num, s->out_sar.den);
+
+    if (s->w <= 0 || s->h <= 0 ||
+        s->w > link->w || s->h > link->h) {
+        av_log(ctx, AV_LOG_ERROR,
+               "Invalid too big or non positive size for width '%d' or height '%d'\n",
+               s->w, s->h);
+        return AVERROR(EINVAL);
+    }
+
+    /* set default, required in the case the first computed value for x/y is NAN */
+    s->x = (link->w - s->w) / 2;
+    s->y = (link->h - s->h) / 2;
+
+    s->x &= ~((1 << s->hsub) - 1);
+    s->y &= ~((1 << s->vsub) - 1);
+
+    return 0;
+
+fail_expr:
+    av_log(NULL, AV_LOG_ERROR, "Error when evaluating the expression '%s'\n", expr);
+    return ret;
+}
+
+static int init_out_pool(AVFilterContext *ctx) {
+    NetIntCropContext *s = ctx->priv;
+    AVHWFramesContext *out_frames_ctx;
+
+    if (!ctx->inputs[0]->hw_frames_ctx) {
+        return AVERROR(EINVAL);
+    }
+
+    out_frames_ctx   = (AVHWFramesContext*)s->out_frames_ref->data;
+
+    /* Don't check return code, this will intentionally fail */
+    av_hwframe_ctx_init(s->out_frames_ref);
+
+    /* Create frame pool on device */
+    return ff_ni_build_frame_pool(&s->api_ctx,
+                                  out_frames_ctx->width, out_frames_ctx->height,
+                                  out_frames_ctx->sw_format,
+                                  DEFAULT_NI_FILTER_POOL_SIZE);
+}
+
+static int config_output(AVFilterLink *link)
+{
+    NetIntCropContext *s = link->src->priv;
+    AVHWFramesContext *in_frames_ctx;
+    AVHWFramesContext *out_frames_ctx;
+    AVFilterContext *ctx;
+
+    link->w = s->w;
+    link->h = s->h;
+    link->sample_aspect_ratio = s->out_sar;
+
+    ctx           = (AVFilterContext *)link->src;
+    in_frames_ctx = (AVHWFramesContext *)ctx->inputs[0]->hw_frames_ctx->data;
+
+    if (in_frames_ctx->sw_format == AV_PIX_FMT_BGRP) {
+        av_log(ctx, AV_LOG_ERROR, "bgrp not supported\n");
+        return AVERROR(EINVAL);
+    }
+
+    s->out_frames_ref = av_hwframe_ctx_alloc(in_frames_ctx->device_ref);
+    if (!s->out_frames_ref)
+        return AVERROR(ENOMEM);
+
+    out_frames_ctx = (AVHWFramesContext *)s->out_frames_ref->data;
+
+    out_frames_ctx->format    = AV_PIX_FMT_NI_QUAD;
+    out_frames_ctx->width     = s->w;
+    out_frames_ctx->height    = s->h;
+    out_frames_ctx->sw_format = in_frames_ctx->sw_format;
+    out_frames_ctx->initial_pool_size =
+        NI_CROP_ID; // Repurposed as identity code
+
+    av_buffer_unref(&ctx->outputs[0]->hw_frames_ctx);
+    ctx->outputs[0]->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    if (!ctx->outputs[0]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return 0;
+}
+
+static int filter_frame(AVFilterLink *link, AVFrame *frame)
+{
+    AVFilterContext *ctx = link->dst;
+    NetIntCropContext *s = ctx->priv;
+    AVFilterLink *outlink = link->dst->outputs[0];
+    AVFrame *out = NULL;
+    niFrameSurface1_t* frame_surface,*new_frame_surface;
+    AVHWFramesContext *pAVHFWCtx;
+    AVNIDeviceContext *pAVNIDevCtx;
+    ni_retcode_t retcode;
+    uint32_t scaler_format;
+    int cardno;
+    uint16_t tempFID;
+
+    pAVHFWCtx         = (AVHWFramesContext *)frame->hw_frames_ctx->data;
+    pAVNIDevCtx       = (AVNIDeviceContext *)pAVHFWCtx->device_ctx->hwctx;
+    cardno            = ni_get_cardno(frame);
+
+    if (!s->initialized) {
+        retcode = ni_device_session_context_init(&s->api_ctx);
+        if (retcode < 0) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "ni crop filter session context init failure\n");
+            goto fail;
+        }
+
+        s->api_ctx.device_handle = pAVNIDevCtx->cards[cardno];
+        s->api_ctx.blk_io_handle = pAVNIDevCtx->cards[cardno];
+
+        s->api_ctx.hw_id             = cardno;
+        s->api_ctx.device_type       = NI_DEVICE_TYPE_SCALER;
+        s->api_ctx.scaler_operation  = NI_SCALER_OPCODE_CROP;
+        s->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+
+        retcode = ni_device_session_open(&s->api_ctx, NI_DEVICE_TYPE_SCALER);
+        if (retcode < 0) {
+            av_log(ctx, AV_LOG_ERROR, "Can't open device session on card %d\n",
+                   cardno);
+            goto fail;
+        }
+
+        s->session_opened = 1;
+
+        retcode = init_out_pool(ctx);
+
+        if (retcode < 0)
+        {
+            av_log(ctx, AV_LOG_ERROR,
+                   "Internal output allocation failed rc = %d\n", retcode);
+            goto fail;
+        }
+
+        ff_ni_clone_hwframe_ctx(pAVHFWCtx,
+                                (AVHWFramesContext *)s->out_frames_ref->data);
+
+        if (frame->color_range == AVCOL_RANGE_JPEG) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "WARNING: Full color range input, limited color output\n");
+        }
+
+        s->initialized = 1;
+    }
+
+    s->var_values[VAR_N] = link->frame_count_out;
+    s->var_values[VAR_T] = frame->pts == AV_NOPTS_VALUE ?
+        NAN : frame->pts * av_q2d(link->time_base);
+    s->var_values[VAR_POS] = frame->pkt_pos == -1 ?
+        NAN : frame->pkt_pos;
+    s->var_values[VAR_X] = av_expr_eval(s->x_pexpr, s->var_values, NULL);
+    s->var_values[VAR_Y] = av_expr_eval(s->y_pexpr, s->var_values, NULL);
+    s->var_values[VAR_X] = av_expr_eval(s->x_pexpr, s->var_values, NULL);
+
+    normalize_double(&s->x, s->var_values[VAR_X]);
+    normalize_double(&s->y, s->var_values[VAR_Y]);
+
+    if (s->x < 0)
+        s->x = 0;
+    if (s->y < 0)
+        s->y = 0;
+    if ((unsigned)s->x + (unsigned)s->w > link->w)
+        s->x = link->w - s->w;
+    if ((unsigned)s->y + (unsigned)s->h > link->h)
+        s->y = link->h - s->h;
+
+    s->x &= ~((1 << s->hsub) - 1);
+    s->y &= ~((1 << s->vsub) - 1);
+
+    av_log(ctx, AV_LOG_TRACE, "n:%d t:%f pos:%f x:%d y:%d x+w:%d y+h:%d\n",
+            (int)s->var_values[VAR_N], s->var_values[VAR_T], s->var_values[VAR_POS],
+            s->x, s->y, s->x+s->w, s->y+s->h);
+
+    frame_surface = (niFrameSurface1_t *) frame->data[3];
+    if (frame_surface == NULL) {
+        return AVERROR(EINVAL);
+    }
+
+    scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(pAVHFWCtx->sw_format);
+
+    retcode = ni_frame_buffer_alloc_hwenc(&s->api_dst_frame.data.frame,
+                                          outlink->w,
+                                          outlink->h,
+                                          0);
+
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    /*
+     * Allocate device input frame. This call won't actually allocate a frame,
+     * but sends the incoming hardware frame index to the scaler manager
+     */
+    retcode = ni_device_alloc_frame(&s->api_ctx,               //
+                                    FFALIGN(frame->width, 2),  //
+                                    FFALIGN(frame->height, 2), //
+                                    scaler_format,             //
+                                    0,                         // input frame
+                                    s->w, // src rectangle width
+                                    s->h, // src rectangle height
+                                    s->x, // src rectangle x
+                                    s->y, // src rectangle y
+                                    frame_surface->ui32nodeAddress,
+                                    frame_surface->ui16FrameIdx,
+                                    NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        av_log(ctx, AV_LOG_DEBUG, "Can't allocate device input frame %d\n",
+               retcode);
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    if ((outlink->w != s->frame_out.picture_width) ||
+        (outlink->h != s->frame_out.picture_height) ||
+        (scaler_format != s->frame_out.picture_format)) {
+        s->frame_out.picture_width  = outlink->w;
+        s->frame_out.picture_height = outlink->h;
+        s->frame_out.picture_format = scaler_format;
+
+        /* Allocate device destination frame. This acquires a frame from the
+         * pool
+         */
+        retcode = ni_device_alloc_frame(&s->api_ctx,            //
+                                        FFALIGN(outlink->w, 2), //
+                                        FFALIGN(outlink->h, 2), //
+                                        scaler_format,          //
+                                        NI_SCALER_FLAG_IO,      //
+                                        0,                      //
+                                        0,                      //
+                                        0,                      //
+                                        0,                      //
+                                        0,                      //
+                                        -1,                     //
+                                        NI_DEVICE_TYPE_SCALER);
+
+        if (retcode != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_DEBUG, "Can't allocate device output frame %d\n",
+                   retcode);
+            retcode = AVERROR(ENOMEM);
+            goto fail;
+        }
+    }
+
+    out = av_frame_alloc();
+    if (!out)
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    av_frame_copy_props(out,frame);
+
+    out->width  = s->w;
+    out->height = s->h;
+
+    out->format = AV_PIX_FMT_NI_QUAD;
+
+    /* Quadra 2D engine always outputs limited color range */
+    out->color_range = AVCOL_RANGE_MPEG;
+
+    /* Reference the new hw frames context */
+    out->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    out->data[3] = av_malloc(sizeof(niFrameSurface1_t));
+
+    if (!out->data[3])
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    /* Copy the frame surface from the incoming frame */
+    memcpy(out->data[3], frame->data[3], sizeof(niFrameSurface1_t));
+
+    /* Set the new frame index */
+    retcode = ni_device_session_read_hwdesc(&s->api_ctx, &s->api_dst_frame,
+                                            NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR,
+               "Can't acquire output frame %d\n",retcode);
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    tempFID           = frame_surface->ui16FrameIdx;
+    frame_surface = (niFrameSurface1_t *) out->data[3];
+    new_frame_surface = (niFrameSurface1_t *) s->api_dst_frame.data.frame.p_data[3];
+    frame_surface->ui16FrameIdx = new_frame_surface->ui16FrameIdx;
+    frame_surface->ui16session_ID = new_frame_surface->ui16session_ID;
+    frame_surface->device_handle  = pAVNIDevCtx->cards[cardno];
+    frame_surface->output_idx     = new_frame_surface->output_idx;
+    frame_surface->src_cpu = new_frame_surface->src_cpu;
+
+    /*Remove ni-split specific assets*/
+    frame_surface->ui32nodeAddress = 0;
+
+    frame_surface->ui16width = out->width;
+    frame_surface->ui16height = out->height;
+
+    av_log(ctx, AV_LOG_DEBUG,
+           "vf_crop_ni.c:IN trace ui16FrameIdx = [%d] --> out = [%d] \n",
+           tempFID, frame_surface->ui16FrameIdx);
+
+    out->buf[0] = av_buffer_create(out->data[3], sizeof(niFrameSurface1_t), ff_ni_frame_free, NULL, 0);
+
+    av_frame_free(&frame);
+
+    return ff_filter_frame(link->dst->outputs[0], out);
+
+fail:
+    av_frame_free(&frame);
+    av_frame_free(&out);
+    return retcode;
+}
+
+#define OFFSET(x) offsetof(NetIntCropContext, x)
+#define FLAGS (AV_OPT_FLAG_FILTERING_PARAM | AV_OPT_FLAG_VIDEO_PARAM)
+
+static const AVOption crop_options[] = {
+    { "out_w",       "set the width crop area expression",   OFFSET(w_expr), AV_OPT_TYPE_STRING, {.str = "iw"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "w",           "set the width crop area expression",   OFFSET(w_expr), AV_OPT_TYPE_STRING, {.str = "iw"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "out_h",       "set the height crop area expression",  OFFSET(h_expr), AV_OPT_TYPE_STRING, {.str = "ih"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "h",           "set the height crop area expression",  OFFSET(h_expr), AV_OPT_TYPE_STRING, {.str = "ih"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "x",           "set the x crop area expression",       OFFSET(x_expr), AV_OPT_TYPE_STRING, {.str = "(in_w-out_w)/2"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "y",           "set the y crop area expression",       OFFSET(y_expr), AV_OPT_TYPE_STRING, {.str = "(in_h-out_h)/2"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "keep_aspect", "keep aspect ratio",                    OFFSET(keep_aspect), AV_OPT_TYPE_BOOL, {.i64=0}, 0, 1, FLAGS }, 
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSET(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     FLAGS,
+     "keep_alive_timeout"},
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(crop);
+
+static const AVFilterPad avfilter_vf_crop_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = filter_frame,
+        .config_props = config_input,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad avfilter_vf_crop_outputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_output,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_crop_ni_quadra = {
+    .name            = "ni_quadra_crop",
+    .description     = NULL_IF_CONFIG_SMALL("NetInt Quadra crop the input video v" NI_XCODER_REVISION),
+    .priv_size       = sizeof(NetIntCropContext),
+    .priv_class      = &crop_class,
+    .uninit          = uninit,
+    .flags_internal  = FF_FILTER_FLAG_HWFRAME_AWARE,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(avfilter_vf_crop_inputs),
+    FILTER_OUTPUTS(avfilter_vf_crop_outputs),
+    FILTER_QUERY_FUNC(query_formats),
+#else
+    .inputs          = avfilter_vf_crop_inputs,
+    .outputs         = avfilter_vf_crop_outputs,
+    .query_formats   = query_formats,
+#endif
+};
diff --git a/libavfilter/vf_hwupload_ni_logan.c b/libavfilter/vf_hwupload_ni_logan.c
new file mode 100644
index 0000000000..962a99e6b9
--- /dev/null
+++ b/libavfilter/vf_hwupload_ni_logan.c
@@ -0,0 +1,230 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/buffer.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_ni_logan.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/log.h"
+#include "libavutil/opt.h"
+
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+
+typedef struct NiUploadContext {
+    const AVClass *class;
+    int device_idx;
+
+    AVBufferRef *hwdevice;
+    AVBufferRef *hwframe;
+} NiUploadContext;
+
+static av_cold int niupload_init(AVFilterContext *ctx)
+{
+    NiUploadContext *s = ctx->priv;
+    char buf[64] = { 0 };
+
+    snprintf(buf, sizeof(buf), "%d", s->device_idx);
+
+    return av_hwdevice_ctx_create(&s->hwdevice, AV_HWDEVICE_TYPE_NI_LOGAN, buf, NULL, 0);
+}
+
+static av_cold void niupload_uninit(AVFilterContext *ctx)
+{
+    NiUploadContext *s = ctx->priv;
+
+    av_buffer_unref(&s->hwframe);
+    av_buffer_unref(&s->hwdevice);
+}
+
+static int niupload_query_formats(AVFilterContext *ctx)
+{
+    NiUploadContext *nictx = ctx->priv;
+    AVHWFramesConstraints *constraints = NULL;
+    const enum AVPixelFormat *input_pix_fmts, *output_pix_fmts;
+    AVFilterFormats *input_formats = NULL;
+    int err, i;
+
+    if (!nictx->hwdevice)
+        return AVERROR(ENOMEM);
+
+    constraints = av_hwdevice_get_hwframe_constraints(nictx->hwdevice, NULL);
+    if (!constraints) {
+        err = AVERROR(EINVAL);
+        goto fail;
+    }
+
+    input_pix_fmts  = constraints->valid_sw_formats;
+    output_pix_fmts = constraints->valid_hw_formats;
+
+    input_formats = ff_make_format_list(output_pix_fmts);
+    if (!input_formats) {
+        err = AVERROR(ENOMEM);
+        goto fail;
+    }
+    if (input_pix_fmts) {
+        for (i = 0; input_pix_fmts[i] != AV_PIX_FMT_NONE; i++) {
+            err = ff_add_format(&input_formats, input_pix_fmts[i]);
+            if (err < 0)
+                goto fail;
+        }
+    }
+
+// Needed for FFmpeg-n4.4+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 ||  (LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110))
+    if ((err = ff_formats_ref(input_formats, &ctx->inputs[0]->outcfg.formats)) < 0 ||
+        (err = ff_formats_ref(ff_make_format_list(output_pix_fmts),
+                              &ctx->outputs[0]->incfg.formats)) < 0)
+#else
+    if ((err = ff_formats_ref(input_formats, &ctx->inputs[0]->out_formats)) < 0 ||
+        (err = ff_formats_ref(ff_make_format_list(output_pix_fmts),
+                              &ctx->outputs[0]->in_formats)) < 0)
+#endif
+        goto fail;
+
+    av_hwframe_constraints_free(&constraints);
+    return 0;
+
+fail:
+    av_buffer_unref(&nictx->hwdevice);
+    av_hwframe_constraints_free(&constraints);
+    return err;
+}
+
+static int niupload_config_output(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    AVFilterLink *inlink = ctx->inputs[0];
+    NiUploadContext *s = ctx->priv;
+
+    AVHWFramesContext *hwframe_ctx;
+    int ret;
+
+    av_buffer_unref(&s->hwframe);
+    s->hwframe = av_hwframe_ctx_alloc(s->hwdevice);
+    if (!s->hwframe)
+        return AVERROR(ENOMEM);
+
+    hwframe_ctx            = (AVHWFramesContext*)s->hwframe->data;
+    hwframe_ctx->format    = AV_PIX_FMT_NI_LOGAN;
+    hwframe_ctx->sw_format = inlink->format;
+    hwframe_ctx->width     = inlink->w;
+    hwframe_ctx->height    = inlink->h;
+
+    ret = av_hwframe_ctx_init(s->hwframe);
+    if (ret < 0)
+        return ret;
+
+    outlink->hw_frames_ctx = av_buffer_ref(s->hwframe);
+    if (!outlink->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return 0;
+}
+
+static int niupload_filter_frame(AVFilterLink *link, AVFrame *in)
+{
+    AVFilterContext   *ctx = link->dst;
+    AVFilterLink  *outlink = ctx->outputs[0];
+
+    AVFrame *out = NULL;
+    int ret;
+
+    out = ff_get_video_buffer(outlink, outlink->w, outlink->h);
+    if (!out) {
+        ret = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    out->width  = in->width;
+    out->height = in->height;
+
+    ret = av_hwframe_transfer_data(out, in, 0);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "niupload_filter_frame(): Error transferring data to the NI devices\n");
+        goto fail;
+    }
+
+    ret = av_frame_copy_props(out, in);
+    if (ret < 0)
+        goto fail;
+
+    av_frame_free(&in);
+
+    return ff_filter_frame(ctx->outputs[0], out);
+fail:
+    av_frame_free(&in);
+    av_frame_free(&out);
+    return ret;
+}
+
+#define OFFSET(x) offsetof(NiUploadContext, x)
+#define FLAGS (AV_OPT_FLAG_FILTERING_PARAM | AV_OPT_FLAG_VIDEO_PARAM)
+static const AVOption niupload_options[] = {
+    { "device", "Number of the device to use", OFFSET(device_idx), AV_OPT_TYPE_INT, { .i64 = 0 }, 0, INT_MAX, FLAGS },
+    { NULL },
+};
+
+AVFILTER_DEFINE_CLASS(niupload);
+
+static const AVFilterPad niupload_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = niupload_filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad niupload_outputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = niupload_config_output,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_hwupload_ni_logan = {
+    .name        = "ni_logan_hwupload",
+    .description = NULL_IF_CONFIG_SMALL("NetInt Logan upload a system memory frame to a device v" NI_LOGAN_XCODER_REVISION),
+
+    .init      = niupload_init,
+    .uninit    = niupload_uninit,
+
+    .priv_size  = sizeof(NiUploadContext),
+    .priv_class = &niupload_class,
+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(niupload_inputs),
+    FILTER_OUTPUTS(niupload_outputs),
+    FILTER_QUERY_FUNC(niupload_query_formats),
+#else
+    .inputs    = niupload_inputs,
+    .outputs   = niupload_outputs,
+    .query_formats = niupload_query_formats,
+#endif
+
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+};
diff --git a/libavfilter/vf_hwupload_ni_quadra.c b/libavfilter/vf_hwupload_ni_quadra.c
new file mode 100644
index 0000000000..55e60459cd
--- /dev/null
+++ b/libavfilter/vf_hwupload_ni_quadra.c
@@ -0,0 +1,270 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/buffer.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_ni_quad.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/log.h"
+#include "libavutil/opt.h"
+
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+
+typedef struct NiUploadContext {
+    const AVClass *class;
+    int device_idx;
+
+    AVBufferRef *hwdevice;
+    AVBufferRef *hwframe;
+    int keep_alive_timeout; /* keep alive timeout setting */
+} NiUploadContext;
+
+static av_cold int niupload_init(AVFilterContext *ctx)
+{
+    NiUploadContext *s = ctx->priv;
+    char buf[64] = { 0 };
+
+    snprintf(buf, sizeof(buf), "%d", s->device_idx);
+
+    return av_hwdevice_ctx_create(&s->hwdevice, AV_HWDEVICE_TYPE_NI_QUADRA, buf, NULL, 0);
+}
+
+static av_cold void niupload_uninit(AVFilterContext *ctx)
+{
+    NiUploadContext *s = ctx->priv;
+
+    av_buffer_unref(&s->hwframe);
+    av_buffer_unref(&s->hwdevice);
+}
+
+static int niupload_query_formats(AVFilterContext *ctx)
+{
+    NiUploadContext *nictx = ctx->priv;
+    AVHWFramesConstraints *constraints = NULL;
+    const enum AVPixelFormat *input_pix_fmts, *output_pix_fmts;
+    AVFilterFormats *input_formats = NULL;
+    int err, i;
+
+    if (!nictx->hwdevice)
+        return AVERROR(ENOMEM);
+
+    constraints = av_hwdevice_get_hwframe_constraints(nictx->hwdevice, NULL);
+    if (!constraints) {
+        err = AVERROR(EINVAL);
+        goto fail;
+    }
+
+    input_pix_fmts  = constraints->valid_sw_formats;
+    output_pix_fmts = constraints->valid_hw_formats;
+
+    input_formats = ff_make_format_list(output_pix_fmts);
+    if (!input_formats) {
+        err = AVERROR(ENOMEM);
+        goto fail;
+    }
+    if (input_pix_fmts) {
+        for (i = 0; input_pix_fmts[i] != AV_PIX_FMT_NONE; i++) {
+            err = ff_add_format(&input_formats, input_pix_fmts[i]);
+            if (err < 0)
+                goto fail;
+        }
+    }
+
+// Needed for FFmpeg-n4.4+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+    if ((err = ff_formats_ref(input_formats, &ctx->inputs[0]->outcfg.formats)) < 0 ||
+        (err = ff_formats_ref(ff_make_format_list(output_pix_fmts),
+                              &ctx->outputs[0]->incfg.formats)) < 0)
+#else
+    if ((err = ff_formats_ref(input_formats, &ctx->inputs[0]->out_formats)) < 0 ||
+        (err = ff_formats_ref(ff_make_format_list(output_pix_fmts),
+                              &ctx->outputs[0]->in_formats)) < 0)
+#endif
+        goto fail;
+
+    av_hwframe_constraints_free(&constraints);
+    return 0;
+
+fail:
+    av_buffer_unref(&nictx->hwdevice);
+    av_hwframe_constraints_free(&constraints);
+    return err;
+}
+
+static int niupload_config_output(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    AVFilterLink *inlink = ctx->inputs[0];
+    NiUploadContext *s = ctx->priv;
+    AVNIFramesContext *pub_ctx;
+    AVHWFramesContext *hwframe_ctx;
+    int ret;
+
+    av_buffer_unref(&s->hwframe);
+
+    if (inlink->format == outlink->format) {
+        // The input is already a hardware format, so we just want to
+        // pass through the input frames in their own hardware context.
+        if (!inlink->hw_frames_ctx) {
+            av_log(ctx, AV_LOG_ERROR, "No input hwframe context.\n");
+            return AVERROR(EINVAL);
+        }
+
+        outlink->hw_frames_ctx = av_buffer_ref(inlink->hw_frames_ctx);
+        if (!outlink->hw_frames_ctx)
+            return AVERROR(ENOMEM);
+
+        return 0;
+    }
+
+    s->hwframe = av_hwframe_ctx_alloc(s->hwdevice);
+    if (!s->hwframe)
+        return AVERROR(ENOMEM);
+
+    hwframe_ctx            = (AVHWFramesContext*)s->hwframe->data;
+    hwframe_ctx->format    = AV_PIX_FMT_NI_QUAD;
+    hwframe_ctx->sw_format = inlink->format;
+    hwframe_ctx->width     = inlink->w;
+    hwframe_ctx->height    = inlink->h;
+    pub_ctx = (AVNIFramesContext*)hwframe_ctx->hwctx;
+    pub_ctx->keep_alive_timeout = s->keep_alive_timeout;
+
+    ret = av_hwframe_ctx_init(s->hwframe);
+    if (ret < 0)
+        return ret;
+
+    outlink->hw_frames_ctx = av_buffer_ref(s->hwframe);
+    if (!outlink->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return 0;
+}
+
+static int niupload_filter_frame(AVFilterLink *link, AVFrame *in)
+{
+    AVFilterContext   *ctx = link->dst;
+    AVFilterLink  *outlink = ctx->outputs[0];
+
+    AVFrame *out = NULL;
+    int ret;
+
+    if (in->format == outlink->format)
+        return ff_filter_frame(outlink, in);
+
+    out = ff_get_video_buffer(outlink, outlink->w, outlink->h);
+    if (!out) {
+        ret = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    out->width  = in->width;
+    out->height = in->height;
+
+    ret = av_hwframe_transfer_data(out, in, 0);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "Error transferring data to the Quadra\n");
+        goto fail;
+    }
+
+    ret = av_frame_copy_props(out, in);
+    if (ret < 0)
+        goto fail;
+
+    av_frame_free(&in);
+
+    return ff_filter_frame(ctx->outputs[0], out);
+fail:
+    av_frame_free(&in);
+    av_frame_free(&out);
+    return ret;
+}
+
+#define OFFSET(x) offsetof(NiUploadContext, x)
+#define FLAGS (AV_OPT_FLAG_FILTERING_PARAM | AV_OPT_FLAG_VIDEO_PARAM)
+// default device_idx -1 for uploader to auto balance
+static const AVOption niupload_options[] = {
+    {"device",
+     "Number of the device to use",
+     OFFSET(device_idx),
+     AV_OPT_TYPE_INT,
+     {.i64 = -1},
+     -1,
+     INT_MAX,
+     FLAGS},
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSET(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     FLAGS,
+     "keep_alive_timeout"},
+    {NULL},
+};
+
+AVFILTER_DEFINE_CLASS(niupload);
+
+static const AVFilterPad niupload_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = niupload_filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad niupload_outputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = niupload_config_output,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_hwupload_ni_quadra = {
+    .name        = "ni_quadra_hwupload",
+    .description = NULL_IF_CONFIG_SMALL("NetInt Quadra upload a system memory frame to a device v" NI_XCODER_REVISION),
+
+    .init      = niupload_init,
+    .uninit    = niupload_uninit,
+
+    .priv_size  = sizeof(NiUploadContext),
+    .priv_class = &niupload_class,
+
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(niupload_inputs),
+    FILTER_OUTPUTS(niupload_outputs),
+    FILTER_QUERY_FUNC(niupload_query_formats),
+#else
+    .inputs        = niupload_inputs,
+    .outputs       = niupload_outputs,
+    .query_formats = niupload_query_formats,
+#endif
+};
diff --git a/libavfilter/vf_overlay_ni.c b/libavfilter/vf_overlay_ni.c
new file mode 100644
index 0000000000..7cf9b8e528
--- /dev/null
+++ b/libavfilter/vf_overlay_ni.c
@@ -0,0 +1,729 @@
+/*
+ * Copyright (c) 2010 Stefano Sabatini
+ * Copyright (c) 2010 Baptiste Coudurier
+ * Copyright (c) 2007 Bobby Bingham
+ * Copyright (c) 2021 NetInt
+ *
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * overlay one video on top of another
+ */
+
+#include "avfilter.h"
+#include "formats.h"
+#include "libavutil/common.h"
+#include "libavutil/eval.h"
+#include "libavutil/avstring.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/mathematics.h"
+#include "libavutil/opt.h"
+#include "libavutil/timestamp.h"
+#include "libavutil/hwcontext.h"
+#include "internal.h"
+#include "drawutils.h"
+#include "framesync.h"
+#include "video.h"
+#include "nifilter.h"
+#include <ni_device_api.h>
+
+static const char *const var_names[] = {
+    "main_w",    "W", ///< width  of the main    video
+    "main_h",    "H", ///< height of the main    video
+    "overlay_w", "w", ///< width  of the overlay video
+    "overlay_h", "h", ///< height of the overlay video
+    "hsub",
+    "vsub",
+    "x",
+    "y",
+    "n",            ///< number of frame
+    "pos",          ///< position in the file
+    "t",            ///< timestamp expressed in seconds
+    NULL
+};
+
+enum var_name {
+    VAR_MAIN_W,    VAR_MW,
+    VAR_MAIN_H,    VAR_MH,
+    VAR_OVERLAY_W, VAR_OW,
+    VAR_OVERLAY_H, VAR_OH,
+    VAR_HSUB,
+    VAR_VSUB,
+    VAR_X,
+    VAR_Y,
+    VAR_N,
+    VAR_POS,
+    VAR_T,
+    VAR_VARS_NB
+};
+
+#define MAIN    0
+#define OVERLAY 1
+
+#define R 0
+#define G 1
+#define B 2
+#define A 3
+
+#define Y 0
+#define U 1
+#define V 2
+
+enum OverlayFormat {
+    OVERLAY_FORMAT_YUV420,
+    OVERLAY_FORMAT_YUV422,
+    OVERLAY_FORMAT_YUV444,
+    OVERLAY_FORMAT_RGB,
+    OVERLAY_FORMAT_GBRP,
+    OVERLAY_FORMAT_AUTO,
+    OVERLAY_FORMAT_NB
+};
+
+typedef struct NetIntOverlayContext {
+    const AVClass *class;
+    int x, y;                   ///< position of overlaid picture
+
+    uint8_t main_is_packed_rgb;
+    uint8_t main_rgba_map[4];
+    uint8_t main_has_alpha;
+    uint8_t overlay_is_packed_rgb;
+    uint8_t overlay_rgba_map[4];
+    uint8_t overlay_has_alpha;
+    int alpha_format;
+
+    FFFrameSync fs;
+
+    int main_pix_step[4];       ///< steps per pixel for each plane of the main output
+    int overlay_pix_step[4];    ///< steps per pixel for each plane of the overlay
+    int hsub, vsub;             ///< chroma subsampling values
+    const AVPixFmtDescriptor *main_desc; ///< format descriptor for main input
+
+    double var_values[VAR_VARS_NB];
+    char *x_expr, *y_expr;
+
+    AVExpr *x_pexpr, *y_pexpr;
+
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+
+    AVBufferRef* out_frames_ref;
+
+    int initialized;
+    int session_opened;
+    int keep_alive_timeout; /* keep alive timeout setting */
+} NetIntOverlayContext;
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    NetIntOverlayContext *s = ctx->priv;
+
+    ff_framesync_uninit(&s->fs);
+    av_expr_free(s->x_pexpr); s->x_pexpr = NULL;
+    av_expr_free(s->y_pexpr); s->y_pexpr = NULL;
+
+    if (s->api_dst_frame.data.frame.p_buffer)
+        ni_frame_buffer_free(&s->api_dst_frame.data.frame);
+
+    if (s->session_opened) {
+        /* Close operation will free the device frames */
+        ni_device_session_close(&s->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ni_device_session_context_clear(&s->api_ctx);
+    }
+
+    av_buffer_unref(&s->out_frames_ref);
+}
+
+static inline int normalize_xy(double d, int chroma_sub)
+{
+    if (isnan(d))
+        return INT_MAX;
+    return (int)d & ~((1 << chroma_sub) - 1);
+}
+
+static void eval_expr(AVFilterContext *ctx)
+{
+    NetIntOverlayContext *s = ctx->priv;
+
+    s->var_values[VAR_X] = av_expr_eval(s->x_pexpr, s->var_values, NULL);
+    s->var_values[VAR_Y] = av_expr_eval(s->y_pexpr, s->var_values, NULL);
+    s->var_values[VAR_X] = av_expr_eval(s->x_pexpr, s->var_values, NULL);
+    s->x = normalize_xy(s->var_values[VAR_X], s->hsub);
+    s->y = normalize_xy(s->var_values[VAR_Y], s->vsub);
+}
+
+static int set_expr(AVExpr **pexpr, const char *expr, const char *option, void *log_ctx)
+{
+    int ret;
+    AVExpr *old = NULL;
+
+    if (*pexpr)
+        old = *pexpr;
+    ret = av_expr_parse(pexpr, expr, var_names,
+                        NULL, NULL, NULL, NULL, 0, log_ctx);
+    if (ret < 0) {
+        av_log(log_ctx, AV_LOG_ERROR,
+               "Error when evaluating the expression '%s' for %s\n",
+               expr, option);
+        *pexpr = old;
+        return ret;
+    }
+
+    av_expr_free(old);
+    return 0;
+}
+
+static const enum AVPixelFormat alpha_pix_fmts[] = {
+    AV_PIX_FMT_YUVA420P, AV_PIX_FMT_YUVA422P, AV_PIX_FMT_YUVA444P,
+    AV_PIX_FMT_ARGB, AV_PIX_FMT_ABGR, AV_PIX_FMT_RGBA,
+    AV_PIX_FMT_BGRA, AV_PIX_FMT_GBRAP, AV_PIX_FMT_NONE
+};
+
+static int query_formats(AVFilterContext *ctx)
+{
+    /* We only accept hardware frames */
+    static const enum AVPixelFormat pix_fmts[] =
+        {AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE};
+    AVFilterFormats *formats;
+
+    formats = ff_make_format_list(pix_fmts);
+
+    if (!formats)
+        return AVERROR(ENOMEM);
+
+    return ff_set_common_formats(ctx, formats);
+}
+
+static int config_input_overlay(AVFilterLink *inlink)
+{
+    AVFilterContext *ctx  = inlink->dst;
+    NetIntOverlayContext  *s = inlink->dst->priv;
+    int ret;
+    const AVPixFmtDescriptor *pix_desc = av_pix_fmt_desc_get(inlink->format);
+
+    av_image_fill_max_pixsteps(s->overlay_pix_step, NULL, pix_desc);
+
+    /* Finish the configuration by evaluating the expressions
+       now when both inputs are configured. */
+    s->var_values[VAR_MAIN_W   ] = s->var_values[VAR_MW] = ctx->inputs[MAIN   ]->w;
+    s->var_values[VAR_MAIN_H   ] = s->var_values[VAR_MH] = ctx->inputs[MAIN   ]->h;
+    s->var_values[VAR_OVERLAY_W] = s->var_values[VAR_OW] = ctx->inputs[OVERLAY]->w;
+    s->var_values[VAR_OVERLAY_H] = s->var_values[VAR_OH] = ctx->inputs[OVERLAY]->h;
+    s->var_values[VAR_HSUB]  = 1<<pix_desc->log2_chroma_w;
+    s->var_values[VAR_VSUB]  = 1<<pix_desc->log2_chroma_h;
+    s->var_values[VAR_X]     = NAN;
+    s->var_values[VAR_Y]     = NAN;
+    s->var_values[VAR_N]     = 0;
+    s->var_values[VAR_T]     = NAN;
+    s->var_values[VAR_POS]   = NAN;
+
+    if ((ret = set_expr(&s->x_pexpr,      s->x_expr,      "x",      ctx)) < 0 ||
+        (ret = set_expr(&s->y_pexpr,      s->y_expr,      "y",      ctx)) < 0)
+        return ret;
+
+    s->overlay_is_packed_rgb =
+        ff_fill_rgba_map(s->overlay_rgba_map, inlink->format) >= 0;
+    s->overlay_has_alpha = ff_fmt_is_in(inlink->format, alpha_pix_fmts);
+
+    av_log(ctx, AV_LOG_VERBOSE,
+           "main w:%d h:%d fmt:%s overlay w:%d h:%d fmt:%s\n",
+           ctx->inputs[MAIN]->w, ctx->inputs[MAIN]->h,
+           av_get_pix_fmt_name(ctx->inputs[MAIN]->format),
+           ctx->inputs[OVERLAY]->w, ctx->inputs[OVERLAY]->h,
+           av_get_pix_fmt_name(ctx->inputs[OVERLAY]->format));
+    return 0;
+}
+
+static int init_out_pool(AVFilterContext *ctx) {
+    NetIntOverlayContext *s = ctx->priv;
+    AVHWFramesContext *out_frames_ctx;
+
+    if (!ctx->inputs[0]->hw_frames_ctx) {
+        return AVERROR(EINVAL);
+    }
+
+    out_frames_ctx = (AVHWFramesContext *)s->out_frames_ref->data;
+
+    /* Don't check return code, this will intentionally fail */
+    av_hwframe_ctx_init(s->out_frames_ref);
+
+    /* Create frame pool on device */
+    return ff_ni_build_frame_pool(&s->api_ctx, out_frames_ctx->width,
+                                  out_frames_ctx->height, out_frames_ctx->sw_format,
+                                  DEFAULT_NI_FILTER_POOL_SIZE);
+}
+
+static int process_frame(FFFrameSync *fs)
+{
+    AVFilterContext      *ctx = fs->parent;
+    NetIntOverlayContext *s = (NetIntOverlayContext *) ctx->priv;
+    AVHWFramesContext    *main_frame_ctx,*ovly_frame_ctx;
+    AVNIDeviceContext *pAVNIDevCtx;
+    AVFilterLink         *inlink,*outlink;
+    AVFrame              *frame = NULL;
+    AVFrame              *overlay = NULL;
+    AVFrame              *out = NULL;
+    niFrameSurface1_t    *frame_surface,*new_frame_surface;
+    int flags, main_cardno, ovly_cardno;
+    int main_scaler_format, ovly_scaler_format;
+    ni_retcode_t retcode;
+    int64_t pos;
+    uint16_t tempFIDOverlay = 0;
+    uint16_t tempFIDFrame   = 0;
+
+    /* ff_framesync_get_frame() always returns 0 for hw frames */
+    ff_framesync_get_frame(fs, OVERLAY, &overlay, 0);
+
+    if (!overlay) {
+        ff_framesync_get_frame(fs, MAIN, &frame, 1);
+        return ff_filter_frame(ctx->outputs[0], frame);
+    }
+
+    ff_framesync_get_frame(fs, MAIN, &frame, 0);
+
+    frame->pts =
+        av_rescale_q(fs->pts, fs->time_base, ctx->outputs[0]->time_base);
+
+    inlink = ctx->inputs[MAIN];
+
+    pos = frame->pkt_pos;
+
+    s->var_values[VAR_N] = inlink->frame_count_out;
+    s->var_values[VAR_T] = frame->pts == AV_NOPTS_VALUE ?
+        NAN : frame->pts * av_q2d(inlink->time_base);
+    s->var_values[VAR_POS] = pos == -1 ? NAN : pos;
+
+    if (overlay)
+    {
+        s->var_values[VAR_OVERLAY_W] = s->var_values[VAR_OW] = overlay->width;
+        s->var_values[VAR_OVERLAY_H] = s->var_values[VAR_OH] = overlay->height;
+    }
+
+    s->var_values[VAR_MAIN_W   ] = s->var_values[VAR_MW] = frame->width;
+    s->var_values[VAR_MAIN_H   ] = s->var_values[VAR_MH] = frame->height;
+
+    //This can satisfy some customers or demos to modify the location when using ni_overlay
+    set_expr(&s->x_pexpr, s->x_expr,"x", ctx);
+    set_expr(&s->y_pexpr, s->y_expr,"y", ctx);
+
+    eval_expr(ctx);
+    av_log(ctx, AV_LOG_DEBUG, "n:%f t:%f pos:%f x:%f xi:%d y:%f yi:%d\n",
+           s->var_values[VAR_N], s->var_values[VAR_T], s->var_values[VAR_POS],
+           s->var_values[VAR_X], s->x,
+           s->var_values[VAR_Y], s->y);
+
+    main_frame_ctx = (AVHWFramesContext *) frame->hw_frames_ctx->data;
+    main_scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(main_frame_ctx->sw_format);
+    outlink = ctx->outputs[0];
+
+    main_cardno = ni_get_cardno(frame);
+
+    if (overlay)
+    {
+        ovly_frame_ctx = (AVHWFramesContext *) overlay->hw_frames_ctx->data;
+        ovly_scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(ovly_frame_ctx->sw_format);
+        ovly_cardno        = ni_get_cardno(overlay);
+
+        if (main_cardno != ovly_cardno) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "Main/Overlay frames on different cards\n");
+            return AVERROR(EINVAL);
+        }
+    }
+    else
+    {
+        ovly_scaler_format = 0;
+    }
+
+    if (!s->initialized) {
+        retcode = ni_device_session_context_init(&s->api_ctx);
+        if (retcode < 0) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "ni overlay filter session context init failure\n");
+            return retcode;
+        }
+
+        pAVNIDevCtx = (AVNIDeviceContext *)main_frame_ctx->device_ctx->hwctx;
+        s->api_ctx.device_handle = pAVNIDevCtx->cards[main_cardno];
+        s->api_ctx.blk_io_handle = pAVNIDevCtx->cards[main_cardno];
+
+        s->api_ctx.hw_id             = main_cardno;
+        s->api_ctx.device_type       = NI_DEVICE_TYPE_SCALER;
+        s->api_ctx.scaler_operation  = NI_SCALER_OPCODE_OVERLAY;
+        s->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+
+        retcode = ni_device_session_open(&s->api_ctx, NI_DEVICE_TYPE_SCALER);
+        if (retcode < 0) {
+            av_log(ctx, AV_LOG_ERROR, "Can't open device session on card %d\n",
+                   main_cardno);
+            return retcode;
+        }
+
+        s->session_opened = 1;
+
+        retcode = init_out_pool(inlink->dst);
+
+        if (retcode < 0)
+        {
+            av_log(ctx, AV_LOG_ERROR,
+                   "Internal output allocation failed rc = %d\n", retcode);
+            return retcode;
+        }
+
+        ff_ni_clone_hwframe_ctx(main_frame_ctx,
+                                (AVHWFramesContext *)s->out_frames_ref->data);
+
+        if ((frame && frame->color_range == AVCOL_RANGE_JPEG) ||
+            (overlay && overlay->color_range == AVCOL_RANGE_JPEG)) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "WARNING: Full color range input, limited color output\n");
+        }
+
+        s->initialized = 1;
+    }
+
+    /* Allocate a ni_frame for the overlay output */
+    retcode = ni_frame_buffer_alloc_hwenc(&s->api_dst_frame.data.frame,
+                                          outlink->w,
+                                          outlink->h,
+                                          0);
+
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        return AVERROR(ENOMEM);
+    }
+
+    if (overlay)
+    {
+      frame_surface = (niFrameSurface1_t *)overlay->data[3];
+      tempFIDOverlay = frame_surface->ui16FrameIdx;
+    }
+    else
+    {
+      frame_surface = NULL;
+    }
+    /*
+     * Allocate device input frame for overlay picture. This call won't actually
+     * allocate a frame, but sends the incoming hardware frame index to the
+     * scaler manager.
+     */
+    retcode = ni_device_alloc_frame(
+        &s->api_ctx,                                             //
+        overlay ? FFALIGN(overlay->width, 2) : 0,                //
+        overlay ? FFALIGN(overlay->height, 2) : 0,               //
+        ovly_scaler_format,                                      //
+        0,                                                       //
+        overlay ? FFALIGN(overlay->width, 2) : 0,                //
+        overlay ? FFALIGN(overlay->height, 2) : 0,               //
+        s->x,                                                    //
+        s->y,                                                    //
+        frame_surface ? (int)frame_surface->ui32nodeAddress : 0, //
+        frame_surface ? frame_surface->ui16FrameIdx : 0,         //
+        NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        av_log(ctx, AV_LOG_DEBUG, "Can't assign frame for overlay input %d\n",
+               retcode);
+        return AVERROR(ENOMEM);
+    }
+
+    frame_surface = (niFrameSurface1_t *) frame->data[3];
+    if (frame_surface == NULL) {
+        return AVERROR(EINVAL);
+    }
+
+    tempFIDFrame = frame_surface->ui16FrameIdx;
+    /*
+     * Allocate device output frame from the pool. We also send down the frame index
+     * of the background frame to the scaler manager.
+     */
+    flags = (s->alpha_format ? NI_SCALER_FLAG_PA : 0) | NI_SCALER_FLAG_IO;
+    retcode = ni_device_alloc_frame(&s->api_ctx,                    //
+                                    FFALIGN(frame->width, 2),       //
+                                    FFALIGN(frame->height, 2),      //
+                                    main_scaler_format,             //
+                                    flags,                          //
+                                    FFALIGN(frame->width, 2),       //
+                                    FFALIGN(frame->height, 2),      //
+                                    0,                              // x
+                                    0,                              // y
+                                    frame_surface->ui32nodeAddress, //
+                                    frame_surface->ui16FrameIdx,    //
+                                    NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        av_log(ctx, AV_LOG_DEBUG, "Can't allocate frame for output %d\n",
+               retcode);
+        return AVERROR(ENOMEM);
+    }
+
+    out = av_frame_alloc();
+    if (!out)
+    {
+        return AVERROR(ENOMEM);
+    }
+
+    av_frame_copy_props(out,frame);
+
+    out->width = outlink->w;
+    out->height = outlink->h;
+    out->format = AV_PIX_FMT_NI_QUAD;
+
+    /* Quadra 2D engine always outputs limited color range */
+    out->color_range = AVCOL_RANGE_MPEG;
+
+    /* Reference the new hw frames context */
+    out->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+    out->data[3] = av_malloc(sizeof(niFrameSurface1_t));
+
+    if (!out->data[3])
+    {
+        av_frame_free(&out);
+        return AVERROR(ENOMEM);
+    }
+
+    /* Copy the frame surface from the incoming frame */
+    memcpy(out->data[3], frame->data[3], sizeof(niFrameSurface1_t));
+
+    /* Set the new frame index */
+    retcode = ni_device_session_read_hwdesc(&s->api_ctx, &s->api_dst_frame,
+                                            NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR,
+               "Can't acquire output frame %d\n", retcode);
+        av_frame_free(&out);
+        return AVERROR(ENOMEM);
+    }
+
+    frame_surface = (niFrameSurface1_t *) out->data[3];
+    new_frame_surface = (niFrameSurface1_t *) s->api_dst_frame.data.frame.p_data[3];
+    frame_surface->ui16FrameIdx = new_frame_surface->ui16FrameIdx;
+    frame_surface->ui16session_ID = new_frame_surface->ui16session_ID;
+    frame_surface->device_handle = new_frame_surface->device_handle;
+    frame_surface->output_idx     = new_frame_surface->output_idx;
+    frame_surface->src_cpu = new_frame_surface->src_cpu;
+
+#if 0
+    /* Enable this code after QDFW-276 merged */
+    /* Remove ni-split specific assets */
+    frame_surface->ui32nodeAddress = 0;
+
+    frame_surface->ui16width = out->width;
+    frame_surface->ui16height = out->height;
+#endif
+
+    av_log(
+        ctx, AV_LOG_DEBUG,
+        "vf_overlay_ni.c:IN trace ui16FrameIdx = [%d] and [%d] --> out [%d] \n",
+        tempFIDFrame, tempFIDOverlay, frame_surface->ui16FrameIdx);
+
+    out->buf[0] = av_buffer_create(out->data[3], sizeof(niFrameSurface1_t), ff_ni_frame_free, NULL, 0);
+
+    return ff_filter_frame(ctx->outputs[0], out);
+
+}
+
+static int init_framesync(AVFilterContext *ctx)
+{
+    NetIntOverlayContext *s = ctx->priv;
+    int ret, i;
+
+    s->fs.on_event = process_frame;
+    s->fs.opaque   = s;
+    ret = ff_framesync_init(&s->fs, ctx, ctx->nb_inputs);
+    if (ret < 0)
+        return ret;
+
+    for (i = 0; i < ctx->nb_inputs; i++) {
+        FFFrameSyncIn *in = &s->fs.in[i];
+        in->before    = EXT_STOP;
+        in->after     = EXT_INFINITY;
+        in->sync      = i ? 1 : 2;
+        in->time_base = ctx->inputs[i]->time_base;
+    }
+
+    return ff_framesync_configure(&s->fs);
+}
+
+static int config_output(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    NetIntOverlayContext *s = ctx->priv;
+    AVHWFramesContext *in_frames_ctx,*in_frames_ctx_ovly;
+    AVHWFramesContext *out_frames_ctx;
+    int ret;
+
+    av_log(ctx, AV_LOG_DEBUG, "Output is of %s.\n", av_get_pix_fmt_name(outlink->format));
+
+    outlink->w = ctx->inputs[MAIN]->w;
+    outlink->h = ctx->inputs[MAIN]->h;
+    outlink->frame_rate = ctx->inputs[MAIN]->frame_rate;
+    outlink->time_base = ctx->inputs[MAIN]->time_base;
+
+    ret = init_framesync(ctx);
+    if (ret < 0)
+        return ret;
+
+    in_frames_ctx = (AVHWFramesContext *)ctx->inputs[0]->hw_frames_ctx->data;
+
+    if (in_frames_ctx->sw_format == AV_PIX_FMT_BGRP) {
+        av_log(ctx, AV_LOG_ERROR, "bgrp not supported for background\n");
+        return AVERROR(EINVAL);
+    }
+
+    in_frames_ctx_ovly = (AVHWFramesContext *)ctx->inputs[1]->hw_frames_ctx->data;
+
+    if (in_frames_ctx_ovly->sw_format == AV_PIX_FMT_BGRP) {
+        av_log(ctx, AV_LOG_ERROR, "bgrp not supported for overlay\n");
+        return AVERROR(EINVAL);
+    }
+
+    s->out_frames_ref = av_hwframe_ctx_alloc(in_frames_ctx->device_ref);
+    if (!s->out_frames_ref)
+        return AVERROR(ENOMEM);
+
+    out_frames_ctx = (AVHWFramesContext *)s->out_frames_ref->data;
+
+    out_frames_ctx->format    = AV_PIX_FMT_NI_QUAD;
+    out_frames_ctx->width     = outlink->w;
+    out_frames_ctx->height    = outlink->h;
+    out_frames_ctx->sw_format = in_frames_ctx->sw_format;
+    out_frames_ctx->initial_pool_size =
+        NI_OVERLAY_ID; // Repurposed as identity code
+
+    av_buffer_unref(&ctx->outputs[0]->hw_frames_ctx);
+    ctx->outputs[0]->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    if (!ctx->outputs[0]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return 0;
+}
+
+// divide by 255 and round to nearest
+// apply a fast variant: (X+127)/255 = ((X+127)*257+257)>>16 = ((X+128)*257)>>16
+#define FAST_DIV255(x) ((((x) + 128) * 257) >> 16)
+
+// calculate the unpremultiplied alpha, applying the general equation:
+// alpha = alpha_overlay / ( (alpha_main + alpha_overlay) - (alpha_main * alpha_overlay) )
+// (((x) << 16) - ((x) << 9) + (x)) is a faster version of: 255 * 255 * x
+// ((((x) + (y)) << 8) - ((x) + (y)) - (y) * (x)) is a faster version of: 255 * (x + y)
+#define UNPREMULTIPLY_ALPHA(x, y) ((((x) << 16) - ((x) << 9) + (x)) / ((((x) + (y)) << 8) - ((x) + (y)) - (y) * (x)))
+
+/**
+ * Blend image in src to destination buffer dst at position (x, y).
+ */
+
+static int config_input_main(AVFilterLink *inlink)
+{
+    /*
+     * TODO: should we check pixel format of input
+     * link and reject if background video is RGBA?
+     */
+    return 0;
+}
+
+static int activate(AVFilterContext *ctx)
+{
+    NetIntOverlayContext *s = ctx->priv;
+    return ff_framesync_activate(&s->fs);
+}
+
+#define OFFSET(x) offsetof(NetIntOverlayContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+static const AVOption overlay_ni_options[] = {
+    { "x", "set the x expression", OFFSET(x_expr), AV_OPT_TYPE_STRING, {.str = "0"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "y", "set the y expression", OFFSET(y_expr), AV_OPT_TYPE_STRING, {.str = "0"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "eof_action", "Action to take when encountering EOF from secondary input ",
+        OFFSET(fs.opt_eof_action), AV_OPT_TYPE_INT, { .i64 = EOF_ACTION_REPEAT },
+        EOF_ACTION_REPEAT, EOF_ACTION_PASS, .flags = FLAGS, "eof_action" },
+    { "repeat", "Repeat the previous frame.",   0, AV_OPT_TYPE_CONST, { .i64 = EOF_ACTION_REPEAT }, .flags = FLAGS, "eof_action" },
+    { "endall", "End both streams.",            0, AV_OPT_TYPE_CONST, { .i64 = EOF_ACTION_ENDALL }, .flags = FLAGS, "eof_action" },
+    { "pass",   "Pass through the main input.", 0, AV_OPT_TYPE_CONST, { .i64 = EOF_ACTION_PASS },   .flags = FLAGS, "eof_action" },
+    { "shortest", "force termination when the shortest input terminates", OFFSET(fs.opt_shortest), AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, FLAGS },
+    { "repeatlast", "repeat overlay of the last overlay frame", OFFSET(fs.opt_repeatlast), AV_OPT_TYPE_BOOL, {.i64=1}, 0, 1, FLAGS },
+    { "alpha", "alpha format", OFFSET(alpha_format), AV_OPT_TYPE_INT, {.i64=0}, 0, 1, FLAGS, "alpha_format" },
+        { "straight",      "", 0, AV_OPT_TYPE_CONST, {.i64=0}, .flags = FLAGS, .unit = "alpha_format" },
+        { "premultiplied", "", 0, AV_OPT_TYPE_CONST, {.i64=1}, .flags = FLAGS, .unit = "alpha_format" },
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSET(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     FLAGS,
+     "keep_alive_timeout"},
+    { NULL }
+};
+
+// NOLINTNEXTLINE(clang-diagnostic-deprecated-declarations)
+FRAMESYNC_DEFINE_CLASS(overlay_ni, NetIntOverlayContext, fs);
+
+static const AVFilterPad avfilter_vf_overlay_inputs[] = {
+    {
+        .name         = "main",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_input_main,
+    },
+    {
+        .name         = "overlay",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_input_overlay,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad avfilter_vf_overlay_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .config_props  = config_output,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_overlay_ni_quadra = {
+    .name          = "ni_quadra_overlay",
+    .description   = NULL_IF_CONFIG_SMALL("NetInt Quadra overlay a video source on top of the input v" NI_XCODER_REVISION),
+    .preinit       = overlay_ni_framesync_preinit,
+    .uninit        = uninit,
+    .priv_size     = sizeof(NetIntOverlayContext),
+    .priv_class    = &overlay_ni_class,
+    .activate      = activate,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(avfilter_vf_overlay_inputs),
+    FILTER_OUTPUTS(avfilter_vf_overlay_outputs),
+    FILTER_QUERY_FUNC(query_formats),
+#else
+    .inputs        = avfilter_vf_overlay_inputs,
+    .outputs       = avfilter_vf_overlay_outputs,
+    .query_formats = query_formats,
+#endif
+    .flags_internal= FF_FILTER_FLAG_HWFRAME_AWARE
+};
diff --git a/libavfilter/vf_pad_ni.c b/libavfilter/vf_pad_ni.c
new file mode 100644
index 0000000000..669987a375
--- /dev/null
+++ b/libavfilter/vf_pad_ni.c
@@ -0,0 +1,633 @@
+/*
+ * Copyright (c) 2007 Bobby Bingham
+ * Copyright (c) 2020 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * video padding filter
+ */
+
+#include <float.h>  /* DBL_MAX */
+
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+#include "libavutil/avstring.h"
+#include "libavutil/common.h"
+#include "libavutil/eval.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/colorspace.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/parseutils.h"
+#include "libavutil/mathematics.h"
+#include "libavutil/opt.h"
+#include "nifilter.h"
+#include "drawutils.h"
+
+static const char *const var_names[] = {
+    "in_w",   "iw",
+    "in_h",   "ih",
+    "out_w",  "ow",
+    "out_h",  "oh",
+    "x",
+    "y",
+    "a",
+    "sar",
+    "dar",
+    "hsub",
+    "vsub",
+    NULL
+};
+
+enum var_name {
+    VAR_IN_W,   VAR_IW,
+    VAR_IN_H,   VAR_IH,
+    VAR_OUT_W,  VAR_OW,
+    VAR_OUT_H,  VAR_OH,
+    VAR_X,
+    VAR_Y,
+    VAR_A,
+    VAR_SAR,
+    VAR_DAR,
+    VAR_HSUB,
+    VAR_VSUB,
+    VARS_NB
+};
+
+typedef struct NetIntPadContext {
+    const AVClass *class;
+    int w, h;               ///< output dimensions, a value of 0 will result in the input size
+    int x, y;               ///< offsets of the input area with respect to the padded area
+    int in_w, in_h;         ///< width and height for the padded input video, which has to be aligned to the chroma values in order to avoid chroma issues
+    int inlink_w, inlink_h;
+    AVRational aspect;
+
+    char *w_expr;           ///< width  expression string
+    char *h_expr;           ///< height expression string
+    char *x_expr;           ///< width  expression string
+    char *y_expr;           ///< height expression string
+    uint8_t rgba_color[4];  ///< color for the padding area
+    FFDrawContext draw;
+    FFDrawColor color;
+
+    AVBufferRef *out_frames_ref;
+
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+
+    int initialized;
+    int session_opened;
+    int keep_alive_timeout; /* keep alive timeout setting */
+
+    ni_frame_config_t frame_out;
+} NetIntPadContext;
+
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] =
+        {AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE};
+    AVFilterFormats *formats;
+
+    formats = ff_make_format_list(pix_fmts);
+
+    if (!formats)
+        return AVERROR(ENOMEM);
+
+    return ff_set_common_formats(ctx, formats);
+}
+
+static int init_out_pool(AVFilterContext *ctx) {
+    NetIntPadContext  *s = ctx->priv;
+    AVHWFramesContext *out_frames_ctx;
+
+    if (!ctx->inputs[0]->hw_frames_ctx) {
+        return AVERROR(EINVAL);
+    }
+
+    out_frames_ctx   = (AVHWFramesContext*)s->out_frames_ref->data;
+
+    /* Don't check return code, this will intentionally fail */
+    av_hwframe_ctx_init(s->out_frames_ref);
+
+    /* Create frame pool on device */
+    return ff_ni_build_frame_pool(&s->api_ctx,
+                                  out_frames_ctx->width, out_frames_ctx->height,
+                                  out_frames_ctx->sw_format,
+                                  DEFAULT_NI_FILTER_POOL_SIZE);
+}
+
+static int config_input(AVFilterLink *inlink)
+{
+    AVFilterContext *ctx = inlink->dst;
+    NetIntPadContext *s = ctx->priv;
+    AVRational adjusted_aspect = s->aspect;
+    int ret;
+    double var_values[VARS_NB], res;
+    char *expr;
+
+    if (inlink->format == AV_PIX_FMT_NI_QUAD)
+    {
+        AVBufferRef *avhwref = (AVBufferRef *) inlink->hw_frames_ctx;
+        AVHWFramesContext *avhwctx = (AVHWFramesContext *)  avhwref->data;
+
+        if (ff_draw_init(&s->draw, avhwctx->sw_format, 0) < 0)
+            return AVERROR(EINVAL);
+    }
+    else
+    {
+        if (ff_draw_init(&s->draw, inlink->format, 0) < 0)
+            return AVERROR(EINVAL);
+    }
+
+    ff_draw_color(&s->draw, &s->color, s->rgba_color);
+
+    var_values[VAR_IN_W]  = var_values[VAR_IW] = inlink->w;
+    var_values[VAR_IN_H]  = var_values[VAR_IH] = inlink->h;
+    var_values[VAR_OUT_W] = var_values[VAR_OW] = NAN;
+    var_values[VAR_OUT_H] = var_values[VAR_OH] = NAN;
+    var_values[VAR_A]     = (double) inlink->w / inlink->h;
+    var_values[VAR_SAR]   = inlink->sample_aspect_ratio.num ?
+        (double) inlink->sample_aspect_ratio.num / inlink->sample_aspect_ratio.den : 1;
+    var_values[VAR_DAR]   = var_values[VAR_A] * var_values[VAR_SAR];
+    var_values[VAR_HSUB]  = 1 << s->draw.hsub_max;
+    var_values[VAR_VSUB]  = 1 << s->draw.vsub_max;
+
+    /* evaluate width and height */
+    av_expr_parse_and_eval(&res, s->w_expr, var_names, var_values, NULL, NULL,
+                           NULL, NULL, NULL, 0, ctx);
+    s->w                  = (int)res;
+    var_values[VAR_OUT_W] = var_values[VAR_OW] = res;
+    if ((ret = av_expr_parse_and_eval(&res, (expr = s->h_expr),
+                                      var_names, var_values,
+                                      NULL, NULL, NULL, NULL, NULL, 0, ctx)) < 0)
+        goto eval_fail;
+    s->h                  = (int)res;
+    var_values[VAR_OUT_H] = var_values[VAR_OH] = res;
+    if (!s->h)
+        var_values[VAR_OUT_H] = var_values[VAR_OH] = s->h = inlink->h;
+
+    /* evaluate the width again, as it may depend on the evaluated output height */
+    if ((ret = av_expr_parse_and_eval(&res, (expr = s->w_expr),
+                                      var_names, var_values,
+                                      NULL, NULL, NULL, NULL, NULL, 0, ctx)) < 0)
+        goto eval_fail;
+    s->w                  = (int)res;
+    var_values[VAR_OUT_W] = var_values[VAR_OW] = res;
+    if (!s->w)
+        var_values[VAR_OUT_W] = var_values[VAR_OW] = s->w = inlink->w;
+
+    if (adjusted_aspect.num && adjusted_aspect.den) {
+        adjusted_aspect = av_div_q(adjusted_aspect, inlink->sample_aspect_ratio);
+        if (s->h < av_rescale(s->w, adjusted_aspect.den, adjusted_aspect.num)) {
+            s->h = av_rescale(s->w, adjusted_aspect.den, adjusted_aspect.num);
+            var_values[VAR_OUT_H] = var_values[VAR_OH] = (double)s->h;
+        } else {
+            s->w = av_rescale(s->h, adjusted_aspect.num, adjusted_aspect.den);
+            var_values[VAR_OUT_W] = var_values[VAR_OW] = (double)s->w;
+        }
+    }
+
+    /* evaluate x and y */
+    av_expr_parse_and_eval(&res, s->x_expr, var_names, var_values, NULL, NULL,
+                           NULL, NULL, NULL, 0, ctx);
+    s->x              = (int)res;
+    var_values[VAR_X] = res;
+    if ((ret = av_expr_parse_and_eval(&res, (expr = s->y_expr),
+                                      var_names, var_values,
+                                      NULL, NULL, NULL, NULL, NULL, 0, ctx)) < 0)
+        goto eval_fail;
+    s->y              = (int)res;
+    var_values[VAR_Y] = res;
+    /* evaluate x again, as it may depend on the evaluated y value */
+    if ((ret = av_expr_parse_and_eval(&res, (expr = s->x_expr),
+                                      var_names, var_values,
+                                      NULL, NULL, NULL, NULL, NULL, 0, ctx)) < 0)
+        goto eval_fail;
+    s->x              = (int)res;
+    var_values[VAR_X] = res;
+
+    if (s->x < 0 || s->x + inlink->w > s->w) {
+        var_values[VAR_X] = (double)(s->w - inlink->w) / 2.0;
+        s->x              = (int)var_values[VAR_X];
+    }
+    if (s->y < 0 || s->y + inlink->h > s->h) {
+        var_values[VAR_Y] = (double)(s->h - inlink->h) / 2.0;
+        s->y              = (int)var_values[VAR_Y];
+    }
+
+    /* sanity check params */
+    if (s->w < 0 || s->h < 0) {
+        av_log(ctx, AV_LOG_ERROR, "Negative values are not acceptable.\n");
+        return AVERROR(EINVAL);
+    }
+
+    s->w    = ff_draw_round_to_sub(&s->draw, 0, -1, s->w);
+    s->h    = ff_draw_round_to_sub(&s->draw, 1, -1, s->h);
+    s->x    = ff_draw_round_to_sub(&s->draw, 0, -1, s->x);
+    s->y    = ff_draw_round_to_sub(&s->draw, 1, -1, s->y);
+    s->in_w = ff_draw_round_to_sub(&s->draw, 0, -1, inlink->w);
+    s->in_h = ff_draw_round_to_sub(&s->draw, 1, -1, inlink->h);
+    s->inlink_w = inlink->w;
+    s->inlink_h = inlink->h;
+
+    av_log(ctx, AV_LOG_VERBOSE, "w:%d h:%d -> w:%d h:%d x:%d y:%d color:0x%02X%02X%02X%02X\n",
+           inlink->w, inlink->h, s->w, s->h, s->x, s->y,
+           s->rgba_color[0], s->rgba_color[1], s->rgba_color[2], s->rgba_color[3]);
+
+    if (s->x <  0 || s->y <  0                      ||
+        s->w <= 0 || s->h <= 0                      ||
+        (unsigned)s->x + (unsigned)inlink->w > s->w ||
+        (unsigned)s->y + (unsigned)inlink->h > s->h) {
+        av_log(ctx, AV_LOG_ERROR,
+               "Input area %d:%d:%d:%d not within the padded area 0:0:%d:%d or zero-sized\n",
+               s->x, s->y, s->x + inlink->w, s->y + inlink->h, s->w, s->h);
+        return AVERROR(EINVAL);
+    }
+
+    if (s->w > NI_MAX_RESOLUTION_WIDTH || s->h > NI_MAX_RESOLUTION_HEIGHT) {
+        av_log(ctx, AV_LOG_ERROR, "Padded value (%dx%d) > 8192, not allowed\n", s->w, s->h);
+        return AVERROR(EINVAL);
+    }
+
+    return 0;
+
+eval_fail:
+    av_log(ctx, AV_LOG_ERROR,
+           "Error when evaluating the expression '%s'\n", expr);
+    return ret;
+
+}
+
+static int config_output(AVFilterLink *outlink)
+{
+    NetIntPadContext *s = outlink->src->priv;
+    AVHWFramesContext *in_frames_ctx;
+    AVHWFramesContext *out_frames_ctx;
+    AVFilterContext *ctx;
+
+    outlink->w = s->w;
+    outlink->h = s->h;
+
+    ctx           = (AVFilterContext *)outlink->src;
+    in_frames_ctx = (AVHWFramesContext *)ctx->inputs[0]->hw_frames_ctx->data;
+
+    if (in_frames_ctx->sw_format == AV_PIX_FMT_BGRP ||
+        in_frames_ctx->sw_format == AV_PIX_FMT_YUYV422 ||
+        in_frames_ctx->sw_format == AV_PIX_FMT_UYVY422) {
+        av_log(ctx, AV_LOG_ERROR, "bgrp/yuyv/uyvy not supported\n");
+        return AVERROR(EINVAL);
+    }
+
+    s->out_frames_ref = av_hwframe_ctx_alloc(in_frames_ctx->device_ref);
+    if (!s->out_frames_ref)
+        return AVERROR(ENOMEM);
+
+    out_frames_ctx = (AVHWFramesContext *)s->out_frames_ref->data;
+
+    out_frames_ctx->format    = AV_PIX_FMT_NI_QUAD;
+    out_frames_ctx->width     = s->w;
+    out_frames_ctx->height    = s->h;
+    out_frames_ctx->sw_format = in_frames_ctx->sw_format;
+    out_frames_ctx->initial_pool_size =
+        NI_PAD_ID; // Repurposed as identity code
+
+    av_buffer_unref(&ctx->outputs[0]->hw_frames_ctx);
+    ctx->outputs[0]->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    if (!ctx->outputs[0]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return 0;
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in)
+{
+    NetIntPadContext *s = inlink->dst->priv;
+    AVFilterLink *outlink = inlink->dst->outputs[0];
+    AVFrame *out = NULL;
+    niFrameSurface1_t* frame_surface,*new_frame_surface;
+    AVHWFramesContext *pAVHFWCtx;
+    AVNIDeviceContext *pAVNIDevCtx;
+    ni_retcode_t retcode;
+    uint32_t ui32RgbaColor, scaler_format;
+    uint16_t tempFID;
+    int cardno;
+
+    frame_surface = (niFrameSurface1_t *) in->data[3];
+    if (frame_surface == NULL) {
+        return AVERROR(EINVAL);
+    }
+
+    pAVHFWCtx = (AVHWFramesContext *) in->hw_frames_ctx->data;
+    pAVNIDevCtx       = (AVNIDeviceContext *)pAVHFWCtx->device_ctx->hwctx;
+    cardno            = ni_get_cardno(in);
+
+    if (!s->initialized) {
+        retcode = ni_device_session_context_init(&s->api_ctx);
+        if (retcode < 0) {
+            av_log(inlink->dst, AV_LOG_ERROR,
+                   "ni pad filter session context init failure\n");
+            goto fail;
+        }
+
+        s->api_ctx.device_handle = pAVNIDevCtx->cards[cardno];
+        s->api_ctx.blk_io_handle = pAVNIDevCtx->cards[cardno];
+
+        s->api_ctx.hw_id             = cardno;
+        s->api_ctx.device_type       = NI_DEVICE_TYPE_SCALER;
+        s->api_ctx.scaler_operation  = NI_SCALER_OPCODE_PAD;
+        s->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+
+        retcode = ni_device_session_open(&s->api_ctx, NI_DEVICE_TYPE_SCALER);
+        if (retcode < 0) {
+            av_log(inlink->dst, AV_LOG_ERROR,
+                   "Can't open device session on card %d\n", cardno);
+            goto fail;
+        }
+
+        s->session_opened = 1;
+
+        retcode = init_out_pool(inlink->dst);
+
+        if (retcode < 0)
+        {
+            av_log(inlink->dst, AV_LOG_ERROR,
+                   "Internal output allocation failed rc = %d\n", retcode);
+            goto fail;
+        }
+
+        ff_ni_clone_hwframe_ctx(pAVHFWCtx,
+                                (AVHWFramesContext *)s->out_frames_ref->data);
+
+        if (in->color_range == AVCOL_RANGE_JPEG) {
+            av_log(inlink->dst, AV_LOG_ERROR,
+                   "WARNING: Full color range input, limited color output\n");
+        }
+
+        s->initialized = 1;
+    }
+
+    scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(pAVHFWCtx->sw_format);
+
+    retcode = ni_frame_buffer_alloc_hwenc(&s->api_dst_frame.data.frame,
+                                          outlink->w,
+                                          outlink->h,
+                                          0);
+
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    av_log(inlink->dst, AV_LOG_DEBUG,
+           "inlink->w = %d;inlink->h = %d;outlink->w = %d;outlink->h = %d\n",
+           inlink->w, inlink->h, outlink->w, outlink->h);
+    av_log(inlink->dst, AV_LOG_DEBUG,
+           "s->w=%d;s->h=%d;s->x=%d;s->y=%d;c=%02x:%02x:%02x:%02x\n", s->w,
+           s->h, s->x, s->y, s->rgba_color[0], s->rgba_color[1],
+           s->rgba_color[2], s->rgba_color[3]);
+
+    /*
+     * Allocate device input frame. This call won't actually allocate a frame,
+     * but sends the incoming hardware frame index to the scaler manager
+     */
+    retcode = ni_device_alloc_frame(&s->api_ctx,            //
+                                    FFALIGN(in->width, 2),  //
+                                    FFALIGN(in->height, 2), //
+                                    scaler_format,          //
+                                    0,                      // input frame
+                                    in->width,  // src rectangle width
+                                    in->height, // src rectangle height
+                                    0,          // src rectangle x = 0
+                                    0,          // src rectangle y = 0
+                                    frame_surface->ui32nodeAddress,
+                                    frame_surface->ui16FrameIdx,
+                                    NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        av_log(inlink->dst, AV_LOG_DEBUG, "Can't allocate device input frame %d\n", retcode);
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    /* Scaler uses BGRA color, or ARGB in little-endian */
+    ui32RgbaColor = (s->rgba_color[3] << 24) | (s->rgba_color[0] << 16) |
+                    (s->rgba_color[1] << 8) | s->rgba_color[2];
+
+    if ((outlink->w != s->frame_out.picture_width) ||
+        (outlink->h != s->frame_out.picture_height) ||
+        (scaler_format != s->frame_out.picture_format) ||
+        (in->width != s->frame_out.rectangle_width) ||
+        (in->height != s->frame_out.rectangle_height) ||
+        (s->x != s->frame_out.rectangle_x) ||
+        (s->y != s->frame_out.rectangle_y) ||
+        (ui32RgbaColor != s->frame_out.rgba_color)) {
+        s->frame_out.picture_width    = outlink->w;
+        s->frame_out.picture_height   = outlink->h;
+        s->frame_out.picture_format   = scaler_format;
+        s->frame_out.rectangle_width  = in->width;
+        s->frame_out.rectangle_height = in->height;
+        s->frame_out.rectangle_x      = s->x;
+        s->frame_out.rectangle_y      = s->y;
+        s->frame_out.rgba_color       = ui32RgbaColor;
+
+        /* Allocate device destination frame. This acquires a frame from the
+         * pool
+         */
+        retcode = ni_device_alloc_frame(&s->api_ctx,            //
+                                        FFALIGN(outlink->w, 2), //
+                                        FFALIGN(outlink->h, 2), //
+                                        scaler_format,          //
+                                        NI_SCALER_FLAG_IO,      //
+                                        in->width,              //
+                                        in->height,             //
+                                        s->x,                   //
+                                        s->y,                   //
+                                        ui32RgbaColor,          //
+                                        -1,                     //
+                                        NI_DEVICE_TYPE_SCALER);
+
+        if (retcode != NI_RETCODE_SUCCESS) {
+            av_log(inlink->dst, AV_LOG_DEBUG,
+                   "Can't allocate device output frame %d\n", retcode);
+            retcode = AVERROR(ENOMEM);
+            goto fail;
+        }
+    }
+
+    out = av_frame_alloc();
+    if (!out)
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    av_frame_copy_props(out,in);
+
+    out->width  = s->w;
+    out->height = s->h;
+
+    out->format = AV_PIX_FMT_NI_QUAD;
+
+    /* Quadra 2D engine always outputs limited color range */
+    out->color_range = AVCOL_RANGE_MPEG;
+
+    /* Reference the new hw frames context */
+    out->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    out->data[3] = av_malloc(sizeof(niFrameSurface1_t));
+
+    if (!out->data[3])
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    /* Copy the frame surface from the incoming frame */
+    memcpy(out->data[3], in->data[3], sizeof(niFrameSurface1_t));
+
+    /* Set the new frame index */
+    retcode = ni_device_session_read_hwdesc(&s->api_ctx, &s->api_dst_frame,
+                                            NI_DEVICE_TYPE_SCALER);
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(inlink->dst, AV_LOG_ERROR,
+               "Can't acquire output frame %d\n",retcode);
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    tempFID = frame_surface->ui16FrameIdx;
+    frame_surface = (niFrameSurface1_t *) out->data[3];
+    new_frame_surface = (niFrameSurface1_t *) s->api_dst_frame.data.frame.p_data[3];
+    frame_surface->ui16FrameIdx = new_frame_surface->ui16FrameIdx;
+    frame_surface->ui16session_ID = new_frame_surface->ui16session_ID;
+    frame_surface->device_handle = new_frame_surface->device_handle;
+    frame_surface->output_idx     = new_frame_surface->output_idx;
+    frame_surface->src_cpu = new_frame_surface->src_cpu;
+
+    /*Remove ni-split specific assets*/
+    frame_surface->ui32nodeAddress = 0;
+
+    frame_surface->ui16width = out->width;
+    frame_surface->ui16height = out->height;
+
+    av_log(inlink->dst, AV_LOG_DEBUG,
+           "vf_pad_ni.c:IN trace ui16FrameIdx = [%d] --> out [%d] \n", tempFID,
+           frame_surface->ui16FrameIdx);
+
+    out->buf[0] = av_buffer_create(out->data[3], sizeof(niFrameSurface1_t), ff_ni_frame_free, NULL, 0);
+
+    av_frame_free(&in);
+
+    return ff_filter_frame(inlink->dst->outputs[0], out);
+
+fail:
+    av_frame_free(&in);
+    av_frame_free(&out);
+    return retcode;
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    NetIntPadContext *s = ctx->priv;
+
+    if (s->api_dst_frame.data.frame.p_buffer)
+        ni_frame_buffer_free(&s->api_dst_frame.data.frame);
+
+    if (s->session_opened) {
+        /* Close operation will free the device frames */
+        ni_device_session_close(&s->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ni_device_session_context_clear(&s->api_ctx);
+    }
+
+    av_buffer_unref(&s->out_frames_ref);
+}
+
+
+#define OFFSET(x) offsetof(NetIntPadContext, x)
+#define FLAGS (AV_OPT_FLAG_FILTERING_PARAM | AV_OPT_FLAG_VIDEO_PARAM)
+
+static const AVOption pad_options[] = {
+    { "width",  "set the pad area width expression",       OFFSET(w_expr), AV_OPT_TYPE_STRING, {.str = "iw"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "w",      "set the pad area width expression",       OFFSET(w_expr), AV_OPT_TYPE_STRING, {.str = "iw"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "height", "set the pad area height expression",      OFFSET(h_expr), AV_OPT_TYPE_STRING, {.str = "ih"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "h",      "set the pad area height expression",      OFFSET(h_expr), AV_OPT_TYPE_STRING, {.str = "ih"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "x",      "set the x offset expression for the input image position", OFFSET(x_expr), AV_OPT_TYPE_STRING, {.str = "0"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "y",      "set the y offset expression for the input image position", OFFSET(y_expr), AV_OPT_TYPE_STRING, {.str = "0"}, CHAR_MIN, CHAR_MAX, FLAGS },
+    { "color",  "set the color of the padded area border", OFFSET(rgba_color), AV_OPT_TYPE_COLOR, {.str = "black"}, .flags = FLAGS },
+    { "aspect",  "pad to fit an aspect instead of a resolution", OFFSET(aspect), AV_OPT_TYPE_RATIONAL, {.dbl = 0}, 0, DBL_MAX, FLAGS },
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSET(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     FLAGS,
+     "keep_alive_timeout"},
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(pad);
+
+static const AVFilterPad avfilter_vf_pad_inputs[] = {
+    {
+        .name             = "default",
+        .type             = AVMEDIA_TYPE_VIDEO,
+        .config_props     = config_input,
+        .filter_frame     = filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad avfilter_vf_pad_outputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_output,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_pad_ni_quadra = {
+    .name          = "ni_quadra_pad",
+    .description   = NULL_IF_CONFIG_SMALL("NetInt Quadra pad the input video v" NI_XCODER_REVISION),
+    .priv_size     = sizeof(NetIntPadContext),
+    .priv_class    = &pad_class,
+    .uninit        = uninit,
+    .flags_internal= FF_FILTER_FLAG_HWFRAME_AWARE,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(avfilter_vf_pad_inputs),
+    FILTER_OUTPUTS(avfilter_vf_pad_outputs),
+    FILTER_QUERY_FUNC(query_formats),
+#else
+    .inputs        = avfilter_vf_pad_inputs,
+    .outputs       = avfilter_vf_pad_outputs,
+    .query_formats = query_formats,
+#endif
+};
diff --git a/libavfilter/vf_roi_ni.c b/libavfilter/vf_roi_ni.c
new file mode 100644
index 0000000000..2c18b704e1
--- /dev/null
+++ b/libavfilter/vf_roi_ni.c
@@ -0,0 +1,1339 @@
+/*
+ * Copyright (c) 2022 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <float.h>
+#include <math.h>
+#include <stdio.h>
+#include <string.h>
+#include <unistd.h>
+
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#if HAVE_IO_H
+#include <io.h>
+#endif
+#include "libavutil/avassert.h"
+#include "libavutil/avstring.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/internal.h"
+#include "libavutil/mathematics.h"
+#include "libavutil/opt.h"
+#include "libavutil/parseutils.h"
+#include "libavutil/pixdesc.h"
+#include "libswscale/swscale.h"
+#include "ni_device_api.h"
+#include "ni_util.h"
+#include "nifilter.h"
+#include "video.h"
+
+#define NI_NUM_FRAMES_IN_QUEUE 8
+
+typedef struct _ni_roi_network_layer {
+    int32_t width;
+    int32_t height;
+    int32_t channel;
+    int32_t classes;
+    int32_t component;
+    int32_t mask[3];
+    float biases[12];
+    int32_t output_number;
+    float *output;
+} ni_roi_network_layer_t;
+
+typedef struct _ni_roi_network {
+    int32_t netw;
+    int32_t neth;
+    ni_network_data_t raw;
+    ni_roi_network_layer_t *layers;
+} ni_roi_network_t;
+
+typedef struct box {
+    float x, y, w, h;
+} box;
+
+typedef struct detection {
+    box bbox;
+    float objectness;
+    int classes;
+    int color;
+    float *prob;
+    int prob_class;
+    float max_prob;
+} detection;
+
+typedef struct detetion_cache {
+    detection *dets;
+    int capacity;
+    int dets_num;
+} detection_cache;
+
+struct roi_box {
+    int left;
+    int right;
+    int top;
+    int bottom;
+    int color;
+    float objectness;
+    int cls;
+};
+
+typedef struct HwScaleContext {
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+} HwScaleContext;
+
+typedef struct AiContext {
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_src_frame;
+    ni_session_data_io_t api_dst_pkt;
+} AiContext;
+
+typedef struct NetIntRoiContext {
+    const AVClass *class;
+    const char *nb_file;  /* path to network binary */
+    AVRational qp_offset; /* default qp offset. */
+    int initialized;
+    int devid;
+    float obj_thresh;
+    float nms_thresh;
+
+    AiContext *ai_ctx;
+
+    AVBufferRef *out_frames_ref;
+
+    ni_roi_network_t network;
+    detection_cache det_cache;
+    struct SwsContext *img_cvt_ctx;
+    AVFrame rgb_picture;
+
+    HwScaleContext *hws_ctx;
+    int keep_alive_timeout; /* keep alive timeout setting */
+} NetIntRoiContext;
+
+/* class */
+// int g_masks[2][3] = { { 3, 4, 5 }, { 1, 2, 3 } };
+// float g_biases[] = { 10, 14, 23, 27, 37, 58, 81, 82, 135, 169, 344, 319 };
+
+/* human face */
+static int g_masks[2][3] = {{3, 4, 5}, {0, 1, 2}};
+static float g_biases[] = {10, 16, 25, 37, 49, 71, 85, 118, 143, 190, 274, 283};
+
+static int entry_index(ni_roi_network_layer_t *l, int batch, int location,
+                       int entry) {
+    int n   = location / (l->width * l->height);
+    int loc = location % (l->width * l->height);
+    return batch * l->output_number +
+           n * l->width * l->height * (4 + l->classes + 1) +
+           entry * l->width * l->height + loc;
+}
+
+static float sigmoid(float x) {
+    return (float)(1.0 / (1.0 + (float)exp((double)(-x))));
+}
+
+/*
+ * nw: network input width
+ * nh: network input height
+ * lw: layer width
+ * lh: layer height
+ */
+static box get_yolo_box(float *x, float *biases, int n, int index, int col,
+                        int row, int lw, int lh, int nw, int nh, int stride) {
+    box b;
+
+    b.x = (float)((float)col + sigmoid(x[index + 0 * stride])) / (float)lw;
+    b.y = (float)((float)row + sigmoid(x[index + 1 * stride])) / (float)lh;
+    b.w = (float)exp((double)x[index + 2 * stride]) * biases[2 * n] / (float)nw;
+    b.h = (float)exp((double)x[index + 3 * stride]) * biases[2 * n + 1] /
+          (float)nh;
+
+    b.x -= (float)(b.w / 2.0);
+    b.y -= (float)(b.h / 2.0);
+
+    return b;
+}
+
+static int get_yolo_detections(void *ctx, ni_roi_network_layer_t *l, int netw,
+                               int neth, float thresh,
+                               detection_cache *det_cache, int *dets_num) {
+    int i, n, k;
+    float *predictions = l->output;
+    float max_prob;
+    int prob_class;
+    // This snippet below is not necessary
+    // Need to comment it in order to batch processing >= 2 images
+    // if (l.batch == 2) avg_flipped_yolo(l);
+    int count       = 0;
+    detection *dets = det_cache->dets;
+
+    *dets_num = 0;
+
+    av_log(ctx, AV_LOG_TRACE,
+           "pic %dx%d, comp=%d, class=%d, net %dx%d, thresh=%f\n", l->width,
+           l->height, l->component, l->classes, netw, neth, thresh);
+    for (i = 0; i < l->width * l->height; ++i) {
+        int row = i / l->width;
+        int col = i % l->width;
+        for (n = 0; n < l->component; ++n) {
+            int obj_index = entry_index(l, 0, n * l->width * l->height + i, 4);
+            float objectness = predictions[obj_index];
+            objectness       = sigmoid(objectness);
+
+            prob_class = -1;
+            max_prob   = thresh;
+            for (k = 0; k < l->classes; k++) {
+                int class_index =
+                    entry_index(l, 0, n * l->width * l->height + i, 4 + 1 + k);
+                double prob = objectness * sigmoid(predictions[class_index]);
+                if (prob >= max_prob) {
+                    prob_class = k;
+                    max_prob   = (float)prob;
+                }
+            }
+
+            if (prob_class >= 0) {
+                box bbox;
+                int box_index =
+                    entry_index(l, 0, n * l->width * l->height + i, 0);
+
+                if (det_cache->dets_num >= det_cache->capacity) {
+                    dets =
+                        realloc(det_cache->dets,
+                                sizeof(detection) * (det_cache->capacity + 10));
+                    if (!dets) {
+                        av_log(ctx, AV_LOG_ERROR,
+                               "failed to realloc detections capacity %d\n",
+                               det_cache->capacity);
+                        return AVERROR(ENOMEM);
+                    }
+                    det_cache->dets = dets;
+                    det_cache->capacity += 10;
+                    if (det_cache->capacity >= 100) {
+                        av_log(ctx, AV_LOG_WARNING, "too many detections %d\n",
+                               det_cache->dets_num);
+                    }
+                }
+
+                av_log(ctx, AV_LOG_TRACE, "max_prob %f, class %d\n", max_prob,
+                       prob_class);
+                bbox = get_yolo_box(predictions, l->biases, l->mask[n],
+                                    box_index, col, row, l->width, l->height,
+                                    netw, neth, l->width * l->height);
+
+                dets[det_cache->dets_num].max_prob   = max_prob;
+                dets[det_cache->dets_num].prob_class = prob_class;
+                dets[det_cache->dets_num].bbox       = bbox;
+                dets[det_cache->dets_num].objectness = objectness;
+                dets[det_cache->dets_num].classes    = l->classes;
+                dets[det_cache->dets_num].color      = n;
+
+                av_log(ctx, AV_LOG_TRACE, "%d, x %f, y %f, w %f, h %f\n",
+                       det_cache->dets_num, dets[det_cache->dets_num].bbox.x,
+                       dets[det_cache->dets_num].bbox.y,
+                       dets[det_cache->dets_num].bbox.w,
+                       dets[det_cache->dets_num].bbox.h);
+                det_cache->dets_num++;
+                count++;
+            }
+        }
+    }
+    *dets_num = count;
+    return 0;
+}
+
+static int nms_comparator(const void *pa, const void *pb) {
+    detection *a = (detection *)pa;
+    detection *b = (detection *)pb;
+
+    if (a->prob_class > b->prob_class)
+        return 1;
+    else if (a->prob_class < b->prob_class)
+        return -1;
+    else {
+        if (a->max_prob < b->max_prob)
+            return 1;
+        else if (a->max_prob > b->max_prob)
+            return -1;
+    }
+    return 0;
+}
+
+static float overlap(float x1, float w1, float x2, float w2) {
+    float l1    = x1 - w1 / 2;
+    float l2    = x2 - w2 / 2;
+    float left  = l1 > l2 ? l1 : l2;
+    float r1    = x1 + w1 / 2;
+    float r2    = x2 + w2 / 2;
+    float right = r1 < r2 ? r1 : r2;
+    return right - left;
+}
+
+static float box_intersection(box a, box b) {
+    float w = overlap(a.x, a.w, b.x, b.w);
+    float h = overlap(a.y, a.h, b.y, b.h);
+    float area;
+
+    if (w < 0 || h < 0)
+        return 0;
+
+    area = w * h;
+    return area;
+}
+
+static float box_union(box a, box b) {
+    float i = box_intersection(a, b);
+    float u = a.w * a.h + b.w * b.h - i;
+    return u;
+}
+
+static float box_iou(box a, box b) {
+    // return box_intersection(a, b)/box_union(a, b);
+
+    float I = box_intersection(a, b);
+    float U = box_union(a, b);
+    if (I == 0 || U == 0)
+        return 0;
+
+    return I / U;
+}
+
+static int nms_sort(void *ctx, detection *dets, int dets_num,
+                    float nms_thresh) {
+    int i, j;
+    box boxa, boxb;
+
+    for (i = 0; i < (dets_num - 1); i++) {
+        int class = dets[i].prob_class;
+        if (dets[i].max_prob == 0)
+            continue;
+
+        if (dets[i].prob_class != dets[i + 1].prob_class)
+            continue;
+
+        boxa = dets[i].bbox;
+        for (j = i + 1; j < dets_num && dets[j].prob_class == class; j++) {
+            if (dets[j].max_prob == 0)
+                continue;
+
+            boxb = dets[j].bbox;
+            if (box_iou(boxa, boxb) > nms_thresh)
+                dets[j].max_prob = 0;
+        }
+    }
+
+    return 0;
+}
+
+static int resize_coords(void *ctx, detection *dets, int dets_num,
+                         uint32_t img_width, uint32_t img_height,
+                         struct roi_box **roi_box, int *roi_num) {
+    int i;
+    int left, right, top, bot;
+    struct roi_box *rbox;
+    int rbox_num = 0;
+
+    if (dets_num == 0) {
+        return 0;
+    }
+
+    rbox = malloc(sizeof(struct roi_box) * dets_num);
+    if (!rbox)
+        return AVERROR(ENOMEM);
+
+    for (i = 0; i < dets_num; i++) {
+        av_log(ctx, AV_LOG_TRACE, "index %d, max_prob %f, class %d\n", i,
+               dets[i].max_prob, dets[i].prob_class);
+        if (dets[i].max_prob == 0)
+            continue;
+
+        top   = (int)floor(dets[i].bbox.y * img_height + 0.5);
+        left  = (int)floor(dets[i].bbox.x * img_width + 0.5);
+        right = (int)floor((dets[i].bbox.x + dets[i].bbox.w) * img_width + 0.5);
+        bot = (int)floor((dets[i].bbox.y + dets[i].bbox.h) * img_height + 0.5);
+
+        if (top < 0)
+            top = 0;
+
+        if (left < 0)
+            left = 0;
+
+        if (right > img_width)
+            right = img_width;
+
+        if (bot > img_height)
+            bot = img_height;
+
+        av_log(ctx, AV_LOG_DEBUG, "top %d, left %d, right %d, bottom %d\n", top,
+               left, right, bot);
+
+        rbox[rbox_num].left       = left;
+        rbox[rbox_num].right      = right;
+        rbox[rbox_num].top        = top;
+        rbox[rbox_num].bottom     = bot;
+        rbox[rbox_num].cls        = dets[i].prob_class;
+        rbox[rbox_num].objectness = dets[i].objectness;
+        rbox[rbox_num].color      = dets[i].color;
+        rbox_num++;
+    }
+
+    if (rbox_num == 0) {
+        free(rbox);
+        *roi_num = rbox_num;
+        *roi_box = NULL;
+    } else {
+        *roi_num = rbox_num;
+        *roi_box = rbox;
+    }
+
+    return 0;
+}
+
+static int ni_get_detections(void *ctx, ni_roi_network_t *network,
+                             detection_cache *det_cache, uint32_t img_width,
+                             uint32_t img_height, float obj_thresh,
+                             float nms_thresh, struct roi_box **roi_box,
+                             int *roi_num) {
+    int i;
+    int ret;
+    int dets_num    = 0;
+    detection *dets = NULL;
+
+    *roi_box = NULL;
+    *roi_num = 0;
+
+    for (i = 0; i < network->raw.output_num; i++) {
+        ret = get_yolo_detections(ctx, &network->layers[i], network->netw,
+                                  network->neth, obj_thresh, det_cache,
+                                  &dets_num);
+        if (ret != 0) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "failed to get yolo detection at layer %d\n", i);
+            return ret;
+        }
+        av_log(ctx, AV_LOG_TRACE, "layer %d, yolo detections: %d\n", i,
+               dets_num);
+    }
+
+    if (det_cache->dets_num == 0)
+        return 0;
+
+    dets     = det_cache->dets;
+    dets_num = det_cache->dets_num;
+    for (i = 0; i < dets_num; i++) {
+        av_log(ctx, AV_LOG_TRACE,
+               "orig dets %d: x %f,y %f,w %f,h %f,c %d,p %f\n", i,
+               dets[i].bbox.x, dets[i].bbox.y, dets[i].bbox.w, dets[i].bbox.h,
+               dets[i].prob_class, dets[i].max_prob);
+    }
+
+    qsort(dets, dets_num, sizeof(detection), nms_comparator);
+    for (i = 0; i < dets_num; i++) {
+        av_log(ctx, AV_LOG_TRACE,
+               "sorted dets %d: x %f,y %f,w %f,h %f,c %d,p %f\n", i,
+               dets[i].bbox.x, dets[i].bbox.y, dets[i].bbox.w, dets[i].bbox.h,
+               dets[i].prob_class, dets[i].max_prob);
+    }
+
+    nms_sort(ctx, dets, dets_num, nms_thresh);
+    ret = resize_coords(ctx, dets, dets_num, img_width, img_height, roi_box,
+                        roi_num);
+    if (ret != 0) {
+        av_log(ctx, AV_LOG_ERROR, "cannot resize coordinates\n");
+        return ret;
+    }
+
+    return 0;
+}
+
+static int ni_roi_query_formats(AVFilterContext *ctx) {
+    AVFilterFormats *formats;
+
+    static const enum AVPixelFormat pix_fmts[] = {
+        AV_PIX_FMT_NI_QUAD,
+        AV_PIX_FMT_YUV420P,
+        AV_PIX_FMT_NONE,
+    };
+
+    formats = ff_make_format_list(pix_fmts);
+    if (!formats)
+        return AVERROR(ENOMEM);
+
+    return ff_set_common_formats(ctx, formats);
+}
+
+static void cleanup_ai_context(AVFilterContext *ctx, NetIntRoiContext *s) {
+    ni_retcode_t retval;
+    AiContext *ai_ctx = s->ai_ctx;
+
+    if (ai_ctx) {
+        ni_frame_buffer_free(&ai_ctx->api_src_frame.data.frame);
+        ni_packet_buffer_free(&ai_ctx->api_dst_pkt.data.packet);
+
+        retval =
+            ni_device_session_close(&ai_ctx->api_ctx, 1, NI_DEVICE_TYPE_AI);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "%s: failed to close ai session. retval %d\n", __func__,
+                   retval);
+        }
+        ni_device_session_context_clear(&ai_ctx->api_ctx);
+        av_free(ai_ctx);
+        s->ai_ctx = NULL;
+    }
+}
+
+static int init_ai_context(AVFilterContext *ctx, NetIntRoiContext *s,
+                           AVFrame *frame) {
+    ni_retcode_t retval;
+    AiContext *ai_ctx;
+    ni_roi_network_t *network = &s->network;
+    int ret;
+    int hwframe = frame->format == AV_PIX_FMT_NI_QUAD ? 1 : 0;
+
+#if HAVE_IO_H
+    if ((s->nb_file == NULL) || (_access(s->nb_file, R_OK) != 0)) {
+#else
+    if ((s->nb_file == NULL) || (access(s->nb_file, R_OK) != 0)) {
+#endif
+        av_log(ctx, AV_LOG_ERROR, "invalid network binary path\n");
+        return AVERROR(EINVAL);
+    }
+
+    ai_ctx = av_mallocz(sizeof(AiContext));
+    if (!ai_ctx) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate ai context\n");
+        return AVERROR(ENOMEM);
+    }
+
+    retval = ni_device_session_context_init(&ai_ctx->api_ctx);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "ai session context init failure\n");
+        return AVERROR(EIO);
+    }
+
+    if (hwframe) {
+        AVHWFramesContext *pAVHFWCtx;
+        AVNIDeviceContext *pAVNIDevCtx;
+        int cardno;
+
+        pAVHFWCtx   = (AVHWFramesContext *)frame->hw_frames_ctx->data;
+        pAVNIDevCtx = (AVNIDeviceContext *)pAVHFWCtx->device_ctx->hwctx;
+        cardno      = ni_get_cardno(frame);
+
+        ai_ctx->api_ctx.device_handle = pAVNIDevCtx->cards[cardno];
+        ai_ctx->api_ctx.blk_io_handle = pAVNIDevCtx->cards[cardno];
+        ai_ctx->api_ctx.hw_action     = NI_CODEC_HW_ENABLE;
+        ai_ctx->api_ctx.hw_id         = cardno;
+    } else
+        ai_ctx->api_ctx.hw_id = s->devid;
+
+    ai_ctx->api_ctx.device_type = NI_DEVICE_TYPE_AI;
+    ai_ctx->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+    retval = ni_device_session_open(&ai_ctx->api_ctx, NI_DEVICE_TYPE_AI);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to open ai session. retval %d\n",
+               retval);
+        return AVERROR(EIO);
+    }
+
+    retval = ni_ai_config_network_binary(&ai_ctx->api_ctx, &network->raw,
+                                         s->nb_file);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to configure ai session. retval %d\n",
+               retval);
+        ret = AVERROR(EIO);
+        goto failed_out;
+    }
+
+    if (!hwframe) {
+        retval = ni_ai_frame_buffer_alloc(&ai_ctx->api_src_frame.data.frame,
+                                          &network->raw);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR, "failed to allocate ni frame\n");
+            ret = AVERROR(ENOMEM);
+            goto failed_out;
+        }
+    }
+
+    retval = ni_ai_packet_buffer_alloc(&ai_ctx->api_dst_pkt.data.packet,
+                                       &network->raw);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate ni packet\n");
+        ret = AVERROR(ENOMEM);
+        goto failed_out;
+    }
+
+    s->ai_ctx = ai_ctx;
+    return 0;
+
+failed_out:
+    cleanup_ai_context(ctx, s);
+    return ret;
+}
+
+static void ni_destroy_network(AVFilterContext *ctx,
+                               ni_roi_network_t *network) {
+    if (network) {
+        int i;
+
+        for (i = 0; i < network->raw.output_num; i++) {
+            if (network->layers[i].output) {
+                free(network->layers[i].output);
+                network->layers[i].output = NULL;
+            }
+        }
+
+        free(network->layers);
+        network->layers = NULL;
+    }
+}
+
+static int ni_create_network(AVFilterContext *ctx, ni_roi_network_t *network) {
+    int ret;
+    int i;
+    ni_network_data_t *ni_network = &network->raw;
+
+    av_log(ctx, AV_LOG_VERBOSE, "network input number %d, output number %d\n",
+           ni_network->input_num, ni_network->output_num);
+
+    if (ni_network->input_num == 0 || ni_network->output_num == 0) {
+        av_log(ctx, AV_LOG_ERROR, "invalid network layer\n");
+        return AVERROR(EINVAL);
+    }
+
+    /* only support one input for now */
+    if (ni_network->input_num != 1) {
+        av_log(ctx, AV_LOG_ERROR,
+               "network input layer number %d not supported\n",
+               ni_network->input_num);
+        return AVERROR(EINVAL);
+    }
+
+    /*
+     * create network and its layers. i don't know whether this is platform
+     * specific or not. maybe i shall add a create network api to do this.
+     */
+    network->layers =
+        malloc(sizeof(ni_roi_network_layer_t) * ni_network->output_num);
+    if (!network->layers) {
+        av_log(ctx, AV_LOG_ERROR, "cannot allocate network layer memory\n");
+        return AVERROR(ENOMEM);
+    }
+
+    for (i = 0; i < ni_network->output_num; i++) {
+        network->layers[i].width     = ni_network->linfo.out_param[i].sizes[0];
+        network->layers[i].height    = ni_network->linfo.out_param[i].sizes[1];
+        network->layers[i].channel   = ni_network->linfo.out_param[i].sizes[2];
+        network->layers[i].component = 3;
+        network->layers[i].classes =
+            (network->layers[i].channel / network->layers[i].component) -
+            (4 + 1);
+        network->layers[i].output_number =
+            ni_ai_network_layer_dims(&ni_network->linfo.out_param[i]);
+        av_assert0(network->layers[i].output_number ==
+                   network->layers[i].width * network->layers[i].height *
+                       network->layers[i].channel);
+
+        network->layers[i].output =
+            malloc(network->layers[i].output_number * sizeof(float));
+        if (!network->layers[i].output) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "failed to allocate network layer %d output buffer\n", i);
+            ret = AVERROR(ENOMEM);
+            goto out;
+        }
+        memcpy(network->layers[i].mask, &g_masks[i][0],
+               sizeof(network->layers[i].mask));
+        memcpy(network->layers[i].biases, &g_biases[0],
+               sizeof(network->layers[i].biases));
+
+        av_log(ctx, AV_LOG_DEBUG,
+               "network layer %d: w %d, h %d, ch %d, co %d, cl %d\n", i,
+               network->layers[i].width, network->layers[i].height,
+               network->layers[i].channel, network->layers[i].component,
+               network->layers[i].classes);
+    }
+
+    network->netw = ni_network->linfo.in_param[0].sizes[0];
+    network->neth = ni_network->linfo.in_param[0].sizes[1];
+
+    return 0;
+out:
+    ni_destroy_network(ctx, network);
+    return ret;
+}
+
+static av_cold int init_hwframe_scale(AVFilterContext *ctx, NetIntRoiContext *s,
+                                      enum AVPixelFormat format,
+                                      AVFrame *frame) {
+    ni_retcode_t retval;
+    HwScaleContext *hws_ctx;
+    int ret;
+    AVHWFramesContext *pAVHFWCtx;
+    AVNIDeviceContext *pAVNIDevCtx;
+    int cardno;
+
+    hws_ctx = av_mallocz(sizeof(HwScaleContext));
+    if (!hws_ctx) {
+        av_log(ctx, AV_LOG_ERROR, "could not allocate hwframe ctx\n");
+        return AVERROR(ENOMEM);
+    }
+
+    retval = ni_device_session_context_init(&hws_ctx->api_ctx);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "hw scaler session context init failure\n");
+        return AVERROR(EIO);
+    }
+
+    pAVHFWCtx   = (AVHWFramesContext *)frame->hw_frames_ctx->data;
+    pAVNIDevCtx = (AVNIDeviceContext *)pAVHFWCtx->device_ctx->hwctx;
+    cardno      = ni_get_cardno(frame);
+
+    hws_ctx->api_ctx.device_handle     = pAVNIDevCtx->cards[cardno];
+    hws_ctx->api_ctx.blk_io_handle     = pAVNIDevCtx->cards[cardno];
+    hws_ctx->api_ctx.device_type       = NI_DEVICE_TYPE_SCALER;
+    hws_ctx->api_ctx.scaler_operation  = NI_SCALER_OPCODE_SCALE;
+    hws_ctx->api_ctx.hw_id             = cardno;
+    hws_ctx->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+
+    retval = ni_device_session_open(&hws_ctx->api_ctx, NI_DEVICE_TYPE_SCALER);
+    if (retval < 0) {
+        av_log(ctx, AV_LOG_ERROR, "could not open scaler session\n");
+        ret = AVERROR(EIO);
+        goto out;
+    }
+
+    /* Create scale frame pool on device */
+    retval = ff_ni_build_frame_pool(&hws_ctx->api_ctx, s->network.netw,
+                                    s->network.neth, format,
+                                    DEFAULT_NI_FILTER_POOL_SIZE);
+    if (retval < 0) {
+        av_log(ctx, AV_LOG_ERROR, "could not build frame pool\n");
+        ni_device_session_close(&hws_ctx->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ni_device_session_context_clear(&hws_ctx->api_ctx);
+        ret = AVERROR(EIO);
+        goto out;
+    }
+
+    s->hws_ctx = hws_ctx;
+    return 0;
+out:
+    av_free(hws_ctx);
+    return ret;
+}
+
+static void cleanup_hwframe_scale(AVFilterContext *ctx, NetIntRoiContext *s) {
+    HwScaleContext *hws_ctx = s->hws_ctx;
+
+    if (hws_ctx) {
+        ni_frame_buffer_free(&hws_ctx->api_dst_frame.data.frame);
+        ni_device_session_close(&hws_ctx->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ni_device_session_context_clear(&hws_ctx->api_ctx);
+
+        av_free(hws_ctx);
+        s->hws_ctx = NULL;
+    }
+}
+
+static int ni_roi_config_input(AVFilterContext *ctx, AVFrame *frame) {
+    NetIntRoiContext *s = ctx->priv;
+    int ret;
+
+    if (s->initialized)
+        return 0;
+
+    ret = init_ai_context(ctx, s, frame);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to initialize ai context\n");
+        return ret;
+    }
+
+    ret = ni_create_network(ctx, &s->network);
+    if (ret != 0) {
+        goto fail_out;
+    }
+
+    if (frame->format != AV_PIX_FMT_NI_QUAD) {
+        memset(&s->rgb_picture, 0, sizeof(s->rgb_picture));
+        s->rgb_picture.width  = s->network.netw;
+        s->rgb_picture.height = s->network.neth;
+        s->rgb_picture.format = AV_PIX_FMT_RGB24;
+        if (av_frame_get_buffer(&s->rgb_picture, 32)) {
+            av_log(ctx, AV_LOG_ERROR, "Out of memory for RGB pack data!\n");
+            goto fail_out;
+        }
+
+        s->img_cvt_ctx = sws_getContext(frame->width, frame->height,
+                                        frame->format, s->network.netw,
+                                        s->network.neth, s->rgb_picture.format,
+                                        SWS_BICUBIC, NULL, NULL, NULL);
+        if (!s->img_cvt_ctx) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "could not create SwsContext for conversion and scaling\n");
+            ret = AVERROR(ENOMEM);
+            goto fail_out;
+        }
+    } else {
+        ret = init_hwframe_scale(ctx, s, AV_PIX_FMT_BGRP, frame);
+        if (ret < 0) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "could not initialized hwframe scale context\n");
+            goto fail_out;
+        }
+    }
+
+    s->initialized = 1;
+    return 0;
+
+fail_out:
+    cleanup_ai_context(ctx, s);
+
+    ni_destroy_network(ctx, &s->network);
+
+    av_frame_unref(&s->rgb_picture);
+    if (s->img_cvt_ctx) {
+        sws_freeContext(s->img_cvt_ctx);
+        s->img_cvt_ctx = NULL;
+    }
+    return ret;
+}
+
+static av_cold int ni_roi_init(AVFilterContext *ctx) {
+    NetIntRoiContext *s = ctx->priv;
+
+    s->det_cache.dets_num = 0;
+    s->det_cache.capacity = 20;
+    s->det_cache.dets     = malloc(sizeof(detection) * s->det_cache.capacity);
+    if (!s->det_cache.dets) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate detection cache\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return 0;
+}
+
+static av_cold void ni_roi_uninit(AVFilterContext *ctx) {
+    NetIntRoiContext *s       = ctx->priv;
+    ni_roi_network_t *network = &s->network;
+
+    cleanup_ai_context(ctx, s);
+
+    ni_destroy_network(ctx, network);
+
+    if (s->det_cache.dets) {
+        free(s->det_cache.dets);
+        s->det_cache.dets = NULL;
+    }
+
+    av_buffer_unref(&s->out_frames_ref);
+    s->out_frames_ref = NULL;
+
+    av_frame_unref(&s->rgb_picture);
+    sws_freeContext(s->img_cvt_ctx);
+    s->img_cvt_ctx = NULL;
+
+    cleanup_hwframe_scale(ctx, s);
+}
+
+static int ni_roi_output_config_props(AVFilterLink *outlink) {
+    AVFilterContext *ctx = outlink->src;
+    AVFilterLink *inlink = outlink->src->inputs[0];
+    AVHWFramesContext *in_frames_ctx;
+    AVHWFramesContext *out_frames_ctx;
+    NetIntRoiContext *s = ctx->priv;
+
+    if (inlink->hw_frames_ctx == NULL)
+        return 0;
+
+    outlink->w = inlink->w;
+    outlink->h = inlink->h;
+
+    in_frames_ctx = (AVHWFramesContext *)ctx->inputs[0]->hw_frames_ctx->data;
+
+    if (in_frames_ctx->sw_format == AV_PIX_FMT_BGRP) {
+        av_log(ctx, AV_LOG_ERROR, "bgrp not supported\n");
+        return AVERROR(EINVAL);
+    }
+
+    s->out_frames_ref = av_hwframe_ctx_alloc(in_frames_ctx->device_ref);
+    if (!s->out_frames_ref)
+        return AVERROR(ENOMEM);
+
+    out_frames_ctx = (AVHWFramesContext *)s->out_frames_ref->data;
+
+    ff_ni_clone_hwframe_ctx(in_frames_ctx, out_frames_ctx);
+
+    out_frames_ctx->format            = AV_PIX_FMT_NI_QUAD;
+    out_frames_ctx->width             = outlink->w;
+    out_frames_ctx->height            = outlink->h;
+    out_frames_ctx->sw_format         = in_frames_ctx->sw_format;
+    out_frames_ctx->initial_pool_size = NI_ROI_ID;
+
+    av_hwframe_ctx_init(s->out_frames_ref);
+
+    av_buffer_unref(&ctx->outputs[0]->hw_frames_ctx);
+    ctx->outputs[0]->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    if (!ctx->outputs[0]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return 0;
+}
+
+static int ni_read_roi(AVFilterContext *ctx, ni_session_data_io_t *p_dst_pkt,
+                       AVFrame *out, int pic_width, int pic_height) {
+    NetIntRoiContext *s = ctx->priv;
+    ni_retcode_t retval;
+    ni_roi_network_t *network = &s->network;
+    AVFrameSideData *sd;
+    AVRegionOfInterest *roi;
+    struct roi_box *roi_box = NULL;
+    int roi_num             = 0;
+    int ret;
+    int i;
+    int width, height;
+
+    for (i = 0; i < network->raw.output_num; i++) {
+        retval = ni_network_layer_convert_output(
+            network->layers[i].output,
+            network->layers[i].output_number * sizeof(float),
+            &p_dst_pkt->data.packet, &network->raw, i);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "failed to read layer %d output. retval %d\n", i, retval);
+            return AVERROR(EIO);
+        }
+    }
+
+    width  = pic_width;
+    height = pic_height;
+
+    s->det_cache.dets_num = 0;
+    ret = ni_get_detections(ctx, network, &s->det_cache, width, height,
+                            s->obj_thresh, s->nms_thresh, &roi_box, &roi_num);
+    if (ret < 0) {
+        av_log(ctx, AV_LOG_ERROR, "failed to get roi.\n");
+        return ret;
+    }
+
+    if (roi_num == 0) {
+        av_log(ctx, AV_LOG_DEBUG, "no roi available\n");
+        return 0;
+    }
+
+    sd = av_frame_new_side_data(out, AV_FRAME_DATA_REGIONS_OF_INTEREST,
+                                (int)(roi_num * sizeof(AVRegionOfInterest)));
+    if (!sd) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate roi sidedata\n");
+        free(roi_box);
+        return AVERROR(ENOMEM);
+    }
+
+    roi = (AVRegionOfInterest *)sd->data;
+    for (i = 0; i < roi_num; i++) {
+        roi[i].self_size = sizeof(*roi);
+        roi[i].top       = roi_box[i].top;
+        roi[i].bottom    = roi_box[i].bottom;
+        roi[i].left      = roi_box[i].left;
+        roi[i].right     = roi_box[i].right;
+        roi[i].qoffset   = s->qp_offset;
+        av_log(ctx, AV_LOG_DEBUG,
+               "roi %d: top %d, bottom %d, left %d, right %d, qpo %d/%d\n", i,
+               roi[i].top, roi[i].bottom, roi[i].left, roi[i].right,
+               roi[i].qoffset.num, roi[i].qoffset.den);
+    }
+
+    free(roi_box);
+    return 0;
+}
+
+static int ni_recreate_frame(ni_frame_t *ni_frame, AVFrame *frame) {
+    uint8_t *p_data = ni_frame->p_data[0];
+
+    av_log(NULL, AV_LOG_DEBUG,
+           "linesize %d/%d/%d, data %p/%p/%p, pixel %dx%d\n",
+           frame->linesize[0], frame->linesize[1], frame->linesize[2],
+           frame->data[0], frame->data[1], frame->data[2], frame->width,
+           frame->height);
+
+    if (frame->format == AV_PIX_FMT_GBRP) {
+        int i;
+        /* GBRP -> BGRP */
+        for (i = 0; i < frame->height; i++) {
+            memcpy((void *)(p_data + i * frame->linesize[1]),
+                   frame->data[1] + i * frame->linesize[1], frame->linesize[1]);
+        }
+
+        p_data += frame->height * frame->linesize[1];
+        for (i = 0; i < frame->height; i++) {
+            memcpy((void *)(p_data + i * frame->linesize[0]),
+                   frame->data[0] + i * frame->linesize[0], frame->linesize[0]);
+        }
+
+        p_data += frame->height * frame->linesize[0];
+        for (i = 0; i < frame->height; i++) {
+            memcpy((void *)(p_data + i * frame->linesize[2]),
+                   frame->data[2] + i * frame->linesize[2], frame->linesize[2]);
+        }
+    } else if (frame->format == AV_PIX_FMT_RGB24) {
+        /* RGB24 -> BGRP */
+        uint8_t *r_data = p_data + frame->width * frame->height * 2;
+        uint8_t *g_data = p_data + frame->width * frame->height * 1;
+        uint8_t *b_data = p_data + frame->width * frame->height * 0;
+        uint8_t *fdata  = frame->data[0];
+        int x, y;
+
+        av_log(NULL, AV_LOG_DEBUG,
+               "%s(): rgb24 to bgrp, pix %dx%d, linesize %d\n", __func__,
+               frame->width, frame->height, frame->linesize[0]);
+
+        for (y = 0; y < frame->height; y++) {
+            for (x = 0; x < frame->width; x++) {
+                int fpos  = y * frame->linesize[0];
+                int ppos  = y * frame->width;
+                uint8_t r = fdata[fpos + x * 3 + 0];
+                uint8_t g = fdata[fpos + x * 3 + 1];
+                uint8_t b = fdata[fpos + x * 3 + 2];
+
+                r_data[ppos + x] = r;
+                g_data[ppos + x] = g;
+                b_data[ppos + x] = b;
+            }
+        }
+    }
+    return 0;
+}
+
+static int ni_hwframe_scale(AVFilterContext *ctx, NetIntRoiContext *s,
+                            AVFrame *in, int w, int h,
+                            niFrameSurface1_t **filt_frame_surface) {
+    HwScaleContext *scale_ctx = s->hws_ctx;
+    int scaler_format;
+    ni_retcode_t retcode;
+    niFrameSurface1_t *frame_surface, *new_frame_surface;
+    AVHWFramesContext *pAVHFWCtx;
+
+    frame_surface = (niFrameSurface1_t *)in->data[3];
+
+    av_log(ctx, AV_LOG_DEBUG, "in frame surface frameIdx %d\n",
+           frame_surface->ui16FrameIdx);
+
+    pAVHFWCtx = (AVHWFramesContext *)in->hw_frames_ctx->data;
+
+    scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(pAVHFWCtx->sw_format);
+
+    retcode = ni_frame_buffer_alloc_hwenc(&scale_ctx->api_dst_frame.data.frame,
+                                          w, h, 0);
+    if (retcode != NI_RETCODE_SUCCESS)
+        return AVERROR(ENOMEM);
+
+    /*
+     * Allocate device input frame. This call won't actually allocate a frame,
+     * but sends the incoming hardware frame index to the scaler manager
+     */
+    retcode = ni_device_alloc_frame(
+        &scale_ctx->api_ctx, FFALIGN(in->width, 2), FFALIGN(in->height, 2),
+        scaler_format, 0, 0, 0, 0, 0, frame_surface->ui32nodeAddress,
+        frame_surface->ui16FrameIdx, NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(NULL, AV_LOG_DEBUG, "Can't allocate device input frame %d\n",
+               retcode);
+        return AVERROR(ENOMEM);
+    }
+
+    /* Allocate hardware device destination frame. This acquires a frame from
+     * the pool */
+    retcode = ni_device_alloc_frame(
+        &scale_ctx->api_ctx, FFALIGN(w, 2), FFALIGN(h, 2),
+        ff_ni_ffmpeg_to_gc620_pix_fmt(AV_PIX_FMT_BGRP), NI_SCALER_FLAG_IO, 0, 0,
+        0, 0, 0, -1, NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(NULL, AV_LOG_DEBUG, "Can't allocate device output frame %d\n",
+               retcode);
+        return AVERROR(ENOMEM);
+    }
+
+    /* Set the new frame index */
+    ni_device_session_read_hwdesc(
+        &scale_ctx->api_ctx, &scale_ctx->api_dst_frame, NI_DEVICE_TYPE_SCALER);
+    new_frame_surface =
+        (niFrameSurface1_t *)scale_ctx->api_dst_frame.data.frame.p_data[3];
+
+    *filt_frame_surface = new_frame_surface;
+
+    return 0;
+}
+
+static int ni_roi_filter_frame(AVFilterLink *link, AVFrame *in) {
+    AVFilterContext *ctx = link->dst;
+    NetIntRoiContext *s  = ctx->priv;
+    AVFrame *out         = NULL;
+    ni_roi_network_t *network;
+    ni_retcode_t retval;
+    int ret;
+    AiContext *ai_ctx;
+
+    if (in == NULL) {
+        av_log(ctx, AV_LOG_WARNING, "in frame is null\n");
+        return AVERROR(EINVAL);
+    }
+
+    if (!s->initialized) {
+        ret = ni_roi_config_input(ctx, in);
+        if (ret) {
+            av_log(ctx, AV_LOG_ERROR, "failed to config input\n");
+            return ret;
+        }
+    }
+
+    ai_ctx  = s->ai_ctx;
+    network = &s->network;
+    retval  = ni_ai_packet_buffer_alloc(&ai_ctx->api_dst_pkt.data.packet,
+                                       &network->raw);
+    if (retval != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR, "failed to allocate packet\n");
+        return AVERROR(EAGAIN);
+    }
+
+    out = av_frame_clone(in);
+    if (!out)
+        return AVERROR(ENOMEM);
+
+    if (in->format == AV_PIX_FMT_NI_QUAD) {
+        niFrameSurface1_t *filt_frame_surface;
+
+        ret = ni_hwframe_scale(ctx, s, in, network->netw, network->neth,
+                               &filt_frame_surface);
+        if (ret < 0) {
+            av_log(ctx, AV_LOG_ERROR, "Error run hwframe scale\n");
+            goto failed_out;
+        }
+
+        av_log(ctx, AV_LOG_DEBUG, "filt frame surface frameIdx %d\n",
+               filt_frame_surface->ui16FrameIdx);
+
+        /* allocate output buffer */
+        retval = ni_device_alloc_frame(&ai_ctx->api_ctx, 0, 0, 0, 0, 0, 0, 0, 0,
+                                       filt_frame_surface->ui32nodeAddress,
+                                       filt_frame_surface->ui16FrameIdx,
+                                       NI_DEVICE_TYPE_AI);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR, "failed to alloc hw input frame\n");
+            ret = AVERROR(ENOMEM);
+            goto failed_out;
+        }
+
+        do {
+            retval = ni_device_session_read(
+                &ai_ctx->api_ctx, &ai_ctx->api_dst_pkt, NI_DEVICE_TYPE_AI);
+            if (retval < 0) {
+                av_log(ctx, AV_LOG_ERROR, "read hwdesc retval %d\n", retval);
+                ret = AVERROR(EIO);
+                goto failed_out;
+            } else if (retval > 0) {
+                ret = ni_read_roi(ctx, &ai_ctx->api_dst_pkt, out, out->width,
+                                  out->height);
+                if (ret != 0) {
+                    av_log(ctx, AV_LOG_ERROR,
+                           "failed to read roi from packet\n");
+                    goto failed_out;
+                }
+            }
+        } while (retval == 0);
+
+        ni_hwframe_buffer_recycle(filt_frame_surface,
+                                  filt_frame_surface->device_handle);
+
+        av_buffer_unref(&out->hw_frames_ctx);
+        /* Reference the new hw frames context */
+        out->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+    } else {
+        ret = sws_scale(s->img_cvt_ctx, (const uint8_t *const *)in->data,
+                        in->linesize, 0, in->height, s->rgb_picture.data,
+                        s->rgb_picture.linesize);
+        if (ret < 0) {
+            av_log(ctx, AV_LOG_ERROR, "failed to do sws scale\n");
+            goto failed_out;
+        }
+
+        retval = ni_ai_frame_buffer_alloc(&ai_ctx->api_src_frame.data.frame,
+                                          &network->raw);
+        if (retval != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_ERROR, "cannot allocate ai frame\n");
+            ret = AVERROR(ENOMEM);
+            goto failed_out;
+        }
+
+        ret = ni_recreate_frame(&ai_ctx->api_src_frame.data.frame,
+                                &s->rgb_picture);
+        if (ret != 0) {
+            av_log(ctx, AV_LOG_ERROR, "cannot re-create ai frame\n");
+            goto failed_out;
+        }
+
+        /* write frame */
+        do {
+            retval = ni_device_session_write(
+                &ai_ctx->api_ctx, &ai_ctx->api_src_frame, NI_DEVICE_TYPE_AI);
+            if (retval < 0) {
+                av_log(ctx, AV_LOG_ERROR,
+                       "failed to write ai session: retval %d\n", retval);
+                ret = AVERROR(EIO);
+                goto failed_out;
+            }
+        } while (retval == 0);
+
+        /* read roi result */
+        do {
+            retval = ni_device_session_read(
+                &ai_ctx->api_ctx, &ai_ctx->api_dst_pkt, NI_DEVICE_TYPE_AI);
+            if (retval < 0) {
+                av_log(ctx, AV_LOG_ERROR, "read hwdesc retval %d\n", retval);
+                ret = AVERROR(EIO);
+                goto failed_out;
+            } else if (retval > 0) {
+                ret = ni_read_roi(ctx, &ai_ctx->api_dst_pkt, out, out->width,
+                                  out->height);
+                if (ret != 0) {
+                    av_log(ctx, AV_LOG_ERROR,
+                           "failed to read roi from packet\n");
+                    goto failed_out;
+                }
+            }
+        } while (retval == 0);
+    }
+
+    av_frame_free(&in);
+    return ff_filter_frame(link->dst->outputs[0], out);
+
+failed_out:
+    if (out)
+        av_frame_free(&out);
+
+    av_frame_free(&in);
+    return ret;
+}
+
+#define OFFSET(x) offsetof(NetIntRoiContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+static const AVOption ni_roi_options[] = {{"nb", "path to network binary file",
+                                           OFFSET(nb_file), AV_OPT_TYPE_STRING,
+                                           .flags = FLAGS},
+                                          {"qpoffset",
+                                           "qp offset ratio",
+                                           OFFSET(qp_offset),
+                                           AV_OPT_TYPE_RATIONAL,
+                                           {.dbl = 0},
+                                           -1.0,
+                                           1.0,
+                                           .flags = FLAGS,
+                                           "range"},
+                                          {"devid",
+                                           "device to operate in swframe mode",
+                                           OFFSET(devid),
+                                           AV_OPT_TYPE_INT,
+                                           {.i64 = 0},
+                                           -1,
+                                           INT_MAX,
+                                           .flags = FLAGS,
+                                           "range"},
+                                          {"obj_thresh",
+                                           "objectness thresh",
+                                           OFFSET(obj_thresh),
+                                           AV_OPT_TYPE_FLOAT,
+                                           {.dbl = 0.25},
+                                           -FLT_MAX,
+                                           FLT_MAX,
+                                           .flags = FLAGS,
+                                           "range"},
+                                          {"nms_thresh",
+                                           "nms thresh",
+                                           OFFSET(nms_thresh),
+                                           AV_OPT_TYPE_FLOAT,
+                                           {.dbl = 0.45},
+                                           -FLT_MAX,
+                                           FLT_MAX,
+                                           .flags = FLAGS,
+                                           "range"},
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSET(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     FLAGS,
+     "keep_alive_timeout"},
+                                          {NULL}};
+
+static const AVClass ni_roi_class = {
+    .class_name = "ni_roi",
+    .item_name  = av_default_item_name,
+    .option     = ni_roi_options,
+    .version    = LIBAVUTIL_VERSION_INT,
+    .category   = AV_CLASS_CATEGORY_FILTER,
+    //    .child_class_next = child_class_next,
+};
+
+static const AVFilterPad avfilter_vf_roi_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = ni_roi_filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    {NULL}
+#endif
+};
+
+static const AVFilterPad avfilter_vf_roi_outputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = ni_roi_output_config_props,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    {NULL}
+#endif
+};
+
+AVFilter ff_vf_roi_ni_quadra = {
+    .name           = "ni_quadra_roi",
+    .description    = NULL_IF_CONFIG_SMALL("NetInt Quadra video roi v" NI_XCODER_REVISION),
+    .init           = ni_roi_init,
+    .uninit         = ni_roi_uninit,
+    .priv_size      = sizeof(NetIntRoiContext),
+    .priv_class     = &ni_roi_class,
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(avfilter_vf_roi_inputs),
+    FILTER_OUTPUTS(avfilter_vf_roi_outputs),
+    FILTER_QUERY_FUNC(ni_roi_query_formats),
+#else
+    .inputs         = avfilter_vf_roi_inputs,
+    .outputs        = avfilter_vf_roi_outputs,
+    .query_formats  = ni_roi_query_formats,
+#endif
+};
diff --git a/libavfilter/vf_rotate_ni.c b/libavfilter/vf_rotate_ni.c
new file mode 100644
index 0000000000..0e350b9e75
--- /dev/null
+++ b/libavfilter/vf_rotate_ni.c
@@ -0,0 +1,708 @@
+/*
+ * Copyright (c) 2013 Stefano Sabatini
+ * Copyright (c) 2008 Vitor Sessak
+ * Copyright (c) 2022 NETINT Technologies Inc.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * rotation filter, based on the FFmpeg rotate filter
+*/
+
+#include <string.h>
+
+#include "libavutil/eval.h"
+#include "libavutil/parseutils.h"
+#include "libavutil/opt.h"
+
+#include "internal.h"
+#include "nifilter.h"
+
+#define BUFFER_WIDTH_PIXEL_ALIGNMENT 16
+
+static const char * const var_names[] = {
+    "in_w" , "iw",  ///< width of the input video
+    "in_h" , "ih",  ///< height of the input video
+    "out_w", "ow",  ///< width of the input video
+    "out_h", "oh",  ///< height of the input video
+    "hsub", "vsub",
+    NULL
+};
+
+enum var_name {
+    VAR_IN_W , VAR_IW,
+    VAR_IN_H , VAR_IH,
+    VAR_OUT_W, VAR_OW,
+    VAR_OUT_H, VAR_OH,
+    VAR_HSUB, VAR_VSUB,
+    VAR_VARS_NB
+};
+
+typedef struct NetIntRotContext {
+    const AVClass *class;
+
+    char *angle_expr_str;
+    AVExpr *angle_expr;
+
+    char *outw_expr_str, *outh_expr_str;
+    int outw, outh;
+
+    char *fillcolor_str;
+    uint8_t fillcolor[4];
+    bool fillcolor_enable;
+
+    int hsub, vsub;
+
+    double var_values[VAR_VARS_NB];
+
+    AVBufferRef *out_frames_ref;
+
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+
+    ni_frame_config_t output_frame_config;
+
+    bool initialized;
+    bool session_opened;
+    int64_t keep_alive_timeout;
+} NetIntRotContext;
+
+#define OFFSET(x) offsetof(NetIntRotContext, x)
+#define FLAGS (AV_OPT_FLAG_FILTERING_PARAM|AV_OPT_FLAG_VIDEO_PARAM)
+
+static const AVOption rotate_options[] = {
+    { "angle",     "set angle (in radians)",       OFFSET(angle_expr_str), AV_OPT_TYPE_STRING, {.str="0"},     0, 0, FLAGS },
+    { "a",         "set angle (in radians)",       OFFSET(angle_expr_str), AV_OPT_TYPE_STRING, {.str="0"},     0, 0, FLAGS },
+    { "out_w",     "set output width expression",  OFFSET(outw_expr_str),  AV_OPT_TYPE_STRING, {.str="iw"},    0, 0, FLAGS },
+    { "ow",        "set output width expression",  OFFSET(outw_expr_str),  AV_OPT_TYPE_STRING, {.str="iw"},    0, 0, FLAGS },
+    { "out_h",     "set output height expression", OFFSET(outh_expr_str),  AV_OPT_TYPE_STRING, {.str="ih"},    0, 0, FLAGS },
+    { "oh",        "set output height expression", OFFSET(outh_expr_str),  AV_OPT_TYPE_STRING, {.str="ih"},    0, 0, FLAGS },
+    { "fillcolor", "set background fill color",    OFFSET(fillcolor_str),  AV_OPT_TYPE_STRING, {.str="black"}, 0, 0, FLAGS },
+    { "c",         "set background fill color",    OFFSET(fillcolor_str),  AV_OPT_TYPE_STRING, {.str="black"}, 0, 0, FLAGS },
+    { "keep_alive_timeout",
+      "specify a custom session keep alive timeout in seconds",
+      OFFSET(keep_alive_timeout),
+      AV_OPT_TYPE_INT64,
+      { .i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT },
+      NI_MIN_KEEP_ALIVE_TIMEOUT,
+      NI_MAX_KEEP_ALIVE_TIMEOUT,
+      FLAGS },
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(rotate);
+
+static av_cold int init(AVFilterContext *ctx)
+{
+    NetIntRotContext *rot = ctx->priv;
+
+    av_log(ctx, AV_LOG_DEBUG, "Entered %s\n", __func__);
+
+    if (!strcmp(rot->fillcolor_str, "none"))
+    {
+        rot->fillcolor_enable = false;
+    }
+    else if (av_parse_color(rot->fillcolor, rot->fillcolor_str, -1, ctx) >= 0)
+    {
+        rot->fillcolor_enable = true;
+    }
+    else
+    {
+        av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+        return AVERROR(EINVAL);
+    }
+
+    av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+    return 0;
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    NetIntRotContext *rot = ctx->priv;
+
+    av_log(ctx, AV_LOG_DEBUG, "Entered %s\n", __func__);
+
+    av_expr_free(rot->angle_expr);
+    rot->angle_expr = NULL;
+
+    if (rot->api_dst_frame.data.frame.p_buffer)
+    {
+        ni_frame_buffer_free(&rot->api_dst_frame.data.frame);
+    }
+
+    if (rot->session_opened)
+    {
+        /* Close operation will free the device frames */
+        ni_device_session_close(&rot->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ni_device_session_context_clear(&rot->api_ctx);
+    }
+
+    av_buffer_unref(&rot->out_frames_ref);
+
+    av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+}
+
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] = { AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE };
+    AVFilterFormats *fmts_list = NULL;
+
+    av_log(ctx, AV_LOG_DEBUG, "Entered %s\n", __func__);
+
+    fmts_list = ff_make_format_list(pix_fmts);
+    if (!fmts_list)
+    {
+        av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+        return AVERROR(ENOMEM);
+    }
+
+    av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+    return ff_set_common_formats(ctx, fmts_list);
+}
+
+static double get_rotated_w(void *opaque, double angle)
+{
+    NetIntRotContext *rot = opaque;
+    double inw = rot->var_values[VAR_IN_W];
+    double inh = rot->var_values[VAR_IN_H];
+    float sinx = (float)sin(angle);
+    float cosx = (float)cos(angle);
+
+    return FFMAX(0, inh * sinx) + FFMAX(0, -inw * cosx) +
+           FFMAX(0, inw * cosx) + FFMAX(0, -inh * sinx);
+}
+
+static double get_rotated_h(void *opaque, double angle)
+{
+    NetIntRotContext *rot = opaque;
+    double inw = rot->var_values[VAR_IN_W];
+    double inh = rot->var_values[VAR_IN_H];
+    float sinx = (float)sin(angle);
+    float cosx = (float)cos(angle);
+
+    return FFMAX(0, -inh * cosx) + FFMAX(0, -inw * sinx) +
+           FFMAX(0,  inh * cosx) + FFMAX(0,  inw * sinx);
+}
+
+static double (* const func1[])(void *, double) = {
+    get_rotated_w,
+    get_rotated_h,
+    NULL
+};
+
+static const char * const func1_names[] = {
+    "rotw",
+    "roth",
+    NULL
+};
+
+static int config_props(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    NetIntRotContext *rot = ctx->priv;
+    AVFilterLink *inlink = ctx->inputs[0];
+    AVHWFramesContext *in_frames_ctx, *out_frames_ctx;
+    const AVPixFmtDescriptor *pixdesc = av_pix_fmt_desc_get(inlink->format);
+    int ret;
+    double res;
+    char *expr;
+
+    av_log(ctx, AV_LOG_DEBUG, "Entered %s\n", __func__);
+
+    rot->hsub = pixdesc->log2_chroma_w;
+    rot->vsub = pixdesc->log2_chroma_h;
+
+    rot->var_values[VAR_IN_W] = rot->var_values[VAR_IW] = inlink->w;
+    rot->var_values[VAR_IN_H] = rot->var_values[VAR_IH] = inlink->h;
+    rot->var_values[VAR_HSUB] = 1<<rot->hsub;
+    rot->var_values[VAR_VSUB] = 1<<rot->vsub;
+    rot->var_values[VAR_OUT_W] = rot->var_values[VAR_OW] = NAN;
+    rot->var_values[VAR_OUT_H] = rot->var_values[VAR_OH] = NAN;
+
+    av_expr_free(rot->angle_expr);
+    rot->angle_expr = NULL;
+    ret = av_expr_parse(&rot->angle_expr,
+                        // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores)
+                        expr = rot->angle_expr_str,
+                        var_names,
+                        func1_names,
+                        func1,
+                        NULL,
+                        NULL,
+                        0,
+                        ctx);
+    if (ret < 0)
+    {
+        av_log(ctx,
+               AV_LOG_ERROR,
+               "Error occurred parsing angle expression '%s'\n",
+               rot->angle_expr_str);
+        return ret;
+    }
+
+#define SET_SIZE_EXPR(name, opt_name) do {                                         \
+    ret = av_expr_parse_and_eval(&res, expr = rot->name##_expr_str,                \
+                                 var_names, rot->var_values,                       \
+                                 func1_names, func1, NULL, NULL, rot, 0, ctx);     \
+    if (ret < 0 || isnan(res) || isinf(res) || res <= 0) {                         \
+        av_log(ctx, AV_LOG_ERROR,                                                  \
+               "Error parsing or evaluating expression for option %s: "            \
+               "invalid expression '%s' or non-positive or indefinite value %f\n", \
+               opt_name, expr, res);                                               \
+        return ret;                                                                \
+    }                                                                              \
+} while (0)
+
+    /* evaluate width and height */
+    av_expr_parse_and_eval(&res,
+                           // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores)
+                           expr = rot->outw_expr_str,
+                           var_names,
+                           rot->var_values,
+                           func1_names,
+                           func1,
+                           NULL,
+                           NULL,
+                           rot,
+                           0,
+                           ctx);
+    rot->var_values[VAR_OUT_W] = rot->var_values[VAR_OW] = res;
+    // NOLINTNEXTLINE(bugprone-incorrect-roundings, bugprone-narrowing-conversions)
+    rot->outw = res + 0.5;
+    SET_SIZE_EXPR(outh, "out_h");
+    rot->var_values[VAR_OUT_H] = rot->var_values[VAR_OH] = res;
+    // NOLINTNEXTLINE(bugprone-incorrect-roundings, bugprone-narrowing-conversions)
+    rot->outh = res + 0.5;
+
+    /* evaluate the width again, as it may depend on the evaluated output height */
+    SET_SIZE_EXPR(outw, "out_w");
+    rot->var_values[VAR_OUT_W] = rot->var_values[VAR_OW] = res;
+    // NOLINTNEXTLINE(bugprone-incorrect-roundings, bugprone-narrowing-conversions)
+    rot->outw = res + 0.5;
+
+    outlink->w = rot->outw;
+    outlink->h = rot->outh;
+
+    in_frames_ctx = (AVHWFramesContext *) ctx->inputs[0]->hw_frames_ctx->data;
+
+    rot->out_frames_ref = av_hwframe_ctx_alloc(in_frames_ctx->device_ref);
+    if (!rot->out_frames_ref)
+    {
+        av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+        return AVERROR(ENOMEM);
+    }
+
+    out_frames_ctx = (AVHWFramesContext *) rot->out_frames_ref->data;
+
+    out_frames_ctx->format = AV_PIX_FMT_NI_QUAD;
+    out_frames_ctx->width = rot->outw;
+    out_frames_ctx->height = rot->outh;
+    out_frames_ctx->sw_format = in_frames_ctx->sw_format;
+    out_frames_ctx->initial_pool_size = NI_ROTATE_ID; // Repurposed as identity code
+
+    av_buffer_unref(&ctx->outputs[0]->hw_frames_ctx);
+    ctx->outputs[0]->hw_frames_ctx = av_buffer_ref(rot->out_frames_ref);
+
+    if (!ctx->outputs[0]->hw_frames_ctx)
+    {
+        av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+        return AVERROR(ENOMEM);
+    }
+
+    av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+    return 0;
+}
+
+static int init_out_pool(AVFilterContext *ctx)
+{
+    NetIntRotContext *rot = ctx->priv;
+    AVHWFramesContext *out_frames_context;
+
+    if (!ctx->inputs[0]->hw_frames_ctx)
+    {
+        return AVERROR(EINVAL);
+    }
+
+    out_frames_context = (AVHWFramesContext*)rot->out_frames_ref->data;
+
+    /* Don't check return code, this will intentionally fail */
+    av_hwframe_ctx_init(rot->out_frames_ref);
+
+    /* Create frame pool on device */
+    return ff_ni_build_frame_pool(&rot->api_ctx,
+                                  out_frames_context->width,
+                                  out_frames_context->height,
+                                  out_frames_context->sw_format,
+                                  DEFAULT_NI_FILTER_POOL_SIZE);
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in)
+{
+    AVFilterContext *ctx = inlink->dst;
+    AVFilterLink *outlink = inlink->dst->outputs[0];
+    AVFrame *out = NULL;
+    NetIntRotContext *rot = ctx->priv;
+    AVBufferRef *out_buffer_ref = rot->out_frames_ref;
+    AVHWFramesContext *in_frames_context = (AVHWFramesContext *) in->hw_frames_ctx->data;
+    AVNIDeviceContext *av_ni_device_context = (AVNIDeviceContext *) in_frames_context->device_ctx->hwctx;
+    ni_retcode_t ni_retcode = NI_RETCODE_SUCCESS;
+    niFrameSurface1_t *frame_surface = (niFrameSurface1_t *) in->data[3], *frame_surface2 = NULL;
+    ni_frame_config_t input_frame_config = {0};
+    uint32_t scaler_format;
+    int retcode = 0, rgba_color = 255 /* black opaque */, card_number = (int) in->opaque;
+    int aligned_picture_width, rotated_picture_width, rotated_picture_height;
+    double angle;
+
+    av_log(ctx, AV_LOG_DEBUG, "Entered %s\n", __func__);
+
+    if (!frame_surface)
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni rotate filter frame_surface should not be NULL\n");
+        return AVERROR(EINVAL);
+    }
+
+    if (!rot->initialized)
+    {
+        if (in_frames_context->sw_format == AV_PIX_FMT_BGRP)
+        {
+            av_log(ctx, AV_LOG_ERROR, "bgrp not supported\n");
+            goto FAIL;
+        }
+
+        ni_retcode = ni_device_session_context_init(&rot->api_ctx);
+        if (ni_retcode != NI_RETCODE_SUCCESS)
+        {
+            av_log(ctx, AV_LOG_ERROR, "ni rotate filter session context init failed with %d\n", ni_retcode);
+            retcode = AVERROR(EINVAL);
+            goto FAIL;
+        }
+
+        rot->api_ctx.device_handle = rot->api_ctx.blk_io_handle = av_ni_device_context->cards[card_number];
+
+        rot->api_ctx.hw_id = card_number;
+        rot->api_ctx.device_type = NI_DEVICE_TYPE_SCALER;
+        rot->api_ctx.scaler_operation = NI_SCALER_OPCODE_ROTATE;
+        rot->api_ctx.keep_alive_timeout = rot->keep_alive_timeout;
+
+        ni_retcode = ni_device_session_open(&rot->api_ctx, NI_DEVICE_TYPE_SCALER);
+        if (ni_retcode != NI_RETCODE_SUCCESS)
+        {
+            av_log(ctx, AV_LOG_ERROR, "ni rotate filter device session open failed with %d\n", ni_retcode);
+            retcode = AVERROR(EAGAIN);
+            goto FAIL;
+        }
+
+        rot->session_opened = true;
+
+        ni_retcode = init_out_pool(inlink->dst);
+        if (ni_retcode != NI_RETCODE_SUCCESS)
+        {
+            av_log(ctx, AV_LOG_ERROR, "ni rotate filter init out pool failed with %d\n", ni_retcode);
+            goto FAIL;
+        }
+
+        ff_ni_clone_hwframe_ctx(in_frames_context, (AVHWFramesContext *)out_buffer_ref->data);
+
+        if (in->color_range == AVCOL_RANGE_JPEG)
+        {
+            av_log(ctx, AV_LOG_WARNING, "Full color range input, limited color output\n");
+        }
+
+        rot->initialized = true;
+    }
+
+    av_log(ctx, AV_LOG_DEBUG, "outlink %dx%d\n", outlink->w, outlink->h);
+
+    ni_retcode = ni_frame_buffer_alloc_hwenc(&rot->api_dst_frame.data.frame,
+                                             outlink->w,
+                                             outlink->h,
+                                             0);
+    if (ni_retcode != NI_RETCODE_SUCCESS)
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni rotate filter frame buffer alloc hwenc failed with %d\n", ni_retcode);
+        retcode = AVERROR(ENOMEM);
+        goto FAIL;
+    }
+
+    // Input.
+
+    scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(in_frames_context->sw_format);
+    input_frame_config.picture_format = scaler_format;
+
+    input_frame_config.rgba_color = frame_surface->ui32nodeAddress;
+    input_frame_config.frame_index = frame_surface->ui16FrameIdx;
+
+    aligned_picture_width = FFALIGN(in->width, BUFFER_WIDTH_PIXEL_ALIGNMENT);
+
+    angle = av_expr_eval(rot->angle_expr, rot->var_values, rot);
+    if (angle == 0.0)
+    {
+        // input_frame_config.orientation = 0; // initialized to zero, unnecessary assignment
+        input_frame_config.picture_width = in->width;
+        input_frame_config.picture_height = in->height;
+
+        input_frame_config.rectangle_width = FFMIN(outlink->w, in->width);
+        input_frame_config.rectangle_height = FFMIN(outlink->h, in->height);
+
+        rotated_picture_width = in->width;
+        rotated_picture_height = in->height;
+    }
+    else if ((angle == -M_PI_2 * 3.0) || (angle == M_PI_2)) // -270.0° || 90.0°
+    {
+        input_frame_config.orientation = 1;
+        input_frame_config.picture_width = aligned_picture_width;
+        input_frame_config.picture_height = in->height;
+
+        input_frame_config.rectangle_width = FFMIN(outlink->w, in->height);
+        input_frame_config.rectangle_height = FFMIN(outlink->h, in->width);
+
+        rotated_picture_width = in->height;
+        rotated_picture_height = aligned_picture_width;
+    }
+    else if ((angle == -M_PI) || (angle == M_PI)) // -180.0° || 180.0°
+    {
+        input_frame_config.orientation = 2;
+        input_frame_config.picture_width = aligned_picture_width;
+        input_frame_config.picture_height = in->height;
+
+        input_frame_config.rectangle_width = FFMIN(outlink->w, in->width);
+        input_frame_config.rectangle_height = FFMIN(outlink->h, in->height);
+
+        rotated_picture_width = aligned_picture_width;
+        rotated_picture_height = in->height;
+    }
+    else if ((angle == -M_PI_2) || (angle == M_PI_2 * 3.0)) // -90.0° || 270.0°
+    {
+        input_frame_config.orientation = 3;
+        input_frame_config.picture_width = aligned_picture_width;
+        input_frame_config.picture_height = in->height;
+
+        input_frame_config.rectangle_width = FFMIN(outlink->w, in->height);
+        input_frame_config.rectangle_height = FFMIN(outlink->h, in->width);
+
+        rotated_picture_width = in->height;
+        rotated_picture_height = aligned_picture_width;
+    }
+    else
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni rotate filter does not support rotation of %.1f radians\n", angle);
+        return AVERROR(EINVAL);
+    }
+
+    input_frame_config.rectangle_x =
+        (rotated_picture_width > input_frame_config.rectangle_width) ?
+        (rotated_picture_width / 2) - (input_frame_config.rectangle_width / 2) : 0;
+    input_frame_config.rectangle_y =
+        (rotated_picture_height > input_frame_config.rectangle_height) ?
+        (rotated_picture_height / 2) - (input_frame_config.rectangle_height / 2) : 0;
+    if (aligned_picture_width - in->width)
+    {
+        switch (input_frame_config.orientation)
+        {
+        case 1: // 90°
+            input_frame_config.rectangle_y =
+                (in->width > input_frame_config.rectangle_height) ?
+                (in->width / 2) - (input_frame_config.rectangle_height / 2) : 0;
+            break;
+        case 2: // 180°
+            input_frame_config.rectangle_x =
+                aligned_picture_width - in->width +
+                ((in->width > input_frame_config.rectangle_width) ?
+                 (in->width / 2) - (input_frame_config.rectangle_width / 2) : 0);
+            break;
+        case 3: // 270°
+            input_frame_config.rectangle_y =
+                aligned_picture_width - in->width +
+                ((in->width > input_frame_config.rectangle_height) ?
+                 (in->width / 2) - (input_frame_config.rectangle_height / 2) : 0);
+            break;
+        }
+    }
+
+    // use ni_device_config_frame() instead of ni_device_alloc_frame()
+    // such that input_frame_config's orientation can be configured
+    ni_retcode = ni_device_config_frame(&rot->api_ctx, &input_frame_config);
+    if (ni_retcode != NI_RETCODE_SUCCESS)
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni rotate filter device config input frame failed with %d\n", ni_retcode);
+        retcode = AVERROR(EAGAIN);
+        goto FAIL;
+    }
+
+    // Output.
+
+    if (rot->fillcolor_enable)
+    {
+        rgba_color = (rot->fillcolor[3] << 24) |
+                     (rot->fillcolor[0] << 16) |
+                     (rot->fillcolor[1] <<  8) |
+                      rot->fillcolor[2];
+    }
+
+    if ((outlink->w != rot->output_frame_config.picture_width) ||
+        (outlink->h != rot->output_frame_config.picture_height) ||
+        (input_frame_config.rectangle_width != rot->output_frame_config.rectangle_width) ||
+        (input_frame_config.rectangle_height != rot->output_frame_config.rectangle_height) ||
+        (scaler_format != rot->output_frame_config.picture_format) ||
+        (rgba_color != rot->output_frame_config.rgba_color)) {
+        rot->output_frame_config.picture_width = outlink->w;
+        rot->output_frame_config.picture_height = outlink->h;
+        rot->output_frame_config.rectangle_width = input_frame_config.rectangle_width;
+        rot->output_frame_config.rectangle_height = input_frame_config.rectangle_height;
+        rot->output_frame_config.rectangle_x =
+            (rot->output_frame_config.picture_width > rot->output_frame_config.rectangle_width) ?
+            (rot->output_frame_config.picture_width / 2) - (rot->output_frame_config.rectangle_width / 2) : 0;
+        rot->output_frame_config.rectangle_y =
+            (rot->output_frame_config.picture_height > rot->output_frame_config.rectangle_height) ?
+            (rot->output_frame_config.picture_height / 2) - (rot->output_frame_config.rectangle_height / 2) : 0;
+        rot->output_frame_config.rgba_color = rgba_color;
+
+        ni_retcode = ni_device_alloc_frame(&rot->api_ctx,
+                                           rot->output_frame_config.picture_width,
+                                           rot->output_frame_config.picture_height,
+                                           scaler_format,
+                                           NI_SCALER_FLAG_IO,
+                                           rot->output_frame_config.rectangle_width,
+                                           rot->output_frame_config.rectangle_height,
+                                           rot->output_frame_config.rectangle_x,
+                                           rot->output_frame_config.rectangle_y,
+                                           rot->output_frame_config.rgba_color,
+                                           -1,
+                                           NI_DEVICE_TYPE_SCALER);
+
+        if (ni_retcode != NI_RETCODE_SUCCESS)
+        {
+            av_log(ctx, AV_LOG_ERROR, "ni rotate filter device alloc output frame failed with %d\n", ni_retcode);
+            retcode = AVERROR(ENOMEM);
+            goto FAIL;
+        }
+    }
+
+    out = av_frame_alloc();
+    if (!out)
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni rotate filter av_frame_alloc returned NULL\n");
+        retcode = AVERROR(ENOMEM);
+        goto FAIL;
+    }
+
+    av_frame_copy_props(out, in);
+
+    out->width = rot->outw;
+    out->height = rot->outh;
+    out->format = AV_PIX_FMT_NI_QUAD;
+    out->color_range = AVCOL_RANGE_MPEG;
+
+    out->hw_frames_ctx = av_buffer_ref(out_buffer_ref);
+    out->data[3] = av_malloc(sizeof(niFrameSurface1_t));
+    if (!out->data[3])
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni rotate filter av_alloc returned NULL\n");
+        retcode = AVERROR(ENOMEM);
+        goto FAIL;
+    }
+    memcpy(out->data[3], frame_surface, sizeof(niFrameSurface1_t));
+
+    ni_retcode = ni_device_session_read_hwdesc(&rot->api_ctx,
+                                               &rot->api_dst_frame,
+                                               NI_DEVICE_TYPE_SCALER);
+    if (ni_retcode != NI_RETCODE_SUCCESS)
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni rotate filter read hwdesc failed with %d\n", ni_retcode);
+        retcode = AVERROR(ENOMEM);
+        goto FAIL;
+    }
+
+    frame_surface2 = (niFrameSurface1_t *) rot->api_dst_frame.data.frame.p_data[3];
+
+    frame_surface = (niFrameSurface1_t *) out->data[3];
+    frame_surface->ui16FrameIdx = frame_surface2->ui16FrameIdx;
+    frame_surface->ui16session_ID = frame_surface2->ui16session_ID;
+    frame_surface->device_handle = frame_surface2->device_handle;
+    frame_surface->output_idx = frame_surface2->output_idx;
+    frame_surface->src_cpu = frame_surface2->src_cpu;
+    frame_surface->ui32nodeAddress = 0;
+    frame_surface->ui16width = out->width;
+    frame_surface->ui16height = out->height;
+
+    out->buf[0] = av_buffer_create(out->data[3],
+                                   sizeof(niFrameSurface1_t),
+                                   ff_ni_frame_free,
+                                   NULL,
+                                   0);
+    if (!out->buf[0])
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni rotate filter av_buffer_create returned NULL\n");
+        retcode = AVERROR(ENOMEM);
+        goto FAIL;
+    }
+
+    av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+
+    av_frame_free(&in);
+    return ff_filter_frame(inlink->dst->outputs[0], out);
+
+FAIL:
+    av_log(ctx, AV_LOG_DEBUG, "Exiting %s\n", __func__);
+
+    av_frame_free(&in);
+    av_frame_free(&out);
+    return retcode;
+}
+
+static const AVFilterPad avfilter_vf_rotate_inputs[] = {
+    {
+        .name = "default",
+        .type = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad avfilter_vf_rotate_outputs[] = {
+    {
+        .name = "default",
+        .type = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_props,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_rotate_ni_quadra = {
+    .name = "ni_quadra_rotate",
+    .description = NULL_IF_CONFIG_SMALL("NetInt Quadra rotate the input video v" NI_XCODER_REVISION),
+    .priv_size = sizeof(NetIntRotContext),
+    .priv_class = &rotate_class,
+    .init = init,
+    .uninit = uninit,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_QUERY_FUNC(query_formats),
+    FILTER_INPUTS(avfilter_vf_rotate_inputs),
+    FILTER_OUTPUTS(avfilter_vf_rotate_outputs),
+#else
+    .query_formats = query_formats,
+    .inputs = avfilter_vf_rotate_inputs,
+    .outputs = avfilter_vf_rotate_outputs,
+#endif
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+};
diff --git a/libavfilter/vf_scale_ni.c b/libavfilter/vf_scale_ni.c
new file mode 100644
index 0000000000..98b5a1824b
--- /dev/null
+++ b/libavfilter/vf_scale_ni.c
@@ -0,0 +1,634 @@
+/*
+ * Copyright (c) 2007 Bobby Bingham
+ * Copyright (c) 2020 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * scale video filter
+ */
+
+#include <stdio.h>
+#include <string.h>
+
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+
+ // Needed for FFmpeg-n4.3+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 85)
+#include "scale_eval.h"
+#else
+#include "scale.h"
+#endif
+#include "video.h"
+#include "libavutil/avstring.h"
+#include "libavutil/internal.h"
+#include "libavutil/mathematics.h"
+#include "libavutil/opt.h"
+#include "libavutil/parseutils.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/avassert.h"
+#include "libswscale/swscale.h"
+#include "nifilter.h"
+
+enum OutputFormat {
+    OUTPUT_FORMAT_YUV420P,
+    OUTPUT_FORMAT_YUYV422,
+    OUTPUT_FORMAT_UYVY422,
+    OUTPUT_FORMAT_NV12,
+    OUTPUT_FORMAT_ARGB,
+    OUTPUT_FORMAT_RGBA,
+    OUTPUT_FORMAT_ABGR,
+    OUTPUT_FORMAT_BGRA,
+    OUTPUT_FORMAT_YUV420P10LE,
+    OUTPUT_FORMAT_NV16,
+    OUTPUT_FORMAT_BGR0,
+    OUTPUT_FORMAT_P010LE,
+    OUTPUT_FORMAT_BGRP,
+    OUTPUT_FORMAT_AUTO,
+    OUTPUT_FORMAT_NB
+};
+
+enum AVPixelFormat ff_output_fmt[] = {
+    AV_PIX_FMT_YUV420P, AV_PIX_FMT_YUYV422, AV_PIX_FMT_UYVY422,
+    AV_PIX_FMT_NV12,    AV_PIX_FMT_ARGB,    AV_PIX_FMT_RGBA,
+    AV_PIX_FMT_ABGR,    AV_PIX_FMT_BGRA,    AV_PIX_FMT_YUV420P10LE,
+    AV_PIX_FMT_NV16,    AV_PIX_FMT_BGR0,    AV_PIX_FMT_P010LE,
+    AV_PIX_FMT_BGRP};
+
+typedef struct NetIntScaleContext {
+    const AVClass *class;
+    AVDictionary *opts;
+
+    /**
+     * New dimensions. Special values are:
+     *   0 = original width/height
+     *  -1 = keep original aspect
+     *  -N = try to keep aspect but make sure it is divisible by N
+     */
+    int w, h;
+    char *size_str;
+
+    char *w_expr;               ///< width  expression string
+    char *h_expr;               ///< height expression string
+
+    int force_original_aspect_ratio;
+    int force_divisible_by;
+    int format;
+
+    enum AVPixelFormat out_format;
+    AVBufferRef *out_frames_ref;
+
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+    ni_scaler_params_t params;
+
+    int initialized;
+    int session_opened;
+    int keep_alive_timeout; /* keep alive timeout setting */
+
+    ni_frame_config_t frame_in;
+    ni_frame_config_t frame_out;
+} NetIntScaleContext;
+
+AVFilter ff_vf_scale_ni;
+
+static av_cold int init_dict(AVFilterContext *ctx, AVDictionary **opts)
+{
+    NetIntScaleContext *scale = ctx->priv;
+
+    if (scale->size_str && (scale->w_expr || scale->h_expr)) {
+        av_log(ctx, AV_LOG_ERROR,
+               "Size and width/height expressions cannot be set at the same time.\n");
+            return AVERROR(EINVAL);
+    }
+
+    if (scale->w_expr && !scale->h_expr)
+        FFSWAP(char *, scale->w_expr, scale->size_str);
+
+    if (scale->size_str) {
+        char buf[32];
+        int ret;
+
+        if ((ret = av_parse_video_size(&scale->w, &scale->h, scale->size_str)) < 0) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "Invalid size '%s'\n", scale->size_str);
+            return ret;
+        }
+        snprintf(buf, sizeof(buf)-1, "%d", scale->w);
+        av_opt_set(scale, "w", buf, 0);
+        snprintf(buf, sizeof(buf)-1, "%d", scale->h);
+        av_opt_set(scale, "h", buf, 0);
+    }
+    if (!scale->w_expr)
+        av_opt_set(scale, "w", "iw", 0);
+    if (!scale->h_expr)
+        av_opt_set(scale, "h", "ih", 0);
+
+    av_log(ctx, AV_LOG_VERBOSE, "w:%s h:%s\n", scale->w_expr, scale->h_expr);
+
+    scale->opts = *opts;
+    *opts = NULL;
+
+    return 0;
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    NetIntScaleContext *scale = ctx->priv;
+
+    av_dict_free(&scale->opts);
+
+    if (scale->api_dst_frame.data.frame.p_buffer)
+        ni_frame_buffer_free(&scale->api_dst_frame.data.frame);
+
+    if (scale->session_opened) {
+        /* Close operation will free the device frames */
+        ni_device_session_close(&scale->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ni_device_session_context_clear(&scale->api_ctx);
+    }
+
+    av_buffer_unref(&scale->out_frames_ref);
+}
+
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] =
+        {AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE};
+    AVFilterFormats *formats;
+
+    formats = ff_make_format_list(pix_fmts);
+
+    if (!formats)
+        return AVERROR(ENOMEM);
+
+    return ff_set_common_formats(ctx, formats);
+}
+
+static int init_out_pool(AVFilterContext *ctx) {
+    NetIntScaleContext *s = ctx->priv;
+    AVHWFramesContext *out_frames_ctx;
+
+    if (!ctx->inputs[0]->hw_frames_ctx) {
+        return AVERROR(EINVAL);
+    }
+
+    out_frames_ctx   = (AVHWFramesContext*)s->out_frames_ref->data;
+
+    /* Don't check return code, this will intentionally fail */
+    av_hwframe_ctx_init(s->out_frames_ref);
+
+    /* Create frame pool on device */
+    return ff_ni_build_frame_pool(&s->api_ctx, out_frames_ctx->width,
+                                  out_frames_ctx->height, s->out_format,
+                                  DEFAULT_NI_FILTER_POOL_SIZE);
+}
+
+static int config_props(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    AVFilterLink *inlink0 = outlink->src->inputs[0];
+    AVFilterLink *inlink = outlink->src->inputs[0];
+    AVHWFramesContext *in_frames_ctx;
+    AVHWFramesContext *out_frames_ctx;
+    NetIntScaleContext *scale = ctx->priv;
+    int w, h, ret, h_shift, v_shift;
+
+    if ((ret = ff_scale_eval_dimensions(ctx,
+                                        scale->w_expr, scale->h_expr,
+                                        inlink, outlink,
+                                        &w, &h)) < 0)
+        goto fail;
+
+    /* Note that force_original_aspect_ratio may overwrite the previous set
+     * dimensions so that it is not divisible by the set factors anymore
+     * unless force_divisible_by is defined as well */
+    if (scale->force_original_aspect_ratio) {
+        int tmp_w = av_rescale(h, inlink->w, inlink->h);
+        int tmp_h = av_rescale(w, inlink->h, inlink->w);
+
+        if (scale->force_original_aspect_ratio == 1) {
+             w = FFMIN(tmp_w, w);
+             h = FFMIN(tmp_h, h);
+             if (scale->force_divisible_by > 1) {
+                 // round down
+                 w = w / scale->force_divisible_by * scale->force_divisible_by;
+                 h = h / scale->force_divisible_by * scale->force_divisible_by;
+             }
+        } else {
+             w = FFMAX(tmp_w, w);
+             h = FFMAX(tmp_h, h);
+             if (scale->force_divisible_by > 1) {
+                 // round up
+                 w = (w + scale->force_divisible_by - 1) / scale->force_divisible_by * scale->force_divisible_by;
+                 h = (h + scale->force_divisible_by - 1) / scale->force_divisible_by * scale->force_divisible_by;
+             }
+        }
+    }
+
+    if (w > NI_MAX_RESOLUTION_WIDTH || h > NI_MAX_RESOLUTION_HEIGHT) {
+        av_log(ctx, AV_LOG_ERROR, "Scaled value (%dx%d) > 8192 not allowed\n", w, h);
+        return AVERROR(EINVAL);
+    }
+
+    if ((w <= 0) || (h <= 0)) {
+        av_log(ctx, AV_LOG_ERROR, "Scaled value (%dx%d) not allowed\n", w, h);
+        return AVERROR(EINVAL);
+    }
+
+    in_frames_ctx = (AVHWFramesContext *)ctx->inputs[0]->hw_frames_ctx->data;
+
+    if (in_frames_ctx->sw_format == AV_PIX_FMT_BGRP) {
+        av_log(ctx, AV_LOG_ERROR, "bgrp not supported\n");
+        return AVERROR(EINVAL);
+    }
+
+    /* Set the output format */
+    if (scale->format == OUTPUT_FORMAT_AUTO) {
+        scale->out_format = in_frames_ctx->sw_format;
+    } else {
+        scale->out_format = ff_output_fmt[scale->format];
+    }
+
+    av_pix_fmt_get_chroma_sub_sample(scale->out_format, &h_shift, &v_shift);
+
+    outlink->w = FFALIGN(w, (1 << h_shift));
+    outlink->h = FFALIGN(h, (1 << v_shift));
+
+    /* TODO: make algorithm configurable */
+
+    if (inlink0->sample_aspect_ratio.num){
+        outlink->sample_aspect_ratio = av_mul_q((AVRational){outlink->h * inlink0->w, outlink->w * inlink0->h}, inlink0->sample_aspect_ratio);
+    } else
+        outlink->sample_aspect_ratio = inlink0->sample_aspect_ratio;
+
+    av_log(ctx, AV_LOG_VERBOSE,
+           "w:%d h:%d fmt:%s sar:%d/%d -> w:%d h:%d fmt:%s sar:%d/%d\n",
+           inlink->w, inlink->h, av_get_pix_fmt_name(inlink->format),
+           inlink->sample_aspect_ratio.num, inlink->sample_aspect_ratio.den,
+           outlink->w, outlink->h, av_get_pix_fmt_name(outlink->format),
+           outlink->sample_aspect_ratio.num, outlink->sample_aspect_ratio.den);
+
+    scale->out_frames_ref = av_hwframe_ctx_alloc(in_frames_ctx->device_ref);
+    if (!scale->out_frames_ref)
+        return AVERROR(ENOMEM);
+
+    out_frames_ctx = (AVHWFramesContext *)scale->out_frames_ref->data;
+
+    out_frames_ctx->format    = AV_PIX_FMT_NI_QUAD;
+    out_frames_ctx->width     = outlink->w;
+    out_frames_ctx->height    = outlink->h;
+    out_frames_ctx->sw_format = scale->out_format;
+    out_frames_ctx->initial_pool_size =
+        NI_SCALE_ID; // Repurposed as identity code
+
+    av_buffer_unref(&ctx->outputs[0]->hw_frames_ctx);
+    ctx->outputs[0]->hw_frames_ctx = av_buffer_ref(scale->out_frames_ref);
+
+    if (!ctx->outputs[0]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return 0;
+
+fail:
+    return ret;
+}
+
+/* Process a received frame */
+static int filter_frame(AVFilterLink *link, AVFrame *in)
+{
+    NetIntScaleContext *scale = link->dst->priv;
+    AVFilterLink *outlink = link->dst->outputs[0];
+    AVFrame *out = NULL;
+    niFrameSurface1_t* frame_surface,*new_frame_surface;
+    AVHWFramesContext *pAVHFWCtx;
+    AVNIDeviceContext *pAVNIDevCtx;
+    ni_retcode_t retcode;
+    int scaler_format, cardno;
+    uint16_t tempFID;
+
+    frame_surface = (niFrameSurface1_t *) in->data[3];
+    if (frame_surface == NULL) {
+        return AVERROR(EINVAL);
+    }
+
+    pAVHFWCtx = (AVHWFramesContext *) in->hw_frames_ctx->data;
+    pAVNIDevCtx       = (AVNIDeviceContext *)pAVHFWCtx->device_ctx->hwctx;
+    cardno            = ni_get_cardno(in);
+
+    if (!scale->initialized) {
+        retcode = ni_device_session_context_init(&scale->api_ctx);
+        if (retcode < 0) {
+            av_log(link->dst, AV_LOG_ERROR,
+                   "ni scale filter session context init failure\n");
+            goto fail;
+        }
+
+        scale->api_ctx.device_handle = pAVNIDevCtx->cards[cardno];
+        scale->api_ctx.blk_io_handle = pAVNIDevCtx->cards[cardno];
+
+        scale->api_ctx.hw_id             = cardno;
+        scale->api_ctx.device_type       = NI_DEVICE_TYPE_SCALER;
+        scale->api_ctx.scaler_operation  = NI_SCALER_OPCODE_SCALE;
+        scale->api_ctx.keep_alive_timeout = scale->keep_alive_timeout;
+
+        av_log(link->dst, AV_LOG_ERROR,
+               "Open scaler session to card %d, hdl %d, blk_hdl %d\n", cardno,
+               scale->api_ctx.device_handle, scale->api_ctx.blk_io_handle);
+
+        retcode =
+            ni_device_session_open(&scale->api_ctx, NI_DEVICE_TYPE_SCALER);
+        if (retcode < 0) {
+            av_log(link->dst, AV_LOG_ERROR,
+                   "Can't open device session on card %d\n", cardno);
+            goto fail;
+        }
+
+        scale->session_opened = 1;
+
+        if (scale->params.filterblit) {
+            retcode = ni_scaler_set_params(&scale->api_ctx, &(scale->params));
+            if (retcode < 0)
+                goto fail;
+        }
+
+        retcode = init_out_pool(link->dst);
+
+        if (retcode < 0)
+        {
+            av_log(link->dst, AV_LOG_ERROR, 
+                   "Internal output allocation failed rc = %d\n", retcode);
+            goto fail;
+        }
+
+        ff_ni_clone_hwframe_ctx(
+            pAVHFWCtx, (AVHWFramesContext *)scale->out_frames_ref->data);
+
+        if (in->color_range == AVCOL_RANGE_JPEG) {
+            av_log(link->dst, AV_LOG_ERROR,
+                   "WARNING: Full color range input, limited color output\n");
+        }
+
+        scale->initialized = 1;
+    }
+
+    scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(pAVHFWCtx->sw_format);
+
+    retcode = ni_frame_buffer_alloc_hwenc(&scale->api_dst_frame.data.frame,
+                                          outlink->w,
+                                          outlink->h,
+                                          0);
+
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    if ((FFALIGN(in->width, 2) != scale->frame_in.picture_width) ||
+        (FFALIGN(in->height, 2) != scale->frame_in.picture_height) ||
+        (scaler_format != scale->frame_in.picture_format) ||
+        (frame_surface->ui16session_ID != scale->frame_in.session_id)) {
+        scale->frame_in.picture_width  = FFALIGN(in->width, 2);
+        scale->frame_in.picture_height = FFALIGN(in->height, 2);
+        scale->frame_in.picture_format = scaler_format;
+        scale->frame_in.session_id     = frame_surface->ui16session_ID;
+        scale->frame_in.output_index   = frame_surface->output_idx;
+        scale->frame_in.frame_index    = frame_surface->ui16FrameIdx;
+
+        /*
+         * Config device input frame parameters
+         */
+        retcode = ni_device_config_frame(&scale->api_ctx, &scale->frame_in);
+
+        if (retcode != NI_RETCODE_SUCCESS) {
+            av_log(link->dst, AV_LOG_DEBUG,
+                   "Can't allocate device input frame %d\n", retcode);
+            retcode = AVERROR(ENOMEM);
+            goto fail;
+        }
+    }
+
+    scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(scale->out_format);
+
+    if ((outlink->w != scale->frame_out.picture_width) ||
+        (outlink->h != scale->frame_out.picture_height) ||
+        (scaler_format != scale->frame_out.picture_format)) {
+        scale->frame_out.picture_width  = outlink->w;
+        scale->frame_out.picture_height = outlink->h;
+        scale->frame_out.picture_format = scaler_format;
+
+        /* Allocate hardware device destination frame. This acquires a frame
+         * from the pool
+         */
+        retcode = ni_device_alloc_frame(&scale->api_ctx,        //
+                                        FFALIGN(outlink->w, 2), //
+                                        FFALIGN(outlink->h, 2), //
+                                        scaler_format,          //
+                                        NI_SCALER_FLAG_IO,      //
+                                        0,                      //
+                                        0,                      //
+                                        0,                      //
+                                        0,                      //
+                                        0,                      //
+                                        -1,                     //
+                                        NI_DEVICE_TYPE_SCALER);
+
+        if (retcode != NI_RETCODE_SUCCESS) {
+            av_log(link->dst, AV_LOG_DEBUG,
+                   "Can't allocate device output frame %d\n", retcode);
+            retcode = AVERROR(ENOMEM);
+            goto fail;
+        }
+    }
+
+    out = av_frame_alloc();
+    if (!out)
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    av_frame_copy_props(out,in);
+
+    out->width  = outlink->w;
+    out->height = outlink->h;
+
+    out->format = AV_PIX_FMT_NI_QUAD;
+
+    /* Quadra 2D engine always outputs limited color range */
+    out->color_range = AVCOL_RANGE_MPEG;
+
+    /* Reference the new hw frames context */
+    out->hw_frames_ctx = av_buffer_ref(scale->out_frames_ref);
+
+    out->data[3] = av_malloc(sizeof(niFrameSurface1_t));
+
+    if (!out->data[3])
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    /* Copy the frame surface from the incoming frame */
+    memcpy(out->data[3], in->data[3], sizeof(niFrameSurface1_t));
+
+    /* Set the new frame index */
+    retcode = ni_device_session_read_hwdesc(&scale->api_ctx, &scale->api_dst_frame,
+                                            NI_DEVICE_TYPE_SCALER);
+
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(link->dst, AV_LOG_ERROR,
+               "Can't acquire output frame %d\n",retcode);
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    tempFID = frame_surface->ui16FrameIdx;
+    frame_surface = (niFrameSurface1_t *)out->data[3];
+    new_frame_surface = (niFrameSurface1_t *)scale->api_dst_frame.data.frame.p_data[3];
+    frame_surface->ui16FrameIdx = new_frame_surface->ui16FrameIdx;
+    frame_surface->ui16session_ID = new_frame_surface->ui16session_ID;
+    frame_surface->device_handle = new_frame_surface->device_handle;
+    frame_surface->output_idx     = new_frame_surface->output_idx;
+    frame_surface->src_cpu = new_frame_surface->src_cpu;
+    frame_surface->bit_depth = ((scale->out_format == AV_PIX_FMT_YUV420P10LE) ||
+                                (scale->out_format == AV_PIX_FMT_P010LE))
+                                   ? 2
+                                   : 1;
+
+    /* Remove ni-split specific assets */
+    frame_surface->ui32nodeAddress = 0;
+
+    frame_surface->ui16width  = out->width;
+    frame_surface->ui16height = out->height;
+
+    av_log(link->dst, AV_LOG_DEBUG,
+           "vf_scale_ni.c:IN trace ui16FrameIdx = [%d] --> out [%d] \n",
+           tempFID, frame_surface->ui16FrameIdx);
+
+    out->buf[0] = av_buffer_create(out->data[3], sizeof(niFrameSurface1_t),
+                                   ff_ni_frame_free, NULL, 0);
+
+    av_frame_free(&in);
+
+    return ff_filter_frame(link->dst->outputs[0], out);
+
+fail:
+    av_frame_free(&in);
+    av_frame_free(&out);
+    return retcode;
+}
+
+#define OFFSET(x) offsetof(NetIntScaleContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+/* clang-format off */
+static const AVOption scale_options[] = {
+    { "w",     "Output video width",          OFFSET(w_expr),    AV_OPT_TYPE_STRING,        .flags = FLAGS },
+    { "width", "Output video width",          OFFSET(w_expr),    AV_OPT_TYPE_STRING,        .flags = FLAGS },
+    { "h",     "Output video height",         OFFSET(h_expr),    AV_OPT_TYPE_STRING,        .flags = FLAGS },
+    { "height","Output video height",         OFFSET(h_expr),    AV_OPT_TYPE_STRING,        .flags = FLAGS },
+    { "size",   "set video size",          OFFSET(size_str), AV_OPT_TYPE_STRING, {.str = NULL}, 0, FLAGS },
+    { "s",      "set video size",          OFFSET(size_str), AV_OPT_TYPE_STRING, {.str = NULL}, 0, FLAGS },
+    { "force_original_aspect_ratio", "decrease or increase w/h if necessary to keep the original AR", OFFSET(force_original_aspect_ratio), AV_OPT_TYPE_INT, { .i64 = 0}, 0, 2, FLAGS, "force_oar" },
+    { "format", "set_output_format", OFFSET(format), AV_OPT_TYPE_INT, {.i64=OUTPUT_FORMAT_AUTO}, 0, OUTPUT_FORMAT_NB-1, FLAGS, "format" },
+        { "yuv420p", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_YUV420P}, .flags = FLAGS, .unit = "format" },
+        { "yuyv422", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_YUYV422}, .flags = FLAGS, .unit = "format" },
+        { "uyvy422", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_UYVY422}, .flags = FLAGS, .unit = "format" },
+        { "nv12", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_NV12}, .flags = FLAGS, .unit = "format" },
+        { "argb", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_ARGB}, .flags = FLAGS, .unit = "format" },
+        { "rgba", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_RGBA}, .flags = FLAGS, .unit = "format" },
+        { "abgr", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_ABGR}, .flags = FLAGS, .unit = "format" },
+        { "bgra", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_BGRA}, .flags = FLAGS, .unit = "format" },
+        { "yuv420p10le", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_YUV420P10LE}, .flags = FLAGS, .unit = "format" },
+        { "nv16", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_NV16}, .flags = FLAGS, .unit = "format" },
+        { "bgr0", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_BGR0}, .flags = FLAGS, .unit = "format" },
+        { "p010le", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_P010LE}, .flags = FLAGS, .unit = "format" },
+        { "bgrp", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_BGRP}, .flags = FLAGS, .unit = "format" },
+        { "auto", "", 0, AV_OPT_TYPE_CONST, {.i64=OUTPUT_FORMAT_AUTO}, .flags = FLAGS, .unit="format"},
+    { "disable",  NULL, 0, AV_OPT_TYPE_CONST, {.i64 = 0 }, 0, 0, FLAGS, "force_oar" },
+    { "decrease", NULL, 0, AV_OPT_TYPE_CONST, {.i64 = 1 }, 0, 0, FLAGS, "force_oar" },
+    { "increase", NULL, 0, AV_OPT_TYPE_CONST, {.i64 = 2 }, 0, 0, FLAGS, "force_oar" },
+    { "force_divisible_by", "enforce that the output resolution is divisible by a defined integer when force_original_aspect_ratio is used", OFFSET(force_divisible_by), AV_OPT_TYPE_INT, { .i64 = 1}, 1, 256, FLAGS },
+    { "filterblit", "filterblit enable", OFFSET(params.filterblit), AV_OPT_TYPE_BOOL, {.i64=0}, 0, 1, FLAGS },
+
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSET(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     FLAGS,
+     "keep_alive_timeout"},
+    { NULL }
+};
+/* clang-format on */
+
+static const AVClass scale_class = {
+    .class_name       = "ni_scale",
+    .item_name        = av_default_item_name,
+    .option           = scale_options,
+    .version          = LIBAVUTIL_VERSION_INT,
+    .category         = AV_CLASS_CATEGORY_FILTER,
+};
+
+static const AVFilterPad avfilter_vf_scale_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad avfilter_vf_scale_outputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_props,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_scale_ni_quadra = {
+    .name            = "ni_quadra_scale",
+    .description     = NULL_IF_CONFIG_SMALL("NetInt Quadra video scaler v" NI_XCODER_REVISION),
+    .init_dict       = init_dict,
+    .uninit          = uninit,
+    .priv_size       = sizeof(NetIntScaleContext),
+    .priv_class      = &scale_class,
+    .flags_internal  = FF_FILTER_FLAG_HWFRAME_AWARE,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(avfilter_vf_scale_inputs),
+    FILTER_OUTPUTS(avfilter_vf_scale_outputs),
+    FILTER_QUERY_FUNC(query_formats),
+#else
+    .inputs          = avfilter_vf_scale_inputs,
+    .outputs         = avfilter_vf_scale_outputs,
+    .query_formats   = query_formats,
+#endif
+};
diff --git a/libavfilter/vf_sdl_ni.c b/libavfilter/vf_sdl_ni.c
new file mode 100644
index 0000000000..3a137f06af
--- /dev/null
+++ b/libavfilter/vf_sdl_ni.c
@@ -0,0 +1,209 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/avassert.h"
+#include "libavutil/opt.h"
+#include "libavutil/time.h"
+#include "libswscale/swscale.h"
+#include "avfilter.h"
+#include "internal.h"
+#include <SDL.h>
+
+
+typedef struct SdlContext {
+    const AVClass *class;
+    int quit;
+    int width;
+    int height;
+    SDL_Renderer *renderer;
+    SDL_Texture *texture;
+    SDL_Window *window;
+} SdlContext;
+
+static int query_formats(AVFilterContext *avctx)
+{
+    static const enum AVPixelFormat pix_fmts[] = {
+        AV_PIX_FMT_RGB24, AV_PIX_FMT_BGR24,
+        AV_PIX_FMT_GRAY8, AV_PIX_FMT_GRAYF32,
+        AV_PIX_FMT_YUV420P, AV_PIX_FMT_YUV422P,
+        AV_PIX_FMT_YUV444P, AV_PIX_FMT_YUV410P, AV_PIX_FMT_YUV411P,
+        AV_PIX_FMT_NONE
+    };
+
+    AVFilterFormats *fmts_list = ff_make_format_list(pix_fmts);
+    if (!fmts_list) {
+        av_log(avctx, AV_LOG_ERROR, "could not create formats list\n");
+        return AVERROR(ENOMEM);
+    }
+
+    return ff_set_common_formats(avctx, fmts_list);
+}
+
+static int sdl_config_input(AVFilterLink *inlink)
+{
+    AVFilterContext *avctx = inlink->dst;
+    SdlContext     *ctx = avctx->priv;
+
+    ctx->window = SDL_CreateWindow("FFmpeg SDL Filter", SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED,
+                                   inlink->w, inlink->h, SDL_WINDOW_RESIZABLE);
+    if (!ctx->window) {
+        av_log(ctx, AV_LOG_ERROR, "Failed to create SDL window %s!", SDL_GetError());
+        return AVERROR(ENOMEM);
+    }
+
+    ctx->renderer = SDL_CreateRenderer(ctx->window, -1, SDL_RENDERER_ACCELERATED | SDL_RENDERER_PRESENTVSYNC);
+    if (!ctx->renderer) {
+        av_log(ctx, AV_LOG_ERROR, "Failed to create SDL renderer %s!", SDL_GetError());
+        return AVERROR(ENOMEM);
+    }
+
+    ctx->texture = SDL_CreateTexture(ctx->renderer, SDL_PIXELFORMAT_IYUV,
+                                     SDL_TEXTUREACCESS_STREAMING, inlink->w , inlink->h);
+    if (!ctx->renderer) {
+        av_log(ctx, AV_LOG_ERROR, "Failed to create SDL texture %s!", SDL_GetError());
+        return AVERROR(ENOMEM);
+    }
+
+    return 0;
+}
+
+static int sdl_filter_frame(AVFilterLink *inlink, AVFrame *frame)
+{
+    AVFilterContext *avctx = inlink->dst;
+    AVFilterLink  *outlink = avctx->outputs[0];
+    SdlContext     *ctx = avctx->priv;
+    SDL_Event event;
+
+    if (SDL_PollEvent(&event)) {
+        switch (event.type) {
+            case SDL_KEYDOWN:
+                switch (event.key.keysym.sym) {
+                case SDLK_ESCAPE:
+                case SDLK_q:
+                    ctx->quit = 1;
+                    break;
+                default:
+                    break;
+                }
+                break;
+            case SDL_QUIT:
+                ctx->quit = 1;
+                break;
+            case SDL_WINDOWEVENT:
+                switch (event.window.event) {
+                    case SDL_WINDOWEVENT_RESIZED:
+                    case SDL_WINDOWEVENT_SIZE_CHANGED:
+                        ctx->width = event.window.data1;
+                        ctx->height = event.window.data2;
+                    default:
+                        break;
+                }
+                break;
+            default:
+                break;
+        }
+    }
+
+    if (ctx->quit) {
+        SDL_DestroyTexture(ctx->texture);
+        SDL_DestroyRenderer(ctx->renderer);
+        SDL_DestroyWindow(ctx->window);
+        SDL_Quit();
+    } else {
+        SDL_UpdateYUVTexture(ctx->texture, NULL,
+                             frame->data[0], frame->linesize[0],
+                             frame->data[1], frame->linesize[1],
+                             frame->data[2], frame->linesize[2]);
+        SDL_RenderClear(ctx->renderer);
+        SDL_RenderCopyEx(ctx->renderer, ctx->texture, NULL, NULL, 0, NULL, 0);
+        SDL_RenderPresent(ctx->renderer);
+    }
+
+    return ff_filter_frame(outlink, frame);
+}
+
+static av_cold int sdl_init(AVFilterContext *avctx)
+{
+    if (SDL_Init(SDL_INIT_VIDEO) < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Failed to init SDL %s!", SDL_GetError());
+        return AVERROR(ENOMEM);
+    }
+
+    return 0;
+}
+
+static av_cold void sdl_uninit(AVFilterContext *avctx)
+{
+    SdlContext *ctx = avctx->priv;
+
+    if (!ctx->quit) {
+        SDL_DestroyTexture(ctx->texture);
+        SDL_DestroyRenderer(ctx->renderer);
+        SDL_DestroyWindow(ctx->window);
+        SDL_Quit();
+    }
+}
+
+#define OFFSET(x) offsetof(SdlContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+static const AVOption sdl_options[] = {
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(sdl);
+
+static const AVFilterPad sdl_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = sdl_config_input,
+        .filter_frame = sdl_filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad sdl_outputs[] = {
+    {
+        .name = "default",
+        .type = AVMEDIA_TYPE_VIDEO,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_sdl_ni_quadra = {
+    .name          = "sdl_ni_quadra",
+    .description   = NULL_IF_CONFIG_SMALL("Use SDL2.0 to display AVFrame."),
+    .init          = sdl_init,
+    .uninit        = sdl_uninit,
+
+    .priv_size     = sizeof(SdlContext),
+    .priv_class    = &sdl_class,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(sdl_inputs),
+    FILTER_OUTPUTS(sdl_outputs),
+    FILTER_QUERY_FUNC(query_formats),
+#else
+    .inputs        = sdl_inputs,
+    .outputs       = sdl_outputs,
+    .query_formats = query_formats,
+#endif
+};
diff --git a/libavfilter/vf_split_ni.c b/libavfilter/vf_split_ni.c
new file mode 100644
index 0000000000..bd1b718825
--- /dev/null
+++ b/libavfilter/vf_split_ni.c
@@ -0,0 +1,527 @@
+/*
+ * Copyright (c) 2007 Bobby Bingham
+ * Copyright (c) 2021 NetInt
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * audio and video splitter
+ */
+
+#include <stdio.h>
+
+#include "libavutil/attributes.h"
+#include "libavutil/internal.h"
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_internal.h"
+#include "libavutil/hwcontext_ni_quad.h"
+#include "libavutil/mem.h"
+#include "libavutil/opt.h"
+#include "nifilter.h"
+#include <ni_device_api.h>
+
+#define FF_INTERNAL_FIELDS 1
+#include "framequeue.h"
+
+#include "avfilter.h"
+#include "audio.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+
+typedef struct SplitContext {
+    const AVClass *class;
+    int nb_output0;
+    int nb_output1;
+    int nb_output2;
+    int total_outputs;
+    int frame_contexts_applied;
+    ni_split_context_t src_ctx;
+    AVBufferRef *out_frames_ref[3];
+} SplitContext;
+
+static int init_out_hwctxs(AVFilterContext *ctx)
+{
+  SplitContext *s = ctx->priv;
+  AVHWFramesContext *in_frames_ctx;
+  AVHWFramesContext *out_frames_ctx[3];
+
+  enum AVPixelFormat out_format;
+  int i,j;
+
+  if (!ctx->inputs[0]->hw_frames_ctx) {
+    return AVERROR(EINVAL);
+  }
+  
+  in_frames_ctx = (AVHWFramesContext*)ctx->inputs[0]->hw_frames_ctx->data;
+  if (s->src_ctx.enabled == 1)
+  {
+    for (i = 0; i < 3; i++)
+    {
+      s->out_frames_ref[i] = av_hwframe_ctx_alloc(in_frames_ctx->device_ref);
+      if (!s->out_frames_ref[i])
+        return AVERROR(ENOMEM);
+      out_frames_ctx[i] = (AVHWFramesContext*)s->out_frames_ref[i]->data;
+
+      out_frames_ctx[i]->format = AV_PIX_FMT_NI_QUAD;
+      out_frames_ctx[i]->width = s->src_ctx.w[i];
+      out_frames_ctx[i]->height = s->src_ctx.h[i];
+
+      if (s->src_ctx.f[i] == -1)
+      {
+        return AVERROR(EINVAL);
+      }
+
+      if (s->src_ctx.f[i] == NI_PIXEL_PLANAR_FORMAT_PLANAR)//yuv420p or p10
+      {
+        out_format = (s->src_ctx.f8b[i] == 1) ? AV_PIX_FMT_YUV420P : AV_PIX_FMT_YUV420P10LE;
+      }
+      else //NV12
+      {
+        out_format = (s->src_ctx.f8b[i] == 1) ? AV_PIX_FMT_NV12 : AV_PIX_FMT_P010LE;
+      }
+      out_frames_ctx[i]->sw_format = out_format;
+      out_frames_ctx[i]->initial_pool_size = -1; //already has its own pool
+
+      /* Don't check return code, this will intentionally fail */
+      av_hwframe_ctx_init(s->out_frames_ref[i]);
+
+      ff_ni_clone_hwframe_ctx(in_frames_ctx, out_frames_ctx[i]);
+    }
+
+    for (i = 0; i < ctx->nb_outputs; i++)
+    {
+      av_buffer_unref(&ctx->outputs[i]->hw_frames_ctx);
+      if (i < s->nb_output0)
+      {
+        j = 0;
+      }
+      else if (i < s->nb_output0 + s->nb_output1)
+      {
+        j = 1;
+      }
+      else
+      {
+        j = 2;
+      }
+      ctx->outputs[i]->hw_frames_ctx = av_buffer_ref(s->out_frames_ref[j]);
+      av_log(ctx, AV_LOG_DEBUG, "NI:%splanar:out\n",
+             (s->src_ctx.f[j] == NI_PIXEL_PLANAR_FORMAT_SEMIPLANAR) ? "semi" : "");
+      if (!ctx->outputs[i]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+      av_log(ctx, AV_LOG_DEBUG,
+             "ni_split superframe config_output_hwctx[%d] %p\n", i,
+             ctx->outputs[i]->hw_frames_ctx);
+    }
+  }
+  else 
+  {//no possibility of using extra outputs
+    for (i = 0; i < ctx->nb_outputs; i++)
+    {
+      av_buffer_unref(&ctx->outputs[i]->hw_frames_ctx);
+      if (i < s->nb_output0)
+      {
+        ctx->outputs[i]->hw_frames_ctx = av_buffer_ref(ctx->inputs[0]->hw_frames_ctx);
+      }
+      if (!ctx->outputs[i]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+      av_log(ctx, AV_LOG_DEBUG, "ni_split config_output_hwctx[%d] %p\n", i,
+             ctx->outputs[i]->hw_frames_ctx);
+    }
+    av_log(ctx, AV_LOG_DEBUG, "ni_split config_output_hwctx set direct to output\n");
+  }
+  return 0;
+}
+
+static int config_output(AVFilterLink *link)
+{
+  //config output sets all outputs at a time since there's no 
+  //easy way to track the target output based on inlink.
+  //fairly trivial assignments here so no performance worries
+  AVFilterContext *ctx = link->src;
+  SplitContext *s = ctx->priv;
+  int i, ret;
+  for (i = 0; i < ctx->nb_outputs; i++)
+  {
+    if (i < s->nb_output0)
+    {
+      ctx->outputs[i]->w = s->src_ctx.w[0];
+      ctx->outputs[i]->h = s->src_ctx.h[0];
+    }
+    else if (i < s->nb_output0 + s->nb_output1)
+    {
+      ctx->outputs[i]->w = s->src_ctx.w[1];
+      ctx->outputs[i]->h = s->src_ctx.h[1];
+    }
+    else 
+    {
+      ctx->outputs[i]->w = s->src_ctx.w[2];
+      ctx->outputs[i]->h = s->src_ctx.h[2];
+    }
+    av_log(ctx, AV_LOG_DEBUG, "ni_split config_output[%d] w x h = %d x %d\n",
+           i, ctx->outputs[i]->w, ctx->outputs[i]->h);
+  }
+  if (s->frame_contexts_applied == 0)
+  {
+    s->frame_contexts_applied = 1; //run once per set ni_split, not per output
+    ret = init_out_hwctxs(ctx);
+    if (ret < 0)
+      return ret;
+  }
+  return 0;
+}
+
+static av_cold int split_init(AVFilterContext *ctx)
+{
+    SplitContext *s = ctx->priv;
+    int i, ret;
+    av_log(ctx, AV_LOG_DEBUG, "nisplitINIT out0,1,2 = %d %d %d ctx->nb_outputs = %d\n",
+        s->nb_output0, s->nb_output1, s->nb_output2,
+        ctx->nb_outputs);
+    if (s->nb_output2 && s->nb_output1 == 0)
+    {
+        //swap them for reorder to use out1 first
+        s->nb_output1 = s->nb_output2;
+        s->nb_output2 = 0;
+        av_log(ctx, AV_LOG_DEBUG, "nisplitINIT out2 moved to out1\n");
+    }
+
+    s->total_outputs = s->nb_output0 + s->nb_output1 + s->nb_output2;
+    //ctx->nb_outputs = s->total_outputs;
+    for (i = 0; i < s->total_outputs; i++) {
+        char name[32];
+        AVFilterPad pad = { 0 };
+
+        snprintf(name, sizeof(name), "output%d", i);
+        pad.type = ctx->filter->inputs[0].type;
+        pad.name = av_strdup(name);
+        pad.config_props = config_output;
+        
+        if (!pad.name)
+            return AVERROR(ENOMEM);
+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+        if ((ret = ff_append_outpad(ctx, &pad)) < 0) {
+#else
+        if ((ret = ff_insert_outpad(ctx, i, &pad)) < 0) {
+#endif
+            av_freep(&pad.name);
+            return ret;
+        }
+    }
+
+    return 0;
+}
+
+
+static int query_formats(AVFilterContext *ctx) 
+{
+    static const enum AVPixelFormat input_pix_fmts[] = {
+        AV_PIX_FMT_YUV420P,
+        AV_PIX_FMT_YUVJ420P,
+        AV_PIX_FMT_YUV420P10LE,
+        AV_PIX_FMT_NV12,
+        AV_PIX_FMT_P010LE,
+        AV_PIX_FMT_NI_QUAD,
+        AV_PIX_FMT_NONE,
+    };
+    static const enum AVPixelFormat output_pix_fmts[] = {
+        AV_PIX_FMT_YUV420P,
+        AV_PIX_FMT_YUVJ420P,
+        AV_PIX_FMT_YUV420P10LE,
+        AV_PIX_FMT_NV12,
+        AV_PIX_FMT_P010LE,
+        AV_PIX_FMT_NI_QUAD,
+        AV_PIX_FMT_NONE,
+    };
+    AVFilterFormats *in_fmts = ff_make_format_list(input_pix_fmts);
+    AVFilterFormats *out_fmts = ff_make_format_list(output_pix_fmts);
+
+    // Needed for FFmpeg-n4.4+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+    // NOLINTNEXTLINE(clang-diagnostic-unused-result)
+    ff_formats_ref(in_fmts, &ctx->inputs[0]->outcfg.formats);
+    // NOLINTNEXTLINE(clang-diagnostic-unused-result)
+    ff_formats_ref(out_fmts, &ctx->outputs[0]->incfg.formats);
+#else
+    // NOLINTNEXTLINE(clang-diagnostic-unused-result)
+    ff_formats_ref(in_fmts, &ctx->inputs[0]->out_formats);
+    // NOLINTNEXTLINE(clang-diagnostic-unused-result)
+    ff_formats_ref(out_fmts, &ctx->outputs[0]->in_formats);
+#endif
+
+    return 0;
+}
+
+static int config_input(AVFilterLink *inlink)
+{
+  AVFilterContext *avctx = inlink->dst;
+  SplitContext *s = avctx->priv;
+  AVHWFramesContext *ctx;
+  ni_split_context_t *p_split_ctx_dst = &s->src_ctx;
+  NIFramesContext *src_ctx;
+  ni_split_context_t *p_split_ctx_src; 
+  //av_log(ctx, AV_LOG_DEBUG, "nisplit config_input 0x%lx\n", inlink->hw_frames_ctx);
+  int i;
+  s->frame_contexts_applied = -1;
+  if (inlink->hw_frames_ctx == NULL)
+  {
+      for (i = 0; i < 3; i++)
+      {
+          s->src_ctx.w[i] = inlink->w;
+          s->src_ctx.h[i] = inlink->h;
+          s->src_ctx.f[i] = -1;
+          s->src_ctx.f8b[i] = -1;
+      }
+  }
+  else
+  {
+      ctx = (AVHWFramesContext*)inlink->hw_frames_ctx->data;
+      src_ctx = ctx->internal->priv;
+      p_split_ctx_src = &src_ctx->split_ctx;
+      memcpy(p_split_ctx_dst, p_split_ctx_src, sizeof(ni_split_context_t));
+      for (i = 0; i < 3; i++)
+      {
+          s->frame_contexts_applied = 0;
+          av_log(avctx, AV_LOG_DEBUG, "[%d] %d x %d  f8b %d\n", 
+                 i, p_split_ctx_dst->w[i], p_split_ctx_dst->h[i], p_split_ctx_dst->f8b[i]);
+                 //p_split_ctx_dst->crop_meta_data_rltb[i][0],
+                 //p_split_ctx_dst->crop_meta_data_rltb[i][1], 
+                 //p_split_ctx_dst->crop_meta_data_rltb[i][2], 
+                 //p_split_ctx_dst->crop_meta_data_rltb[i][3]);
+      }
+      if (p_split_ctx_dst->enabled == 0)
+      {
+          for (i = 0; i < 3; i++)
+          {
+              s->src_ctx.w[i] = inlink->w;
+              s->src_ctx.h[i] = inlink->h;
+              s->src_ctx.f[i] = -1;
+              s->src_ctx.f8b[i] = -1;
+          }
+      }
+  }
+  
+  return 0;
+}
+
+
+
+static int filter_ni_frame(AVFilterLink *inlink, AVFrame *frame)
+{
+    AVFilterContext *ctx = inlink->dst;
+    SplitContext *s = ctx->priv;
+    int i, ret = AVERROR_EOF;
+    int output_index;
+#if 0
+    bool use_crop_from_ctx = true;
+
+    if (!frame->buf[1])
+    {
+        use_crop_from_ctx = false; //source frame not from super frame
+    }
+#endif
+    for (i = 0; i < ctx->nb_outputs; i++) 
+    {
+        AVFrame *buf_out;
+
+        if (ctx->outputs[i]->status_in)
+            continue;
+
+        buf_out = av_frame_alloc();
+        if (!buf_out) {
+          ret = AVERROR(ENOMEM);
+          break;
+        }
+        av_frame_copy_props(buf_out, frame);
+
+        //buf_out->width = frame->width;  //update with real frame info when multioutput ready
+        //buf_out->height = frame->height;//
+        buf_out->format = frame->format;
+        //buf_out->hw_frames_ctx = av_buffer_ref(frame->hw_frames_ctx);//av_buffer_ref(src->hw_frames_ctx);
+
+        if (i < s->nb_output0)
+        {
+            output_index = 0;
+        }
+        else if (i < s->nb_output0 + s->nb_output1)
+        {
+            if (!frame->buf[1])
+            {
+                ret = AVERROR(ENOMEM);
+                av_frame_free(&buf_out);
+                break;
+            }
+            output_index = 1;
+        }
+        else
+        {
+            if (!frame->buf[2])
+            {
+                ret = AVERROR(ENOMEM);
+                av_frame_free(&buf_out);
+                break;
+            }
+            output_index = 2;
+        }
+        buf_out->buf[0]        = av_buffer_ref(frame->buf[output_index]);
+        buf_out->hw_frames_ctx = av_buffer_ref(s->out_frames_ref[output_index]);
+#if 0
+        if (use_crop_from_ctx) //overwrites work from av_frame_copy_props
+        {
+            buf_out->crop_right = s->src_ctx.crop_meta_data_rltb[output_index][0];
+            buf_out->crop_left = s->src_ctx.crop_meta_data_rltb[output_index][1];
+            buf_out->crop_top = s->src_ctx.crop_meta_data_rltb[output_index][2];
+            buf_out->crop_bottom = s->src_ctx.crop_meta_data_rltb[output_index][3];
+        }
+#endif 
+        buf_out->data[3] = buf_out->buf[0]->data;
+        niFrameSurface1_t* p_data3 = (niFrameSurface1_t*)((uint8_t*)buf_out->data[3]);
+        
+        buf_out->width = ctx->outputs[i]->w = p_data3->ui16width;
+        buf_out->height = ctx->outputs[i]->h = p_data3->ui16height;
+        
+
+        av_log(ctx, AV_LOG_DEBUG, "output %d supplied WxH = %d x %d FID %d offset %d\n",
+               i, buf_out->width, buf_out->height,
+                p_data3->ui16FrameIdx, p_data3->ui32nodeAddress);
+
+        ret = ff_filter_frame(ctx->outputs[i], buf_out);
+        if (ret < 0)
+            break;
+    }
+    return ret;
+}
+
+static int filter_std_frame(AVFilterLink *inlink, AVFrame *frame)
+{//basically clone of native split
+    AVFilterContext *ctx = inlink->dst;
+    SplitContext *s = ctx->priv;
+    int i, ret = AVERROR_EOF;
+    if (s->nb_output0 < 2)
+    {
+        av_log(ctx, AV_LOG_ERROR, "ni_split must have at least 2 outputs for Standard split!\n");
+        ret = AVERROR(EINVAL);
+        return ret;
+    }
+    if (s->nb_output1) {
+        av_log(ctx, AV_LOG_ERROR, "ni_split output1 or output2 param must not be used for Standard splitting!\n");
+        ret = AVERROR(E2BIG);
+        return ret;
+    }
+
+    for (i = 0; i < ctx->nb_outputs; i++) {
+        AVFrame *buf_out;
+
+        if (ctx->outputs[i]->status_in)
+            continue;
+        buf_out = av_frame_clone(frame);
+        if (!buf_out) {
+            ret = AVERROR(ENOMEM);
+            break;
+        }
+
+        ret = ff_filter_frame(ctx->outputs[i], buf_out);
+        if (ret < 0)
+            break;
+    }
+    return ret;
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *frame) //separate hw and sw into different function calls
+{
+    AVFilterContext *ctx = inlink->dst;
+    SplitContext *s = ctx->priv;
+    int ret              = AVERROR_EOF;
+    av_log(ctx, AV_LOG_TRACE, "out0,1,2 = %d %d %d total = %d\n",
+        s->nb_output0, s->nb_output1, s->nb_output2,
+        ctx->nb_outputs);
+    av_log(ctx, AV_LOG_DEBUG, "ni_split: filter_frame, in format=%d, Sctx %d\n",
+      frame->format,
+      s->src_ctx.enabled);
+    if (frame->format == AV_PIX_FMT_NI_QUAD && s->src_ctx.enabled == 1)
+    {
+        ret = filter_ni_frame(inlink, frame);
+    }
+    else
+    {
+        ret = filter_std_frame(inlink, frame);
+    }
+    av_frame_free(&frame);
+    return ret;
+}
+
+#define OFFSET(x) offsetof(SplitContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+static const AVOption options[] = {
+    { "output0", "Copies of output0", OFFSET(nb_output0), AV_OPT_TYPE_INT, { .i64 = 2 }, 0, INT_MAX, FLAGS },
+    { "output1", "Copies of output1", OFFSET(nb_output1), AV_OPT_TYPE_INT, { .i64 = 0 }, 0, INT_MAX, FLAGS },
+    { "output2", "Copies of output2", OFFSET(nb_output2), AV_OPT_TYPE_INT, { .i64 = 0 }, 0, INT_MAX, FLAGS },
+    { NULL }
+};
+
+#define split_options options
+AVFILTER_DEFINE_CLASS(split);
+
+static av_cold void split_uninit(AVFilterContext *ctx)
+{
+  int i;
+  SplitContext *s = ctx->priv;
+  for (i = 0; i < ctx->nb_outputs-1; i++)
+    av_freep(&ctx->output_pads[i].name);
+
+  for (i = 0; i < 3; i++)
+  {
+    if(s->out_frames_ref[i])
+      av_buffer_unref(&s->out_frames_ref[i]);
+  }
+}
+
+static const AVFilterPad avfilter_vf_split_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_input,
+        .filter_frame = filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_split_ni_quadra = {
+    .name = "ni_quadra_split",
+    .description =
+        NULL_IF_CONFIG_SMALL("NetInt Quadra pass on the input to N video outputs v" NI_XCODER_REVISION),
+    .priv_size      = sizeof(SplitContext),
+    .priv_class     = &split_class,
+    .init           = split_init,
+    .uninit         = split_uninit,
+    .flags          = AVFILTER_FLAG_DYNAMIC_OUTPUTS,
+    .flags_internal = FF_FILTER_FLAG_HWFRAME_AWARE,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(avfilter_vf_split_inputs),
+    FILTER_QUERY_FUNC(query_formats),
+#else
+    .inputs         = avfilter_vf_split_inputs,
+    .query_formats  = query_formats,
+#endif
+};
diff --git a/libavfilter/vf_stack_ni.c b/libavfilter/vf_stack_ni.c
new file mode 100644
index 0000000000..4cdbf9596d
--- /dev/null
+++ b/libavfilter/vf_stack_ni.c
@@ -0,0 +1,718 @@
+/*
+ * Copyright (c) 2015 Paul B. Mahol
+ * Copyright (c) 2022 NETINT
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/avstring.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/opt.h"
+#include "libavutil/parseutils.h"
+#include "libavutil/pixdesc.h"
+
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "framesync.h"
+#include "video.h"
+
+#include "nifilter.h"
+
+#define MAX_INPUTS 8
+#define MAX_XSTACK_INPUTS 50
+
+typedef struct StackItem {
+    int x, y;
+    int w;
+    int h;
+} StackItem;
+
+typedef struct StackContext {
+    const AVClass *class;
+    const AVPixFmtDescriptor *desc;
+    int nb_inputs;
+    char *layout;
+    char *size;
+    int shortest;
+    uint8_t fillcolor[4];
+    char *fillcolor_str;
+    int fillcolor_enable;
+    int sync;
+
+    StackItem *items;
+    AVFrame **frames;
+    FFFrameSync fs;
+
+    // The rest of this structure is NETINT HW specific
+
+    enum AVPixelFormat out_format;
+    AVBufferRef *out_frames_ref;
+
+    ni_session_context_t api_ctx;
+    ni_session_data_io_t api_dst_frame;
+    ni_scaler_params_t params;
+
+    int initialized;
+    int session_opened;
+    int keep_alive_timeout;
+
+    ni_frame_config_t frame_in[MAX_XSTACK_INPUTS];
+    ni_frame_config_t frame_out;
+} StackContext;
+
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] =
+        {AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE};
+    AVFilterFormats *formats;
+
+    formats = ff_make_format_list(pix_fmts);
+
+    if (!formats)
+        return AVERROR(ENOMEM);
+
+    return ff_set_common_formats(ctx, formats);
+}
+
+static av_cold int init(AVFilterContext *ctx)
+{
+    StackContext *s = ctx->priv;
+    int i, ret;
+
+    // NOLINTNEXTLINE(bugprone-sizeof-expression)
+    s->frames = av_calloc(s->nb_inputs, sizeof(*s->frames));
+    if (!s->frames)
+        return AVERROR(ENOMEM);
+
+    s->items = av_calloc(s->nb_inputs, sizeof(*s->items));
+    if (!s->items)
+        return AVERROR(ENOMEM);
+
+    if ((strcmp(s->fillcolor_str, "none") != 0) &&
+        av_parse_color(s->fillcolor, s->fillcolor_str, -1, ctx) >= 0) {
+        s->fillcolor_enable = 1;
+    } else {
+        s->fillcolor_enable = 0;
+    }
+    if (!s->layout) {
+        if (s->nb_inputs == 2) {
+            s->layout = av_strdup("0_0|w0_0");
+            if (!s->layout)
+                return AVERROR(ENOMEM);
+        } else {
+            av_log(ctx, AV_LOG_ERROR, "No layout specified.\n");
+            return AVERROR(EINVAL);
+        }
+    }
+
+    for (i = 0; i < s->nb_inputs; i++) {
+        AVFilterPad pad = { 0 };
+
+        pad.type = AVMEDIA_TYPE_VIDEO;
+        pad.name = av_asprintf("input%d", i);
+        if (!pad.name)
+            return AVERROR(ENOMEM);
+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+        if ((ret = ff_append_inpad(ctx, &pad)) < 0) {
+#else
+        if ((ret = ff_insert_inpad(ctx, i, &pad)) < 0) {
+#endif
+            av_freep(&pad.name);
+            return ret;
+        }
+    }
+
+    return 0;
+}
+
+static int init_out_pool(AVFilterContext *ctx)
+{
+    StackContext *s = ctx->priv;
+    AVHWFramesContext *out_frames_ctx;
+
+    if (!ctx->inputs[0]->hw_frames_ctx) {
+        return AVERROR(EINVAL);
+    }
+
+    out_frames_ctx = (AVHWFramesContext *)s->out_frames_ref->data;
+
+    /* Don't check return code, this will intentionally fail */
+    av_hwframe_ctx_init(s->out_frames_ref);
+
+    /* Create frame pool on device */
+    return ff_ni_build_frame_pool(&s->api_ctx, out_frames_ctx->width,
+                                  out_frames_ctx->height, s->out_format,
+                                  DEFAULT_NI_FILTER_POOL_SIZE);
+}
+
+static int process_frame(FFFrameSync *fs)
+{
+    AVFilterContext *ctx = fs->parent;
+    AVFilterLink *outlink = ctx->outputs[0];
+    StackContext *s = fs->opaque;
+    AVFrame **in = s->frames;
+    AVFrame *out = NULL;
+    niFrameSurface1_t *frame_surface, *new_frame_surface;
+    int i, p, ret;
+
+    AVFilterLink *inlink0 = ctx->inputs[0];
+    AVHWFramesContext *pAVHFWCtx;
+    AVNIDeviceContext *pAVNIDevCtx;
+    ni_retcode_t retcode;
+    int scaler_format;
+    uint16_t outFrameIdx;
+    int num_cfg_inputs = MAX_INPUTS;
+
+    for (i = 0; i < s->nb_inputs; i++) {
+        if ((ret = ff_framesync_get_frame(&s->fs, i, &in[i], 0)) < 0)
+            return ret;
+    }
+
+    if (!s->initialized) {
+        int cardno, tmp_cardno;
+        cardno = ni_get_cardno(in[0]);
+
+        for (i = 1; i < s->nb_inputs; i++) {
+            tmp_cardno = ni_get_cardno(in[i]);
+            if (tmp_cardno != cardno) {
+                // All inputs must be on the same Quadra device
+                return AVERROR(EINVAL);
+            }
+        }
+
+        pAVHFWCtx = (AVHWFramesContext *) inlink0->hw_frames_ctx->data;
+        pAVNIDevCtx = (AVNIDeviceContext *) pAVHFWCtx->device_ctx->hwctx;
+
+        retcode = ni_device_session_context_init(&s->api_ctx);
+        if (retcode < 0) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "ni stack filter session context init failure\n");
+            goto fail;
+        }
+
+        s->api_ctx.device_handle = pAVNIDevCtx->cards[cardno];
+        s->api_ctx.blk_io_handle = pAVNIDevCtx->cards[cardno];
+
+        s->api_ctx.hw_id             = cardno;
+        s->api_ctx.device_type       = NI_DEVICE_TYPE_SCALER;
+        s->api_ctx.scaler_operation  = NI_SCALER_OPCODE_STACK;
+        s->api_ctx.keep_alive_timeout = s->keep_alive_timeout;
+
+        av_log(ctx, AV_LOG_ERROR,
+               "Open scaler session to card %d, hdl %d, blk_hdl %d\n", cardno,
+               s->api_ctx.device_handle, s->api_ctx.blk_io_handle);
+
+        retcode =
+            ni_device_session_open(&s->api_ctx, NI_DEVICE_TYPE_SCALER);
+        if (retcode < 0) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "Can't open device session on card %d\n", cardno);
+            goto fail;
+        }
+
+        s->session_opened = 1;
+
+        retcode = init_out_pool(ctx);
+
+        if (retcode < 0)
+        {
+            av_log(ctx, AV_LOG_ERROR,
+                   "Internal output allocation failed rc = %d\n", retcode);
+            goto fail;
+        }
+
+        if (s->nb_inputs < MAX_INPUTS) {
+            s->params.nb_inputs = s->nb_inputs;
+        } else {
+            s->params.nb_inputs = MAX_INPUTS;
+        }
+        retcode = ni_scaler_set_params(&s->api_ctx, &(s->params));
+        if (retcode < 0)
+            goto fail;
+
+        ff_ni_clone_hwframe_ctx(
+            pAVHFWCtx, (AVHWFramesContext *)s->out_frames_ref->data);
+
+        s->initialized = 1;
+    }
+
+    out = av_frame_alloc();
+    if (!out)
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    av_frame_copy_props(out, in[s->sync]);
+
+    out->width  = outlink->w;
+    out->height = outlink->h;
+
+    out->format = AV_PIX_FMT_NI_QUAD;
+
+    /* Reference the new hw frames context */
+    out->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    out->data[3] = av_malloc(sizeof(niFrameSurface1_t));
+    if (!out->data[3])
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    /* Copy the frame surface from the incoming frame */
+    memcpy(out->data[3], in[0]->data[3], sizeof(niFrameSurface1_t));
+
+    retcode = ni_frame_buffer_alloc_hwenc(&s->api_dst_frame.data.frame,
+                                          outlink->w,
+                                          outlink->h,
+                                          0);
+    if (retcode != NI_RETCODE_SUCCESS)
+    {
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    /* Allocate hardware device destination frame. This acquires a frame
+     * from the pool
+     */
+    retcode = ni_device_session_read_hwdesc(&s->api_ctx, &s->api_dst_frame,
+                                            NI_DEVICE_TYPE_SCALER);
+    if (retcode != NI_RETCODE_SUCCESS) {
+        av_log(ctx, AV_LOG_ERROR,
+               "Can't acquire output frame %d\n",retcode);
+        retcode = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    frame_surface = (niFrameSurface1_t *)out->data[3];
+    new_frame_surface = (niFrameSurface1_t *)s->api_dst_frame.data.frame.p_data[3];
+    frame_surface->ui16FrameIdx = new_frame_surface->ui16FrameIdx;
+    frame_surface->ui16session_ID = new_frame_surface->ui16session_ID;
+    frame_surface->device_handle = new_frame_surface->device_handle;
+    frame_surface->output_idx = new_frame_surface->output_idx;
+    frame_surface->src_cpu = new_frame_surface->src_cpu;
+    frame_surface->bit_depth = ((s->out_format == AV_PIX_FMT_YUV420P10LE) ||
+                                (s->out_format == AV_PIX_FMT_P010LE))
+                                   ? 2
+                                   : 1;
+
+    /* Remove ni-split specific assets */
+    frame_surface->ui32nodeAddress = 0;
+
+    frame_surface->ui16width  = out->width;
+    frame_surface->ui16height = out->height;
+
+    av_log(ctx, AV_LOG_DEBUG,
+           "vf_stack_ni.c:OUT trace ui16FrameIdx = [%d]\n",
+           frame_surface->ui16FrameIdx);
+
+    out->pts = av_rescale_q(s->fs.pts, s->fs.time_base, outlink->time_base);
+    out->sample_aspect_ratio = outlink->sample_aspect_ratio;
+
+    outFrameIdx = frame_surface->ui16FrameIdx;
+
+    if (s->fillcolor_enable == 1) {
+        s->frame_out.options = NI_SCALER_FLAG_FCE;
+    }
+
+    i = 0;
+    for (p = s->nb_inputs; p > 0; p -= MAX_INPUTS) {
+        int start = i;
+        int end = i + MAX_INPUTS;
+
+        if (end > s->nb_inputs) {
+           num_cfg_inputs = p;
+           end = s->nb_inputs;
+        }
+
+        for ( ; i < end; i++) {
+            AVFilterLink *inlink = ctx->inputs[i];
+            pAVHFWCtx = (AVHWFramesContext *) inlink->hw_frames_ctx->data;
+
+            scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(pAVHFWCtx->sw_format);
+
+            frame_surface = (niFrameSurface1_t *) in[i]->data[3];
+            if (frame_surface == NULL) {
+                return AVERROR(EINVAL);
+            }
+
+            s->frame_in[i].picture_width  = FFALIGN(in[i]->width, 2);
+            s->frame_in[i].picture_height = FFALIGN(in[i]->height, 2);
+            s->frame_in[i].picture_format = scaler_format;
+            s->frame_in[i].session_id     = frame_surface->ui16session_ID;
+            s->frame_in[i].output_index   = frame_surface->output_idx;
+            s->frame_in[i].frame_index    = frame_surface->ui16FrameIdx;
+
+            // Where to place the input into the output
+            s->frame_in[i].rectangle_x    = s->items[i].x;
+            s->frame_in[i].rectangle_y    = s->items[i].y;
+            s->frame_in[i].rectangle_width = s->items[i].w;
+            s->frame_in[i].rectangle_height = s->items[i].h;
+
+            av_log(ctx, AV_LOG_DEBUG,
+               "vf_stack_ni.c:IN %d, ui16FrameIdx = [%d]\n",
+               i, frame_surface->ui16FrameIdx);
+        }
+
+        scaler_format = ff_ni_ffmpeg_to_gc620_pix_fmt(s->out_format);
+
+        s->frame_out.picture_width  = FFALIGN(outlink->w, 2);
+        s->frame_out.picture_height = FFALIGN(outlink->h, 2);
+        s->frame_out.picture_format = scaler_format;
+        s->frame_out.frame_index    = outFrameIdx;
+        s->frame_out.options        |= NI_SCALER_FLAG_IO;
+        if (s->frame_out.options & NI_SCALER_FLAG_FCE) {
+            s->frame_out.rgba_color = (s->fillcolor[3] << 24) | (s->fillcolor[0] << 16) |
+                                      (s->fillcolor[1] << 8) | s->fillcolor[2];
+        } else {
+            s->frame_out.rgba_color = 0;
+        }
+
+        /*
+         * Config device frame parameters
+         */
+        retcode = ni_device_multi_config_frame(&s->api_ctx, &s->frame_in[start],
+                                               num_cfg_inputs, &s->frame_out);
+
+        if (retcode != NI_RETCODE_SUCCESS) {
+            av_log(ctx, AV_LOG_DEBUG,
+                   "Can't transfer config frames %d\n", retcode);
+            retcode = AVERROR(ENOMEM);
+            goto fail;
+        }
+
+        /* Only fill the output frame once each process_frame */
+        s->frame_out.options &= ~NI_SCALER_FLAG_FCE;
+    }
+
+    out->buf[0] = av_buffer_create(out->data[3], sizeof(niFrameSurface1_t),
+                                   ff_ni_frame_free, NULL, 0);
+
+    return ff_filter_frame(outlink, out);
+
+fail:
+    av_frame_free(&out);
+    return retcode;
+}
+
+static int config_output(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    StackContext *s = ctx->priv;
+    AVRational frame_rate = ctx->inputs[0]->frame_rate;
+    AVRational sar = ctx->inputs[0]->sample_aspect_ratio;
+    int height, width;
+    FFFrameSyncIn *in;
+    int i, ret;
+    AVHWFramesContext *in_frames_ctx;
+    AVHWFramesContext *in_frames_ctx0;
+    AVHWFramesContext *out_frames_ctx;
+    char *arg, *p, *saveptr = NULL;
+    char *arg2, *p2, *saveptr2 = NULL;
+    char *arg3, *p3, *saveptr3 = NULL;
+    int inw, inh, size;
+
+    s->desc = av_pix_fmt_desc_get(outlink->format);
+    if (!s->desc)
+        return AVERROR_BUG;
+
+    if (s->size == NULL) {
+        for (i = 0; i < s->nb_inputs; i++) {
+            AVFilterLink *inlink = ctx->inputs[i];
+            StackItem *item = &s->items[i];
+
+            item->w = FFALIGN(inlink->w,2);
+            item->h = FFALIGN(inlink->h,2);
+        }
+    } else {
+        p = s->size;
+        for (i = 0; i < s->nb_inputs; i++) {
+            StackItem *item = &s->items[i];
+
+            if (!(arg = av_strtok(p, "|", &saveptr)))
+               return AVERROR(EINVAL);
+
+            p = NULL;
+
+            p2 = arg;
+            inw = inh = 0;
+
+            for (int j = 0; j < 2; j++) {
+                if (!(arg2 = av_strtok(p2, "_", &saveptr2)))
+                    return AVERROR(EINVAL);
+
+                p2 = NULL;
+                p3 = arg2;
+                while ((arg3 = av_strtok(p3, "+", &saveptr3))) {
+                    p3 = NULL;
+                    if (sscanf(arg3, "%d", &size) == 1) {
+                        if (size < 0)
+                            return AVERROR(EINVAL);
+
+                        if (!j)
+                            inw += size;
+                        else
+                            inh += size;
+                    } else {
+                        return AVERROR(EINVAL);
+                    }
+                }
+            }
+
+            item->w = FFALIGN(inw,2);
+            item->h = FFALIGN(inh,2);
+        }
+    }
+
+    width = 0;
+    height = 0;
+    p = s->layout;
+    saveptr = NULL;
+    saveptr2 = NULL;
+    saveptr3 = NULL;
+    for (i = 0; i < s->nb_inputs; i++) {
+        StackItem *item = &s->items[i];
+
+        if (!(arg = av_strtok(p, "|", &saveptr)))
+            return AVERROR(EINVAL);
+
+        p = NULL;
+
+        p2 = arg;
+        inw = inh = 0;
+
+        for (int j = 0; j < 2; j++) {
+            if (!(arg2 = av_strtok(p2, "_", &saveptr2)))
+                return AVERROR(EINVAL);
+
+            p2 = NULL;
+            p3 = arg2;
+            while ((arg3 = av_strtok(p3, "+", &saveptr3))) {
+                p3 = NULL;
+                if (sscanf(arg3, "w%d", &size) == 1) {
+                    if (size == i || size < 0 || size >= s->nb_inputs)
+                        return AVERROR(EINVAL);
+
+                    if (!j)
+                        inw += s->items[size].w;
+                    else
+                        inh += s->items[size].w;
+                } else if (sscanf(arg3, "h%d", &size) == 1) {
+                    if (size == i || size < 0 || size >= s->nb_inputs)
+                        return AVERROR(EINVAL);
+
+                    if (!j)
+                        inw += s->items[size].h;
+                    else
+                        inh += s->items[size].h;
+                } else if (sscanf(arg3, "%d", &size) == 1) {
+                    if (size < 0)
+                        return AVERROR(EINVAL);
+
+                    if (!j)
+                        inw += size;
+                    else
+                        inh += size;
+                } else {
+                    return AVERROR(EINVAL);
+                }
+            }
+        }
+
+        item->x = FFALIGN(inw,2);
+        item->y = FFALIGN(inh,2);
+
+        width  = FFMAX(width,  item->w + inw);
+        height = FFMAX(height, item->h + inh);
+    }
+
+    outlink->w          = width;
+    outlink->h          = height;
+    outlink->frame_rate = frame_rate;
+    outlink->sample_aspect_ratio = sar;
+
+    in_frames_ctx0 = (AVHWFramesContext *)ctx->inputs[0]->hw_frames_ctx->data;
+
+    if (in_frames_ctx0->sw_format == AV_PIX_FMT_BGRP) {
+        av_log(ctx, AV_LOG_ERROR, "bgrp not supported\n");
+        return AVERROR(EINVAL);
+    }
+
+    for (i = 1; i < s->nb_inputs; i++) {
+        in_frames_ctx = (AVHWFramesContext *)ctx->inputs[i]->hw_frames_ctx->data;
+        if (in_frames_ctx0->sw_format != in_frames_ctx->sw_format) {
+            av_log(ctx, AV_LOG_ERROR,
+                   "All inputs must have the same pixel format!!!\n");
+            return AVERROR(EINVAL);
+        }
+
+        if (in_frames_ctx->sw_format == AV_PIX_FMT_BGRP) {
+            av_log(ctx, AV_LOG_ERROR, "bgrp not supported\n");
+            return AVERROR(EINVAL);
+        }
+    }
+
+    for (i = 1; i < s->nb_inputs; i++) {
+        AVFilterLink *inlink = ctx->inputs[i];
+        if (outlink->frame_rate.num != inlink->frame_rate.num ||
+            outlink->frame_rate.den != inlink->frame_rate.den) {
+            av_log(ctx, AV_LOG_VERBOSE,
+                    "Video inputs have different frame rates, output will be VFR\n");
+            outlink->frame_rate = av_make_q(1, 0);
+            break;
+        }
+    }
+
+    if ((ret = ff_framesync_init(&s->fs, ctx, s->nb_inputs)) < 0)
+        return ret;
+
+    in = s->fs.in;
+    s->fs.opaque = s;
+    s->fs.on_event = process_frame;
+
+    /* Give each input a unique sync priority */
+    for (i = 0; i < s->nb_inputs; i++) {
+        AVFilterLink *inlink = ctx->inputs[i];
+
+        in[i].time_base = inlink->time_base;
+        in[i].sync   = i+1;
+        in[i].before = EXT_STOP;
+        in[i].after  = s->shortest ? EXT_STOP : EXT_INFINITY;
+    }
+
+    if ((s->sync < 0) || (s->sync >= s->nb_inputs)) {
+        av_log(ctx, AV_LOG_ERROR,
+               "Cannot sync to %d, valid range is 0 to %d!!!, Defaulting to 0.\n",
+               s->sync, s->nb_inputs - 1);
+        s->sync = 0;
+    }
+    /* Give the sync input highest priority */
+    in[s->sync].sync = s->nb_inputs + 1;
+
+    ret = ff_framesync_configure(&s->fs);
+    outlink->time_base = s->fs.time_base;
+
+    s->out_frames_ref = av_hwframe_ctx_alloc(in_frames_ctx0->device_ref);
+    if (!s->out_frames_ref)
+        return AVERROR(ENOMEM);
+
+    out_frames_ctx = (AVHWFramesContext *)s->out_frames_ref->data;
+
+    out_frames_ctx->format    = AV_PIX_FMT_NI_QUAD;
+    out_frames_ctx->width     = outlink->w;
+    out_frames_ctx->height    = outlink->h;
+    out_frames_ctx->sw_format = in_frames_ctx0->sw_format;
+    out_frames_ctx->initial_pool_size =
+        NI_STACK_ID; // Repurposed as identity code
+
+    s->out_format = out_frames_ctx->sw_format;
+
+    av_buffer_unref(&ctx->outputs[0]->hw_frames_ctx);
+    ctx->outputs[0]->hw_frames_ctx = av_buffer_ref(s->out_frames_ref);
+
+    if (!ctx->outputs[0]->hw_frames_ctx)
+        return AVERROR(ENOMEM);
+
+    return ret;
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    StackContext *s = ctx->priv;
+    int i;
+
+    ff_framesync_uninit(&s->fs);
+    av_freep(&s->frames);
+    av_freep(&s->items);
+
+    for (i = 0; i < ctx->nb_inputs; i++)
+        av_freep(&ctx->input_pads[i].name);
+
+    if (s->api_dst_frame.data.frame.p_buffer)
+        ni_frame_buffer_free(&s->api_dst_frame.data.frame);
+
+    if (s->session_opened) {
+        /* Close operation will free the device frames */
+        ni_device_session_close(&s->api_ctx, 1, NI_DEVICE_TYPE_SCALER);
+        ni_device_session_context_clear(&s->api_ctx);
+    }
+
+    av_buffer_unref(&s->out_frames_ref);
+}
+
+static int activate(AVFilterContext *ctx)
+{
+    StackContext *s = ctx->priv;
+    return ff_framesync_activate(&s->fs);
+}
+
+#define OFFSET(x) offsetof(StackContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+static const AVFilterPad outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .config_props  = config_output,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVOption xstack_options[] = {
+    { "inputs", "set number of inputs", OFFSET(nb_inputs), AV_OPT_TYPE_INT, {.i64=2}, 2, MAX_XSTACK_INPUTS, .flags = FLAGS },
+    { "layout", "set custom layout", OFFSET(layout), AV_OPT_TYPE_STRING, {.str=NULL}, 0, 0, .flags = FLAGS },
+    { "size", "set custom size", OFFSET(size), AV_OPT_TYPE_STRING, {.str=NULL}, 0, 0, .flags = FLAGS },
+    { "shortest", "force termination when the shortest input terminates", OFFSET(shortest), AV_OPT_TYPE_BOOL, {.i64=0}, 0, 1, .flags = FLAGS },
+    { "fill",  "set the color for unused pixels", OFFSET(fillcolor_str), AV_OPT_TYPE_STRING, {.str = "none"}, .flags = FLAGS },
+    { "sync", "input to sync to", OFFSET(sync), AV_OPT_TYPE_INT, {.i64=0}, 0, MAX_XSTACK_INPUTS - 1, .flags = FLAGS },
+    {"keep_alive_timeout",
+     "Specify a custom session keep alive timeout in seconds.",
+     OFFSET(keep_alive_timeout),
+     AV_OPT_TYPE_INT,
+     {.i64 = NI_DEFAULT_KEEP_ALIVE_TIMEOUT},
+     NI_MIN_KEEP_ALIVE_TIMEOUT,
+     NI_MAX_KEEP_ALIVE_TIMEOUT,
+     FLAGS,
+     "keep_alive_timeout"},
+    { NULL },
+};
+
+AVFILTER_DEFINE_CLASS(xstack);
+
+AVFilter ff_vf_xstack_ni_quadra = {
+    .name          = "ni_quadra_xstack",
+    .description   = NULL_IF_CONFIG_SMALL("NetInt Quadra stack video inputs into custom layout v" NI_XCODER_REVISION),
+    .priv_size     = sizeof(StackContext),
+    .priv_class    = &xstack_class,
+    .init          = init,
+    .uninit        = uninit,
+    .activate      = activate,
+    .flags         = AVFILTER_FLAG_DYNAMIC_INPUTS,
+    .flags_internal= FF_FILTER_FLAG_HWFRAME_AWARE,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_OUTPUTS(outputs),
+    FILTER_QUERY_FUNC(query_formats),
+#else
+    .outputs       = outputs,
+    .query_formats = query_formats,
+#endif
+};
diff --git a/libavfilter/vf_yuv420to444_ni.c b/libavfilter/vf_yuv420to444_ni.c
new file mode 100644
index 0000000000..e9b387f5cc
--- /dev/null
+++ b/libavfilter/vf_yuv420to444_ni.c
@@ -0,0 +1,258 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * yuv420 to yuv444
+ */
+
+#include "avfilter.h"
+#include "formats.h"
+#include "libavutil/common.h"
+#include "libavutil/eval.h"
+#include "libavutil/avstring.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/mathematics.h"
+#include "libavutil/opt.h"
+#include "libavutil/timestamp.h"
+#include "internal.h"
+#include "drawutils.h"
+#include "framesync.h"
+#include "video.h"
+
+#include <ni_device_api.h>
+
+typedef struct YUVTransContext {
+    const AVClass *class;
+    FFFrameSync fs;
+} YUVTransContext;
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    YUVTransContext *s = ctx->priv;
+
+    ff_framesync_uninit(&s->fs);
+}
+
+static int query_formats(AVFilterContext *ctx)
+{
+    AVFilterFormats *formats;
+    int ret;
+    enum AVPixelFormat input_pix_fmt = AV_PIX_FMT_YUV420P;
+    enum AVPixelFormat output_pix_fmt = AV_PIX_FMT_YUV444P;
+
+    if (ctx->inputs[0]) {
+        formats = NULL;
+        if ((ret = ff_add_format(&formats, input_pix_fmt)) < 0)
+            return ret;
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+        if ((ret = ff_formats_ref(formats, &ctx->inputs[0]->outcfg.formats)) < 0)
+#else
+        if ((ret = ff_formats_ref(formats, &ctx->inputs[0]->out_formats)) < 0)
+#endif
+            return ret;
+    }
+    if (ctx->inputs[1]) {
+        formats = NULL;
+        if ((ret = ff_add_format(&formats, input_pix_fmt)) < 0)
+            return ret;
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+        if ((ret = ff_formats_ref(formats, &ctx->inputs[1]->outcfg.formats)) < 0)
+#else
+        if ((ret = ff_formats_ref(formats, &ctx->inputs[1]->out_formats)) < 0)
+#endif
+            return ret;
+    }
+    if (ctx->outputs[0]) {
+        formats = NULL;
+
+        if ((ret = ff_add_format(&formats, output_pix_fmt)) < 0)
+            return ret;
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+        if ((ret = ff_formats_ref(formats, &ctx->outputs[0]->incfg.formats)) < 0)
+#else
+        if ((ret = ff_formats_ref(formats, &ctx->outputs[0]->in_formats)) < 0)
+#endif
+            return ret;
+    }
+
+    return 0;
+}
+
+static int config_output(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    YUVTransContext *s = ctx->priv;
+    int ret;
+
+    if ((ret = ff_framesync_init_dualinput(&s->fs, ctx)) < 0)
+        return ret;
+
+    outlink->w = ctx->inputs[0]->w;
+    outlink->h = ctx->inputs[0]->h;
+    outlink->format = AV_PIX_FMT_YUV444P;
+    outlink->time_base = ctx->inputs[0]->time_base;
+    av_log(ctx, AV_LOG_INFO,
+           "output w:%d h:%d fmt:%s \n",
+           outlink->w, outlink->h,
+           av_get_pix_fmt_name(outlink->format));
+
+    return ff_framesync_configure(&s->fs);
+}
+
+static int do_blend(FFFrameSync *fs)
+{
+    AVFilterContext *ctx = fs->parent;
+    AVFrame *mainpic, *second, *out;
+    int luma_size;
+
+    ff_framesync_get_frame(fs, 0, &mainpic, 0);
+    ff_framesync_get_frame(fs, 1, &second, 0);
+
+    mainpic->pts =
+        av_rescale_q(fs->pts, fs->time_base, ctx->outputs[0]->time_base);
+    {
+        //allocate a new buffer, data is null
+        out = ff_get_video_buffer(ctx->outputs[0], ctx->outputs[0]->w, ctx->outputs[0]->h);
+        if (!out) {
+            return AVERROR(ENOMEM);
+        }
+
+        av_frame_copy_props(out, mainpic);
+        out->format = ctx->outputs[0]->format;
+
+        luma_size = out->width * out->height;
+
+        //y compnent
+        if (mainpic->linesize[0] == out->linesize[0])
+        {
+            memcpy(out->data[0],               mainpic->data[0], sizeof(char) * luma_size/2);
+            memcpy(out->data[0] + luma_size/2, second->data[0], sizeof(char) * luma_size/2);
+        } else {
+            for (int i = 0; i < out->height/2; i++)
+            {
+                memcpy(out->data[0] + i * out->linesize[0],               mainpic->data[0] + i * mainpic->linesize[0], out->linesize[0]);
+                memcpy(out->data[0] + i * out->linesize[0] + luma_size/2, second->data[0]  + i * second->linesize[0],  out->linesize[0]);
+            }
+	}
+
+	if (mainpic->linesize[1] == out->linesize[1]/2 &&
+            mainpic->linesize[2] == out->linesize[2]/2)
+        {
+            //u compnent
+            memcpy(out->data[1],               mainpic->data[0]+luma_size/2, sizeof(char) * luma_size/2); //u compnent
+            memcpy(out->data[1]+luma_size/2,   mainpic->data[1],             sizeof(char) * luma_size/4); //u compnent
+            memcpy(out->data[1]+luma_size*3/4, mainpic->data[2],             sizeof(char) * luma_size/4); //u compnent
+
+            //v compnent
+            memcpy(out->data[2],               second->data[0]+luma_size/2, sizeof(char) * luma_size/2); //v compnent
+            memcpy(out->data[2]+luma_size/2,   second->data[1],             sizeof(char) * luma_size/4); //v compnent
+            memcpy(out->data[2]+luma_size*3/4, second->data[2],             sizeof(char) * luma_size/4); //v compnent
+        } else
+        {
+            for (int i = 0; i < out->height/2; i++)
+            {
+                memcpy(out->data[1] + i * out->linesize[1],                    mainpic->data[0] + i * mainpic->linesize[0] + luma_size/2, out->linesize[1]);
+                memcpy(out->data[1] + i * out->linesize[1]/2 + luma_size / 2,  mainpic->data[1]  + i * mainpic->linesize[1],  out->linesize[1]/2);
+                memcpy(out->data[1] + i * out->linesize[1]/2 + luma_size*3/4,  mainpic->data[2]  + i * mainpic->linesize[2],  out->linesize[1]/2);
+
+                memcpy(out->data[2] + i * out->linesize[1],                    second->data[0]  + i * second->linesize[0]  + luma_size/2, out->linesize[1]);
+                memcpy(out->data[2] + i * out->linesize[1]/2 + luma_size / 2,  second->data[1]  + i * second->linesize[1],  out->linesize[1]/2);
+                memcpy(out->data[2] + i * out->linesize[1]/2 + luma_size*3/4,  second->data[2]  + i * second->linesize[2],  out->linesize[1]/2);
+	    }
+	}
+    }
+
+    return ff_filter_frame(ctx->outputs[0], out);
+}
+
+static av_cold int init(AVFilterContext *ctx)
+{
+    YUVTransContext *s = ctx->priv;
+
+    s->fs.on_event = do_blend;
+    s->fs.opt_shortest = 1;//force termination when the shortest input terminates
+    s->fs.opt_eof_action = EOF_ACTION_ENDALL;
+
+    return 0;
+}
+
+static int activate(AVFilterContext *ctx)
+{
+    YUVTransContext *s = ctx->priv;
+
+    return ff_framesync_activate(&s->fs);
+}
+
+#define OFFSET(x) offsetof(YUVTransContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM|AV_OPT_FLAG_FILTERING_PARAM)
+
+static const AVOption YUVTrans_options[] = {
+    { NULL }
+};
+
+// NOLINTNEXTLINE(clang-diagnostic-deprecated-declarations)
+FRAMESYNC_DEFINE_CLASS(YUVTrans, YUVTransContext, fs);
+
+static const AVFilterPad avfilter_vf_YUVTrans_inputs[] = {
+    {
+        .name         = "input0",
+        .type         = AVMEDIA_TYPE_VIDEO,
+    },
+    {
+        .name         = "input1",
+        .type         = AVMEDIA_TYPE_VIDEO,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+static const AVFilterPad avfilter_vf_YUVTrans_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .config_props  = config_output,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_yuv420to444_ni_quadra = {
+    .name          = "ni_quadra_yuv420to444",
+    .description   = NULL_IF_CONFIG_SMALL("NetInt Quadra YUV420 to YUV444 v" NI_XCODER_REVISION),
+    .preinit       = YUVTrans_framesync_preinit,
+    .init          = init,
+    .uninit        = uninit,
+    .priv_size     = sizeof(YUVTransContext),
+    .priv_class    = &YUVTrans_class,
+    .activate      = activate,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_INPUTS(avfilter_vf_YUVTrans_inputs),
+    FILTER_OUTPUTS(avfilter_vf_YUVTrans_outputs),
+    FILTER_QUERY_FUNC(query_formats),
+#else
+    .inputs        = avfilter_vf_YUVTrans_inputs,
+    .outputs       = avfilter_vf_YUVTrans_outputs,
+    .query_formats = query_formats,
+#endif
+    .flags         = AVFILTER_FLAG_SUPPORT_TIMELINE_INTERNAL |
+                     AVFILTER_FLAG_SLICE_THREADS,
+};
diff --git a/libavfilter/vf_yuv444to420_ni.c b/libavfilter/vf_yuv444to420_ni.c
new file mode 100644
index 0000000000..7bbca193d3
--- /dev/null
+++ b/libavfilter/vf_yuv444to420_ni.c
@@ -0,0 +1,206 @@
+/*
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * yuv444 to yuv420
+ */
+
+#include <stdio.h>
+#include "libavutil/attributes.h"
+#include "libavutil/avstring.h"
+#include "libavutil/internal.h"
+#include "libavutil/mem.h"
+#include "libavutil/opt.h"
+
+#include "avfilter.h"
+#include "audio.h"
+#include "filters.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+
+#include <ni_device_api.h>
+
+typedef struct TransContext {
+    const AVClass *class;
+    int nb_output0;
+    int nb_output1;
+} TransContext;
+
+static av_cold int trans_init(AVFilterContext *ctx)
+{
+    int i, ret;
+
+    for (i = 0; i < 2; i++) {
+        AVFilterPad pad = { 0 };
+
+        pad.type = ctx->filter->inputs[0].type;
+        pad.name = av_asprintf("output%d", i);
+        if (!pad.name)
+            return AVERROR(ENOMEM);
+
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+        if ((ret = ff_append_outpad(ctx, &pad)) < 0) {
+#else
+        if ((ret = ff_insert_outpad(ctx, i, &pad)) < 0) {
+#endif
+            av_freep(&pad.name);
+            return ret;
+        }
+    }
+
+    return 0;
+}
+
+static av_cold void trans_uninit(AVFilterContext *ctx)
+{
+    int i;
+
+    for (i = 0; i < 2; i++)
+        av_freep(&ctx->output_pads[i].name);
+}
+
+static int query_formats(AVFilterContext *ctx)
+{
+    AVFilterFormats *formats;
+    int ret;
+    enum AVPixelFormat input_pix_fmt = AV_PIX_FMT_YUV444P;
+    enum AVPixelFormat output_pix_fmt = AV_PIX_FMT_YUV420P;
+
+    if (ctx->inputs[0]) {
+        formats = NULL;
+        if ((ret = ff_add_format(&formats, input_pix_fmt)) < 0)
+            return ret;
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+        if ((ret = ff_formats_ref(formats, &ctx->inputs[0]->outcfg.formats)) < 0)
+#else
+        if ((ret = ff_formats_ref(formats, &ctx->inputs[0]->out_formats)) < 0)
+#endif
+            return ret;
+    }
+    if (ctx->outputs[0]) {
+        formats = NULL;
+
+        if ((ret = ff_add_format(&formats, output_pix_fmt)) < 0)
+            return ret;
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+        if ((ret = ff_formats_ref(formats, &ctx->inputs[0]->incfg.formats)) < 0)
+#else
+        if ((ret = ff_formats_ref(formats, &ctx->outputs[0]->in_formats)) < 0)
+#endif
+            return ret;
+    }
+    if (ctx->outputs[1]) {
+        formats = NULL;
+
+        if ((ret = ff_add_format(&formats, output_pix_fmt)) < 0)
+            return ret;
+#if (LIBAVFILTER_VERSION_MAJOR >= 8 || LIBAVFILTER_VERSION_MAJOR >= 7 && LIBAVFILTER_VERSION_MINOR >= 110)
+        if ((ret = ff_formats_ref(formats, &ctx->inputs[0]->incfg.formats)) < 0)
+#else
+        if ((ret = ff_formats_ref(formats, &ctx->outputs[1]->in_formats)) < 0)
+#endif
+            return ret;
+    }
+
+    return 0;
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *frame)
+{
+    AVFilterContext *ctx = inlink->dst;
+    AVFrame *out;
+    int luma_size, ret;
+
+    ////output1
+    ctx->outputs[1]->format = AV_PIX_FMT_YUV420P;
+    out = ff_get_video_buffer(ctx->outputs[1], ctx->outputs[1]->w, ctx->outputs[1]->h);
+    if (!out) {
+        av_frame_free(&frame);
+	    return AVERROR(ENOMEM);
+    }
+    av_frame_copy_props(out, frame);
+    out->format = ctx->outputs[1]->format;
+    luma_size = out->width * out->height;
+
+    memcpy(out->data[0],               frame->data[0] + luma_size/2,     sizeof(char) * luma_size/2); //y compnent
+    memcpy(out->data[0] + luma_size/2, frame->data[2],                   sizeof(char) * luma_size/2); //v compnent
+    memcpy(out->data[1],               frame->data[2] + luma_size/2,     sizeof(char) * luma_size/4); //v compnent
+    memcpy(out->data[2],               frame->data[2] + luma_size * 3/4, sizeof(char) * luma_size/4); //v compnent
+
+    ret = ff_filter_frame(ctx->outputs[1], out);
+    if (ret)
+      return ret;
+
+    ////output0
+    ctx->outputs[0]->format = AV_PIX_FMT_YUV420P;
+    frame->format = AV_PIX_FMT_YUV420P;
+    frame->linesize[1] = frame->linesize[1]/2;
+    frame->linesize[2] = frame->linesize[2]/2;
+
+    memcpy(frame->data[0] + luma_size/2, frame->data[1],                   sizeof(char) * luma_size/2); //u compnent
+    memcpy(frame->data[1],               frame->data[1] + luma_size/2,     sizeof(char) * luma_size/4); //u compnent
+    memcpy(frame->data[2],               frame->data[1] + luma_size * 3/4, sizeof(char) * luma_size/4); //u compnent
+
+    ret = ff_filter_frame(ctx->outputs[0], frame);
+
+    return ret;
+}
+
+#define OFFSET(x) offsetof(TransContext, x)
+#define FLAGS (AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM)
+
+static const AVOption options[] = {
+    { "output0", "yuv420 of output0", OFFSET(nb_output0), AV_OPT_TYPE_INT, { .i64 = 0 }, 0, INT_MAX, FLAGS },
+    { "output1", "yuv420 of output1", OFFSET(nb_output1), AV_OPT_TYPE_INT, { .i64 = 0 }, 0, INT_MAX, FLAGS },
+    { NULL }
+};
+
+#define trans_options options
+
+AVFILTER_DEFINE_CLASS(trans);
+
+static const AVFilterPad avfilter_vf_trans_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = filter_frame,
+    },
+#if (LIBAVFILTER_VERSION_MAJOR < 8)
+    { NULL }
+#endif
+};
+
+AVFilter ff_vf_yuv444to420_ni_quadra = {
+    .name          = "ni_quadra_yuv444to420",
+    .description   = NULL_IF_CONFIG_SMALL("NetInt Quadra YUV444 to YUV420 v" NI_XCODER_REVISION),
+    .priv_size     = sizeof(TransContext),
+    .priv_class    = &trans_class,
+    .init          = trans_init,
+    .uninit        = trans_uninit,
+#if (LIBAVFILTER_VERSION_MAJOR >= 8)
+    FILTER_QUERY_FUNC(query_formats),
+    FILTER_INPUTS(avfilter_vf_trans_inputs),
+#else
+    .query_formats = query_formats,
+    .inputs        = avfilter_vf_trans_inputs,
+#endif
+    .flags         = AVFILTER_FLAG_DYNAMIC_OUTPUTS,
+};
diff --git a/libavformat/flv.h b/libavformat/flv.h
index 3571b90279..91f006520c 100644
--- a/libavformat/flv.h
+++ b/libavformat/flv.h
@@ -110,6 +110,7 @@ enum {
     FLV_CODECID_H264    = 7,
     FLV_CODECID_REALH263= 8,
     FLV_CODECID_MPEG4   = 9,
+    FLV_CODECID_HEVC    = 12,
 };
 
 enum {
diff --git a/libavformat/flvdec.c b/libavformat/flvdec.c
index 7242296f7f..ef0e7f19c7 100644
--- a/libavformat/flvdec.c
+++ b/libavformat/flvdec.c
@@ -37,6 +37,7 @@
 #include "demux.h"
 #include "internal.h"
 #include "flv.h"
+#include "hevc.h"
 
 #define VALIDATE_INDEX_TS_THRESH 2500
 
@@ -321,6 +322,8 @@ static int flv_same_video_codec(AVCodecParameters *vpar, int flags)
         return vpar->codec_id == AV_CODEC_ID_VP6A;
     case FLV_CODECID_H264:
         return vpar->codec_id == AV_CODEC_ID_H264;
+    case FLV_CODECID_HEVC:
+        return vpar->codec_id == AV_CODEC_ID_HEVC;
     default:
         return vpar->codec_tag == flv_codecid;
     }
@@ -367,6 +370,11 @@ static int flv_set_video_codec(AVFormatContext *s, AVStream *vstream,
         vstreami->need_parsing = AVSTREAM_PARSE_HEADERS;
         ret = 3;     // not 4, reading packet type will consume one byte
         break;
+    case FLV_CODECID_HEVC:
+        par->codec_id = AV_CODEC_ID_HEVC;
+        vstreami->need_parsing = AVSTREAM_PARSE_NONE;
+        ret = 3;     // not 4, reading packet type will consume one byte
+        break;
     case FLV_CODECID_MPEG4:
         par->codec_id = AV_CODEC_ID_MPEG4;
         ret = 3;
@@ -1240,7 +1248,8 @@ retry_duration:
 
     if (st->codecpar->codec_id == AV_CODEC_ID_AAC ||
         st->codecpar->codec_id == AV_CODEC_ID_H264 ||
-        st->codecpar->codec_id == AV_CODEC_ID_MPEG4) {
+        st->codecpar->codec_id == AV_CODEC_ID_MPEG4 ||
+        st->codecpar->codec_id == AV_CODEC_ID_HEVC) {
         int type = avio_r8(s->pb);
         size--;
 
@@ -1249,7 +1258,9 @@ retry_duration:
             goto leave;
         }
 
-        if (st->codecpar->codec_id == AV_CODEC_ID_H264 || st->codecpar->codec_id == AV_CODEC_ID_MPEG4) {
+        if (st->codecpar->codec_id == AV_CODEC_ID_H264 ||
+            st->codecpar->codec_id == AV_CODEC_ID_MPEG4 ||
+            st->codecpar->codec_id == AV_CODEC_ID_HEVC) {
             // sign extension
             int32_t cts = (avio_rb24(s->pb) + 0xff800000) ^ 0xff800000;
             pts = av_sat_add64(dts, cts);
@@ -1265,7 +1276,7 @@ retry_duration:
             }
         }
         if (type == 0 && (!st->codecpar->extradata || st->codecpar->codec_id == AV_CODEC_ID_AAC ||
-            st->codecpar->codec_id == AV_CODEC_ID_H264)) {
+            st->codecpar->codec_id == AV_CODEC_ID_H264 || st->codecpar->codec_id == AV_CODEC_ID_HEVC)) {
             AVDictionaryEntry *t;
 
             if (st->codecpar->extradata) {
diff --git a/libavformat/flvenc.c b/libavformat/flvenc.c
index 770ca319ed..b767ed654d 100644
--- a/libavformat/flvenc.c
+++ b/libavformat/flvenc.c
@@ -29,6 +29,7 @@
 #include "avc.h"
 #include "avformat.h"
 #include "flv.h"
+#include "hevc.h"
 #include "internal.h"
 #include "mux.h"
 #include "libavutil/opt.h"
@@ -45,6 +46,7 @@ static const AVCodecTag flv_video_codec_ids[] = {
     { AV_CODEC_ID_VP6,      FLV_CODECID_VP6 },
     { AV_CODEC_ID_VP6A,     FLV_CODECID_VP6A },
     { AV_CODEC_ID_H264,     FLV_CODECID_H264 },
+    { AV_CODEC_ID_HEVC,     FLV_CODECID_HEVC },
     { AV_CODEC_ID_NONE,     0 }
 };
 
@@ -235,13 +237,15 @@ static void put_timestamp(AVIOContext *pb, int64_t ts) {
     avio_w8(pb, (ts >> 24) & 0x7F);
 }
 
-static void put_avc_eos_tag(AVIOContext *pb, unsigned ts)
+// NETINT: add 'id' param
+static void put_avc_eos_tag(AVIOContext *pb, unsigned ts, enum AVCodecID id)
 {
     avio_w8(pb, FLV_TAG_TYPE_VIDEO);
     avio_wb24(pb, 5);               /* Tag Data Size */
     put_timestamp(pb, ts);
     avio_wb24(pb, 0);               /* StreamId = 0 */
-    avio_w8(pb, 23);                /* ub[4] FrameType = 1, ub[4] CodecId = 7 */
+    // NETINT: 44(0x2C) for HEVC
+    avio_w8(pb, (id==AV_CODEC_ID_H264)? 23:44);   /* ub[4] FrameType = 1, ub[4] CodecId = 7 */
     avio_w8(pb, 2);                 /* AVC end of sequence */
     avio_wb24(pb, 0);               /* Always 0 for AVC EOS. */
     avio_wb32(pb, 16);              /* Size of FLV tag */
@@ -489,8 +493,13 @@ static void flv_write_codec_header(AVFormatContext* s, AVCodecParameters* par, i
     AVIOContext *pb = s->pb;
     FLVContext *flv = s->priv_data;
 
-    if (par->codec_id == AV_CODEC_ID_AAC || par->codec_id == AV_CODEC_ID_H264
-            || par->codec_id == AV_CODEC_ID_MPEG4) {
+    // NETINT: do not attempt to write FLV headers if they are not available
+    if ((par->codec_id == AV_CODEC_ID_H264 || par->codec_id == AV_CODEC_ID_HEVC) &&
+        ! (par->extradata && par->extradata_size > 0))
+        return;
+
+    if (par->codec_id == AV_CODEC_ID_AAC || par->codec_id == AV_CODEC_ID_H264 ||
+        par->codec_id == AV_CODEC_ID_MPEG4 || par->codec_id == AV_CODEC_ID_HEVC) {
         int64_t pos;
         avio_w8(pb,
                 par->codec_type == AVMEDIA_TYPE_VIDEO ?
@@ -536,7 +545,11 @@ static void flv_write_codec_header(AVFormatContext* s, AVCodecParameters* par, i
             avio_w8(pb, par->codec_tag | FLV_FRAME_KEY); // flags
             avio_w8(pb, 0); // AVC sequence header
             avio_wb24(pb, 0); // composition time
-            ff_isom_write_avcc(pb, par->extradata, par->extradata_size);
+            if (par->codec_id == AV_CODEC_ID_HEVC) {
+                ff_isom_write_hvcc(pb, par->extradata, par->extradata_size, 0);
+            } else {
+                ff_isom_write_avcc(pb, par->extradata, par->extradata_size);
+            }
         }
         data_size = avio_tell(pb) - pos;
         avio_seek(pb, -data_size - 10, SEEK_CUR);
@@ -783,8 +796,8 @@ end:
             AVCodecParameters *par = s->streams[i]->codecpar;
             FLVStreamContext *sc = s->streams[i]->priv_data;
             if (par->codec_type == AVMEDIA_TYPE_VIDEO &&
-                    (par->codec_id == AV_CODEC_ID_H264 || par->codec_id == AV_CODEC_ID_MPEG4))
-                put_avc_eos_tag(pb, sc->last_ts);
+                    (par->codec_id == AV_CODEC_ID_H264 || par->codec_id == AV_CODEC_ID_MPEG4 || par->codec_id == AV_CODEC_ID_HEVC))
+                put_avc_eos_tag(pb, sc->last_ts, par->codec_id); // NETINT: add codec_id
         }
     }
 
@@ -834,13 +847,15 @@ static int flv_write_packet(AVFormatContext *s, AVPacket *pkt)
     if (par->codec_id == AV_CODEC_ID_VP6F || par->codec_id == AV_CODEC_ID_VP6A ||
         par->codec_id == AV_CODEC_ID_VP6  || par->codec_id == AV_CODEC_ID_AAC)
         flags_size = 2;
-    else if (par->codec_id == AV_CODEC_ID_H264 || par->codec_id == AV_CODEC_ID_MPEG4)
+    else if (par->codec_id == AV_CODEC_ID_H264 ||
+             par->codec_id == AV_CODEC_ID_MPEG4 ||
+             par->codec_id == AV_CODEC_ID_HEVC)
         flags_size = 5;
     else
         flags_size = 1;
 
-    if (par->codec_id == AV_CODEC_ID_AAC || par->codec_id == AV_CODEC_ID_H264
-            || par->codec_id == AV_CODEC_ID_MPEG4) {
+    if (par->codec_id == AV_CODEC_ID_AAC || par->codec_id == AV_CODEC_ID_H264 ||
+        par->codec_id == AV_CODEC_ID_MPEG4 || par->codec_id == AV_CODEC_ID_HEVC) {
         size_t side_size;
         uint8_t *side = av_packet_get_side_data(pkt, AV_PKT_DATA_NEW_EXTRADATA, &side_size);
         if (side && side_size > 0 && (side_size != par->extradata_size || memcmp(side, par->extradata, side_size))) {
@@ -905,6 +920,10 @@ static int flv_write_packet(AVFormatContext *s, AVPacket *pkt)
         if (par->extradata_size > 0 && *(uint8_t*)par->extradata != 1)
             if ((ret = ff_avc_parse_nal_units_buf(pkt->data, &data, &size)) < 0)
                 return ret;
+    } else if (par->codec_id == AV_CODEC_ID_HEVC) {
+        if (par->extradata_size > 0 && *(uint8_t*)par->extradata != 1)
+            if ((ret = ff_hevc_annexb2mp4_buf(pkt->data, &data, &size, 0, NULL)) < 0)
+                return ret;
     } else if (par->codec_id == AV_CODEC_ID_AAC && pkt->size > 2 &&
                (AV_RB16(pkt->data) & 0xfff0) == 0xfff0) {
         if (!s->streams[pkt->stream_index]->nb_frames) {
@@ -977,7 +996,7 @@ static int flv_write_packet(AVFormatContext *s, AVPacket *pkt)
                              (FFALIGN(par->height, 16) - par->height));
         } else if (par->codec_id == AV_CODEC_ID_AAC)
             avio_w8(pb, 1); // AAC raw
-        else if (par->codec_id == AV_CODEC_ID_H264 || par->codec_id == AV_CODEC_ID_MPEG4) {
+        else if (par->codec_id == AV_CODEC_ID_H264 || par->codec_id == AV_CODEC_ID_MPEG4 || par->codec_id == AV_CODEC_ID_HEVC) {
             avio_w8(pb, 1); // AVC NALU
             avio_wb24(pb, pkt->pts - pkt->dts);
         }
diff --git a/libavformat/movenc.c b/libavformat/movenc.c
index 5608afde42..4aea22caa8 100644
--- a/libavformat/movenc.c
+++ b/libavformat/movenc.c
@@ -6318,7 +6318,9 @@ static int mov_write_single_packet(AVFormatContext *s, AVPacket *pkt)
     if (trk->par->codec_id == AV_CODEC_ID_MP4ALS ||
             trk->par->codec_id == AV_CODEC_ID_AAC ||
             trk->par->codec_id == AV_CODEC_ID_AV1 ||
-            trk->par->codec_id == AV_CODEC_ID_FLAC) {
+            trk->par->codec_id == AV_CODEC_ID_FLAC ||
+            trk->par->codec_id == AV_CODEC_ID_HEVC || // NETINT: apply h264/h265 video extradata to mov outpu
+            trk->par->codec_id == AV_CODEC_ID_H264) {
         size_t side_size;
         uint8_t *side = av_packet_get_side_data(pkt, AV_PKT_DATA_NEW_EXTRADATA, &side_size);
         if (side && side_size > 0 && (side_size != par->extradata_size || memcmp(side, par->extradata, side_size))) {
@@ -7687,8 +7689,8 @@ static const AVCodecTag codec_mp4_tags[] = {
     { AV_CODEC_ID_MPEG4,           MKTAG('m', 'p', '4', 'v') },
     { AV_CODEC_ID_H264,            MKTAG('a', 'v', 'c', '1') },
     { AV_CODEC_ID_H264,            MKTAG('a', 'v', 'c', '3') },
-    { AV_CODEC_ID_HEVC,            MKTAG('h', 'e', 'v', '1') },
     { AV_CODEC_ID_HEVC,            MKTAG('h', 'v', 'c', '1') },
+    { AV_CODEC_ID_HEVC,            MKTAG('h', 'e', 'v', '1') },
     { AV_CODEC_ID_MPEG2VIDEO,      MKTAG('m', 'p', '4', 'v') },
     { AV_CODEC_ID_MPEG1VIDEO,      MKTAG('m', 'p', '4', 'v') },
     { AV_CODEC_ID_MJPEG,           MKTAG('m', 'p', '4', 'v') },
diff --git a/libavformat/mpegenc.c b/libavformat/mpegenc.c
index 3ab4bd3f9b..a64c1b38f7 100644
--- a/libavformat/mpegenc.c
+++ b/libavformat/mpegenc.c
@@ -426,7 +426,7 @@ static av_cold int mpeg_mux_init(AVFormatContext *ctx)
 
             /* This value HAS to be used for VCD (see VCD standard, p. IV-7).
              * Right now it is also used for everything else. */
-            stream->max_buffer_size = 4 * 1024;
+            stream->max_buffer_size = 4 * 1024 * 2; // NETINT: Increase MPG buffers to avoid overflow at higher bitrates
             s->audio_bound++;
             break;
         case AVMEDIA_TYPE_VIDEO:
@@ -437,14 +437,14 @@ static av_cold int mpeg_mux_init(AVFormatContext *ctx)
 
             props = (AVCPBProperties*)av_stream_get_side_data(st, AV_PKT_DATA_CPB_PROPERTIES, NULL);
             if (props && props->buffer_size)
-                stream->max_buffer_size = 6 * 1024 + props->buffer_size / 8;
+                stream->max_buffer_size = (6 * 1024 + props->buffer_size / 8) * 2; // NETINT: Increase MPG buffers to avoid overflow at higher bitrates
             else {
                 av_log(ctx, AV_LOG_WARNING,
                        "VBV buffer size not set, using default size of 230KB\n"
                        "If you want the mpeg file to be compliant to some specification\n"
                        "Like DVD, VCD or others, make sure you set the correct buffer size\n");
                 // FIXME: this is probably too small as default
-                stream->max_buffer_size = 230 * 1024;
+                stream->max_buffer_size = 230 * 1024 * 2; // NETINT: Increase MPG buffers to avoid overflow at higher bitrates
             }
             if (stream->max_buffer_size > 1024 * 8191) {
                 av_log(ctx, AV_LOG_WARNING, "buffer size %d, too large\n", stream->max_buffer_size);
diff --git a/libavformat/mpegts.h b/libavformat/mpegts.h
index a48f14e768..f5ec78c55d 100644
--- a/libavformat/mpegts.h
+++ b/libavformat/mpegts.h
@@ -137,6 +137,7 @@
 #define STREAM_TYPE_AUDIO_AC3       0x81
 #define STREAM_TYPE_AUDIO_DTS       0x82
 #define STREAM_TYPE_AUDIO_TRUEHD    0x83
+#define STREAM_TYPE_SCTE_35         0x86 // NETINT: add scte35 type to mpegts muxer as PSI
 #define STREAM_TYPE_AUDIO_EAC3      0x87
 
 /* ISO/IEC 13818-1 Table 2-22 */
diff --git a/libavformat/mpegtsenc.c b/libavformat/mpegtsenc.c
index c964d58c8e..e2f4c83af9 100644
--- a/libavformat/mpegtsenc.c
+++ b/libavformat/mpegtsenc.c
@@ -77,6 +77,7 @@ typedef struct MpegTSWrite {
     const AVClass *av_class;
     MpegTSSection pat; /* MPEG-2 PAT table */
     MpegTSSection sdt; /* MPEG-2 SDT table context */
+    MpegTSSection scte35; /* MPEG-2 scte35 signaling */ // NETINT: add scte35 type to mpegts muxer as PSI
     MpegTSSection nit; /* MPEG-2 NIT table context */
     MpegTSService **services;
     AVPacket *pkt;
@@ -123,6 +124,12 @@ typedef struct MpegTSWrite {
 
     uint8_t provider_name[256];
 
+    // NETINT: add scte35 type to mpegts muxer as PSI
+    int64_t last_scte35_ts;
+    int64_t scte35_period_us;
+    int scte35_packet_count;
+    int64_t scte35_period;
+
     int omit_video_pes_length;
 } MpegTSWrite;
 
@@ -225,6 +232,24 @@ static int mpegts_write_section1(MpegTSSection *s, int tid, int id,
     return 0;
 }
 
+// NETINT: add scte35 type to mpegts muxer as PSI
+static int mpegts_write_section_scte35(MpegTSSection *s, uint8_t *buf, int len)
+{
+    uint8_t section[1024], *q;
+    unsigned int tot_len;
+
+    tot_len = len;
+    /* check if not too big */
+    if (tot_len > 1024)
+        return AVERROR_INVALIDDATA;
+
+    q    = section;
+    memcpy(q, buf, len);
+
+    mpegts_write_section(s, section, tot_len);
+    return 0;
+}
+
 /*********************************************/
 /* mpegts writer */
 
@@ -418,6 +443,10 @@ static int get_dvb_stream_type(AVFormatContext *s, AVStream *st)
     case AV_CODEC_ID_TIMED_ID3:
         stream_type = STREAM_TYPE_METADATA;
         break;
+	// NETINT: add scte35 type to mpegts muxer as PSI
+    case AV_CODEC_ID_SCTE_35:
+        stream_type = STREAM_TYPE_SCTE_35;
+        break;
     case AV_CODEC_ID_DVB_SUBTITLE:
     case AV_CODEC_ID_DVB_TELETEXT:
     case AV_CODEC_ID_ARIB_CAPTION:
@@ -506,6 +535,20 @@ static int mpegts_write_pmt(AVFormatContext *s, MpegTSService *service)
     q += 2; /* patched after */
 
     /* put program info here */
+	// NETINT: add scte35 type to mpegts muxer as PSI
+    for (i = 0; i < s->nb_streams; i++) {
+        if(s->streams[i]->codecpar->codec_id==AV_CODEC_ID_SCTE_35){
+            *q++ = 0x05; // ANSI SCTE35 descriptor tag
+            *q++ = 0x04; // ANSI SCTE35 descriptor length (4 for CUEI)
+
+            *q++ = 0x43; // 'C'
+            *q++ = 0x55; // 'U'
+            *q++ = 0x45; // 'E'
+            *q++ = 0x49; // 'I'
+            break;
+        }
+    }
+
     if (ts->m2ts_mode) {
         put_registration_descriptor(&q, MKTAG('H', 'D', 'M', 'V'));
         *q++ = 0x88;        // descriptor_tag - hdmv_copy_control_descriptor
@@ -909,6 +952,38 @@ static void mpegts_write_nit(AVFormatContext *s)
                           data, q - data);
 }
 
+// NETINT: add scte35 type to mpegts muxer as PSI
+static void mpegts_write_scte35(AVFormatContext *s, int64_t pts, const uint8_t *payload, int payload_size)
+{
+    MpegTSWrite *ts = s->priv_data;
+    uint8_t payloadSynced[SECTION_LENGTH];
+    uint8_t data[SECTION_LENGTH], *q;
+    if(payload_size > SECTION_LENGTH){
+      av_log(s, AV_LOG_ERROR, "SCTE35 Payload exceeds max section length \n");
+      return;
+    }
+    q = data;
+    memcpy(payloadSynced, payload, payload_size);
+
+    // set ffmpeg pts
+    if (payloadSynced[13] == 6){ // scte 35 time signal type = 6
+        payloadSynced[15] = (pts & 0xFF000000) >> 24;
+        payloadSynced[16] = (pts & 0x00FF0000) >> 16;
+        payloadSynced[17] = (pts & 0x0000FF00) >> 8;
+        payloadSynced[18] = (pts & 0x000000FF);
+    }else if (payloadSynced[13] == 5){ // scte 35 splice insert type = 5
+        payloadSynced[21] = (pts & 0xFF000000) >> 24;
+        payloadSynced[22] = (pts & 0x00FF0000) >> 16;
+        payloadSynced[23] = (pts & 0x0000FF00) >> 8;
+        payloadSynced[24] = (pts & 0x000000FF);
+    }else{
+        av_log(s, AV_LOG_ERROR, "SCTE35 signal type not yet supported\n");
+    }
+    memcpy(q, payloadSynced, payload_size);
+    q+=payload_size;
+    mpegts_write_section_scte35(&ts->scte35, data, q - data);
+}
+
 /* This stores a string in buf with the correct encoding and also sets the
  * first byte as the length. !str is accepted for an empty string.
  * If the string is already encoded, invalid UTF-8 or has no multibyte sequence
@@ -1145,6 +1220,12 @@ static int mpegts_init(AVFormatContext *s)
 
     ts->pkt = ffformatcontext(s)->pkt;
 
+    // NETINT: add scte35 type to mpegts muxer as PSI
+    ts->scte35.cc           = 15;
+    ts->scte35.discontinuity= ts->flags & MPEGTS_FLAG_DISCONT;
+    ts->scte35.write_packet = section_write_packet;
+    ts->scte35.opaque       = s;
+
     /* assign pids to each stream */
     for (i = 0; i < s->nb_streams; i++) {
         AVStream *st = s->streams[i];
@@ -1262,6 +1343,7 @@ static int mpegts_init(AVFormatContext *s)
 
     ts->last_pat_ts = AV_NOPTS_VALUE;
     ts->last_sdt_ts = AV_NOPTS_VALUE;
+    ts->last_scte35_ts = AV_NOPTS_VALUE; // NETINT: add scte35 type to mpegts muxer as PSI
     ts->last_nit_ts = AV_NOPTS_VALUE;
     ts->pat_period = av_rescale(ts->pat_period_us, PCR_TIME_BASE, AV_TIME_BASE);
     ts->sdt_period = av_rescale(ts->sdt_period_us, PCR_TIME_BASE, AV_TIME_BASE);
@@ -1501,6 +1583,13 @@ static void mpegts_write_pes(AVFormatContext *s, AVStream *st,
             pcr = (dts - delay) * 300;
 
         retransmit_si_info(s, force_pat, force_sdt, force_nit, pcr);
+        // NETINT: add scte35 type to mpegts muxer as PSI
+        if(st->codecpar->codec_id == AV_CODEC_ID_SCTE_35){
+          ts->scte35.pid = ts_st->pid;
+          mpegts_write_scte35 (s, pts, payload, payload_size);
+          payload_size = 0;
+          continue;
+        }
         force_pat = 0;
         force_sdt = 0;
         force_nit = 0;
diff --git a/libavformat/mux.c b/libavformat/mux.c
index 31361f9b46..c982e7ee9f 100644
--- a/libavformat/mux.c
+++ b/libavformat/mux.c
@@ -920,6 +920,8 @@ int ff_interleave_packet_per_dts(AVFormatContext *s, AVPacket *pkt,
 {
     FFFormatContext *const si = ffformatcontext(s);
     int stream_count = 0;
+    // NETINT: fix scte35 handling in muxing buffer
+    int scte35_count = 0; // scte35 stream count
     int noninterleaved_count = 0;
     int ret;
     int eof = flush;
@@ -933,7 +935,16 @@ int ff_interleave_packet_per_dts(AVFormatContext *s, AVPacket *pkt,
         const AVStream *const st  = s->streams[i];
         const FFStream *const sti = cffstream(st);
         const AVCodecParameters *const par = st->codecpar;
-        if (sti->last_in_packet_buffer) {
+        // NETINT: fix scte35 handling in muxing buffer
+        // flush scte35 in muxing buffer like regular AV streams to fix ffmpeg muxing output
+        // stall when scte35 packet DTS has large delta compared to regular AV packet DTS
+        if (sti->last_in_packet_buffer || par->codec_id == AV_CODEC_ID_SCTE_35) {
+            // treat scte35 like regular AV stream for flush consideration
+            // NETINT: fix scte35 handling in muxing buffer
+            if (par->codec_id == AV_CODEC_ID_SCTE_35) {
+                // keep count of scte35 stream to avoid flushing when regular AV streams end
+                scte35_count++;
+            }
             ++stream_count;
         } else if (par->codec_type != AVMEDIA_TYPE_ATTACHMENT &&
                    par->codec_id != AV_CODEC_ID_VP8 &&
@@ -1016,7 +1027,10 @@ int ff_interleave_packet_per_dts(AVFormatContext *s, AVPacket *pkt,
         }
     }
 
-    if (stream_count && flush) {
+    // NETINT: fix scte35 handling in muxing buffer
+    // the muxing buffer requires at least one regular AV stream be present during flush, otherwise segfault
+    if (stream_count - scte35_count > 0 && flush) {
+        // flush if there are non-scte35 streams present AND it is required
         PacketListEntry *pktl = si->packet_buffer.head;
         AVStream *const st = s->streams[pktl->pkt.stream_index];
         FFStream *const sti = ffstream(st);
diff --git a/libavformat/options.c b/libavformat/options.c
index 0079a06d9a..126ab432a4 100644
--- a/libavformat/options.c
+++ b/libavformat/options.c
@@ -281,7 +281,7 @@ AVStream *avformat_new_stream(AVFormatContext *s, const AVCodec *c)
         sti->info->fps_last_dts  = AV_NOPTS_VALUE;
 
         /* default pts setting is MPEG-like */
-        avpriv_set_pts_info(st, 33, 1, 90000);
+        avpriv_set_pts_info(st, 62, 1, 90000);
         /* we set the current DTS to 0 so that formats without any timestamps
          * but durations get some timestamps, formats with some unknown
          * timestamps have their first few packets buffered and the
diff --git a/libavformat/utils.c.rejxBZF1P b/libavformat/utils.c.rejxBZF1P
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/libavutil/Makefile b/libavutil/Makefile
index 9435a0bfb0..93ac9503f0 100644
--- a/libavutil/Makefile
+++ b/libavutil/Makefile
@@ -42,6 +42,8 @@ HEADERS = adler32.h                                                     \
           hwcontext_d3d11va.h                                           \
           hwcontext_drm.h                                               \
           hwcontext_dxva2.h                                             \
+          hwcontext_ni_quad.h                                           \
+          hwcontext_ni_logan.h                                          \
           hwcontext_qsv.h                                               \
           hwcontext_mediacodec.h                                        \
           hwcontext_opencl.h                                            \
@@ -191,6 +193,8 @@ OBJS-$(CONFIG_MACOS_KPERF)              += macos_kperf.o
 OBJS-$(CONFIG_MEDIACODEC)               += hwcontext_mediacodec.o
 OBJS-$(CONFIG_OPENCL)                   += hwcontext_opencl.o
 OBJS-$(CONFIG_QSV)                      += hwcontext_qsv.o
+OBJS-$(CONFIG_NI_QUADRA)                += hwcontext_ni_quad.o
+OBJS-$(CONFIG_NI_LOGAN)                 += hwcontext_ni_logan.o
 OBJS-$(CONFIG_VAAPI)                    += hwcontext_vaapi.o
 OBJS-$(CONFIG_VIDEOTOOLBOX)             += hwcontext_videotoolbox.o
 OBJS-$(CONFIG_VDPAU)                    += hwcontext_vdpau.o
diff --git a/libavutil/buffer.c b/libavutil/buffer.c
index 54590be566..16b6233dfb 100644
--- a/libavutil/buffer.c
+++ b/libavutil/buffer.c
@@ -90,6 +90,22 @@ AVBufferRef *av_buffer_alloc(size_t size)
     return ret;
 }
 
+AVBufferRef *av_buffer_alloc_quadra(int size)
+{
+    AVBufferRef *ret = NULL;
+    uint8_t    *data = NULL;
+
+    data = av_malloc_quadra(size);
+    if (!data)
+        return NULL;
+
+    ret = av_buffer_create(data, size, av_buffer_default_free, NULL, 0);
+    if (!ret)
+        av_freep(&data);
+
+    return ret;
+}
+
 AVBufferRef *av_buffer_allocz(size_t size)
 {
     AVBufferRef *ret = av_buffer_alloc(size);
diff --git a/libavutil/buffer.h b/libavutil/buffer.h
index e1ef5b7f07..35ff98b75b 100644
--- a/libavutil/buffer.h
+++ b/libavutil/buffer.h
@@ -94,6 +94,13 @@ typedef struct AVBufferRef {
     size_t   size;
 } AVBufferRef;
 
+/**
+* Allocate an AVBuffer of the given size using av_malloc_quadra().
+*
+* @return an AVBufferRef of given size or NULL when out of memory
+*/
+AVBufferRef *av_buffer_alloc_quadra(int size);
+
 /**
  * Allocate an AVBuffer of the given size using av_malloc().
  *
diff --git a/libavutil/frame.c b/libavutil/frame.c
index 4c16488c66..46aaf3afb9 100644
--- a/libavutil/frame.c
+++ b/libavutil/frame.c
@@ -27,6 +27,9 @@
 #include "mem.h"
 #include "samplefmt.h"
 #include "hwcontext.h"
+#if CONFIG_NI_QUADRA
+#include "ni_util.h"
+#endif
 
 #if FF_API_OLD_CHANNEL_LAYOUT
 #define CHECK_CHANNELS_CONSISTENCY(frame) \
@@ -117,6 +120,58 @@ void av_frame_free(AVFrame **frame)
     av_freep(frame);
 }
 
+#if CONFIG_NI_QUADRA
+static int get_video_buffer_quadra(AVFrame *frame)
+{ //#include "ni_util.h"
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(frame->format);
+    int ret, i;
+    int plane_stride[4];
+    int plane_height[4];
+    if (!desc)
+        return AVERROR(EINVAL);
+
+    int bit_depth_factor = (desc->comp[0].depth + 7) / 8;
+    int is_nv12 = desc->nb_components == 2;
+    ni_get_hw_yuv420p_dim(frame->width, frame->height, bit_depth_factor,
+                          is_nv12, plane_stride, plane_height);
+    av_log(NULL, AV_LOG_TRACE, "get_video_buffer_quadra frame->width %d "
+           "bit_depth_factor %d nv12 %d dst_stride[0/1/2] %d/%d/%d\n",
+           frame->width, bit_depth_factor, is_nv12, plane_stride[0],
+           plane_stride[1], plane_stride[2]);
+    for (i = 0; i < AV_NUM_DATA_POINTERS; i++)
+    {
+        frame->linesize[i] = (i < desc->nb_components) ? plane_stride[i] : 0;
+    }
+    //beware the 2-pass unhandled condition
+
+    if ((ret = av_image_fill_pointers(frame->data, frame->format, plane_height[0],
+        NULL, frame->linesize)) < 0)
+        return ret;
+
+    frame->buf[0] = av_buffer_alloc_quadra(ret + 4096); //4k aligned with 4KB extra
+
+    if (!frame->buf[0]) {
+        ret = AVERROR(ENOMEM);
+        goto fail;
+    }
+
+    if ((ret = av_image_fill_pointers(frame->data, frame->format, plane_height[0],
+        frame->buf[0]->data, frame->linesize)) < 0)
+        goto fail;
+    frame->data[3] = frame->data[2 - is_nv12] + plane_stride[2 - is_nv12]
+        * plane_height[2 - is_nv12]; //metadata space available
+
+    frame->extended_data = frame->data;
+    av_log(NULL, AV_LOG_TRACE, "D0 %p D1 %p D2 %p D3%p\n", frame->data[0],
+           frame->data[1], frame->data[2], frame->data[3]);
+
+    return 0;
+fail:
+    av_frame_unref(frame);
+    return ret;
+}
+#endif
+
 static int get_video_buffer(AVFrame *frame, int align)
 {
     const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(frame->format);
@@ -251,6 +306,19 @@ FF_ENABLE_DEPRECATION_WARNINGS
 
 }
 
+#if CONFIG_NI_QUADRA
+int av_frame_get_buffer_quadra(AVFrame *frame)
+{
+    if (frame->format < 0)
+        return AVERROR(EINVAL);
+
+    if (frame->width > 0 && frame->height > 0)
+        return get_video_buffer_quadra(frame);
+
+    return AVERROR(EINVAL);
+}
+#endif
+
 int av_frame_get_buffer(AVFrame *frame, int align)
 {
     if (frame->format < 0)
diff --git a/libavutil/frame.h b/libavutil/frame.h
index 33fac2054c..2d483e7535 100644
--- a/libavutil/frame.h
+++ b/libavutil/frame.h
@@ -177,6 +177,35 @@ enum AVFrameSideDataType {
      */
     AV_FRAME_DATA_SEI_UNREGISTERED,
 
+    // NETINT: User data unregistered SEI data
+    /**
+     * This side data takes SEI payload type USER_DATA_UNREGISTERED.
+     * There will be no byte reordering.
+     * Usually this payload would be: 16B UUID + other payload Bytes.
+     */
+    AV_FRAME_DATA_NETINT_UDU_SEI,
+
+    // NETINT: Custom SEI data
+    /**
+     * This side data takes SEI payload custom types.
+     * There will be no byte reordering.
+     * Usually this payload would be: 1B Custom SEI type + 16B UUID + other payload Bytes.
+     */
+    AV_FRAME_DATA_NETINT_CUSTOM_SEI,
+
+    // NETINT: custom bitrate adjustment
+    /**
+     * This side data takes int32_t type data as payload which indicates the new target bitrate value.
+     */
+    AV_FRAME_DATA_NETINT_BITRATE,
+
+    // NETINT: long term reference frame support
+    /**
+     * This side data is a struct of AVNetintLongTermRef that specifies a
+     * frame's support of long term reference frame.
+     */
+    AV_FRAME_DATA_NETINT_LONG_TERM_REF,
+
     /**
      * Film grain parameters for a frame, described by AVFilmGrainParams.
      * Must be present for every frame which should have film grain applied.
@@ -292,6 +321,20 @@ typedef struct AVRegionOfInterest {
     AVRational qoffset;
 } AVRegionOfInterest;
 
+/**
+ * NETINT: Structure describing long term reference frame support.
+ *
+ */
+typedef struct AVNetintLongTermRef {
+  // A flag for the current picture to be used as a long term reference
+  // picture later at other pictures' encoding
+  uint8_t use_cur_src_as_long_term_pic;
+
+  // A flag to use a long term reference picture in DPB when encoding the
+  // current picture
+  uint8_t use_long_term_ref;
+} AVNetintLongTermRef;
+
 /**
  * This structure describes decoded (raw) audio or video data.
  *
@@ -775,6 +818,29 @@ void av_frame_unref(AVFrame *frame);
  */
 void av_frame_move_ref(AVFrame *dst, AVFrame *src);
 
+#if CONFIG_NI_QUADRA
+/**
+ * Allocate new buffer(s) for video data.
+ *
+ * The following fields must be set on frame before calling this function:
+ * - format (pixel format for video)
+ * - width and height for video
+ *
+ * This function will fill AVFrame.data and AVFrame.buf arrays and, if
+ * necessary, allocate and fill AVFrame.extended_data and AVFrame.extended_buf.
+ * For planar formats, one buffer will be allocated for all planes.
+ *
+ * @warning: if frame already has been allocated, calling this function will
+ *           leak memory. In addition, undefined behavior can occur in certain
+ *           cases. 2pass encoding not yet supported
+ *
+ * @param frame frame in which to store the new buffers.
+ *
+ * @return 0 on success, a negative AVERROR on error.
+ */
+int av_frame_get_buffer_quadra(AVFrame *frame);
+#endif
+
 /**
  * Allocate new buffer(s) for audio or video data.
  *
diff --git a/libavutil/hwcontext.c b/libavutil/hwcontext.c
index ab9ad3703e..2d81012606 100644
--- a/libavutil/hwcontext.c
+++ b/libavutil/hwcontext.c
@@ -51,6 +51,12 @@ static const HWContextType * const hw_table[] = {
 #if CONFIG_VAAPI
     &ff_hwcontext_type_vaapi,
 #endif
+#if CONFIG_NI_LOGAN
+    &ff_hwcontext_type_ni_logan,
+#endif
+#if CONFIG_NI_QUADRA
+    &ff_hwcontext_type_ni_quadra,
+#endif
 #if CONFIG_VDPAU
     &ff_hwcontext_type_vdpau,
 #endif
@@ -73,6 +79,8 @@ static const char *const hw_type_names[] = {
     [AV_HWDEVICE_TYPE_D3D11VA] = "d3d11va",
     [AV_HWDEVICE_TYPE_OPENCL] = "opencl",
     [AV_HWDEVICE_TYPE_QSV]    = "qsv",
+    [AV_HWDEVICE_TYPE_NI_LOGAN]  = "ni_logan",
+    [AV_HWDEVICE_TYPE_NI_QUADRA] = "ni_quadra",
     [AV_HWDEVICE_TYPE_VAAPI]  = "vaapi",
     [AV_HWDEVICE_TYPE_VDPAU]  = "vdpau",
     [AV_HWDEVICE_TYPE_VIDEOTOOLBOX] = "videotoolbox",
diff --git a/libavutil/hwcontext.h b/libavutil/hwcontext.h
index c18b7e1e8b..17f3b8ed61 100644
--- a/libavutil/hwcontext.h
+++ b/libavutil/hwcontext.h
@@ -30,6 +30,8 @@ enum AVHWDeviceType {
     AV_HWDEVICE_TYPE_CUDA,
     AV_HWDEVICE_TYPE_VAAPI,
     AV_HWDEVICE_TYPE_DXVA2,
+    AV_HWDEVICE_TYPE_NI_LOGAN,
+    AV_HWDEVICE_TYPE_NI_QUADRA,
     AV_HWDEVICE_TYPE_QSV,
     AV_HWDEVICE_TYPE_VIDEOTOOLBOX,
     AV_HWDEVICE_TYPE_D3D11VA,
diff --git a/libavutil/hwcontext_internal.h b/libavutil/hwcontext_internal.h
index e6266494ac..28d15b6694 100644
--- a/libavutil/hwcontext_internal.h
+++ b/libavutil/hwcontext_internal.h
@@ -170,6 +170,8 @@ extern const HWContextType ff_hwcontext_type_dxva2;
 extern const HWContextType ff_hwcontext_type_opencl;
 extern const HWContextType ff_hwcontext_type_qsv;
 extern const HWContextType ff_hwcontext_type_vaapi;
+extern const HWContextType ff_hwcontext_type_ni_logan;
+extern const HWContextType ff_hwcontext_type_ni_quadra;
 extern const HWContextType ff_hwcontext_type_vdpau;
 extern const HWContextType ff_hwcontext_type_videotoolbox;
 extern const HWContextType ff_hwcontext_type_mediacodec;
diff --git a/libavutil/hwcontext_ni_logan.c b/libavutil/hwcontext_ni_logan.c
new file mode 100644
index 0000000000..a536110cee
--- /dev/null
+++ b/libavutil/hwcontext_ni_logan.c
@@ -0,0 +1,904 @@
+/*
+* This file is part of FFmpeg.
+*
+* FFmpeg is free software; you can redistribute it and/or
+* modify it under the terms of the GNU Lesser General Public
+* License as published by the Free Software Foundation; either
+* version 2.1 of the License, or (at your option) any later version.
+*
+* FFmpeg is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+* Lesser General Public License for more details.
+*
+* You should have received a copy of the GNU Lesser General Public
+* License along with FFmpeg; if not, write to the Free Software
+* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+*/
+
+#include "config.h"
+
+#include <fcntl.h>
+#if HAVE_UNISTD_H
+#   include <unistd.h>
+#endif
+
+
+#include "avassert.h"
+#include "buffer.h"
+#include "common.h"
+#include "hwcontext.h"
+#include "hwcontext_internal.h"
+#include "hwcontext_ni_logan.h"
+#include "libavutil/imgutils.h"
+#include "mem.h"
+#include "pixdesc.h"
+#include "pixfmt.h"
+#include "ni_util_logan.h"
+
+static enum AVPixelFormat supported_pixel_formats[] = {
+  AV_PIX_FMT_YUV420P,
+  AV_PIX_FMT_YUV420P10BE,
+  AV_PIX_FMT_YUV420P10LE
+};
+
+typedef struct NILOGANDeviceContext {
+  ni_device_handle_t  handle;
+} NILOGANDeviceContext;
+
+static inline void ni_logan_buffer_free(void *opaque, uint8_t *data)
+{
+  ni_logan_aligned_free(data);
+}
+
+static inline void ni_logan_frame_free(void *opaque, uint8_t *data)
+{
+  av_log(NULL, AV_LOG_VERBOSE, "ni_logan_frame_free\n");
+  if (data)
+  {
+    ni_logan_session_context_t *p_ctx = (ni_logan_session_context_t *) opaque;
+    ni_logan_hwframe_surface_t* p_data3 = (ni_logan_hwframe_surface_t *) data; //assuming for hwframes there is no data0,1,2?
+    //TODO use int32t device_handle to kill the buffer!
+    av_log(NULL, AV_LOG_VERBOSE, "ni_logan_frame_free:%d, %p\n", p_data3->i8FrameIdx, data);
+    if (p_data3->i8FrameIdx != NI_LOGAN_INVALID_HW_FRAME_IDX)
+    {
+#ifdef _WIN32
+      int64_t handle = (((int64_t) p_data3->device_handle_ext) << 32) | p_data3->device_handle;
+      ni_logan_decode_buffer_free(p_data3, (ni_device_handle_t) handle, p_ctx->event_handle);
+#else
+      ni_logan_decode_buffer_free(p_data3, (ni_device_handle_t) p_data3->device_handle, p_ctx->event_handle);
+#endif
+    }
+    free(data);
+  }
+}
+
+static int ni_logan_device_create(AVHWDeviceContext *ctx,
+                            const char *device,
+                            AVDictionary *opts,
+                            int flags)
+{
+  AVNILOGANDeviceContext *ni_logan_ctx;
+  int ret = 0;
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_device_create %s\n", device);
+
+  ni_logan_ctx = (AVNILOGANDeviceContext *) ctx->hwctx;
+  ni_logan_ctx->device_idx = (int) NI_INVALID_DEVICE_HANDLE;
+  if (device)
+  {
+    ni_logan_ctx->device_idx = atoi(device);
+    if (ni_logan_ctx->device_idx < 0)
+    {
+      av_log(ctx, AV_LOG_ERROR, "ni_logan_device_create(): error device index = %d\n",
+             ni_logan_ctx->device_idx);
+      return AVERROR(EINVAL);
+    }
+  }
+
+  return ret;
+}
+
+static int ni_logan_frames_get_constraints(AVHWDeviceContext *ctx,
+                                     const void *hwconfig,
+                                     AVHWFramesConstraints *constraints)
+{
+  int i;
+
+  int num_pix_fmts_supported;
+
+  num_pix_fmts_supported = FF_ARRAY_ELEMS(supported_pixel_formats);
+
+  constraints->valid_sw_formats = av_malloc_array(num_pix_fmts_supported + 1,
+                                  sizeof(*constraints->valid_sw_formats));
+
+  if (!constraints->valid_sw_formats)
+    return AVERROR(ENOMEM);
+
+  for (i = 0; i < num_pix_fmts_supported; i++)
+    constraints->valid_sw_formats[i] = supported_pixel_formats[i];
+
+  constraints->valid_sw_formats[num_pix_fmts_supported] = AV_PIX_FMT_NONE;
+
+  constraints->valid_hw_formats = av_malloc_array(2, sizeof(*constraints->valid_hw_formats));
+
+  if (!constraints->valid_hw_formats)
+    return AVERROR(ENOMEM);
+
+  constraints->valid_hw_formats[0] = AV_PIX_FMT_NI_LOGAN;
+  constraints->valid_hw_formats[1] = AV_PIX_FMT_NONE;
+
+  return 0;
+}
+
+static int ni_logan_get_buffer(AVHWFramesContext *ctx, AVFrame *frame)
+{
+  int ret = 0, buf_size;
+  uint8_t *buf;
+  ni_logan_frame_t *xfme;
+  NILOGANFramesContext *ni_logan_ctx = ctx->internal->priv;
+  ni_logan_session_data_io_t dst_session_io_data = { 0 };
+  ni_logan_session_data_io_t * p_dst_session_data = &dst_session_io_data;
+
+  av_log(ctx, AV_LOG_TRACE, "ni_logan_get_buffer() enter\n");
+
+  //alloc dest avframe buff
+  ret = ni_logan_frame_buffer_alloc(&(p_dst_session_data->data.frame),
+                              ctx->width,
+                              ctx->height,
+                              0,
+                              1, //codec type does not matter, metadata exists
+                              ni_logan_ctx->api_ctx.bit_depth_factor,
+                              1);
+  if (ret != 0)
+    return AVERROR(ENOMEM);
+
+  xfme = &(p_dst_session_data->data.frame);
+  buf_size = xfme->data_len[0] + xfme->data_len[1] + xfme->data_len[2] + xfme->data_len[3];
+  buf = xfme->p_data[0];
+  memset(buf, 0, buf_size);
+  frame->buf[0] = av_buffer_create(buf, buf_size, ni_logan_frame_free, &ni_logan_ctx->api_ctx, 0);
+  buf = frame->buf[0]->data;
+  if (!frame->buf[0])
+    return AVERROR(ENOMEM);
+
+  // init AVFrame
+  frame->data[3] = (uint8_t*) xfme->p_buffer + xfme->data_len[0] + xfme->data_len[1] + xfme->data_len[2];
+  ((ni_logan_hwframe_surface_t *)frame->data[3])->i8FrameIdx = NI_LOGAN_INVALID_HW_FRAME_IDX;
+  frame->format = AV_PIX_FMT_NI_LOGAN;
+  frame->width = ctx->width;
+  frame->height = ctx->height;
+  av_log(ctx, AV_LOG_TRACE, "ni_logan_get_buffer() exit\n");
+
+  return 0;
+}
+
+static int ni_logan_transfer_get_formats(AVHWFramesContext *ctx,
+                                   enum AVHWFrameTransferDirection dir,
+                                   enum AVPixelFormat **formats)
+{
+  enum AVPixelFormat *fmts;
+
+  fmts = av_malloc_array(2, sizeof(*fmts));
+  if (!fmts)
+    return AVERROR(ENOMEM);
+
+  fmts[0] = ctx->sw_format;
+  fmts[1] = AV_PIX_FMT_NONE;
+
+  *formats = fmts;
+
+  return 0;
+}
+
+static void ni_logan_frames_uninit(AVHWFramesContext *ctx)
+{
+  NILOGANFramesContext *s = ctx->internal->priv;
+  NILOGANFramesContext *ni_logan_ctx = ctx->internal->priv;
+  AVNILOGANDeviceContext * ni_logan_dev_ctx = ctx->device_ctx->hwctx;
+  int dev_idx = ni_logan_dev_ctx->device_idx; //Supplied by init_hw_device ni=<name>:<id>
+
+  av_log(ctx, AV_LOG_TRACE, "ni_logan_frames_uninit() :only close if upload instance, poolsize=%d devid=%d\n",
+         ctx->initial_pool_size, dev_idx);
+  if (dev_idx != -1)
+  {
+    av_log(ctx, AV_LOG_VERBOSE, "SessionID = %d!\n", ni_logan_ctx->api_ctx.session_id);
+    if (ni_logan_ctx->api_ctx.session_id != 0) //assume 0 in invalid ID
+    {
+      ni_logan_device_session_close(&ni_logan_ctx->api_ctx, 1, NI_LOGAN_DEVICE_TYPE_UPLOAD);
+    }
+    //only upload frames init allocates these ones
+    av_freep(&s->surface_ptrs);
+    av_freep(&s->surfaces_internal);
+    av_freep(&s->rsrc_ctx);
+  }
+
+  if (ni_logan_ctx->src_session_io_data)
+  {
+    if (ni_logan_ctx->src_session_io_data->data.frame.p_buffer)
+    {
+      av_log(ctx, AV_LOG_TRACE, "ni_logan_frames_uninit free p_buffer\n");
+      ni_logan_frame_buffer_free(&ni_logan_ctx->src_session_io_data->data.frame);
+    }
+    av_freep(&ni_logan_ctx->src_session_io_data);
+  }
+
+  if (ni_logan_ctx->suspended_device_handle != NI_INVALID_DEVICE_HANDLE)
+  {
+    av_log(ctx, AV_LOG_TRACE, "ni_logan_frames_uninit(): close suspended device "
+	   "handle, =%" SIZE_SPECIFIER "\n",
+	   (int64_t) ni_logan_ctx->suspended_device_handle);
+    ni_logan_device_close(ni_logan_ctx->suspended_device_handle);
+  }
+
+  ni_logan_device_session_context_clear(&s->api_ctx);
+}
+
+static AVBufferRef *ni_logan_pool_alloc(void *opaque, int size)
+{
+  AVHWFramesContext    *ctx = (AVHWFramesContext*)opaque;
+  NILOGANFramesContext       *s = ctx->internal->priv;
+  AVNILOGANFramesContext *frames_hwctx = ctx->hwctx;
+
+  if (s->nb_surfaces_used < frames_hwctx->nb_surfaces) {
+    s->nb_surfaces_used++;
+    return av_buffer_create((uint8_t*)(s->surfaces_internal + s->nb_surfaces_used - 1),
+                            sizeof(*frames_hwctx->surfaces), NULL, NULL, 0);
+  }
+
+  return NULL;
+}
+
+static int ni_logan_init_surface(AVHWFramesContext *ctx, ni_logan_hwframe_surface_t *surf)
+{
+  /* Fill with dummy values. This data is never used. */
+  surf->i8FrameIdx        = NI_LOGAN_INVALID_HW_FRAME_IDX;
+  surf->i8InstID          = 0;
+  surf->ui16SessionID     = NI_LOGAN_INVALID_SESSION_ID;
+  surf->device_handle     = NI_INVALID_DEVICE_HANDLE;
+  surf->device_handle_ext = NI_INVALID_DEVICE_HANDLE;
+  surf->bit_depth         = 0;
+  surf->encoding_type     = 0;
+  surf->seq_change        = 0;
+
+  return 0;
+}
+
+static int ni_logan_init_pool(AVHWFramesContext *ctx)
+{
+  NILOGANFramesContext              *s = ctx->internal->priv; //NI
+  AVNILOGANFramesContext *frames_hwctx = ctx->hwctx;          //NI
+
+  int i, ret = 0;
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_init_pool() enter, pool_size=%d\n", ctx->initial_pool_size);
+  if (ctx->initial_pool_size <= 0) {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_init_pool(): NI requires a fixed frame pool size\n");
+    return AVERROR(EINVAL);
+  }
+
+  s->surfaces_internal = av_mallocz_array(ctx->initial_pool_size,
+                                          sizeof(*s->surfaces_internal));
+  if (!s->surfaces_internal)
+    return AVERROR(ENOMEM);
+
+  for (i = 0; i < ctx->initial_pool_size; i++) {
+    ret = ni_logan_init_surface(ctx, &s->surfaces_internal[i]);
+    if (ret < 0)
+      return ret;
+  }
+
+  ctx->internal->pool_internal = av_buffer_pool_init2(sizeof(ni_logan_hwframe_surface_t),
+                                                      ctx, ni_logan_pool_alloc, NULL);
+  if (!ctx->internal->pool_internal)
+    return AVERROR(ENOMEM);
+
+  frames_hwctx->surfaces = s->surfaces_internal;
+  frames_hwctx->nb_surfaces = ctx->initial_pool_size;
+
+  return 0;
+}
+
+static int ni_logan_init_internal_session(AVHWFramesContext *ctx)
+{
+  NILOGANFramesContext *s = ctx->internal->priv;
+  ni_logan_session_context_t *p_ctx = &s->api_ctx;
+
+  ni_logan_device_session_context_init(p_ctx);
+
+#ifdef _WIN32
+  p_ctx->event_handle = ni_logan_create_event();
+  if (p_ctx->event_handle == NI_INVALID_EVENT_HANDLE)
+  {
+    return AVERROR(EINVAL);
+  }
+
+  p_ctx->thread_event_handle = ni_logan_create_event();
+  if (p_ctx->thread_event_handle == NI_INVALID_EVENT_HANDLE)
+  {
+    return AVERROR(EINVAL);
+  }
+#endif
+
+  return 0;
+}
+
+// hwupload runs this on hwupload_config_output
+static int ni_logan_frames_init(AVHWFramesContext *ctx)
+{
+  NILOGANFramesContext *ni_logan_ctx = ctx->internal->priv;
+  AVNILOGANDeviceContext *device_hwctx = (AVNILOGANDeviceContext *) ctx->device_ctx->hwctx;
+  int dev_idx = device_hwctx->device_idx;
+  int linesize_aligned,height_aligned;
+  int pool_size,ret;
+  ni_log_set_level(ff_to_ni_log_level(av_log_get_level()));
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_frames_init() enter, supplied poolsize = %d, dev_idx=%d\n",
+         ctx->initial_pool_size, dev_idx);
+  
+  ni_logan_ctx->suspended_device_handle = NI_INVALID_DEVICE_HANDLE;
+  pool_size = ctx->initial_pool_size;
+  if (dev_idx == -1)
+  {
+    if (pool_size != -1) // ffmpeg does not sepcify init_hw_device for decoder - so decoder dev_dec_idx is always -1
+    {
+      av_log(ctx, AV_LOG_ERROR, "ni_logan_frames_init(): No device selected!\n");
+      return AVERROR(EINVAL);
+    }
+  }
+
+  ret = ni_logan_init_internal_session(ctx);
+  if (ret < 0)
+  {
+    return ret;
+  }
+  
+  if (pool_size == -1) // decoder returns here
+  {
+    // Init event handler for decoder since it does not invoke
+    // ni_logan_device_session_open here but the event handler is necessary for
+    // Windows event handle.
+    av_log(ctx, AV_LOG_VERBOSE, "ni_logan_frames_init(): Invalid poolsize, assumed decoder mode\n");
+    return ret;
+  }
+  else if (pool_size == 0)
+  {
+    uint32_t pixel_area = ctx->width * ctx->height * (1 + (AV_PIX_FMT_YUV420P10BE == ctx->sw_format)
+                         + (AV_PIX_FMT_YUV420P10LE == ctx->sw_format));
+    if (pixel_area < NI_LOGAN_NUM_OF_PIXELS_720P)
+    {
+      pool_size = ctx->initial_pool_size = 22;
+    }
+    else
+    {
+      pool_size = ctx->initial_pool_size = 20;
+    }
+
+    av_log(ctx, AV_LOG_VERBOSE, "ni_logan_frames_init(): Pool_size autoset to %d\n", pool_size);
+  }
+
+  linesize_aligned = ((ctx->width + 31) / 32) * 32;
+  if (linesize_aligned < NI_LOGAN_MIN_WIDTH)
+  {
+    linesize_aligned = NI_LOGAN_MIN_WIDTH;
+  }
+
+  ctx->width = linesize_aligned;
+
+  height_aligned = ((ctx->height + 15) / 16) * 16;
+  if (height_aligned < NI_LOGAN_MIN_HEIGHT)
+  {
+    ctx->height = NI_LOGAN_MIN_HEIGHT;
+    height_aligned = NI_LOGAN_MIN_HEIGHT;
+  }
+  else if (height_aligned > ctx->height)
+  {
+    ctx->height = height_aligned;
+  }
+
+  ni_logan_ctx->api_ctx.active_video_width = ctx->width;
+  ni_logan_ctx->api_ctx.active_video_height = ctx->height;
+
+  switch (ctx->sw_format)
+  {
+    case AV_PIX_FMT_YUV420P:
+      ni_logan_ctx->api_ctx.bit_depth_factor = 1;
+      ni_logan_ctx->api_ctx.src_bit_depth = 8;
+      ni_logan_ctx->api_ctx.pixel_format = NI_LOGAN_PIX_FMT_YUV420P;
+      break;
+    case AV_PIX_FMT_YUV420P10LE:
+      ni_logan_ctx->api_ctx.bit_depth_factor = 2;
+      ni_logan_ctx->api_ctx.src_bit_depth = 10;
+      ni_logan_ctx->api_ctx.src_endian = NI_LOGAN_FRAME_LITTLE_ENDIAN;
+      ni_logan_ctx->api_ctx.pixel_format = NI_LOGAN_PIX_FMT_YUV420P10LE;
+      break;
+    default:
+      return AVERROR(EINVAL);
+  }
+
+  if (ctx->width > NI_LOGAN_MAX_RESOLUTION_WIDTH ||
+      ctx->height > NI_LOGAN_MAX_RESOLUTION_HEIGHT ||
+      ctx->width * ctx->height > NI_LOGAN_MAX_RESOLUTION_AREA)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_frames_init(): Error XCoder resolution %dx%d not supported\n",
+           ctx->width, ctx->height);
+    av_log(ctx, AV_LOG_ERROR, "Max Supported Width: %d Height %d Area %d\n",
+           NI_LOGAN_MAX_RESOLUTION_WIDTH, NI_LOGAN_MAX_RESOLUTION_HEIGHT, NI_LOGAN_MAX_RESOLUTION_AREA);
+    return AVERROR_EXTERNAL;
+  }
+  else if (dev_idx >= 0)
+  {
+    /* allocate based on what user specifies */
+    if ((ni_logan_ctx->rsrc_ctx = ni_logan_rsrc_allocate_simple_direct(NI_LOGAN_DEVICE_TYPE_DECODER, dev_idx)) == NULL)
+    {
+      av_log(ctx, AV_LOG_ERROR, "ni_logan_frames_init(): Error XCoder resource allocation: inst %d not available!\n",
+             dev_idx);
+      return AVERROR_EXTERNAL;
+    }
+  }
+  else
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_frames_init(): Error XCoder command line options");
+    return AVERROR(EINVAL);
+  }
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_frames_init(): pixel sw_format=%d width = %d height = %d outformat=%d\n",
+         ctx->sw_format, ctx->width, ctx->height, ctx->format);
+
+  ni_logan_ctx->api_ctx.hw_id = dev_idx;
+  ret = ni_logan_device_session_open(&ni_logan_ctx->api_ctx, NI_LOGAN_DEVICE_TYPE_UPLOAD);
+  if (ret < 0)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_frames_init(): Error Something wrong in xcoder open\n");
+    ni_logan_frames_uninit(ctx);
+    return AVERROR_EXTERNAL;
+  }
+  else
+  {
+    av_log(ctx, AV_LOG_VERBOSE, "ni_logan_frames_init(): XCoder %s.%d (inst: %d) opened successfully\n",
+           ni_logan_ctx->rsrc_ctx->p_device_info->dev_name, dev_idx, ni_logan_ctx->api_ctx.session_id);
+  }
+
+  ret = ni_logan_device_session_init_framepool(&ni_logan_ctx->api_ctx, pool_size);
+  if (ret < 0)
+  {
+    return ret;
+  }
+
+  // init src_session_io_data
+  ni_logan_ctx->src_session_io_data =  malloc(sizeof(ni_logan_session_data_io_t));
+  if(!ni_logan_ctx->src_session_io_data)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_frames_init(): Error alloc src_session_io_data memory\n");
+    return AVERROR(ENOMEM);
+  }
+  memset(ni_logan_ctx->src_session_io_data, 0, sizeof(ni_logan_session_data_io_t));
+  ni_logan_ctx->src_session_io_data->data.frame.extra_data_len = NI_LOGAN_APP_ENC_FRAME_META_DATA_SIZE;
+
+  if (!ctx->pool) {
+    ret = ni_logan_init_pool(ctx);
+    if (ret < 0) {
+      av_log(ctx, AV_LOG_ERROR, "ni_logan_frames_init(): Error creating an internal frame pool\n");
+      return ret;
+    }
+  }
+  return 0;
+}
+
+/*!******************************************************************************
+*  \brief  Download frame from the NI devices
+*
+*  \param[in]   ctx    FFmpeg hardware frames context
+*  \param[in]   dst    input hardware frames, fmt AV_PIX_FMT_NI_LOGAN.
+*  \param[out]  src    output frames, fmt YUV420P, etc.
+*
+*  \return On success    0
+*          On failure    <0
+*******************************************************************************/
+static int ni_logan_hwdl_frame(AVHWFramesContext *ctx, AVFrame *dst, const AVFrame *src)
+{
+  int ret = 0, buf_size;
+  uint8_t *buf;
+  ni_logan_frame_t *xfme;
+  NILOGANFramesContext *ni_logan_ctx = ctx->internal->priv;
+  ni_logan_session_data_io_t session_io_data = { 0 };
+  ni_logan_session_data_io_t * p_session_data = &session_io_data;
+
+  ni_logan_hwframe_surface_t* src_surf = (ni_logan_hwframe_surface_t*) src->data[3];
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_hwdl_frame(): dev_handle=%" SIZE_SPECIFIER ""
+	 ", FrameIdx=%d, SessionID=%d\n",
+         src_surf->device_handle,
+         src_surf->i8FrameIdx,
+         src_surf->ui16SessionID);
+
+  av_log(ctx, AV_LOG_DEBUG, "ni_logan_hwdl_frame(): processed width=%d, height=%d\n",
+         src->width, src->height);
+
+  ret = ni_logan_frame_buffer_alloc(&(p_session_data->data.frame),
+                              ni_logan_ctx->pc_width, ni_logan_ctx->pc_height,
+                              src_surf->encoding_type == NI_LOGAN_CODEC_FORMAT_H264,
+                              1,
+                              src_surf->bit_depth,
+                              0);
+  
+  if (NI_LOGAN_RETCODE_SUCCESS != ret)
+  {
+    return AVERROR_EXTERNAL;
+  }
+
+  ret = ni_logan_device_session_hwdl(&ni_logan_ctx->api_ctx, p_session_data, src_surf);
+  if (ret <= 0)
+  {
+    av_log(ctx, AV_LOG_DEBUG, "ni_logan_hwdl_frame(): failed to retrieve frame\n");
+    return AVERROR_EXTERNAL;
+  }
+
+  xfme = &(p_session_data->data.frame);
+  buf_size = xfme->data_len[0] + xfme->data_len[1] + xfme->data_len[2];
+  buf = xfme->p_data[0];
+    
+  dst->buf[0] = av_buffer_create(buf, buf_size, ni_logan_buffer_free, NULL, 0);
+  buf = dst->buf[0]->data;
+  if (!dst->buf[0])
+    return AVERROR(ENOMEM);
+
+  av_log(ctx, AV_LOG_DEBUG, "ni_logan_hwdl_frame(): fill array, linesize[0]=%d, fmt=%d, width=%d, height=%d\n",
+         dst->linesize[0], ctx->sw_format, ni_logan_ctx->pc_width, ni_logan_ctx->pc_height);
+  if ((ret = av_image_fill_arrays(dst->data, dst->linesize,
+       buf, ctx->sw_format,
+       ni_logan_ctx->pc_width,
+       ni_logan_ctx->pc_height, 1)) < 0)
+  {
+    av_buffer_unref(&dst->buf[0]);
+    return ret;
+  }
+
+  dst->format = ctx->sw_format;
+  dst->width = ni_logan_ctx->pc_width;
+  dst->height = ni_logan_ctx->pc_height;
+  dst->crop_bottom = ni_logan_ctx->pc_crop_bottom;
+  dst->crop_right = ni_logan_ctx->pc_crop_right;
+
+  av_log(ctx, AV_LOG_DEBUG, "dst crop right %" SIZE_SPECIFIER " bot "
+	 "%" SIZE_SPECIFIER " width %d height %d\n",
+         dst->crop_right,
+         dst->crop_bottom,
+         dst->width,
+         dst->height);
+  av_frame_apply_cropping(dst, 0); //0 for simplicity
+  av_log(ctx, AV_LOG_DEBUG, "POST dst crop right %" SIZE_SPECIFIER " bot"
+	 "%" SIZE_SPECIFIER " width %d height %d\n",
+         dst->crop_right,
+         dst->crop_bottom,
+         dst->width,
+         dst->height);
+  av_frame_copy_props(dst, src);//should about get the metadata right
+  dst->format = ctx->sw_format;
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_hwdl_frame: frame->width=%d, frame->height=%d, "
+         "crop top %" SIZE_SPECIFIER " bottom %" SIZE_SPECIFIER " "
+         "left %" SIZE_SPECIFIER " right %" SIZE_SPECIFIER ", "
+         "frame->format=%d, frame->linesize=%d/%d/%d\n",
+         dst->width, dst->height,
+         dst->crop_top, dst->crop_bottom,
+         dst->crop_left, dst->crop_right,
+         dst->format, dst->linesize[0], dst->linesize[1], dst->linesize[2]);
+
+  return ret;
+}
+
+/*!******************************************************************************
+*  \brief  Upload frame to the NI devices
+*
+*  \param[in]   ctx    FFmpeg hardware frames context
+*  \param[in]   dst    input frames, fmt YUV420P, etc.
+*  \param[out]  src    output hardware frames, fmt AV_PIX_FMT_NI_LOGAN.
+*
+*  \return On success    0
+*          On failure    <0
+*******************************************************************************/
+static int ni_logan_hwup_frame(AVHWFramesContext *ctx, AVFrame *dst, const AVFrame *src)
+{
+  NILOGANFramesContext *ni_logan_ctx = ctx->internal->priv;
+  int ret = 0;
+  int dst_stride[4],plane_height[4];
+  int height_aligned[4]; //HEIGHT padding for enc
+  int padding_height;
+  int pixel_format;
+  int i,nb_planes=0;
+  const AVPixFmtDescriptor *desc;
+  ni_logan_session_data_io_t * p_src_session_data = ni_logan_ctx->src_session_io_data;
+  ni_logan_hwframe_surface_t* dst_surf = (ni_logan_hwframe_surface_t*)dst->data[3];
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_hwup_frame() enter\n");
+  if (!src)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_hwup_frame(): Src frame is empty! eof?\n");
+  }
+
+  if (ret < 0)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_hwup_frame(): failed to allocate surf\n");
+    return AVERROR_EXTERNAL;
+  }
+
+
+  //alloc src avframe buff--------------
+  switch (src->format)
+  {
+    case AV_PIX_FMT_YUV420P:
+    case AV_PIX_FMT_YUV420P10LE:
+      if (src->width < NI_LOGAN_MIN_WIDTH)
+      {
+        dst_stride[0] = FFALIGN(NI_LOGAN_MIN_WIDTH, 32) * ni_logan_ctx->api_ctx.bit_depth_factor;
+      }
+      else
+      {
+        dst_stride[0] = FFALIGN(src->width,32) * ni_logan_ctx->api_ctx.bit_depth_factor;
+      }
+      dst_stride[1] = dst_stride[2] = dst_stride[0]/2;
+      height_aligned[0] = ((src->height + 7) / 8) * 8;
+      if (1){//avctx->codec_id == AV_CODEC_ID_H264) {
+          height_aligned[0] = ((src->height + 15) / 16) * 16; //force to this for max compat
+      }
+      if (height_aligned[0] < NI_LOGAN_MIN_HEIGHT)
+      {
+          height_aligned[0] = NI_LOGAN_MIN_HEIGHT;
+      }
+      height_aligned[1] = height_aligned[2] = height_aligned[0] / 2;
+      break;
+    default:
+      av_log(ctx, AV_LOG_ERROR, "ni_logan_hwup_frame(): Error Pixel format %s not supported by device %s\n",
+             av_get_pix_fmt_name(src->format),ctx->internal->hw_type->name);
+      return AVERROR(EINVAL);
+  }
+
+  switch (ctx->sw_format)
+  {
+    case AV_PIX_FMT_YUV420P:
+      pixel_format = NI_LOGAN_PIX_FMT_YUV420P;
+      break;
+    case AV_PIX_FMT_YUV420P10LE:
+      pixel_format = NI_LOGAN_PIX_FMT_YUV420P10LE;
+      break;
+    default:
+      av_log(ctx, AV_LOG_ERROR, "ni_logan_hwup_frame(): Error Pixel format %s not supported by devices %s\n",
+             av_get_pix_fmt_name(src->format),ctx->internal->hw_type->name);
+      return AVERROR(EINVAL);
+  }
+
+  ret = ni_logan_frame_buffer_alloc_v4(&(p_src_session_data->data.frame),
+    pixel_format, src->width, src->height, dst_stride,
+    1,
+    p_src_session_data->data.frame.extra_data_len);
+
+  if (ret < 0)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_hwup_frame(): Error Cannot allocate ni_logan_frame %d\n", ret);
+    return ret;
+  }
+
+  if (!p_src_session_data->data.frame.p_data[0])
+  {
+    return AVERROR(ENOMEM);
+  }
+
+  switch (src->format)
+  {
+    case AV_PIX_FMT_YUV420P:
+    case AV_PIX_FMT_YUV420P10LE:
+      plane_height[0] = src->height;
+      plane_height[1] = src->height / 2;
+      plane_height[2] = src->height / 2;
+      plane_height[3] = 0;
+      break;
+    default:
+      av_log(ctx, AV_LOG_ERROR, "ni_logan_hwup_frame(): Error Pixel format %s not supported by device %s\n",
+             av_get_pix_fmt_name(src->format),ctx->internal->hw_type->name);
+      return AVERROR(EINVAL);
+  }
+
+  desc = av_pix_fmt_desc_get(src->format);
+  for (i = 0; i < desc->nb_components; i++)
+  {
+    nb_planes = FFMAX(desc->comp[i].plane, nb_planes);
+  }
+  nb_planes++;
+
+  av_log(ctx, AV_LOG_TRACE, "ni_logan_hwup_frame: src linesize = %d/%d/%d "
+         "dst alloc linesize = %d/%d/%d  height = %d/%d/%d\n",
+         src->linesize[0], src->linesize[1], src->linesize[2],
+         dst_stride[0], dst_stride[1], dst_stride[2],
+         plane_height[0], plane_height[1], plane_height[2]);
+
+  for (i = 0; i < nb_planes; i++)
+  {
+    int height = plane_height[i];
+    uint8_t *dest = p_src_session_data->data.frame.p_data[i];
+    const uint8_t *srce = (const uint8_t *)src->data[i];
+    for (; height > 0; height--)
+    {
+      memcpy(dest, srce, FFMIN(src->linesize[i], dst_stride[i]));
+      dest += dst_stride[i];      
+      srce += src->linesize[i];
+    }
+
+    // height padding if needed
+    switch (src->format)
+    {
+      case AV_PIX_FMT_YUV420P:
+      case AV_PIX_FMT_YUV420P10LE:
+        /*
+         * TODO: This should probably be removed for Quadra. NI_LOGAN_MIN_HEIGHT == 128
+         *       is smaller than what Quadra can support. This looks like a T408
+         *       requirement.
+         */
+        padding_height = height_aligned[i] - plane_height[i];
+        break;
+      default:
+        av_log(ctx, AV_LOG_ERROR, "ni_logan_hwup_frame(): Error Pixel format %s not supported by device %s\n",
+               av_get_pix_fmt_name(src->format),ctx->internal->hw_type->name);
+        return AVERROR(EINVAL);
+    }
+
+    if (padding_height > 0)
+    {
+      av_log(ctx, AV_LOG_TRACE, "ni_logan_hwup_frame(): plane %d padding %d\n",
+             i, padding_height);
+
+      srce = dest - dst_stride[i];
+      for (; padding_height > 0; padding_height--) {
+        memcpy(dest, srce, dst_stride[i]);
+        dest += dst_stride[i];
+      }
+    }
+  }
+
+  ret = ni_logan_device_session_hwup(&ni_logan_ctx->api_ctx, p_src_session_data, dst_surf);
+  if (ret < 0)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_hwup_frame(): failed to upload frame\n");
+    return AVERROR_EXTERNAL;
+  }
+
+  if (!dst->hw_frames_ctx)
+  {
+    dst->hw_frames_ctx = av_hwframe_ctx_alloc(ctx->device_ref);
+    //((AVHWFramesContext*)dst->hw_frames_ctx->data)->format = AV_PIX_FMT_NI_LOGAN_QUAD;
+  }
+  av_log(ctx, AV_LOG_DEBUG, "ni_logan_hwup_frame(): Assigning hw_frames_ctx\n");
+  ((AVHWFramesContext*)dst->hw_frames_ctx->data)->internal->priv = ni_logan_ctx;
+
+  //set additional info to hwdesc
+  dst_surf->ui16width = src->width;
+  dst_surf->ui16height = src->height;
+  //dst_surf->ui32nodeAddress = 0; //always 0 offset for upload
+  //dst_surf->encoding_type = NI_LOGAN_PIXEL_PLANAR_FORMAT_PLANAR;
+  ////Update frames context
+  //ctx->f[0] = dst_surf->encoding_type;
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_hwup_frame(): dev_handle=%" SIZE_SPECIFIER ""
+	 ", FrameIdx=%d, SessionID=%d\n",
+         dst_surf->device_handle,
+         dst_surf->i8FrameIdx,
+         dst_surf->ui16SessionID);
+  //Update frames context
+  //ctx->split_ctx.f[0] = dst_surf->encoding_type;
+
+  //av_log(ctx, AV_LOG_INFO, "original height width %d/%d, processed h/w = %d/%d\n",
+  //    ctx->pc_height, ctx->pc_width, src->height, src->width);
+  //
+  //ret = ni_logan_frame_buffer_alloc(&(p_session_data->data.frame), ctx->pc_width, ctx->pc_height,
+  //    src_surf->encoding_type, 1,
+  //    src_surf->bit_depth,
+  //    false);
+  if (NI_LOGAN_RETCODE_SUCCESS != ret)
+  {
+    return AVERROR_EXTERNAL;
+  }
+
+  av_frame_copy_props(dst, src);//should about get the metadata right
+
+  av_log(ctx, AV_LOG_DEBUG, "ni_logan_hwup_frame(): Upload frame w/h "
+	 "%d/%d crop r/b %" SIZE_SPECIFIER "/%" SIZE_SPECIFIER "\n",
+	 dst->width, dst->height, dst->crop_right, dst->crop_bottom);
+
+  return ret;
+
+}
+
+static int ni_logan_transfer_data_to(AVHWFramesContext *ctx, AVFrame *dst,
+                               const AVFrame *src)
+{
+  int ret = 0;
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_transfer_data_to() enter\n");
+
+  if (src->width > ctx->width || src->height > ctx->height)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_transfer_data_to(): parameter error, dst=%dx%d, src=%dx%d\n",
+           dst->width, dst->height, src->width, src->height);
+    return AVERROR(EINVAL);
+  }
+
+  ret = ni_logan_hwup_frame(ctx, dst, src);
+  if (ret < 0)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_transfer_data_to(): ni_logan_hwup_frame failed, ret=%d\n", ret);
+    av_frame_free(&dst);
+  }
+  ret = 0;
+  return ret;
+}
+
+static int ni_logan_transfer_data_from(AVHWFramesContext *ctx, AVFrame *dst,
+                                 const AVFrame *src)
+{
+  AVFrame *map;
+  int ret = 0;
+
+  av_log(ctx, AV_LOG_VERBOSE, "ni_logan_transfer_data_from() enter\n");
+  if (dst->width > ctx->width || dst->height > ctx->height)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_transfer_data_from(): parameter error, dst=%dx%d, src=%dx%d\n",
+           dst->width, dst->height, src->width, src->height);
+    return AVERROR(EINVAL);
+  }
+
+  map = av_frame_alloc();
+  if (!map)
+    return AVERROR(ENOMEM);
+  
+  ret = ni_logan_hwdl_frame(ctx, map, src);
+  if (ret < 0)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_transfer_data_from(): ni_logan_hwdl_frame failed, ret=%d\n", ret);
+    goto fail;
+  }
+
+  //Roy?
+  if (dst->format != map->format)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_transfer_data_from(): format error, %d, %d\n",
+           dst->format, map->format);
+  }
+  dst->format = map->format;
+  ret = av_frame_copy(dst, map);
+
+  if (ret < 0)
+  {
+    av_log(ctx, AV_LOG_ERROR, "ni_logan_transfer_data_from(): av_frame_copy failed, ret=%d\n", ret);
+    goto fail;
+  }
+fail:
+  av_frame_free(&map);
+  return ret;
+}
+
+const HWContextType ff_hwcontext_type_ni_logan = {
+  .type = AV_HWDEVICE_TYPE_NI_LOGAN,
+  .name = "NI_LOGAN",
+
+  .device_hwctx_size = sizeof(AVNILOGANDeviceContext),
+  .device_priv_size  = sizeof(NILOGANDeviceContext),
+  .frames_hwctx_size = sizeof(AVNILOGANFramesContext),
+  .frames_priv_size  = sizeof(NILOGANFramesContext),
+
+  .device_create = ni_logan_device_create,
+
+  .frames_get_constraints = ni_logan_frames_get_constraints,
+
+  .frames_init   = ni_logan_frames_init,
+  .frames_uninit = ni_logan_frames_uninit,
+
+  .frames_get_buffer = ni_logan_get_buffer,
+
+  .transfer_get_formats = ni_logan_transfer_get_formats,
+  .transfer_data_to     = ni_logan_transfer_data_to,
+  .transfer_data_from   = ni_logan_transfer_data_from,
+
+  .pix_fmts = (const enum AVPixelFormat[]) {
+    AV_PIX_FMT_NI_LOGAN,
+    AV_PIX_FMT_NONE
+  },
+};
diff --git a/libavutil/hwcontext_ni_logan.h b/libavutil/hwcontext_ni_logan.h
new file mode 100644
index 0000000000..c6bd94633a
--- /dev/null
+++ b/libavutil/hwcontext_ni_logan.h
@@ -0,0 +1,61 @@
+/*
+* This file is part of FFmpeg.
+*
+* FFmpeg is free software; you can redistribute it and/or
+* modify it under the terms of the GNU Lesser General Public
+* License as published by the Free Software Foundation; either
+* version 2.1 of the License, or (at your option) any later version.
+*
+* FFmpeg is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+* Lesser General Public License for more details.
+*
+* You should have received a copy of the GNU Lesser General Public
+* License along with FFmpeg; if not, write to the Free Software
+* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+*/
+
+#ifndef AVUTIL_HWCONTEXT_NI_LOGAN_H
+#define AVUTIL_HWCONTEXT_NI_LOGAN_H
+
+#include "hwcontext.h"
+#include <ni_device_api_logan.h>
+#include <ni_rsrc_api_logan.h>
+
+enum
+{
+  NI_LOGAN_MEMTYPE_VIDEO_MEMORY_NONE,
+  NI_LOGAN_MEMTYPE_VIDEO_MEMORY_DECODER_TARGET,
+  NI_LOGAN_MEMTYPE_VIDEO_MEMORY_HWUPLOAD_TARGET,
+};
+
+typedef struct NILOGANFramesContext {
+  ni_logan_hwframe_surface_t    *surfaces_internal;
+  int                     nb_surfaces_used;
+  ni_logan_hwframe_surface_t    **surface_ptrs;
+  ni_logan_device_context_t     *rsrc_ctx;  /* resource management context */
+  ni_logan_session_data_io_t    *src_session_io_data;
+  ni_logan_session_context_t    api_ctx;//for down/uploading frames
+  int                     pc_height, pc_width, pc_crop_bottom, pc_crop_right; //precropped values
+  ni_device_handle_t      suspended_device_handle;
+} NILOGANFramesContext;
+
+/**
+* This struct is allocated as AVHWDeviceContext.hwctx
+*/
+typedef struct AVNILOGANDeviceContext {
+  int device_idx;
+} AVNILOGANDeviceContext;
+
+/**
+* This struct is allocated as AVHWFramesContext.hwctx
+*/
+typedef struct AVNILOGANFramesContext {
+  ni_logan_hwframe_surface_t  *surfaces;
+  int                   nb_surfaces;
+  int                   bit_depth;
+  int                   frame_type;
+} AVNILOGANFramesContext;
+
+#endif /* AVUTIL_HWCONTEXT_NI_LOGAN_H */
diff --git a/libavutil/hwcontext_ni_quad.c b/libavutil/hwcontext_ni_quad.c
new file mode 100644
index 0000000000..ab7119ac51
--- /dev/null
+++ b/libavutil/hwcontext_ni_quad.c
@@ -0,0 +1,1336 @@
+/*
+* This file is part of FFmpeg.
+*
+* FFmpeg is free software; you can redistribute it and/or
+* modify it under the terms of the GNU Lesser General Public
+* License as published by the Free Software Foundation; either
+* version 2.1 of the License, or (at your option) any later version.
+*
+* FFmpeg is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+* Lesser General Public License for more details.
+*
+* You should have received a copy of the GNU Lesser General Public
+* License along with FFmpeg; if not, write to the Free Software
+* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+*/
+
+#include "config.h"
+
+#include <fcntl.h>
+#include <string.h>
+#if HAVE_UNISTD_H
+#   include <unistd.h>
+#endif
+
+#include "avassert.h"
+#include "buffer.h"
+#include "common.h"
+#include "hwcontext.h"
+#include "hwcontext_internal.h"
+#include "hwcontext_ni_quad.h"
+#include "libavutil/imgutils.h"
+#include "mem.h"
+#include "pixdesc.h"
+#include "pixfmt.h"
+
+static enum AVPixelFormat supported_pixel_formats[] = {
+    AV_PIX_FMT_YUV420P, AV_PIX_FMT_YUYV422, AV_PIX_FMT_UYVY422,
+    AV_PIX_FMT_NV12,    AV_PIX_FMT_ARGB,    AV_PIX_FMT_RGBA,
+    AV_PIX_FMT_ABGR,    AV_PIX_FMT_BGRA,    AV_PIX_FMT_YUV420P10LE,
+    AV_PIX_FMT_NV16,    AV_PIX_FMT_BGR0,    AV_PIX_FMT_P010LE,
+    AV_PIX_FMT_BGRP};
+
+static inline void ni_align_free(void *opaque, uint8_t *data)
+{
+    if (data)
+    {
+        free(data);
+    }
+}
+
+static inline void ni_frame_free(void *opaque, uint8_t *data)
+{
+    if (data)
+    {
+        //assuming for hwframes there is no data0,1,2?
+        //TODO use int32t device_handle to kill the buffer!
+        niFrameSurface1_t* p_data3 = (niFrameSurface1_t*)data;
+        if (p_data3->ui16FrameIdx != 0)
+        {
+            ni_hwframe_buffer_recycle(p_data3, p_data3->device_handle);
+        }
+        free(p_data3);
+    }
+}
+
+static int ni_device_create(AVHWDeviceContext *ctx, const char *device,
+                            AVDictionary *opts, int flags)
+{
+    AVNIDeviceContext *ni_hw_ctx;
+    char *blk_name;
+    uint8_t *fw_rev;
+    int i, card_cnt = 0, ret = 0;
+    ni_device_handle_t fd;
+    uint32_t max_io_size;
+    ni_device_t ni_devices;
+
+    ni_hw_ctx              = (AVNIDeviceContext *)ctx->hwctx;
+    ni_hw_ctx->uploader_ID = -2; // -1 is auto load balance, default -2 invalid
+
+    if (device) {
+        /* parse device string and fail if incorrect */
+        av_log(ctx, AV_LOG_VERBOSE, "%s %s\n", __func__, device);
+        ni_hw_ctx->uploader_ID = atoi(device);
+        av_log(ctx, AV_LOG_DEBUG, "%s: given uploader ID %d\n", __func__,
+               ni_hw_ctx->uploader_ID);
+        if (ni_hw_ctx->uploader_ID < -1) {
+            av_log(ctx, AV_LOG_ERROR, "%s: uploader ID %d must be >= -1.\n",
+                   __func__, ni_hw_ctx->uploader_ID);
+            return AVERROR_UNKNOWN;
+        }
+    }
+
+    for (i = 0; i < NI_MAX_DEVICE_CNT; i++) {
+        ni_hw_ctx->cards[i] = NI_INVALID_DEVICE_HANDLE;
+    }
+
+    /* Scan all cards on the host, only look at NETINT cards */
+    if (ni_rsrc_list_all_devices(&ni_devices) == NI_RETCODE_SUCCESS) {
+        // Note: this only checks for Netint encoders
+        for (i = 0; i < ni_devices.xcoder_cnt[NI_DEVICE_TYPE_ENCODER]; i++) {
+            blk_name =
+                &(ni_devices.xcoders[NI_DEVICE_TYPE_ENCODER][i].blk_name[0]);
+            fw_rev =
+                &(ni_devices.xcoders[NI_DEVICE_TYPE_ENCODER][i].fw_rev[0]);
+            av_log(ctx, AV_LOG_DEBUG, "%s blk name %s\n", __func__, blk_name);
+            fd = ni_device_open(blk_name, &max_io_size);
+            if (fd != NI_INVALID_DEVICE_HANDLE) {
+                ni_hw_ctx->cards[card_cnt++] = fd;
+                if (ni_rsrc_is_fw_compat(fw_rev) == 2) {
+                    av_log(ctx, AV_LOG_WARNING,
+                           "WARNING - Query %s FW "
+                           "version: %s is below the minimum support "
+                           "version for this SW version. Some features may "
+                           "be missing.\n",
+                           blk_name, fw_rev);
+                }
+            }
+        }
+    } else {
+        ret = AVERROR_UNKNOWN;
+    }
+
+    return ret;
+}
+
+static void ni_device_uninit(AVHWDeviceContext *ctx)
+{
+    AVNIDeviceContext *ni_hw_ctx;
+    int i;
+
+    ni_hw_ctx = (AVNIDeviceContext *)ctx->hwctx;
+
+    av_log(ctx, AV_LOG_VERBOSE, "%s\n", __func__);
+
+    for (i = 0; i < NI_MAX_DEVICE_CNT; i++) {
+        ni_device_handle_t fd = ni_hw_ctx->cards[i];
+        if (fd != NI_INVALID_DEVICE_HANDLE) {
+            ni_hw_ctx->cards[i] = NI_INVALID_DEVICE_HANDLE;
+            ni_device_close(fd);
+        } else {
+            break;
+        }
+    }
+
+    return;
+}
+
+static int ni_frames_get_constraints(AVHWDeviceContext *ctx,
+                                     const void *hwconfig,
+                                     AVHWFramesConstraints *constraints)
+{
+    int i;
+    int num_pix_fmts_supported;
+
+    num_pix_fmts_supported = FF_ARRAY_ELEMS(supported_pixel_formats);
+
+    constraints->valid_sw_formats = av_malloc_array(num_pix_fmts_supported + 1,
+                                                    sizeof(*constraints->valid_sw_formats));
+    if (!constraints->valid_sw_formats) {
+        return AVERROR(ENOMEM);
+    }
+
+    for (i = 0; i < num_pix_fmts_supported; i++) {
+        constraints->valid_sw_formats[i] = supported_pixel_formats[i];
+    }
+    constraints->valid_sw_formats[num_pix_fmts_supported] = AV_PIX_FMT_NONE;
+
+    constraints->valid_hw_formats = av_malloc_array(2, sizeof(*constraints->valid_hw_formats));
+    if (!constraints->valid_hw_formats) {
+        return AVERROR(ENOMEM);
+    }
+
+    constraints->valid_hw_formats[0] = AV_PIX_FMT_NI_QUAD;
+    constraints->valid_hw_formats[1] = AV_PIX_FMT_NONE;
+
+    return 0;
+}
+
+static int ni_get_buffer(AVHWFramesContext *ctx, AVFrame *frame)
+{
+    int ret = 0;
+    uint8_t *buf;
+    uint32_t buf_size;
+    ni_frame_t *xfme;
+    NIFramesContext *ni_ctx = ctx->internal->priv;
+    ni_session_data_io_t dst_session_io_data;
+    ni_session_data_io_t * p_dst_session_data = &dst_session_io_data;
+    bool isnv12frame = (ctx->sw_format == AV_PIX_FMT_NV12 ||
+                        ctx->sw_format == AV_PIX_FMT_P010LE);
+
+    av_log(ctx, AV_LOG_TRACE, "hwcontext_ni.c:ni_get_buffer()\n");
+
+    // alloc dest avframe buff
+    memset(p_dst_session_data, 0, sizeof(dst_session_io_data));
+    ret = ni_frame_buffer_alloc(&p_dst_session_data->data.frame, ctx->width,
+                                ctx->height, 0, 1, // codec type does not matter, metadata exists
+                                ni_ctx->api_ctx.bit_depth_factor, 1, !isnv12frame);
+    if (ret != 0) {
+        return AVERROR(ENOMEM);
+    }
+
+    xfme = &p_dst_session_data->data.frame;
+    buf_size = xfme->data_len[0] + xfme->data_len[1] +
+               xfme->data_len[2] + xfme->data_len[3];
+    buf = xfme->p_data[0];
+    memset(buf, 0, buf_size);
+    frame->buf[0] = av_buffer_create(buf, buf_size, ni_frame_free, NULL, 0);
+    if (!frame->buf[0]) {
+        return AVERROR(ENOMEM);
+    }
+    frame->data[3] = xfme->p_buffer + xfme->data_len[0] + xfme->data_len[1] +
+                     xfme->data_len[2];
+    frame->format = AV_PIX_FMT_NI_QUAD;
+    frame->width = ctx->width;
+    frame->height = ctx->height;
+
+    return 0;
+}
+
+static int ni_transfer_get_formats(AVHWFramesContext *ctx,
+                                   enum AVHWFrameTransferDirection dir,
+                                   enum AVPixelFormat **formats)
+{
+    enum AVPixelFormat *fmts;
+
+    fmts = av_malloc_array(2, sizeof(*fmts));
+    if (!fmts) {
+        return AVERROR(ENOMEM);
+    }
+
+    fmts[0] = ctx->sw_format;
+    fmts[1] = AV_PIX_FMT_NONE;
+
+    *formats = fmts;
+
+    return 0;
+}
+
+static void ni_frames_uninit(AVHWFramesContext *ctx)
+{
+    NIFramesContext *ni_ctx = ctx->internal->priv;
+    int dev_dec_idx = ni_ctx->uploader_device_id; //Supplied by init_hw_device ni=<name>:<id> or ni_hwupload=<id>
+
+    av_log(ctx, AV_LOG_DEBUG, "%s: only close if upload instance, poolsize=%d "
+                              "devid=%d\n",
+                              __func__, ctx->initial_pool_size, dev_dec_idx);
+
+    if (dev_dec_idx != -2 && ctx->initial_pool_size >= 0)
+    {
+        if (ni_ctx->src_session_io_data.data.frame.p_buffer) {
+            av_log(ctx, AV_LOG_DEBUG, "%s:free upload src frame buffer\n",
+                   __func__);
+            ni_frame_buffer_free(&ni_ctx->src_session_io_data.data.frame);
+        }
+        av_log(ctx, AV_LOG_VERBOSE, "SessionID = %d!\n", ni_ctx->api_ctx.session_id);
+        if (ni_ctx->api_ctx.session_id != NI_INVALID_SESSION_ID) {
+            ni_device_session_close(&ni_ctx->api_ctx, 1, NI_DEVICE_TYPE_UPLOAD);
+        }
+        ni_device_session_context_clear(&ni_ctx->api_ctx);
+
+        //only upload frames init allocates these ones
+        av_freep(&ni_ctx->surface_ptrs);
+        av_freep(&ni_ctx->surfaces_internal);
+    }
+
+    if (ni_ctx->suspended_device_handle != -1)
+    {
+        av_log(ctx, AV_LOG_DEBUG, "%s: close file handle, =%d\n",
+               __func__, ni_ctx->suspended_device_handle);
+        ni_device_close(ni_ctx->suspended_device_handle);
+    }
+}
+
+static AVBufferRef *ni_pool_alloc(void *opaque,
+#if LIBAVUTIL_VERSION_MAJOR >= 58 || LIBAVUTIL_VERSION_MAJOR >= 57 && LIBAVUTIL_VERSION_MINOR >= 17
+                                  size_t size)
+#else
+                                  int size)
+#endif
+{
+    AVHWFramesContext    *ctx = (AVHWFramesContext*)opaque;
+    NIFramesContext       *s = ctx->internal->priv;
+    AVNIFramesContext *frames_hwctx = ctx->hwctx;
+
+    if (s->nb_surfaces_used < frames_hwctx->nb_surfaces) {
+        s->nb_surfaces_used++;
+        return av_buffer_create((uint8_t*)(s->surfaces_internal + s->nb_surfaces_used - 1),
+                                sizeof(*frames_hwctx->surfaces), NULL, NULL, 0);
+    }
+
+    return NULL;
+}
+
+static int ni_init_surface(AVHWFramesContext *ctx, niFrameSurface1_t *surf)
+{
+    /* Fill with dummy values. This data is never used. */
+    surf->ui16FrameIdx    = 0;
+    surf->ui16session_ID  = 0;
+    surf->ui32nodeAddress = 0;
+    surf->device_handle   = 0;
+    surf->bit_depth       = 0;
+    surf->encoding_type   = 0;
+    surf->output_idx      = 0;
+    surf->src_cpu         = 0;
+
+    return 0;
+}
+
+static int ni_init_pool(AVHWFramesContext *ctx)
+{
+    NIFramesContext              *s = ctx->internal->priv; //NI
+    AVNIFramesContext *frames_hwctx = ctx->hwctx;          //NI
+    int i, ret;
+
+    av_log(ctx, AV_LOG_ERROR, "ctx->initial_pool_size = %d\n", ctx->initial_pool_size);
+
+    if (ctx->initial_pool_size <= 0) {
+        av_log(ctx, AV_LOG_ERROR, "NI requires a fixed frame pool size\n");
+        return AVERROR(EINVAL);
+    }
+
+    s->surfaces_internal = av_calloc(ctx->initial_pool_size,
+                                     sizeof(*s->surfaces_internal));
+    if (!s->surfaces_internal) {
+        return AVERROR(ENOMEM);
+    }
+
+    for (i = 0; i < ctx->initial_pool_size; i++) {
+        ret = ni_init_surface(ctx, &s->surfaces_internal[i]);
+        if (ret < 0) {
+            return ret;
+        }
+    }
+
+    ctx->internal->pool_internal = av_buffer_pool_init2(sizeof(niFrameSurface1_t),
+                                                        ctx, ni_pool_alloc, NULL);
+    if (!ctx->internal->pool_internal) {
+        return AVERROR(ENOMEM);
+    }
+
+    frames_hwctx->surfaces = s->surfaces_internal;
+    frames_hwctx->nb_surfaces = ctx->initial_pool_size;
+
+    return 0;
+}
+
+static int ni_init_internal_session(AVHWFramesContext *ctx)
+{
+    NIFramesContext              *s = ctx->internal->priv;
+    ni_log_set_level(ff_to_ni_log_level(av_log_get_level()));
+    av_log(ctx, AV_LOG_ERROR, "hwcontext_ni:ni_init_internal_session()\n");
+    if (ni_device_session_context_init(&(s->api_ctx)) < 0) {
+        av_log(ctx, AV_LOG_ERROR, "ni init context failure\n");
+        return -1;
+    }
+
+    return 0;
+}
+
+static void init_split_rsrc(NIFramesContext *ni_ctx, int w, int h)
+{
+    int i;
+    ni_split_context_t* p_split_ctx = &ni_ctx->split_ctx;
+    memset(p_split_ctx, 0, sizeof(ni_split_context_t));
+    for (i = 0; i < 3; i++)
+    {
+        ni_ctx->split_ctx.w[i] = w;
+        ni_ctx->split_ctx.h[i] = h;
+        ni_ctx->split_ctx.f[i] = -1;
+    }
+}
+
+static int ni_frames_init(AVHWFramesContext *ctx) //hwupload runs this on hwupload_config_output
+{
+  NIFramesContext *ni_ctx = ctx->internal->priv;
+  AVNIDeviceContext *device_hwctx = (AVNIDeviceContext *) ctx->device_ctx->hwctx;
+  int linesize_aligned,height_aligned;
+  int pool_size,ret;
+  AVNIFramesContext *frames_hwctx = (AVNIFramesContext *)ctx->hwctx;
+
+  av_log(ctx, AV_LOG_ERROR, "%s: Enter, supplied poolsize = %d, devid=%d\n",
+         __func__, ctx->initial_pool_size, device_hwctx->uploader_ID);
+
+  ni_ctx->suspended_device_handle = -1;
+  ni_ctx->uploader_device_id = -2; // -1 auto load balance, default -2 invalid
+  pool_size = ctx->initial_pool_size;
+  if (device_hwctx->uploader_ID < -1) {
+      if (pool_size > -1) // ffmpeg does not specify init_hw_device for decoder
+                          // - so decoder device_hwctx->uploader_ID is always -1
+      {
+          av_log(ctx, AV_LOG_ERROR, "%s Error no uploader device selected!\n",
+                 __func__);
+          return AVERROR(EINVAL);
+      }
+  }
+
+  ret = ni_init_internal_session(ctx);
+  if (ret < 0) {
+      return AVERROR(EINVAL);
+  }
+
+  init_split_rsrc(ni_ctx, ctx->width, ctx->height);
+  if (pool_size <= -1) // None upload init returns here
+  {
+    av_log(ctx, AV_LOG_INFO, "%s: poolsize code %d, this code recquires no host pool\n",
+           __func__, pool_size);
+    return ret;
+  }
+  else if (pool_size == 0)
+  {
+    /*Kept in private NIFramesContext for future reference, the device context version gets overwritten*/
+    ni_ctx->uploader_device_id = device_hwctx->uploader_ID; 
+    pool_size = ctx->initial_pool_size = 3;
+    av_log(ctx, AV_LOG_INFO, "%s: Pool_size autoset to %d\n", __func__, pool_size);
+  }
+
+  if ((ctx->width & 0x1) || (ctx->height & 0x1)) {
+      av_log(ctx, AV_LOG_ERROR, "Odd resolution %dx%d not permitted\n",
+             ctx->width, ctx->height);
+      return AVERROR(EINVAL);
+  }
+
+  linesize_aligned = ctx->width;
+  if (QUADRA)
+  {
+      linesize_aligned = NI_VPU_CEIL(linesize_aligned, 2);
+  }
+  else
+  {
+    linesize_aligned = ((ctx->width + 31) / 32) * 32;
+    if (linesize_aligned < NI_MIN_WIDTH)
+    {
+        // p_param->cfg_enc_params.conf_win_right += NI_MIN_WIDTH - ctx->width;
+        linesize_aligned = NI_MIN_WIDTH;
+    }
+    else if (linesize_aligned > ctx->width)
+    {
+        // p_param->cfg_enc_params.conf_win_right += linesize_aligned -
+        // ctx->width;
+    }
+  }
+  ctx->width = linesize_aligned;
+
+  height_aligned = ctx->height;
+  if (QUADRA)
+  {
+      ctx->height = NI_VPU_CEIL(height_aligned, 2);
+  }
+  else
+  {
+    height_aligned = ((ctx->height + 15) / 16) * 16;
+    if (height_aligned < NI_MIN_HEIGHT)
+    {
+        // p_param->cfg_enc_params.conf_win_bottom += NI_MIN_HEIGHT -
+        // ctx->height;
+        ctx->height    = NI_MIN_HEIGHT;
+        height_aligned = NI_MIN_HEIGHT;
+    }
+    else if (height_aligned > ctx->height)
+    {
+        // p_param->cfg_enc_params.conf_win_bottom += height_aligned -
+        // ctx->height;
+        ctx->height = height_aligned;
+    }
+  }
+
+  ni_ctx->api_ctx.active_video_width = ctx->width;
+  ni_ctx->api_ctx.active_video_height = ctx->height;
+
+  switch (ctx->sw_format)
+  {
+    case AV_PIX_FMT_YUV420P:
+      ni_ctx->api_ctx.bit_depth_factor = 1;
+      ni_ctx->api_ctx.src_bit_depth = 8;
+      ni_ctx->api_ctx.pixel_format = NI_PIX_FMT_YUV420P;
+      break;
+    case AV_PIX_FMT_YUV420P10LE:
+      ni_ctx->api_ctx.bit_depth_factor = 2;
+      ni_ctx->api_ctx.src_bit_depth = 10;
+      ni_ctx->api_ctx.src_endian = NI_FRAME_LITTLE_ENDIAN;
+      ni_ctx->api_ctx.pixel_format = NI_PIX_FMT_YUV420P10LE;
+      break;
+    case AV_PIX_FMT_NV12:
+      ni_ctx->api_ctx.bit_depth_factor = 1;
+      ni_ctx->api_ctx.src_bit_depth = 8;
+      ni_ctx->api_ctx.pixel_format = NI_PIX_FMT_NV12;
+      break;
+    case AV_PIX_FMT_P010LE:
+      ni_ctx->api_ctx.bit_depth_factor = 2;
+      ni_ctx->api_ctx.src_bit_depth = 10;
+      ni_ctx->api_ctx.pixel_format = NI_PIX_FMT_P010LE;
+      ni_ctx->api_ctx.src_endian = NI_FRAME_LITTLE_ENDIAN;
+      break;
+    case AV_PIX_FMT_RGBA:
+        ni_ctx->api_ctx.bit_depth_factor = 4;
+        ni_ctx->api_ctx.src_bit_depth    = 32;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_RGBA;
+        break;
+    case AV_PIX_FMT_BGRA:
+        ni_ctx->api_ctx.bit_depth_factor = 4;
+        ni_ctx->api_ctx.src_bit_depth    = 32;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_BGRA;
+        break;
+    case AV_PIX_FMT_ABGR:
+        ni_ctx->api_ctx.bit_depth_factor = 4;
+        ni_ctx->api_ctx.src_bit_depth    = 32;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_ABGR;
+        break;
+    case AV_PIX_FMT_ARGB:
+        ni_ctx->api_ctx.bit_depth_factor = 4;
+        ni_ctx->api_ctx.src_bit_depth    = 32;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_ARGB;
+        break;
+    case AV_PIX_FMT_BGR0:
+        ni_ctx->api_ctx.bit_depth_factor = 4;
+        ni_ctx->api_ctx.src_bit_depth    = 32;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_BGR0;
+        break;
+    case AV_PIX_FMT_BGRP:
+        ni_ctx->api_ctx.bit_depth_factor = 1;
+        ni_ctx->api_ctx.src_bit_depth    = 24;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_BGRP;
+        break;
+    case AV_PIX_FMT_YUYV422:
+        ni_ctx->api_ctx.bit_depth_factor = 1;
+        ni_ctx->api_ctx.src_bit_depth    = 8;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_YUYV422;
+        break;
+    case AV_PIX_FMT_UYVY422:
+        ni_ctx->api_ctx.bit_depth_factor = 1;
+        ni_ctx->api_ctx.src_bit_depth    = 8;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_UYVY422;
+        break;
+    case AV_PIX_FMT_NV16:
+        ni_ctx->api_ctx.bit_depth_factor = 1;
+        ni_ctx->api_ctx.src_bit_depth    = 8;
+        ni_ctx->api_ctx.src_endian       = NI_FRAME_LITTLE_ENDIAN;
+        ni_ctx->api_ctx.pixel_format     = NI_PIX_FMT_NV16;
+        break;
+    default:
+        av_log(ctx, AV_LOG_ERROR, "Pixel format not supported by device.\n");
+        return AVERROR(EINVAL);
+  }
+
+  if (ctx->width > NI_MAX_RESOLUTION_WIDTH ||
+    ctx->height > NI_MAX_RESOLUTION_HEIGHT ||
+    ctx->width * ctx->height > NI_MAX_RESOLUTION_AREA)
+  {
+    av_log(ctx, AV_LOG_ERROR, "Error XCoder resolution %dx%d not supported\n",
+      ctx->width, ctx->height);
+    av_log(ctx, AV_LOG_ERROR, "Max Supported Width: %d Height %d Area %d\n",
+      NI_MAX_RESOLUTION_WIDTH, NI_MAX_RESOLUTION_HEIGHT, NI_MAX_RESOLUTION_AREA);
+    return AVERROR_EXTERNAL;
+  } else if (ni_ctx->uploader_device_id >= -1) {
+      // leave it to ni_device_session_open to handle uploader session open
+      // based on api_ctx.hw_id set to proper value
+  } else {
+      av_log(ctx, AV_LOG_ERROR, "Error XCoder command line options");
+      return AVERROR(EINVAL);
+  }
+
+  av_log(ctx, AV_LOG_VERBOSE,
+         "pixel sw_format=%d width = %d height = %d outformat=%d "
+         "uploader_device_id=%d\n",
+         ctx->sw_format, ctx->width, ctx->height, ctx->format,
+         ni_ctx->uploader_device_id);
+
+  ni_ctx->api_ctx.hw_id = ni_ctx->uploader_device_id;
+  ni_ctx->api_ctx.keep_alive_timeout = frames_hwctx->keep_alive_timeout;
+  if (0 == ni_ctx->api_ctx.keep_alive_timeout )
+  {
+    ni_ctx->api_ctx.keep_alive_timeout = NI_DEFAULT_KEEP_ALIVE_TIMEOUT;
+  }
+  ret = ni_device_session_open(&ni_ctx->api_ctx, NI_DEVICE_TYPE_UPLOAD);
+  if (ret < 0)
+  {
+    av_log(ctx, AV_LOG_ERROR, "Error Something wrong in xcoder open\n");
+    ni_frames_uninit(ctx);
+    return AVERROR_EXTERNAL;
+  }
+  else
+  {
+      av_log(ctx, AV_LOG_VERBOSE,
+             "XCoder %s.%d (inst: %d) opened successfully\n",
+             ni_ctx->api_ctx.dev_xcoder_name, ni_ctx->api_ctx.hw_id,
+             ni_ctx->api_ctx.session_id);
+  }
+  memset(&ni_ctx->src_session_io_data, 0, sizeof(ni_session_data_io_t));
+  ret = ni_device_session_init_framepool(&ni_ctx->api_ctx, pool_size, 0);
+  if (ret < 0)
+  {
+    return ret;
+  }
+
+  if (!ctx->pool) {
+    ret = ni_init_pool(ctx);
+    if (ret < 0) {
+      av_log(ctx, AV_LOG_ERROR, "Error creating an internal frame pool\n");
+      return ret;
+    }
+  }
+  return 0;
+}
+
+static int ni_to_avframe_copy(AVHWFramesContext *hwfc, AVFrame *dst,
+                              const ni_frame_t *src) {
+    int src_linesize[4], src_height[4];
+    int i, h, nb_planes;
+    uint8_t *src_line, *dst_line;
+
+    nb_planes = av_pix_fmt_count_planes(hwfc->sw_format);
+
+    switch (hwfc->sw_format) {
+    case AV_PIX_FMT_YUV420P:
+        src_linesize[0] = FFALIGN(dst->width, 128);
+        src_linesize[1] = FFALIGN(dst->width / 2, 128);
+        src_linesize[2] = src_linesize[1];
+        src_linesize[3] = 0;
+
+        src_height[0] = dst->height;
+        src_height[1] = FFALIGN(dst->height, 2) / 2;
+        src_height[2] = src_height[1];
+        src_height[3] = 0;
+        break;
+
+    case AV_PIX_FMT_YUV420P10LE:
+        src_linesize[0] = FFALIGN(dst->width * 2, 128);
+        src_linesize[1] = FFALIGN(dst->width, 128);
+        src_linesize[2] = src_linesize[1];
+        src_linesize[3] = 0;
+
+        src_height[0] = dst->height;
+        src_height[1] = FFALIGN(dst->height, 2) / 2;
+        src_height[2] = src_height[1];
+        src_height[3] = 0;
+        break;
+
+    case AV_PIX_FMT_NV12:
+        src_linesize[0] = FFALIGN(dst->width, 128);
+        src_linesize[1] = FFALIGN(dst->width, 128);
+        src_linesize[2] = 0;
+        src_linesize[3] = 0;
+
+        src_height[0] = dst->height;
+        src_height[1] = FFALIGN(dst->height, 2) / 2;
+        src_height[2] = 0;
+        src_height[3] = 0;
+        break;
+
+    case AV_PIX_FMT_NV16:
+        src_linesize[0] = dst->width;
+        src_linesize[1] = dst->width;
+        src_linesize[2] = 0;
+        src_linesize[3] = 0;
+
+        src_height[0] = dst->height;
+        src_height[1] = dst->height;
+        src_height[2] = 0;
+        src_height[3] = 0;
+        break;
+
+    case AV_PIX_FMT_YUYV422:
+    case AV_PIX_FMT_UYVY422:
+        src_linesize[0] = FFALIGN(dst->width, 16) * 2;
+        src_linesize[1] = 0;
+        src_linesize[2] = 0;
+        src_linesize[3] = 0;
+
+        src_height[0] = dst->height;
+        src_height[1] = 0;
+        src_height[2] = 0;
+        src_height[3] = 0;
+        break;
+
+    case AV_PIX_FMT_P010LE:
+        src_linesize[0] = FFALIGN(dst->width * 2, 128);
+        src_linesize[1] = FFALIGN(dst->width * 2, 128);
+        src_linesize[2] = 0;
+        src_linesize[3] = 0;
+
+        src_height[0] = dst->height;
+        src_height[1] = FFALIGN(dst->height, 2) / 2;
+        src_height[2] = 0;
+        src_height[3] = 0;
+        break;
+
+    case AV_PIX_FMT_RGBA:
+    case AV_PIX_FMT_BGRA:
+    case AV_PIX_FMT_ABGR:
+    case AV_PIX_FMT_ARGB:
+    case AV_PIX_FMT_BGR0:
+        src_linesize[0] = FFALIGN(dst->width, 16) * 4;
+        src_linesize[1] = 0;
+        src_linesize[2] = 0;
+        src_linesize[3] = 0;
+
+        src_height[0] = dst->height;
+        src_height[1] = 0;
+        src_height[2] = 0;
+        src_height[3] = 0;
+        break;
+
+    case AV_PIX_FMT_BGRP:
+        src_linesize[0] = dst->width;
+        src_linesize[1] = dst->width;
+        src_linesize[2] = dst->width;
+        src_linesize[3] = 0;
+
+        src_height[0] = dst->height;
+        src_height[1] = dst->height;
+        src_height[2] = dst->height;
+        src_height[3] = 0;
+        break;
+
+    default:
+        av_log(hwfc, AV_LOG_ERROR, "Unsupported pixel format %s\n",
+               av_get_pix_fmt_name(hwfc->sw_format));
+        return AVERROR(EINVAL);
+    }
+
+    for (i = 0; i < nb_planes; i++) {
+        dst_line = dst->data[i];
+        src_line = src->p_data[i];
+
+        for (h = 0; h < src_height[i]; h++) {
+            memcpy(dst_line, src_line,
+                   FFMIN(src_linesize[i], dst->linesize[i]));
+            dst_line += dst->linesize[i];
+            src_line += src_linesize[i];
+        }
+    }
+
+    return 0;
+}
+
+static int av_to_niframe_copy(AVHWFramesContext *hwfc, const int dst_stride[4],
+                              ni_frame_t *dst, const AVFrame *src) {
+    int src_height[4], hpad[4], vpad[4];
+    int i, j, h, nb_planes;
+    uint8_t *src_line, *dst_line, YUVsample, *sample, *dest;
+    uint16_t lastidx;
+    bool tenBit;
+
+    nb_planes = av_pix_fmt_count_planes(hwfc->sw_format);
+
+    switch (src->format) {
+    case AV_PIX_FMT_YUV420P:
+        hpad[0] = FFMAX(dst_stride[0] - src->linesize[0], 0);
+        hpad[1] = FFMAX(dst_stride[1] - src->linesize[1], 0);
+        hpad[2] = FFMAX(dst_stride[2] - src->linesize[2], 0);
+        hpad[3] = 0;
+
+        src_height[0] = src->height;
+        src_height[1] = FFALIGN(src->height, 2) / 2;
+        src_height[2] = FFALIGN(src->height, 2) / 2;
+        src_height[3] = 0;
+
+        vpad[0] = FFALIGN(src_height[0], 2) - src_height[0];
+        vpad[1] = FFALIGN(src_height[1], 2) - src_height[1];
+        vpad[2] = FFALIGN(src_height[2], 2) - src_height[2];
+        vpad[3] = 0;
+
+        tenBit = false;
+        break;
+
+    case AV_PIX_FMT_YUV420P10LE:
+        hpad[0] = FFMAX(dst_stride[0] - src->linesize[0], 0);
+        hpad[1] = FFMAX(dst_stride[1] - src->linesize[1], 0);
+        hpad[2] = FFMAX(dst_stride[2] - src->linesize[2], 0);
+        hpad[3] = 0;
+
+        src_height[0] = src->height;
+        src_height[1] = FFALIGN(src->height, 2) / 2;
+        src_height[2] = FFALIGN(src->height, 2) / 2;
+        src_height[3] = 0;
+
+        vpad[0] = FFALIGN(src_height[0], 2) - src_height[0];
+        vpad[1] = FFALIGN(src_height[1], 2) - src_height[1];
+        vpad[2] = FFALIGN(src_height[2], 2) - src_height[2];
+        vpad[3] = 0;
+
+        tenBit = true;
+        break;
+
+    case AV_PIX_FMT_NV12:
+        hpad[0] = FFMAX(dst_stride[0] - src->linesize[0], 0);
+        hpad[1] = FFMAX(dst_stride[1] - src->linesize[1], 0);
+        hpad[2] = 0;
+        hpad[3] = 0;
+
+        src_height[0] = src->height;
+        src_height[1] = FFALIGN(src->height, 2) / 2;
+        src_height[2] = 0;
+        src_height[3] = 0;
+
+        vpad[0] = FFALIGN(src_height[0], 2) - src_height[0];
+        vpad[1] = FFALIGN(src_height[1], 2) - src_height[1];
+        vpad[2] = 0;
+        vpad[3] = 0;
+
+        tenBit = false;
+        break;
+    case AV_PIX_FMT_NV16:
+        hpad[0] = 0;
+        hpad[1] = 0;
+        hpad[2] = 0;
+        hpad[3] = 0;
+
+        src_height[0] = src->height;
+        src_height[1] = src->height;
+        src_height[2] = 0;
+        src_height[3] = 0;
+
+        vpad[0] = 0;
+        vpad[1] = 0;
+        vpad[2] = 0;
+        vpad[3] = 0;
+
+        tenBit = false;
+        break;
+
+    case AV_PIX_FMT_P010LE:
+        hpad[0] = FFMAX(dst_stride[0] - src->linesize[0], 0);
+        hpad[1] = FFMAX(dst_stride[1] - src->linesize[1], 0);
+        hpad[2] = 0;
+        hpad[3] = 0;
+
+        src_height[0] = src->height;
+        src_height[1] = FFALIGN(src->height, 2) / 2;
+        src_height[2] = 0;
+        src_height[3] = 0;
+
+        vpad[0] = FFALIGN(src_height[0], 2) - src_height[0];
+        vpad[1] = FFALIGN(src_height[1], 2) - src_height[1];
+        vpad[2] = 0;
+        vpad[3] = 0;
+
+        tenBit = true;
+        break;
+
+    case AV_PIX_FMT_RGBA:
+    case AV_PIX_FMT_BGRA:
+    case AV_PIX_FMT_ABGR:
+    case AV_PIX_FMT_ARGB:
+    case AV_PIX_FMT_BGR0:
+        hpad[0] = FFMAX(dst_stride[0] - src->linesize[0], 0);
+        hpad[1] = 0;
+        hpad[2] = 0;
+        hpad[3] = 0;
+
+        src_height[0] = src->height;
+        src_height[1] = 0;
+        src_height[2] = 0;
+        src_height[3] = 0;
+
+        vpad[0] = 0;
+        vpad[1] = 0;
+        vpad[2] = 0;
+        vpad[3] = 0;
+
+        tenBit = false;
+        break;
+
+    case AV_PIX_FMT_BGRP:
+        hpad[0] = 0;
+        hpad[1] = 0;
+        hpad[2] = 0;
+        hpad[3] = 0;
+
+        src_height[0] = src->height;
+        src_height[1] = src->height;
+        src_height[2] = src->height;
+        src_height[3] = 0;
+
+        vpad[0] = 0;
+        vpad[1] = 0;
+        vpad[2] = 0;
+        vpad[3] = 0;
+
+        tenBit = false;
+        break;
+
+    case AV_PIX_FMT_YUYV422:
+    case AV_PIX_FMT_UYVY422:
+        hpad[0] = FFMAX(dst_stride[0] - src->linesize[0], 0);
+        hpad[1] = 0;
+        hpad[2] = 0;
+        hpad[3] = 0;
+
+        src_height[0] = src->height;
+        src_height[1] = 0;
+        src_height[2] = 0;
+        src_height[3] = 0;
+
+        vpad[0] = 0;
+        vpad[1] = 0;
+        vpad[2] = 0;
+        vpad[3] = 0;
+
+        tenBit = false;
+        break;
+
+    default:
+        av_log(hwfc, AV_LOG_ERROR, "Pixel format %s not supported\n",
+               av_get_pix_fmt_name(src->format));
+        break;
+    }
+
+    for (i = 0; i < nb_planes; i++) {
+        dst_line = dst->p_data[i];
+        src_line = src->data[i];
+
+        for (h = 0; h < src_height[i]; h++) {
+            memcpy(dst_line, src_line, FFMIN(src->linesize[i], dst_stride[i]));
+
+            if (hpad[i]) {
+                lastidx = src->linesize[i];
+
+                if (tenBit) {
+                    sample = &src_line[lastidx - 2];
+                    dest   = &dst_line[lastidx];
+
+                    /* two bytes per sample */
+                    for (j = 0; j < hpad[i] / 2; j++) {
+                        memcpy(dest, sample, 2);
+                        dest += 2;
+                    }
+                } else {
+                    YUVsample = dst_line[lastidx - 1];
+                    memset(&dst_line[lastidx], YUVsample, hpad[i]);
+                }
+            }
+
+            src_line += src->linesize[i];
+            dst_line += dst_stride[i];
+        }
+
+        /* Extend the height by cloning the last line */
+        src_line = dst_line - dst_stride[i];
+        for (h = 0; h < vpad[i]; h++) {
+            memcpy(dst_line, src_line, dst_stride[i]);
+            dst_line += dst_stride[i];
+        }
+    }
+
+    return 0;
+}
+
+static int ni_hwdl_frame(AVHWFramesContext *hwfc, AVFrame *dst,
+                         const AVFrame *src) {
+    NIFramesContext *ctx = hwfc->internal->priv;
+    ni_session_data_io_t session_io_data;
+    ni_session_data_io_t *p_session_data = &session_io_data;
+    niFrameSurface1_t *src_surf          = (niFrameSurface1_t *)src->data[3];
+    int ret;
+    int pixel_format;
+
+    memset(&session_io_data, 0, sizeof(ni_session_data_io_t));
+
+    av_log(hwfc, AV_LOG_VERBOSE,
+           "%s handle %d trace ui16FrameIdx = [%d] SID %d\n", __func__,
+           src_surf->device_handle, src_surf->ui16FrameIdx,
+           src_surf->ui16session_ID);
+
+    av_log(hwfc, AV_LOG_DEBUG, "%s hwdl processed h/w = %d/%d\n", __func__,
+           src->height, src->width);
+
+    switch (hwfc->sw_format) {
+    case AV_PIX_FMT_YUV420P:
+        pixel_format = NI_PIX_FMT_YUV420P;
+        break;
+    case AV_PIX_FMT_YUV420P10LE:
+        pixel_format = NI_PIX_FMT_YUV420P10LE;
+        break;
+    case AV_PIX_FMT_NV12:
+        pixel_format = NI_PIX_FMT_NV12;
+        break;
+    case AV_PIX_FMT_NV16:
+        pixel_format = NI_PIX_FMT_NV16;
+        break;
+    case AV_PIX_FMT_YUYV422:
+        pixel_format = NI_PIX_FMT_YUYV422;
+        break;
+    case AV_PIX_FMT_UYVY422:
+        pixel_format = NI_PIX_FMT_UYVY422;
+        break;
+    case AV_PIX_FMT_P010LE:
+        pixel_format = NI_PIX_FMT_P010LE;
+        break;
+    case AV_PIX_FMT_RGBA:
+        pixel_format = NI_PIX_FMT_RGBA;
+        break;
+    case AV_PIX_FMT_BGRA:
+        pixel_format = NI_PIX_FMT_BGRA;
+        break;
+    case AV_PIX_FMT_ABGR:
+        pixel_format = NI_PIX_FMT_ABGR;
+        break;
+    case AV_PIX_FMT_ARGB:
+        pixel_format = NI_PIX_FMT_ARGB;
+        break;
+    case AV_PIX_FMT_BGR0:
+        pixel_format = NI_PIX_FMT_BGR0;
+        break;
+    case AV_PIX_FMT_BGRP:
+        pixel_format = NI_PIX_FMT_BGRP;
+        break;
+    default:
+        av_log(hwfc, AV_LOG_ERROR, "Pixel format not supported.\n");
+        return AVERROR(EINVAL);
+    }
+
+    ret = ni_frame_buffer_alloc_dl(&(p_session_data->data.frame), src->width,
+                                   src->height, pixel_format);
+    if (ret != NI_RETCODE_SUCCESS) {
+        av_log(hwfc, AV_LOG_ERROR, "%s Cannot allocate ni_frame\n", __func__);
+        return AVERROR(ENOMEM);
+    }
+
+    ctx->api_ctx.is_auto_dl = false;
+    ret = ni_device_session_hwdl(&ctx->api_ctx, p_session_data, src_surf);
+    if (ret <= 0) {
+        av_log(hwfc, AV_LOG_DEBUG, "%s failed to retrieve frame\n", __func__);
+        ni_frame_buffer_free(&p_session_data->data.frame);
+        return AVERROR_EXTERNAL;
+    }
+
+    ret = ni_to_avframe_copy(hwfc, dst, &p_session_data->data.frame);
+    if (ret < 0) {
+        av_log(hwfc, AV_LOG_ERROR, "Can't copy frame %d\n", ret);
+        ni_frame_buffer_free(&p_session_data->data.frame);
+        return ret;
+    }
+
+    dst->format = hwfc->sw_format;
+
+    av_frame_copy_props(dst, src);
+    ni_frame_buffer_free(&p_session_data->data.frame);
+
+    return 0;
+}
+
+static int ni_hwup_frame(AVHWFramesContext *hwfc, AVFrame *dst, const AVFrame *src)
+{
+    NIFramesContext *ctx = hwfc->internal->priv;
+    ni_session_data_io_t *p_src_session_data;
+    niFrameSurface1_t *dst_surf;
+    AVFrame *src_frame;
+    int ret = 0;
+    int dst_stride[4];
+    int pixel_format;
+    bool isSemiPlanar;
+
+    dst_surf = (niFrameSurface1_t *)dst->data[3];
+
+    if (dst_surf == NULL || dst->hw_frames_ctx == NULL) {
+        av_log(hwfc, AV_LOG_ERROR, "Invalid hw frame\n");
+        return AVERROR(EINVAL);
+    }
+
+    // memset(&ctx->src_session_io_data, 0, sizeof(ni_session_data_io_t));
+    p_src_session_data = &ctx->src_session_io_data;
+
+    switch (src->format) {
+    /* 8-bit YUV420 planar */
+    case AV_PIX_FMT_YUV420P:
+        dst_stride[0] = FFALIGN(src->width, 128);
+        dst_stride[1] = FFALIGN((src->width / 2), 128);
+        dst_stride[2] = dst_stride[1];
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_YUV420P;
+        isSemiPlanar = false;
+        break;
+
+    /* 10-bit YUV420 planar, little-endian, least significant bits */
+    case AV_PIX_FMT_YUV420P10LE:
+        dst_stride[0] = FFALIGN(src->width * 2, 128);
+        dst_stride[1] = FFALIGN(src->width, 128);
+        dst_stride[2] = dst_stride[1];
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_YUV420P10LE;
+        isSemiPlanar = false;
+        break;
+
+    /* 8-bit YUV420 semi-planar */
+    case AV_PIX_FMT_NV12:
+        dst_stride[0] = FFALIGN(src->width, 128);
+        dst_stride[1] = dst_stride[0];
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_NV12;
+        isSemiPlanar = true;
+        break;
+
+    /* 8-bit yuv422 semi-planar */
+    case AV_PIX_FMT_NV16:
+        dst_stride[0] = src->width;
+        dst_stride[1] = dst_stride[0];
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_NV16;
+        isSemiPlanar = false;
+        break;
+
+    /*8-bit yuv422 planar */
+    case AV_PIX_FMT_YUYV422:
+        dst_stride[0] = FFALIGN(src->width, 16) * 2;
+        dst_stride[1] = 0;
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_YUYV422;
+        isSemiPlanar = false;
+        break;
+
+    case AV_PIX_FMT_UYVY422:
+        dst_stride[0] = FFALIGN(src->width, 16) * 2;
+        dst_stride[1] = 0;
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_UYVY422;
+        isSemiPlanar = false;
+        break;
+
+    /* 10-bit YUV420 semi-planar, little endian, most significant bits */
+    case AV_PIX_FMT_P010LE:
+        dst_stride[0] = FFALIGN(src->width * 2, 128);
+        dst_stride[1] = dst_stride[0];
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_P010LE;
+        isSemiPlanar = true;
+        break;
+
+    /* 32-bit RGBA packed */
+    case AV_PIX_FMT_RGBA:
+        /* RGBA for the scaler has a 16-byte width/64-byte stride alignment */
+        dst_stride[0] = FFALIGN(src->width, 16) * 4;
+        dst_stride[1] = 0;
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_RGBA;
+        isSemiPlanar = false;
+        break;
+
+    case AV_PIX_FMT_BGRA:
+        dst_stride[0] = FFALIGN(src->width, 16) * 4;
+        dst_stride[1] = 0;
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_BGRA;
+        isSemiPlanar = false;
+        break;
+
+    case AV_PIX_FMT_ABGR:
+        dst_stride[0] = FFALIGN(src->width, 16) * 4;
+        dst_stride[1] = 0;
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_ABGR;
+        isSemiPlanar = false;
+        break;
+
+    case AV_PIX_FMT_ARGB:
+        dst_stride[0] = FFALIGN(src->width, 16) * 4;
+        dst_stride[1] = 0;
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_ARGB;
+        isSemiPlanar = false;
+        break;
+
+    case AV_PIX_FMT_BGR0:
+        dst_stride[0] = FFALIGN(src->width, 16) * 4;
+        dst_stride[1] = 0;
+        dst_stride[2] = 0;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_BGR0;
+        isSemiPlanar = false;
+        break;
+
+    /* 24-bit BGR planar not supported */
+    /*
+    case AV_PIX_FMT_BGRP:
+        dst_stride[0] = src->width;
+        dst_stride[1] = src->width;
+        dst_stride[2] = src->width;
+        dst_stride[3] = 0;
+
+        pixel_format = NI_PIX_FMT_BGRP;
+        isSemiPlanar = false;
+        break;
+    */
+    default:
+        av_log(hwfc, AV_LOG_ERROR, "Pixel format %s not supported by device %s\n",
+               av_get_pix_fmt_name(src->format), hwfc->internal->hw_type->name);
+        return AVERROR(EINVAL);
+    }
+
+    if (!p_src_session_data->data.frame.p_buffer) {
+        // allocate only once per upload Session when we have frame info
+        p_src_session_data->data.frame.extra_data_len =
+            NI_APP_ENC_FRAME_META_DATA_SIZE;
+
+        ret = ni_frame_buffer_alloc_pixfmt(&p_src_session_data->data.frame,
+                                           pixel_format, src->width,
+                                           src->height, dst_stride,
+                                           1, // force to av_codec_id_h264 for max compat
+                                           (int)p_src_session_data->data.frame.extra_data_len);
+        if (ret < 0) {
+            av_log(hwfc, AV_LOG_ERROR, "Cannot allocate ni_frame %d\n", ret);
+            return ret;
+        }
+    }
+
+    ret = av_to_niframe_copy(hwfc, dst_stride, &p_src_session_data->data.frame, src);
+    if (ret < 0) {
+        av_log(hwfc, AV_LOG_ERROR, "%s can't copy frame\n", __func__);
+        return AVERROR(EINVAL);
+    }
+
+    ret = ni_device_session_hwup(&ctx->api_ctx, p_src_session_data, dst_surf);
+    if (ret < 0) {
+        av_log(hwfc, AV_LOG_ERROR, "%s failed to upload frame %d\n",
+               __func__, ret);
+        return AVERROR_EXTERNAL;
+    }
+
+    dst_surf->ui16width = ctx->split_ctx.w[0] = src->width;
+    dst_surf->ui16height = ctx->split_ctx.h[0] = src->height;
+    dst_surf->ui32nodeAddress = 0; // always 0 offset for upload
+    dst_surf->encoding_type = isSemiPlanar ? NI_PIXEL_PLANAR_FORMAT_SEMIPLANAR
+                                           : NI_PIXEL_PLANAR_FORMAT_PLANAR;
+
+    av_log(hwfc, AV_LOG_VERBOSE, "%s trace ui16FrameIdx = [%u] hdl %d SID%d\n",
+           __func__, dst_surf->ui16FrameIdx, dst_surf->device_handle,
+           dst_surf->ui16session_ID);
+
+    // Update frames context
+    ctx->split_ctx.f[0] = (int)dst_surf->encoding_type;
+
+    /* Remove the const qualifier */
+    src_frame         = (AVFrame *)src;
+    // NOLINTNEXTLINE(clang-diagnostic-int-to-void-pointer-cast)
+    src_frame->opaque = (void *)ctx->api_ctx.hw_id;
+
+    av_frame_copy_props(dst, src); // should get the metadata right
+    av_log(hwfc, AV_LOG_DEBUG, "%s Upload frame w/h %d/%d crop r/b %lu/%lu\n",
+           __func__, dst->width, dst->height, dst->crop_right, dst->crop_bottom);
+
+    return ret;
+}
+
+static int ni_transfer_data_to(AVHWFramesContext *hwfc, AVFrame *dst,
+                               const AVFrame *src)
+{
+    int err;
+    niFrameSurface1_t *dst_surf;
+
+    if (src->width > hwfc->width || src->height > hwfc->height) {
+        return AVERROR(EINVAL);
+    }
+
+    /* should check against MAX frame size */
+    err = ni_hwup_frame(hwfc, dst, src);
+    if (err) {
+        return err;
+    }
+
+    dst_surf = (niFrameSurface1_t *)(dst->data[3]);
+
+    av_log(hwfc, AV_LOG_VERBOSE,
+           "hwcontext.c:ni_hwup_frame() dst_surf FID %d %d\n",
+           dst_surf->ui16FrameIdx, dst_surf->ui16session_ID);
+
+    return 0;
+}
+
+static int ni_transfer_data_from(AVHWFramesContext *hwfc, AVFrame *dst,
+                                 const AVFrame *src)
+{
+    if (dst->width > hwfc->width || dst->height > hwfc->height) {
+        av_log(hwfc, AV_LOG_ERROR, "Invalid frame dimensions\n");
+        return AVERROR(EINVAL);
+    }
+
+    return ni_hwdl_frame(hwfc, dst, src);
+}
+
+const HWContextType ff_hwcontext_type_ni_quadra = {
+    // QUADRA
+    .type = AV_HWDEVICE_TYPE_NI_QUADRA,
+    .name = "NI_QUADRA",
+
+    .device_hwctx_size = sizeof(AVNIDeviceContext),
+    .frames_hwctx_size = sizeof(AVNIFramesContext),
+    .frames_priv_size  = sizeof(NIFramesContext),
+
+    .device_create = ni_device_create,
+    .device_uninit = ni_device_uninit,
+
+    .frames_get_constraints = ni_frames_get_constraints,
+
+    .frames_init   = ni_frames_init,
+    .frames_uninit = ni_frames_uninit,
+
+    .frames_get_buffer = ni_get_buffer,
+
+    .transfer_get_formats = ni_transfer_get_formats,
+    .transfer_data_to     = ni_transfer_data_to,
+    .transfer_data_from   = ni_transfer_data_from,
+
+    .pix_fmts =
+        (const enum AVPixelFormat[]){AV_PIX_FMT_NI_QUAD, AV_PIX_FMT_NONE},
+};
diff --git a/libavutil/hwcontext_ni_quad.h b/libavutil/hwcontext_ni_quad.h
new file mode 100644
index 0000000000..29ad956c8c
--- /dev/null
+++ b/libavutil/hwcontext_ni_quad.h
@@ -0,0 +1,93 @@
+/*
+* This file is part of FFmpeg.
+*
+* FFmpeg is free software; you can redistribute it and/or
+* modify it under the terms of the GNU Lesser General Public
+* License as published by the Free Software Foundation; either
+* version 2.1 of the License, or (at your option) any later version.
+*
+* FFmpeg is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+* Lesser General Public License for more details.
+*
+* You should have received a copy of the GNU Lesser General Public
+* License along with FFmpeg; if not, write to the Free Software
+* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+*/
+
+#ifndef AVUTIL_HWCONTEXT_NI_QUAD_H
+#define AVUTIL_HWCONTEXT_NI_QUAD_H
+
+#include "hwcontext.h"
+#include <ni_device_api.h>
+#include <ni_rsrc_api.h>
+#include <ni_util.h>
+
+enum
+{
+  NI_MEMTYPE_VIDEO_MEMORY_NONE,
+  NI_MEMTYPE_VIDEO_MEMORY_DECODER_TARGET,
+  NI_MEMTYPE_VIDEO_MEMORY_HWUPLOAD_TARGET,
+};
+
+typedef enum _ni_filter_poolsize_code {
+    NI_DECODER_ID = -1,
+    NI_SCALE_ID   = -2,
+    NI_PAD_ID     = -3,
+    NI_CROP_ID    = -4,
+    NI_OVERLAY_ID = -5,
+    NI_ROI_ID     = -6,
+    NI_BG_ID      = -7,
+    NI_STACK_ID   = -8,
+    NI_ROTATE_ID  = -9,
+} ni_filter_poolsize_code;
+
+typedef struct NIFramesContext {
+  niFrameSurface1_t *surfaces_internal;
+  int                nb_surfaces_used;
+  niFrameSurface1_t **surface_ptrs;
+  ni_session_context_t api_ctx;//for down/uploading frames
+  ni_session_data_io_t src_session_io_data; // for upload frame to be sent up
+  // int pc_height, pc_width, pc_crop_bottom, pc_crop_right; //precropped values
+  ni_split_context_t split_ctx;
+  ni_device_handle_t suspended_device_handle;
+  int                uploader_device_id; //same one passed to libxcoder session open
+} NIFramesContext;
+
+/**
+* This struct is allocated as AVHWDeviceContext.hwctx
+*/
+typedef struct AVNIDeviceContext {
+    int uploader_ID;
+
+    ni_device_handle_t cards[NI_MAX_DEVICE_CNT];
+} AVNIDeviceContext;
+
+/**
+* This struct is allocated as AVHWFramesContext.hwctx
+*/
+typedef struct AVNIFramesContext {
+  niFrameSurface1_t *surfaces;
+  int            nb_surfaces;
+  int            keep_alive_timeout;
+  int            frame_type;
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+  int            dev_dec_idx;                /* index of the decoder on the xcoder card */
+#endif
+} AVNIFramesContext;
+
+static inline int ni_get_cardno(const AVFrame *frame) {
+    int cardno;
+
+#ifdef NI_DEC_GSTREAMER_SUPPORT
+    AVHWFramesContext *pAVHFWCtx = (AVHWFramesContext *) frame->hw_frames_ctx->data;
+    AVNIFramesContext *pAVNIFramesCtx = (AVNIFramesContext *) pAVHFWCtx->hwctx;
+    cardno            =  pAVNIFramesCtx->dev_dec_idx;
+#else
+    cardno      = (int)frame->opaque;
+#endif
+    return cardno;
+}
+
+#endif /* AVUTIL_HWCONTEXT_NI_H */
diff --git a/libavutil/mem.c b/libavutil/mem.c
index 18aff5291f..aeae8af0e6 100644
--- a/libavutil/mem.c
+++ b/libavutil/mem.c
@@ -33,6 +33,7 @@
 #include <stdlib.h>
 #include <stdatomic.h>
 #include <string.h>
+#include <unistd.h> // NETINT: in order to use sysconf()
 #if HAVE_MALLOC_H
 #include <malloc.h>
 #endif
@@ -63,6 +64,12 @@ void  free(void *ptr);
 #endif /* MALLOC_PREFIX */
 
 #define ALIGN (HAVE_AVX512 ? 64 : (HAVE_AVX ? 32 : 16))
+#ifndef _WIN32
+#define NETINT_ALIGN sysconf(_SC_PAGESIZE) // NETINT: we need page aligned address in order to reduce cpu usage in the zero copy
+#else
+#define NETINT_ALIGN 4096
+#endif
+#define SIZE_OPT 1024*1024 //   NETINT: threadhold for page aligned memory allocation
 
 /* NOTE: if you want to override these functions with your own
  * implementations (not recommended) you have to link libav* as
@@ -93,6 +100,91 @@ static int size_mult(size_t a, size_t b, size_t *r)
     return 0;
 }
 
+void *av_malloc_quadra(size_t size)
+{
+    void *ptr = NULL;
+
+    /* let's disallow possibly ambiguous cases */
+    if (size > (max_alloc_size - 32))
+        return NULL;
+    if (size > SIZE_OPT) {   // NETINT: use page aligned address to reduce cpu usage on netint hardware when allocate frame buffer
+#if HAVE_POSIX_MEMALIGN
+        if (size) //OS X on SDK 10.6 has a broken posix_memalign implementation
+            if (posix_memalign(&ptr, NETINT_ALIGN, size))
+                ptr = NULL;
+#elif HAVE_ALIGNED_MALLOC
+        ptr = _aligned_malloc(size, NETINT_ALIGN);
+#elif HAVE_MEMALIGN
+#ifndef __DJGPP__
+        ptr = memalign(NETINT_ALIGN, size);
+#else
+        ptr = memalign(size, NETINT_ALIGN);
+#endif
+#else
+        ptr = malloc(size);
+#endif
+        if (!ptr && !size) {
+            size = 1;
+            ptr = av_malloc(1);
+        }
+#if CONFIG_MEMORY_POISONING
+        if (ptr)
+            memset(ptr, FF_MEMORY_POISON, size);
+#endif
+
+    }
+    else {
+#if HAVE_POSIX_MEMALIGN
+        if (size) //OS X on SDK 10.6 has a broken posix_memalign implementation
+            if (posix_memalign(&ptr, ALIGN, size))
+                ptr = NULL;
+#elif HAVE_ALIGNED_MALLOC
+        ptr = _aligned_malloc(size, ALIGN);
+#elif HAVE_MEMALIGN
+#ifndef __DJGPP__
+        ptr = memalign(ALIGN, size);
+#else
+        ptr = memalign(size, ALIGN);
+#endif
+        /* Why 64?
+        * Indeed, we should align it:
+        *   on  4 for 386
+        *   on 16 for 486
+        *   on 32 for 586, PPro - K6-III
+        *   on 64 for K7 (maybe for P3 too).
+        * Because L1 and L2 caches are aligned on those values.
+        * But I don't want to code such logic here!
+        */
+        /* Why 32?
+        * For AVX ASM. SSE / NEON needs only 16.
+        * Why not larger? Because I did not see a difference in benchmarks ...
+        */
+        /* benchmarks with P3
+        * memalign(64) + 1          3071, 3051, 3032
+        * memalign(64) + 2          3051, 3032, 3041
+        * memalign(64) + 4          2911, 2896, 2915
+        * memalign(64) + 8          2545, 2554, 2550
+        * memalign(64) + 16         2543, 2572, 2563
+        * memalign(64) + 32         2546, 2545, 2571
+        * memalign(64) + 64         2570, 2533, 2558
+        *
+        * BTW, malloc seems to do 8-byte alignment by default here.
+        */
+#else
+        ptr = malloc(size);
+#endif
+        if (!ptr && !size) {
+            size = 1;
+            ptr = av_malloc(1);
+        }
+#if CONFIG_MEMORY_POISONING
+        if (ptr)
+            memset(ptr, FF_MEMORY_POISON, size);
+#endif
+    }
+    return ptr;
+}
+
 void *av_malloc(size_t size)
 {
     void *ptr = NULL;
diff --git a/libavutil/mem.h b/libavutil/mem.h
index d91174196c..0efdbe8219 100644
--- a/libavutil/mem.h
+++ b/libavutil/mem.h
@@ -189,6 +189,15 @@
  * @{
  */
 
+ /**
+ * Allocate a memory block with alignment suitable 4k aligned access
+ *
+ * @param size Size in bytes for the memory block to be allocated
+ * @return Pointer to the allocated block, or `NULL` if the block cannot
+ *         be allocated
+ */
+void *av_malloc_quadra(size_t size) av_malloc_attrib av_alloc_size(1);
+
 /**
  * Allocate a memory block with alignment suitable for all memory accesses
  * (including vectors if available on the CPU).
diff --git a/libavutil/pixdesc.c b/libavutil/pixdesc.c
index 6e57a82cb6..89cf6d70ff 100644
--- a/libavutil/pixdesc.c
+++ b/libavutil/pixdesc.c
@@ -2085,6 +2085,16 @@ static const AVPixFmtDescriptor av_pix_fmt_descriptors[AV_PIX_FMT_NB] = {
         .name = "qsv",
         .flags = AV_PIX_FMT_FLAG_HWACCEL,
     },
+    // NETINT: AV_PIX_FMT_NI_LOGAN pixel format for Logan HW frame.
+    [AV_PIX_FMT_NI_LOGAN] = {
+        .name = "ni_logan",
+        .flags = AV_PIX_FMT_FLAG_HWACCEL,
+    },
+    // NETINT: AV_PIX_FMT_NI_QUAD pixel format for Quadra HW frame
+    [AV_PIX_FMT_NI_QUAD] = {
+        .name = "ni_quadra",
+        .flags = AV_PIX_FMT_FLAG_HWACCEL,
+    },
     [AV_PIX_FMT_MEDIACODEC] = {
         .name = "mediacodec",
         .flags = AV_PIX_FMT_FLAG_HWACCEL,
@@ -2491,6 +2501,19 @@ static const AVPixFmtDescriptor av_pix_fmt_descriptors[AV_PIX_FMT_NB] = {
         },
         .flags = AV_PIX_FMT_FLAG_PLANAR,
     },
+    // NETINT: Add AV_PIX_FMT_BGRP for avfilter with BG and ROI
+    [AV_PIX_FMT_BGRP] = {
+        .name = "bgrp",
+        .nb_components = 3,
+        .log2_chroma_w = 0,
+        .log2_chroma_h = 0,
+        .comp = {
+            { 2, 1, 0, 0, 8, 0, 7, 1 },        /* R */
+            { 1, 1, 0, 0, 8, 0, 7, 1 },        /* G */
+            { 0, 1, 0, 0, 8, 0, 7, 1 },        /* B */
+        },
+        .flags = AV_PIX_FMT_FLAG_PLANAR | AV_PIX_FMT_FLAG_RGB,
+    },
 };
 
 static const char * const color_range_names[] = {
diff --git a/libavutil/pixfmt.h b/libavutil/pixfmt.h
index 2d3927cc3f..5a0175cf6c 100644
--- a/libavutil/pixfmt.h
+++ b/libavutil/pixfmt.h
@@ -210,6 +210,19 @@ enum AVPixelFormat {
      *  mfxFrameSurface1 structure.
      */
     AV_PIX_FMT_QSV,
+
+    /**
+     * HW acceleration through NI, data[3] contains a pointer to the
+     * ni_logan_hwframe_surface_t structure, for Netint Logan.
+     */
+    AV_PIX_FMT_NI_LOGAN,
+
+    /**
+     * HW acceleration through NI, data[3] contains a pointer to the
+     * niFrameSurface1_t structure, for Netint Quadra.
+     */
+    AV_PIX_FMT_NI_QUAD,
+
     /**
      * HW acceleration though MMAL, data[3] contains a pointer to the
      * MMAL_BUFFER_HEADER_T structure.
@@ -350,6 +363,9 @@ enum AVPixelFormat {
     AV_PIX_FMT_Y210BE,    ///< packed YUV 4:2:2 like YUYV422, 20bpp, data in the high bits, big-endian
     AV_PIX_FMT_Y210LE,    ///< packed YUV 4:2:2 like YUYV422, 20bpp, data in the high bits, little-endian
 
+    // NETINT: Add AV_PIX_FMT_BGRP for avfilter with BG and ROI
+    AV_PIX_FMT_BGRP, ///< planar BGR 4:4:4 24bpp
+
     AV_PIX_FMT_X2RGB10LE, ///< packed RGB 10:10:10, 30bpp, (msb)2X 10R 10G 10B(lsb), little-endian, X=unused/undefined
     AV_PIX_FMT_X2RGB10BE, ///< packed RGB 10:10:10, 30bpp, (msb)2X 10R 10G 10B(lsb), big-endian, X=unused/undefined
     AV_PIX_FMT_X2BGR10LE, ///< packed BGR 10:10:10, 30bpp, (msb)2X 10B 10G 10R(lsb), little-endian, X=unused/undefined
diff --git a/mingw_package_ffmpeg.sh b/mingw_package_ffmpeg.sh
new file mode 100644
index 0000000000..ba16cd2e7a
--- /dev/null
+++ b/mingw_package_ffmpeg.sh
@@ -0,0 +1,101 @@
+#!/bin/bash
+
+# This is to package all the bin files.
+# ffmpeg.exe, ffplay.exe, ffprobe.exe and their releated dll.
+
+XCODER_WORKDIR="."
+XCODER_BIN_DIR="bin"
+XCODER_BIN="${XCODER_WORKDIR}/${XCODER_BIN_DIR}"
+XCODER_LOCAL_PATH="/usr/local"
+
+PLATFORM=$(uname -s)
+echo ${PLATFORM}
+
+if [[ $PLATFORM =~ "MINGW32" ]]; then
+    MIGNW_PATH="/mingw32/bin"
+else
+    MIGNW_PATH="/mingw64/bin"
+fi
+
+# Create bin folder for compilation
+if [ ! -d "${XCODER_WORKDIR}/${XCODER_BIN_DIR}" ]; then
+    mkdir ${XCODER_BIN}
+fi
+
+# Remove the old files
+rm -rf  ${XCODER_BIN}/*
+
+echo "Package the exe and libraries to folder ${XCODER_BIN}"
+
+cp -v ${XCODER_WORKDIR}/ffmpeg.exe ${XCODER_BIN}
+
+if $enable_ffplay; then
+    cp -v ${XCODER_WORKDIR}/ffplay.exe ${XCODER_BIN}
+fi
+
+if $enable_ffprobe; then
+    cp -v ${XCODER_WORKDIR}/ffprobe.exe ${XCODER_BIN}
+fi
+
+if $enable_x264; then
+    cp -v ${MIGNW_PATH}/libx264-161.dll ${XCODER_BIN}
+fi
+
+if $enable_x265; then
+    cp -v ${MIGNW_PATH}/libx265.dll ${XCODER_BIN}
+    cp -v ${MIGNW_PATH}/libstdc++-6.dll ${XCODER_BIN}
+    cp -v ${MIGNW_PATH}/libgcc_s_seh-1.dll ${XCODER_BIN}
+fi
+
+if $enable_ffnvcodec; then
+    echo "Warning: don't support to package nvcodec now!!"
+    exit 1
+fi
+
+if $enable_vmaf; then
+    echo "Warning: don't support to package vmaf now!!"
+    exit 1
+fi
+
+cp -v ${MIGNW_PATH}/libbz2-1.dll ${MIGNW_PATH}/libiconv-2.dll ${MIGNW_PATH}/liblzma-5.dll ${MIGNW_PATH}/libwinpthread-1.dll ${MIGNW_PATH}/zlib1.dll ${MIGNW_PATH}/SDL2.dll ${XCODER_BIN}
+
+if [[ $PLATFORM =~ "MINGW32" ]]; then
+    cp -v ${MIGNW_PATH}/libgcc_s_dw2-1.dll ${XCODER_BIN}
+fi
+
+if $enable_shared; then
+    SUBFIX="dll"
+    cp -v ${XCODER_WORKDIR}/libavcodec/*avcodec-*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavdevice/*avdevice-*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavfilter/*avfilter-*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavformat/*avformat-*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavutil/*avutil-*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libpostproc/*postproc-*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libswresample/*swresample-*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libswscale/*swscale-*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_LOCAL_PATH}/bin/libxcoder.${SUBFIX} ${XCODER_BIN}
+
+    SUBFIX="lib"
+    cp -v ${XCODER_WORKDIR}/libavcodec/*avcodec*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavdevice/*avdevice*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavfilter/*avfilter*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavformat/*avformat*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavutil/*avutil*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libpostproc/*postproc*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libswresample/*swresample*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libswscale/*swscale*.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_LOCAL_PATH}/bin/libxcoder.${SUBFIX} ${XCODER_BIN}
+else
+    SUBFIX="a"
+    cp -v ${XCODER_WORKDIR}/libavcodec/*avcodec.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavdevice/*avdevice.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavfilter/*avfilter.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavformat/*avformat.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libavutil/*avutil.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libpostproc/*postproc.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libswresample/*swresample.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_WORKDIR}/libswscale/*swscale.${SUBFIX} ${XCODER_BIN}
+    cp -v ${XCODER_LOCAL_PATH}/lib/libxcoder.${SUBFIX} ${XCODER_BIN}
+fi
+
+exit 0
diff --git a/run_ffmpeg_logan.sh b/run_ffmpeg_logan.sh
new file mode 100644
index 0000000000..d8af226e45
--- /dev/null
+++ b/run_ffmpeg_logan.sh
@@ -0,0 +1,151 @@
+#!/bin/bash
+# Check Ni xcoder basic functions
+
+# generate a YUV file of 1280x720p_Basketball.264 if needed
+function gen_yuv_file_if_needed() {
+    if [ ! -f "../libxcoder_logan/test/1280x720p_Basketball.yuv" ]; then
+        ./ffmpeg -nostdin -vsync 0 -y -i ../libxcoder_logan/test/1280x720p_Basketball.264 -c:v rawvideo ../libxcoder_logan/test/1280x720p_Basketball.yuv &> /dev/null
+        if [[ $? != 0 ]]; then
+            echo -e "\e[31mFAIL\e[0m: cannot generate ../libxcoder_logan/test/1280x720p_Basketball.yuv from ../libxcoder_logan/test/1280x720p_Basketball.264"
+        fi
+    fi
+}
+
+#for some clis output is linked to the root directory
+declare -a Outputs=("output_5.yuv" "output_6.yuv" "output_7.h264" "output_8.h265" "output_9.h265")
+for i in "${Outputs[@]}"
+do
+    rm -f $i
+done
+
+while true;do
+options=("check pci device" "check nvme list" "rsrc_init" "ni_rsrc_mon" "test 264 decoder" "test 265 decoder" "test 264 encoder" "test 265 encoder" "test 264->265 transcoder" "Quit")
+echo -e "\e[31mChoose an option:\e[0m"
+select opt in "${options[@]}"
+do
+    case $opt in
+        "check pci device")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            lspci -d 1d82:
+            echo
+            break
+        ;;
+        "check nvme list")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            sudo nvme list
+            echo
+            break
+        ;;
+        "rsrc_init")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            ../libxcoder_logan/bin/init_rsrc_logan
+            echo
+            break
+        ;;
+        "ni_rsrc_mon")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            ../libxcoder_logan/bin/ni_rsrc_mon_logan
+            echo
+            break
+        ;;
+        "test 264 decoder")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            ##### Decoding H.264 to YUV with NI XCoder (full log) #####
+            cmd="./ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h264_ni_logan_dec -i ../libxcoder_logan/test/1280x720p_Basketball.264 -c:v rawvideo output_5.yuv"
+            echo $cmd
+            $cmd 2>&1 | tee ffmpeg_5.log
+            echo -e "\e[31mComplete! output_5.yuv has been generated.\e[0m"
+            CHECKSUM="be2e62fc528c61a01ac44eae5518e13a"
+            HASH=`md5sum output_5.yuv`
+            if [[ ${HASH%% *} == $CHECKSUM ]]; then
+                echo -e "\e[31mPASS: output_5.yuv matches checksum.\e[0m"
+            else
+                echo -e "\e[31mFAIL: output_5.yuv does not match checksum.\e[0m"
+            fi
+            echo
+            break
+        ;;
+        "test 265 decoder")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            ##### Decoding H.265 to YUV with NI XCoder (full log) #####
+            cmd="./ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h265_ni_logan_dec -i ../libxcoder_logan/test/akiyo_352x288p25.265 -c:v rawvideo output_6.yuv"
+            echo $cmd
+            $cmd 2>&1 | tee ffmpeg_6.log
+            echo -e "\e[31mComplete! output_6.yuv has been generated.\e[0m"
+            CHECKSUM="f5a29fd3fd2581844848519bafd7370d"
+            HASH=`md5sum output_6.yuv`
+            if [[ ${HASH%% *} == $CHECKSUM ]]; then
+                echo -e "\e[31mPASS: output_6.yuv matches checksum.\e[0m"
+            else
+                echo -e "\e[31mFAIL: output_6.yuv does not match checksum.\e[0m"
+            fi
+            echo
+            break
+        ;;
+        "test 264 encoder")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            gen_yuv_file_if_needed
+            ##### Encoding YUV to H.264 with NI XCoder (full log) #####
+            cmd="./ffmpeg -y -hide_banner -nostdin -f rawvideo -pix_fmt yuv420p -s:v 1280x720 -r 25 -i ../libxcoder_logan/test/1280x720p_Basketball.yuv -c:v h264_ni_logan_enc output_7.h264"
+            echo $cmd
+            $cmd 2>&1 | tee ffmpeg_7.log
+            echo -e "\e[31mComplete! output_7.h264 has been generated.\e[0m"
+            CHECKSUM="6713d8cc54cc4d0ab0b912a54338e4ee"
+            HASH=`md5sum output_7.h264`
+            if [[ ${HASH%% *} == $CHECKSUM ]]; then
+                echo -e "\e[31mPASS: output_7.h264 matches checksum.\e[0m"
+            else
+                echo -e "\e[31mFAIL: output_7.h264 does not match checksum.\e[0m"
+            fi
+            echo
+            break
+        ;;
+        "test 265 encoder")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            gen_yuv_file_if_needed
+            ##### Encoding YUV to H.265 with NI XCoder (full log) #####
+            cmd="./ffmpeg -y -hide_banner -nostdin -f rawvideo -pix_fmt yuv420p -s:v 1280x720 -r 25 -i ../libxcoder_logan/test/1280x720p_Basketball.yuv -c:v h265_ni_logan_enc output_8.h265"
+            echo $cmd
+            $cmd 2>&1 | tee ffmpeg_8.log
+            echo -e "\e[31mComplete! output_8.h265 has been generated.\e[0m"
+            CHECKSUM="f13466948494cbc1217892f55f60f5ff"
+            HASH=`md5sum output_8.h265`
+            if [[ ${HASH%% *} == $CHECKSUM ]]; then
+                echo -e "\e[31mPASS: output_8.h265 matches checksum.\e[0m"
+            else
+                echo -e "\e[31mFAIL: output_8.h265 does not match checksum.\e[0m"
+            fi
+            echo
+            break
+        ;;
+        "test 264->265 transcoder")
+            echo -e "\e[31mYou chose $REPLY which is $opt\e[0m"
+            ##### Transcoding H.264 to H.265 with NI XCoder (full log) #####
+            cmd="./ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h264_ni_logan_dec -i ../libxcoder_logan/test/1280x720p_Basketball.264 -c:v h265_ni_logan_enc output_9.h265"
+            echo $cmd
+            $cmd 2>&1 | tee ffmpeg_9.log
+            echo -e "\e[31mComplete! output_9.h265 has been generated.\e[0m"
+            CHECKSUM="171a75a81ae8a152fa9fb182099629bf"
+            HASH=`md5sum output_9.h265`
+            echo ${HASH%% *}
+            if [[ ${HASH%% *} == $CHECKSUM ]]; then
+                echo -e "\e[31mPASS: output_9.h265 matches checksum.\e[0m"
+            else
+                echo -e "\e[31mFAIL: output_9.h265 does not match checksum.\e[0m"
+            fi
+            echo
+            break
+        ;;
+        "Quit")
+            break 2
+        ;;
+        *)
+            echo -e "\e[31mInvalid choice!\e[0m"
+            echo
+            break
+        ;;
+    esac
+done
+done
+echo -e "\e[31mBye!\e[0m"
+echo
diff --git a/run_ffmpeg_quadra.sh b/run_ffmpeg_quadra.sh
new file mode 100644
index 0000000000..2eab71e4be
--- /dev/null
+++ b/run_ffmpeg_quadra.sh
@@ -0,0 +1,274 @@
+#!/bin/bash
+# Check Ni xcoder basic functions
+
+AVone_check=$(ffmpeg -version 2> /dev/null | grep 'ffmpeg version 3.4.2' 2> /dev/null)
+
+# generate a YUV file of 1280x720p_Basketball.264 if needed
+function gen_yuv_file_if_needed() {
+    if [ ! -f "../libxcoder/test/1280x720p_Basketball.yuv" ]; then
+        ./ffmpeg -nostdin -vsync 0 -y -i ../libxcoder/test/1280x720p_Basketball.264 -c:v rawvideo ../libxcoder/test/1280x720p_Basketball.yuv &> /dev/null
+        if [[ $? != 0 ]]; then
+            echo -e "\e[31mFAIL\e[0m: cannot generate ../libxcoder/test/1280x720p_Basketball.yuv from ../libxcoder/test/1280x720p_Basketball.264"
+        fi
+    fi
+}
+
+# check a file vs expected hash, print result
+# $1 - file
+# $2 - hash
+function check_hash() {
+    HASH=`md5sum ${1}`
+    if [[ ${HASH%% *} == $2 ]]; then
+        echo -e "\e[32mPASS: ${1} matches checksum.\e[0m"
+    else
+        echo -e "\e[31mFAIL: ${1} does not match checksum.\e[0m"
+        echo -e "\e[31m      expected: ${2}\e[0m"
+        echo -e "\e[31m      ${1}: ${HASH%% *}\e[0m"
+    fi
+}
+
+# check a return code vs 0, print result
+# $1 - rc
+function check_rc() {
+    if [[ $1 != 0 ]]; then
+        echo -e "\e[31mFAIL: ${1} return code is ${1}\e[0m"
+    fi
+}
+
+while true;do
+options=("check pci device" "check nvme list" "rsrc_init" "ni_rsrc_mon" "test 264 decoder" "test 265 decoder" "test VP9 decoder" "test 264 encoder" "test 265 encoder" 
+         "test AV1 encoder" "test 264->265 transcoder" "test 264->AV1 transcoder" "test 265->264 transcoder" "test 265->AV1 transcoder" "test VP9->264 transcoder" 
+         "test VP9->265 transcoder" "test VP9->AV1 transcoder" "Quit")
+echo -e "\e[33mChoose an option:\e[0m"
+select opt in "${options[@]}"
+do
+    case $opt in
+        "check pci device")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            sudo lspci -d 1d82:
+            check_rc $?
+            echo
+            break
+        ;;
+        "check nvme list")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            sudo nvme list
+            check_rc $?
+            echo
+            break
+        ;;
+        "rsrc_init")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ../libxcoder/build/init_rsrc
+            check_rc $?
+            echo
+            break
+        ;;
+        "ni_rsrc_mon")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ../libxcoder/build/ni_rsrc_mon
+            check_rc $?
+            echo
+            break
+        ;;
+        "test 264 decoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Decoding H.264 to YUV with NI XCoder (full log) #####
+            output_file="output_5.yuv"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h264_ni_quadra_dec -i ../libxcoder/test/1280x720p_Basketball.264 -c:v rawvideo ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="be2e62fc528c61a01ac44eae5518e13a"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test 265 decoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Decoding H.265 to YUV with NI XCoder (full log) #####
+            output_file="output_6.yuv"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h265_ni_quadra_dec -i ../libxcoder/test/akiyo_352x288p25.265 -c:v rawvideo ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="f5a29fd3fd2581844848519bafd7370d"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test VP9 decoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Decoding VP9 to YUV with NI XCoder (full log) #####
+            output_file="output_7.yuv"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v vp9_ni_quadra_dec -i ../libxcoder/test/akiyo_352x288p25_300.ivf -c:v rawvideo ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="0da8a892f4f835cd8d8f0c02f208e1f6"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test 264 encoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            gen_yuv_file_if_needed
+            ##### Encoding YUV to H.264 with NI XCoder (full log) #####
+            output_file="output_8.h264"
+            cmd="ffmpeg -y -hide_banner -nostdin -f rawvideo -pix_fmt yuv420p -s:v 1280x720 -r 25 -i ../libxcoder/test/1280x720p_Basketball.yuv -c:v h264_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="62872b29f54af7ddddd93dcf2e0d94b7"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test 265 encoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            gen_yuv_file_if_needed
+            ##### Encoding YUV to H.265 with NI XCoder (full log) #####
+            output_file="output_9.h265"
+            cmd="ffmpeg -y -hide_banner -nostdin -f rawvideo -pix_fmt yuv420p -s:v 1280x720 -r 25 -i ../libxcoder/test/1280x720p_Basketball.yuv -c:v h265_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="8a86c89d4e29359ed8072482f7d81ef6"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test AV1 encoder")
+            if [[ $AVone_check ]]; then
+                echo -e "\e[31m AV1 cannot be run on 3.4.2, stopping test.\e[0m"
+                break
+            fi
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            gen_yuv_file_if_needed
+            ##### Encoding YUV to AV1 with NI XCoder (full log) #####
+            output_file="output_10.ivf"
+            cmd="ffmpeg -y -hide_banner -nostdin -f rawvideo -pix_fmt yuv420p -s:v 1280x720 -r 25 -i ../libxcoder/test/1280x720p_Basketball.yuv -c:v av1_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="d1ca90ddce59e324dd4b827bfd73ce0d"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test 264->265 transcoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Transcoding H.264 to H.265 with NI XCoder (full log) #####
+            output_file="output_11.h265"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h264_ni_quadra_dec -i ../libxcoder/test/1280x720p_Basketball.264 -c:v h265_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="e460dc73f8e5daff3c0b7e271e09553b"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test 264->AV1 transcoder")
+            if [[ $AVone_check ]]; then
+                echo -e "\e[31m AV1 cannot be run on 3.4.2, stopping test.\e[0m"
+                break
+            fi
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Transcoding H.264 to AV1 with NI XCoder (full log) #####
+            output_file="output_12.ivf"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h264_ni_quadra_dec -i ../libxcoder/test/1280x720p_Basketball.264 -c:v av1_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="be0c5eab2c6c55b8e9c4f31ba41a40e1"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test 265->264 transcoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Transcoding H.265 to H.264 with NI XCoder (full log) #####
+            output_file="output_13.h264"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h265_ni_quadra_dec -i ../libxcoder/test/akiyo_352x288p25.265 -c:v h264_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="7ab8b4d619ec4f7c19af2400588310df"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test 265->AV1 transcoder")
+            if [[ $AVone_check ]]; then
+                echo -e "\e[31m AV1 cannot be run on 3.4.2, stopping test.\e[0m"
+                break
+            fi
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Transcoding H.265 to AV1 with NI XCoder (full log) #####
+            output_file="output_14.ivf"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v h265_ni_quadra_dec -i ../libxcoder/test/akiyo_352x288p25.265 -c:v av1_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="e0c850b349887296f0dd4a5db5c80f6d"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test VP9->264 transcoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Transcoding VP9 to H.264 with NI XCoder (full log) #####
+            output_file="output_15.h264"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v vp9_ni_quadra_dec -i ../libxcoder/test/akiyo_352x288p25_300.ivf -c:v h264_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="6d603f169e841db7c5a260e1eac11904"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test VP9->265 transcoder")
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Transcoding VP9 to H.265 with NI XCoder (full log) #####
+            output_file="output_16.h265"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v vp9_ni_quadra_dec -i ../libxcoder/test/akiyo_352x288p25_300.ivf -c:v h265_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="23905d53b2f0290a11963fefafd1988b"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "test VP9->AV1 transcoder")
+            if [[ $AVone_check ]]; then
+                echo -e "\e[31m AV1 cannot be run on 3.4.2, stopping test.\e[0m"
+                break
+            fi
+            echo -e "\e[33mYou chose $REPLY which is $opt\e[0m"
+            ##### Transcoding VP9 to H.265 with NI XCoder (full log) #####
+            output_file="output_17.ivf"
+            cmd="ffmpeg -y -hide_banner -nostdin -vsync 0 -c:v vp9_ni_quadra_dec -i ../libxcoder/test/akiyo_352x288p25_300.ivf -c:v av1_ni_quadra_enc ${output_file}"
+            echo $cmd
+            $cmd 2>&1 | tee ${output_file}.log
+            echo -e "\e[33mComplete! ${output_file} has been generated.\e[0m"
+            CHECKSUM="308427dfe7b908c099901b809e03f7e2"
+            check_hash "${output_file}" "${CHECKSUM}"
+            echo
+            break
+        ;;
+        "Quit")
+            break 2
+        ;;
+        *)
+            echo -e "\e[31mInvalid choice!\e[0m"
+            echo
+            break
+        ;;
+    esac
+done
+done
+echo -e "\e[33mBye!\e[0m"
+echo
diff --git a/tests/ref/fate/sws-pixdesc-query b/tests/ref/fate/sws-pixdesc-query
index 76104bc5a6..cfcc55fc11 100644
--- a/tests/ref/fate/sws-pixdesc-query
+++ b/tests/ref/fate/sws-pixdesc-query
@@ -439,6 +439,7 @@ isRGB:
   bgr8
   bgra64be
   bgra64le
+  bgrp
   gbrap
   gbrap10be
   gbrap10le
@@ -587,6 +588,7 @@ AnyRGB:
   bgr8
   bgra64be
   bgra64le
+  bgrp
   gbrap
   gbrap10be
   gbrap10le
@@ -754,6 +756,7 @@ Packed:
   yvyu422
 
 Planar:
+  bgrp
   gbrap
   gbrap10be
   gbrap10le
@@ -898,6 +901,7 @@ PackedRGB:
   bgr8
   bgra64be
   bgra64le
+  bgrp
   rgb0
   rgb24
   rgb32
